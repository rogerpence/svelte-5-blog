[
    {
        "dirent": {
            "name": "postgres-sql.md",
            "path": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech",
            "parentPath": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech"
        },
        "markdownObject": {
            "frontMatter": {
                "title": "Postgres SQL",
                "description": "Postgres SQL",
                "date_created": "2025-06-14T00:00:00.000Z",
                "date_updated": "2025-11-03T00:00:00.000Z",
                "date_published": "2026-01-03T00:00:00.000Z",
                "pinned": false,
                "tags": [
                    "postgres"
                ]
            },
            "content": "```\nCREATE TABLE downloads (\n  id INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  date_requested timestamp,\n  name varchar(120),\n  company varchar(120),\n  country varchar(60),\n  email_address varchar(120),\n  product varchar(120),\n  family varchar(60)\n);\n\n-- To set the starting value for the auto-incrementing ID:\nALTER SEQUENCE downloads_id_seq RESTART WITH 1023;\n```"
        }
    },
    {
        "dirent": {
            "name": "postgres-on-wsl2-with-podman.md",
            "path": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech",
            "parentPath": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech"
        },
        "markdownObject": {
            "frontMatter": {
                "title": "Postgres on WSL 2 with Podman",
                "description": "How to install and configure Postres on WSL2 with Podman",
                "date_updated": "2025-12-29T00:00:00.000Z",
                "date_created": "2025-02-02T00:00:00.000Z",
                "date_published": "2026-01-03T00:00:00.000Z",
                "pinned": false,
                "tags": [
                    "containers",
                    "podman",
                    "postgres"
                ]
            },
            "content": "\n[[supabase-credentials#This is a test]]\n\n```powershell\n/home/user/postgres_data\n/home/user/postgres_storage\n```\n\nSee 9:40 in Scott's video for this connection URL\n\n![[image-68.png]]\n\nThese are the commands that worked\n\n```powershell\npodman volume create pg_data_final\n\npodman run -d --name postgres-wsl  \n-e POSTGRES_PASSWORD=mysecretpassword -e PGDATA=/var/lib/postgresql/data  \n-v pg_data_final:/var/lib/postgresql/data:Z -p 5432:5432  \n--shm-size=1g postgres:latest  \n-c shared_buffers=512MB `  \n-c random_page_cost=1.1\n```\n\n  It is great to hear you are up and running! This setup is a very good setup for running a database on Windows 11 via WSL 2 because it prioritizes native I/O speed while navigating the strict permission rules of rootless Podman. This setup's named volume keeps all DB data on the WSL 2-hosted VM. \n\nHere is a recap of the logic and steps that led to this successful, high-performance configuration:\n\n### 1. The Environment Reset\nWe started by clearing out \"phantom\" WSL distributions. When a WSL distro is deleted manually, Podman’s internal metadata gets out of sync. \n*   **The Fix:** Running `podman machine rm -f` and `podman machine init` created a clean, fresh WSL 2 backend dedicated to Podman.\n\n### 2. Solving the \"Slow Disk\" Problem\nYou correctly identified that mounting Windows folders (like `C:\\Users\\...`) is extremely slow. This is because every database write has to cross the \"9P\" bridge between Linux and Windows.\n*   **The Fix:** We used a **Named Volume** (`pg_data_final`). Unlike a direct folder mount, a named volume is managed entirely by Podman inside the WSL `ext4` virtual disk. This allows Postgres to write to the disk at native SSD speeds.\n\n### 3. Overcoming Postgres 18 Permission Errors\nPostgres 18 introduced a new logic that tries to create a `/18/data` subdirectory inside your mount. In a \"rootless\" Podman environment, the container user doesn't have system permission to create new directories on a volume root, which caused the `Permission Denied` errors.\n*   **The Fix:** We used `-e PGDATA=/var/lib/postgresql/data`. This told Postgres: *\"Don't try to create a new versioned folder; use this exact path I've provided.\"* This bypassed the failing `mkdir` command.\n\n### 4. Performance Tuning for Modern Hardware\nStandard Postgres images are configured conservatively to run on old servers with spinning hard drives.\n*   **The Fix:** We injected high-performance flags directly into the `run` command:\n    *   `--shm-size=1g`: Gave Postgres enough shared memory to handle complex queries.\n    *   `shared_buffers=512MB`: Allocated 25% of your assigned RAM to the database cache.\n    *   `random_page_cost=1.1`: Told the Postgres query planner that you are using an **SSD**, making it much faster at choosing the right indexes.\n\n\n### Maintenance Cheat Sheet\nNow that you have this working, here are the three commands you’ll likely need in the future:\n\n**To stop/start the database:**\n```powershell\npodman stop postgres-wsl\npodman start postgres-wsl\n```\n\n**To see if it's running healthy:**\n```powershell\npodman ps\npodman logs postgres-wsl\n```\n\n**To enter the database command line directly (psql):**\n```powershell\npodman exec -it postgres-wsl psql -U postgres\n```\n\n**A note on Windows Restarts:** \nIf you restart your computer, you will need to run `podman machine start` first, and then `podman start postgres-wsl` to get your database back online. Your data will be exactly where you left it inside that named volume!\n\n\n## More help\n\nWhen you run `podman stop postgres-wsl`, you are stopping **only the container**.\n\nTo understand exactly what is happening in your specific setup, it helps to look at the \"layers\" you have running. Think of it like a Russian nesting doll:\n\n### 1. The Container (`postgres-wsl`) \nThis is the \"process\" of PostgreSQL itself. When you run `podman stop`, you are telling the Postgres engine to perform a graceful shutdown. \n*   **Status:** \"Exited.\"\n*   **Data:** Safe. It stays in the `pg_data_final` volume.\n*   **Resources:** It stops using CPU and RAM.\n\n### 2. The Podman Engine (The \"Machine\") \nBehind the scenes, you have a WSL 2 distribution (the `podman-machine-default` you saw earlier) that acts as the host for your containers. \n*   **Status:** \"Running.\"\n*   **Note:** Even though the container is stopped, the Linux VM (WSL) is still powered on and idling in the background, waiting for you to start a container again.\n\n### 3. WSL 2/Windows 11 \nThe overall Windows subsystem for Linux is still active.\n\n\n### Why this distinction matters:\n\n#### If you want to save RAM on your computer:\nStopping the container helps a little, but the **Podman Machine** (the Linux VM) usually reserves a chunk of your Windows RAM (you saw it was using 4GiB earlier). To get that RAM back, you have to stop the \"Machine\":\n```powershell\npodman machine stop\n```\n\n#### If you want to resume work:\nIf you only stopped the **container**, you only need one command to get back into DBeaver:\n```powershell\npodman start postgres-wsl\n```\n\nIf you stopped the **machine** or **restarted Windows**, you need two steps:\n1. `podman machine start` (Turn on the Linux engine)\n2. `podman start postgres-wsl` (Turn on the Postgres process)\n\n### Summary\n`podman stop postgres-wsl` = **Turning off the App.**\n`podman machine stop` = **Turning off the Computer the app runs on.**\n## \n\nTo answer your question directly: **No, it does not map to a folder on your Windows host.**\n\nThis is exactly why your database is now performing so well. Here is the breakdown of where that data actually lives and how it works:\n\n### 1. Where is the data?\nThe data lives inside the **WSL 2 Virtual Disk (`.vhdx`)**. \n\nWhen you created the named volume `pg_data_final`, Podman carved out a directory inside the Linux file system that belongs to your `podman-machine-default` instance. To Windows, that entire Linux environment is just one giant, high-performance file (usually located at `%USERPROFILE%\\AppData\\Local\\Microsoft\\Windows\\WSL\\...`).\n\n### 2. Does it contain the database data?\n**Yes.** Everything that makes up your Postgres database is inside that volume:\n*   Your tables and actual row data.\n*   Your indexes and schemas.\n*   The Postgres configuration files.\n*   The transaction logs (WAL).\n\n### 3. Can I see the files from Windows?\nYou can actually \"peek\" into the volume from Windows Explorer, but you should **never** move or edit these files from Windows, or you will corrupt the database permissions.\n\nIf you want to see them, paste this into your Windows Explorer address bar:\n`\\\\wsl$\\podman-machine-default\\home\\user\\.local\\share\\containers\\storage\\volumes\\pg_data_final\\_data`\n\n*(Note: The exact path might vary slightly depending on your Podman version, but it is always inside the `\\\\wsl$` network share.)*\n\n### 4. Why this is the \"Performance Secret\"\n*   **No \"Bridge\" Tax:** When Postgres writes to a Windows folder (like `C:\\Users`), it has to use a translation layer called **9P**. This layer acts like a slow network translator between Linux and Windows. It is death for database performance.\n*   **Native Linux Speed:** By using a Named Volume, Postgres talks directly to the **ext4** filesystem. There is no translation. It’s as fast as running Postgres on a dedicated Linux server.\n\n### 5. How do I back it up?\nSince the files aren't in a standard Windows folder, you can't just \"Copy+Paste\" them to a USB drive easily. If you want to back up your data, you have two professional options:\n\n**Option A: The SQL Dump (Safest)**\nRun this from PowerShell to create a backup file on your Windows Desktop:\n```powershell\npodman exec postgres-wsl pg_dumpall -U postgres > C:\\Users\\YourUsername\\Desktop\\backup.sql\n```\n\n**Option B: The Podman Export**\nYou can export the entire volume to a compressed file:\n```powershell\npodman volume export pg_data_final --output pg_backup.tar\n```\n\n### Summary\nYour data is stored in a **high-speed virtual vault** inside WSL 2. It is not on your \"Host\" Windows filesystem, which is exactly why you are no longer seeing the slowness you experienced before!\n\n## Successful steps!\n\nThis is the \"Golden Path\" we discovered to get high-performance PostgreSQL running on Windows 11 via Podman and WSL 2. \n\nBy following these steps, you moved the database from the slow Windows filesystem into the native Linux environment, bypassing permission errors and performance bottlenecks.\n\n### Step 1: Environment Cleanup & Reset\nWe first had to ensure Podman's background engine (the WSL \"machine\") was healthy and in sync.\n*   **The Problem:** Old or manually deleted WSL distros leave \"phantom\" metadata that makes Podman fail to start.\n*   **The Fix:** You removed the broken machine and initialized a fresh one with enough resources:\n```powershell\npodman machine rm -f podman-machine-default\npodman machine init --disk-size 50 --memory 4096\npodman machine start\n```\n\n### Step 2: Using a performant storage strategy\nYou originally had Postgres pointing to a folder on your Windows `C:\\` drive, which was slow.\n*   **The Change:** We switched from a \"Bind Mount\" (a Windows folder) to a **Podman Named Volume**.\n*   **The Result:** This placed the data inside the WSL `ext4` filesystem. This eliminates the **9P protocol overhead**, allowing your SSD to perform at native Linux speeds.\n```powershell\npodman volume create pg_data_final\n```\n\n### Step 3: Resolving Postgres 18 Permission Errors\nPostgres 18 changed its storage logic to use versioned subdirectories (e.g., `/var/lib/postgresql/18/data`). In a rootless Podman environment, this caused a \"Permission Denied\" error because the container couldn't create new directories on the root of the volume.\n*   **The Solution:** We used the **`PGDATA` override**. By explicitly setting the path, we told Postgres to skip the directory-creation logic and use the volume mount point directly.\n\n### Step 4: The Final Performance-Tuned Run Command\nThis specific command combined the storage fix with engine tuning for modern SSDs:\n\n```powershell\npodman run -d `\n  --name postgres-wsl `\n  -e POSTGRES_PASSWORD=mysecretpassword `\n  -e PGDATA=/var/lib/postgresql/data `\n  -v pg_data_final:/var/lib/postgresql/data:Z `\n  -p 5432:5432 `\n  --shm-size=1g `\n  postgres:latest `\n  -c shared_buffers=512MB `\n  -c random_page_cost=1.1\n```\n\n**Why these flags matter:**\n*   **`-v ...:Z`**: Automatically handles the internal Linux security labels so Postgres can read/write to the volume.\n*   **`--shm-size=1g`**: Increases the shared memory limit. Without this, Postgres often crashes on complex queries in containers.\n*   **`-c random_page_cost=1.1`**: Tells Postgres you are on an **SSD**. By default, Postgres assumes you have a slow, spinning hard drive and avoids using certain indexes. This flag makes it much faster.\n\n### Step 5: Connecting DBeaver via the Localhost Relay\nBecause Podman on Windows runs inside a specialized WSL VM, it uses a \"proxy\" to connect that VM to your Windows network.\n*   **The Result:** You were able to connect DBeaver using `localhost:5432` because Podman automatically forwarded the traffic from Windows into the WSL container.\n\n### Summary of the \"Wins\":\n1.  **Speed:** Native `ext4` storage via a Named Volume.\n2.  **Stability:** Overrode `PGDATA` to fix Postgres 18 directory permission issues.\n3.  **Optimization:** Tuned for SSDs and allocated proper shared memory.\n4.  **Cleanliness:** Removed old, failed containers and volumes to keep the system tidy.\n\nYou now have a professional-grade PostgreSQL setup that is significantly faster and more reliable than a standard Windows-native installation or a basic Docker-on-Windows setup.\n## -f flag\n\nYou don't **need** it for the database to function, but since your goal is **high performance**, it is a very good flag to have. \n\nHere is the breakdown of why you might care about it and whether it’s worth \"fixing\" your current container to include it.\n\n### What does `effective_io_concurrency` do?\nThis setting tells PostgreSQL how many simultaneous disk I/O operations the underlying storage can handle at once. \n*   **Default (1):** Assumes a single spinning hard drive platter. Postgres waits for one piece of data to be read before asking for the next.\n*   **High Value (200-300):** Optimized for **SSDs**. Since SSDs don't have a physical \"head\" that has to move, they can fetch dozens of different pieces of data simultaneously. \n\nBy setting this to `200`, you are telling the Postgres query planner: *\"Don't be shy; feel free to initiate up to 200 I/O requests at once.\"* This dramatically speeds up **Bitmap Heap Scans** (a common way Postgres reads data when using indexes).\n\n### Do you care?\n*   **For Development/Small Data:** You won't notice the difference. The database is already very fast because it’s on the WSL native filesystem.\n*   **For Large Data/Complex Queries:** You **do** care. It allows Postgres to fully saturate the speed of your NVMe/SSD.\n\n### How to add it \nBecause this is a startup flag, you cannot add it to a running container. You would have to recreate it. Since your data is safe in the **Volume**, this only takes 30 seconds:\n\n1.  **Stop and Remove the current container:**\n    ```powershell\n    podman stop postgres-wsl\n    podman rm postgres-wsl\n    ```\n\n2.  **Run it again with the \"Ultimate\" performance flags:**\n```powershell\npodman run -d `\n  --name postgres-wsl `\n  -e POSTGRES_PASSWORD=mysecretpassword `\n  -e PGDATA=/var/lib/postgresql/data `\n  -v pg_data_final:/var/lib/postgresql/data:Z `\n  -p 5432:5432 `\n  --shm-size=1g `\n  postgres:latest `\n  -c shared_buffers=512MB `\n  -c random_page_cost=1.1 `\n  -c effective_io_concurrency=200 `\n  -c work_mem=16MB\n```\n\n### Summary\nIf you are happy with the current speed in DBeaver, **leave it as is.** You’ve already fixed the biggest bottleneck (moving from Windows storage to WSL storage). \n\nHowever, if you want to brag that your Postgres is tuned exactly for modern SSD hardware, adding `effective_io_concurrency=200` is the \"cherry on top\" for your configuration!\n\n## Does that named volume persist as long as the WSL distro is installed\n\nThe short answer is **yes**, the named volume persists as long as the **Podman Machine** (which is a WSL distribution) exists. \n\nHowever, because you are using Podman on Windows, there is a specific hierarchy of \"persistence\" you should understand to ensure you never accidentally lose your data.\n\n### 1. What IS safe (Data Persists)\nYour data in `pg_data_final` will survive all of the following:\n*   **Stopping/Starting the container:** `podman stop` / `podman start`.\n*   **Deleting the container:** `podman rm`.\n*   **Restarting Windows:** When you reboot, the WSL virtual disk stays on your drive.\n*   **Stopping the Podman Machine:** `podman machine stop`.\n*   **Updating the Postgres Image:** If you pull a newer version of Postgres and recreate the container using the same volume, your data remains.\n\n### 2. What IS NOT safe (Data is Lost)\nYour data will be destroyed if you do any of the following:\n*   **`podman machine rm`**: If you delete the Podman Machine to \"start fresh,\" you are deleting the entire WSL virtual disk, including all volumes stored inside it.\n*   **`podman volume rm pg_data_final`**: This is the specific command to delete the virtual hard drive you created.\n*   **Uninstalling WSL or Podman Desktop**: Depending on the settings, removing the application may trigger a cleanup of the associated WSL distributions.\n*   **Factory Resetting Windows**: This obviously wipes the WSL subsystem.\n\n### 3. Where exactly is the \"Risk\"?\nIn a standard Linux setup, the volume is just a folder on your physical disk. In your Windows setup, the volume is a folder **inside a virtual disk file** (`.vhdx`). \n\nIf that `.vhdx` file (the `podman-machine-default` distribution) is deleted, the volume goes with it.\n\n\n### Pro-Tip: The \"Golden Rule\" of Database Persistence\nWhile named volumes are great for performance and daily development, **never rely on a container volume as your only backup** for important data. \n\nBecause WSL distributions can occasionally become corrupted or need to be recreated (as we did in Step 1 of your setup), you should occasionally \"export\" your data to the **Windows Host** side where your standard PC backups (like OneDrive, Backblaze, or an external drive) can see it.\n\n**The \"Safety\" Command (Run this once a week):**\n```powershell\npodman exec postgres-wsl pg_dumpall -U postgres > C:\\Users\\YourName\\Documents\\postgres_backups\\weekly_backup.sql\n```\n*This takes the data **out** of the virtual WSL world and puts it into a standard Windows file that will survive even if you delete the Podman Machine.*\n\n### Summary\nAs long as you don't run `podman machine rm`, your Postgres data is as permanent as any other file on your computer. It is stored safely inside the WSL filesystem, optimized for the high performance you've now achieved.\n\n## Podman command summary\n\nHere is a concise cheat sheet of the most useful Podman commands and flags for managing your new PostgreSQL environment and beyond.\n\n### 1. Essential Container Commands\n| Command | Purpose |\n| :--- | :--- |\n| `podman ps` | List **running** containers. |\n| `podman ps -a` | List **all** containers (including stopped/crashed ones). |\n| `podman logs <name>` | View the output of a container (add `-f` to follow live). |\n| `podman stop <name>` | Gracefully shut down a container. |\n| `podman start <name>` | Start an existing, stopped container. |\n| `podman rm -f <name>` | Forcefully delete a container. |\n| `podman exec -it <name> bash` | Enter a container's terminal while it is running. |\n| `podman inspect <name>` | View the full technical configuration (JSON format). |\n\n\n### 2. Common `podman run` Flags\nThese are the building blocks of the \"Run\" script you used:\n*   **`-d`** (Detached): Runs the container in the background.\n*   **`-p 5432:5432`**: Maps a host port to a container port (`Host:Container`).\n*   **`--name <my_name>`**: Assigns a friendly name so you don't have to use long IDs.\n*   **`-e VAR=val`**: Sets environment variables (e.g., `POSTGRES_PASSWORD`).\n*   **`--rm`**: Automatically deletes the container when it stops (great for one-off tests).\n*   **`-it`**: Keeps the terminal interactive (used for `bash` or `psql`).\n*   **`--restart <policy>`**: Set to `always` or `unless-stopped` to auto-restart if it crashes.\n*   **`--shm-size=1g`**: Increases shared memory (crucial for databases like Postgres).\n\n\n### 3. Special Volume Suffixes\nIn Podman, the flags appended to your volume (`-v volume_name:path:suffix`) are critical for rootless environments:\n*   **`:z`**: (Lower-case) Tells SELinux the volume is **shared** between multiple containers.\n*   **`:Z`**: (Upper-case) Tells SELinux the volume is **private** to this container only.\n*   **`:U`**: Tells Podman to **map the ownership** of the host files to the container’s internal user (fixes \"Permission Denied\" errors in rootless mode).\n\n\n### 4. Image & Storage Management\n| Command | Purpose |\n| :--- | :--- |\n| `podman images` | List all downloaded images. |\n| `podman pull <image>` | Download the latest version of an image from a registry. |\n| `podman rmi <id>` | Delete an image from your drive. |\n| `podman volume ls` | List all named volumes (the virtual hard drives). |\n| `podman volume prune` | **Danger:** Deletes all volumes not currently attached to a container. |\n\n\n### 5. Windows/WSL \"Machine\" Commands\nSince you are on Windows, you are managing the Linux \"engine\" that runs Podman:\n*   **`podman machine ls`**: Check the status of your WSL backend (Running vs. Stopped).\n*   **`podman machine start`**: Power on the Linux VM.\n*   **`podman machine stop`**: Power off the VM (saves Windows RAM).\n*   **`podman machine ssh`**: Log directly into the Linux VM backend.\n*   **`podman machine set --cpus 4 --memory 4096`**: Resize the engine's resources.\n\n\n### 6. System Health & Maintenance\n*   **`podman stats`**: View live CPU and RAM usage of your containers.\n*   **`podman info`**: View system-wide settings, storage paths, and registry info.\n*   **`podman system prune -a`**: The \"Spring Cleaning\" command. Deletes all stopped containers, unused networks, and dangling images.\n\n## Shutdown and startup \n\nBecause your data is stored in a **Named Volume** (`pg_data_final`), the data is physically separated from the container's lifecycle. You can stop, start, or even delete the container without losing a single row of data.\n\nHere is the professional workflow for shutting down and starting back up.\n\n\n### Part 1: Shutting Down Safely\nTo ensure your database files are closed correctly and your Windows RAM is freed up, follow these two steps:\n\n**1. Stop the Postgres Container**\nThis sends a signal to Postgres to flush all pending data to the disk and close connections gracefully.\n```powershell\npodman stop postgres-wsl\n```\n\n**2. Stop the Podman Machine (The RAM Saver)**\nEven if the container is stopped, the WSL 2 Linux VM is still running in the background, consuming a few gigabytes of your Windows RAM. Run this to fully \"turn off\" the engine:\n```powershell\npodman machine stop\n```\n\n\n### Part 2: Starting Up (After a Reboot)\nWhen you turn your PC back on, the Podman engine does not start automatically. You must bring the environment back up in this order:\n\n**1. Start the Podman Machine**\nThis \"boots\" the hidden Linux environment that holds your Podman volumes and engine.\n```powershell\npodman machine start\n```\n\n**2. Start the Postgres Container**\nOnce the machine is \"Started,\" run your container. You don't need the long `run` command with all the flags; Podman remembered them all when you first created it.\n```powershell\npodman start postgres-wsl\n```\n\n**3. Verify Health**\nWait about 5 seconds for Postgres to finish its internal checks, then verify it's ready:\n```powershell\npodman logs postgres-wsl\n```\n*Look for: \"database system is ready to accept connections.\"*\n\n\n### Bonus: The \"Easy Button\" (Automation)\nIf you don't want to type these every time you reboot, you can create a simple **Startup Script** on your Windows Desktop.\n\n1.  Right-click your Desktop and select **New > Text Document**.\n2.  Paste the following:\n ```batch\n @echo off\n  echo Starting Podman Machine...\n podman machine start\n echo Starting Postgres Container...\n podman start postgres-wsl\n echo.\n echo Done! Your database is ready for DBeaver.\n pause \n```\n\n1.  Save the file as **`Start-Postgres.bat`** (make sure it ends in `.bat`, not `.txt`).\n\nNow, after a reboot, you just **double-click that file**, and your high-performance Postgres environment will spin up automatically!\n\n### Why your data is safe:\nWhen you run `podman stop` or `podman machine stop`, the data inside the `pg_data_final` volume remains exactly where it is on the WSL virtual disk. It is exactly like turning off a computer; the files on the hard drive don't disappear just because the power is off.\n\n## Full system shutdown\n\nTo fully restart the entire stack—from the database engine up to the Windows Subsystem for Linux itself—you should follow a \"top-down\" shutdown and a \"bottom-up\" startup. \n\nThis is the most reliable way to clear memory leaks, reset \"stuck\" network ports, and ensure your virtual disk is in a safe state.\n\n### Phase 1: The Clean Shutdown (Top-Down)\nThis sequence ensures Postgres finishes its writes before the underlying \"computer\" is turned off.\n\n1.  **Stop the Container:**\n    ```powershell\n    podman stop postgres-wsl\n    ```\n2.  **Stop the Podman Machine:**\n    ```powershell\n    podman machine stop\n    ```\n3.  **Shutdown WSL Entirely:**\n    This command kills **all** running WSL distributions and the background WSL 2 lightweight utility VM. Use this if WSL feels sluggish or if you are having connection issues.\n    ```powershell\n    wsl --shutdown\n    ```\n\n\n### Phase 2: The Fresh Start (Bottom-Up)\nYou don't need a specific \"WSL start\" command; starting the Podman machine will automatically trigger the WSL 2 subsystem to wake up.\n\n1.  **Start the Podman Machine:**\n    ```powershell\n    podman machine start\n    ```\n    *(If this hangs, it’s usually because Windows is still cleaning up the previous WSL shutdown; just wait 10 seconds and try again.)*\n\n2.  **Start the Container:**\n    ```powershell\n    podman start postgres-wsl\n    ```\n\n\n### Why should you use `wsl --shutdown`?\nIn your specific environment, `wsl --shutdown` is a powerful tool for two reasons:\n\n*   **Memory Reclamation:** WSL 2 (and Podman by extension) is notorious for \"holding onto\" Windows RAM even after you stop a container. A full shutdown forces the `vmmem` process in Task Manager to release that memory back to Windows.\n*   **Networking Reset:** If DBeaver suddenly stops connecting to `localhost`, a WSL shutdown resets the virtual network bridge between Windows and Linux, which usually fixes the issue.\n\n### The \"Full Restart\" Script\nIf you want to do this frequently to keep your system fast, you can put this into a `.bat` file:\n\n```batch\n@echo off\necho --- SHUTTING DOWN ---\npodman stop postgres-wsl\npodman machine stop\nwsl --shutdown\necho.\necho --- RESTARTING ---\npodman machine start\npodman start postgres-wsl\necho.\necho System Restarted Successfully.\npause\n```\n\n### Is my data safe during `wsl --shutdown`?\n**Yes.** As long as you ran `podman stop postgres-wsl` first, the database has finished writing its logs to the disk. The `wsl --shutdown` command is essentially like pulling the power plug on a PC that is already turned off—it is perfectly safe for your data.\n\n### SSH into container\n\n```\npodman machine ssh\n```\n\n### Get container info\n\n```\ncat /etc/os-release\n```\n\n```\n[user@DESKTOP-FT1088C ~]$ cat /etc/os-release\nNAME=\"Fedora Linux\"\nVERSION=\"41 (Container Image)\"\nRELEASE_TYPE=stable\nID=fedora\nVERSION_ID=41\nVERSION_CODENAME=\"\"\nPLATFORM_ID=\"platform:f41\"\nPRETTY_NAME=\"Fedora Linux 41 (Container Image)\"\nANSI_COLOR=\"0;38;2;60;110;180\"\nLOGO=fedora-logo-icon\nCPE_NAME=\"cpe:/o:fedoraproject:fedora:41\"\nDEFAULT_HOSTNAME=\"fedora\"\nHOME_URL=\"https://fedoraproject.org/\"\nDOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f41/\"\nSUPPORT_URL=\"https://ask.fedoraproject.org/\"\nBUG_REPORT_URL=\"https://bugzilla.redhat.com/\"\nREDHAT_BUGZILLA_PRODUCT=\"Fedora\"\nREDHAT_BUGZILLA_PRODUCT_VERSION=41\nREDHAT_SUPPORT_PRODUCT=\"Fedora\"\nREDHAT_SUPPORT_PRODUCT_VERSION=41\nSUPPORT_END=2025-12-15\nVARIANT=\"Container Image\"\nVARIANT_ID=container\n```\n\n\n## Podman compose\n\nYes, a **Podman Compose** file would help significantly. While your current command works, it is \"imperative\"—meaning you have to remember and re-type (or copy-paste) that long string every time you want to recreate or update the container.\n\nUsing a `compose.yaml` file (the \"declarative\" way) offers several advantages for your specific setup:\n\n### 1. Benefits for your specific setup\n*   **Version Control:** You can save the file in your project folder. If you change a setting (like `shared_buffers`), you just edit the text file instead of trying to edit a massive terminal command.\n*   **Automatic Restarts:** You can add `restart: always` so the DB starts automatically when your machine/WSL2 starts.\n*   **Readability:** It’s much easier to see the relationship between your shared memory (`shm_size`), volumes, and Postgres flags.\n*   **One-Command Management:** Instead of multiple steps (creating volume, then running), you just run `podman-compose up -d`.\n\n### 2. Your command converted to `compose.yaml`\nCreate a file named `compose.yaml` in your project directory and paste this:\n\n```yaml\nservices:\n  postgres:\n    image: postgres:latest\n    container_name: postgres-wsl\n    restart: always\n    environment:\n      - POSTGRES_PASSWORD=mysecretpassword\n      - PGDATA=/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n    volumes:\n      # The :Z ensures correct SELinux labels (useful if using RHEL/Fedora in WSL)\n      - pg_data_final:/var/lib/postgresql/data:Z\n    shm_size: '1gb'\n    # This translates your -c flags into the startup command\n    command: \n      - postgres\n      - -c\n      - shared_buffers=512MB\n      - -c\n      - random_page_cost=1.1\n      - -c \n      - effective_io_concurrency=200        \n\nvolumes:\n  pg_data_final:\n    external: true  # Tells Podman to use the volume you already created manually\n```\n\n### 3. How to use it\n1.  **Install Podman Compose** (if you haven't yet):\n    ```powershell\n    pip install podman-compose\n    ```\n2.  **Start the container**:\n    In the same folder as your file, run:\n    ```powershell\n    podman-compose up -d\n    ```\n3.  **Stop the container**:\n    ```powershell\n    podman-compose down\n    ```\n\n### Key differences to note:\n*   **`shm_size`**: In your manual command, you used `--shm-size=1g`. In the Compose file, it’s a dedicated setting. This is critical for Postgres performance with large `shared_buffers`.\n*   **`external: true`**: I set the volume as \"external\" because you already created `pg_data_final` manually. If you want Compose to manage the volume lifecycle entirely, you would remove `external: true`.\n*   **Postgres Flags**: The `command` section cleanly breaks down your `-c` configurations, making them much easier to adjust later as your database grows.\n\n## The VHDX \n\n`podman-machine-default` is the standard, automatically assigned name for the Linux virtual machine (VM) that Podman creates on macOS and Windows to run containers, acting as the default target for Podman commands like `podman machine init`. It provides a Linux environment (Fedora CoreOS) for container operations, allowing Podman to function on non-Linux systems, and you use commands like `podman machine start`, `stop`, `list`, and `init` to manage it.\n\n### Using PowerShell to show  WSL distros\n\n```powershell\n(Get-ChildItem HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Lxss | ForEach-Object {Get-ItemProperty $_.PSPath}) | Select-Object DistributionName, BasePath\n```\n\n```\nDistributionName       BasePath\n----------------       --------\nUbuntu                 C:\\Users\\thumb\\AppData\\Local\\wsl\\{73929071-0e78-42fe-a25d-96580d570fe0}\npodman-machine-default \\\\?\\C:\\Users\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\n```\n\n### Using the registry to see WSL distros\n\n```\nHKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Lxss\n```\n\n```\nC:\\Users\\\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\\ext4.vhdx\n```\n\n```\nDistributionName       BasePath\n----------------       --------\nUbuntu                 C:\\Users\\thumb\\AppData\\Local\\wsl\\{73929071-0e78-42fe-a25d-96580d570fe0}\npodman-machine-default \\\\?\\C:\\Users\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\n```\n\n### The Windows Extended Length Path Prefix\n\nThe path prefix for the `podman-machine-default` is a Windows \"[Extended Length Path](https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=registry)\" prefix. \n\n```\n\\\\?\\\n```\n\n- This tells Windows to ignore the standard 260-character path limit.    \n- It's necessary with WSL and Podman because they often use deeply nested folders. Without this prefix, some Windows applications would crash or fail to find the file because the folder path is too long."
        }
    },
    {
        "dirent": {
            "name": "vanilla-css-for-sveltekit.md",
            "path": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech",
            "parentPath": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech"
        },
        "markdownObject": {
            "frontMatter": {
                "title": "Vanilla CSS for Sveltekit",
                "description": "Vanilla CSS for Sveltekit",
                "date_updated": "2025-12-15T00:00:00.000Z",
                "date_created": "2025-02-02T00:00:00.000Z",
                "date_published": "2026-01-03T00:00:00.000Z",
                "pinned": false,
                "tags": [
                    "sveltekit",
                    "css"
                ]
            },
            "content": "This document shows how to use vanilla CSS without an preprocessing. Given how much CSS has improved over the last couple of years, it's hard to make a good case for PostCSS. \n* The Sveltekit build step (with Vite's help) automatically compresses CSS.\n* CSS's @import (which may, or may now, impede performance) runs at SvelteKit so it's performance is a moot point.\n\n```\n└── src/\n    ├── css  /\n    │   ├── components/\n    │   │   ├── _components.css\n    │   │   └── button.css\n    │   ├── elements/\n    │   │   ├── _elements.css\n    │   │   └── select.css\n    │   ├── global/\n    │   │   ├── _global.css\n    │   │   ├── reset.css\n    │   │   ├── tags.css\n    │   │   └── vars.css\n    │   ├── layouts/\n    │   │   └── _layouts.css\n    │   ├── utilities/\n    │   │   └── _utilities.css\n    │   └── _style.css      \n    ├── lib/\n    │   ├── assets/\n    │   │   └── favicon.svg\n    │   └── routes/\n    │       ├── +layout.svelte\n    │       └── +page.svelte    \n    ├── routes/\n    │   └── ..      \n    └── style.css\n```\n\n`+layout.svelte`\n\n```ts\n<script lang=\"ts\">\n\timport favicon from '$lib/assets/favicon.svg';\n\timport '../style.css';\n\n\tlet { children } = $props();\n</script>\n\n<svelte:head>\n\t<link rel=\"icon\" href={favicon} />\n</svelte:head>\n\n{@render children?.()}\n```\n\n`src/css/_style.css_`\n\n```\n@layer global, layouts, elements, components, utilities;\n\n/* @import \"https://unpkg.com/open-props\" layer(global); */\n@import './global/_global.css' layer(global);\n@import './layouts/_layouts.css' layer(layouts);\n@import './elements/_elements.css' layer(elements);\n@import './components/_components.css' layer(components);\n@import './utilities/_utilities.css' layer(utiilties);\n```\n\n`src/style.css`\n\n```\n@import './css/_style.css';\n```\n\nThis is the swindle that makes this all work. At build time (or runtime in dev mode), `_style_.css` imports the various CSS files. Then `/css/style.css` imports the rendered `_style_.css` contents. It's almost magic--CSS `@imports` are driving all of this. At build time, Sveltekit and Vite produce a concrete, compressed, version of `/css/style.css`. That single, static CSS file is produced at (where `<hash>` is the hash file name created.)\n\n`.svelte-kit\\output\\client\\_app\\immutable\\assets\\<hash>.css`\n\nBy the way, this CSS process occurs without any configuration! \n\n`routes/+layout.svelte`\n\n```\n<script lang=\"ts\">\n\timport favicon from '$lib/assets/favicon.svg';\n\timport '../style.css';\n\n\tlet { children } = $props();\n</script>\n\n<svelte:head>\n\t<link rel=\"icon\" href={favicon} />\n</svelte:head>\n\n{@render children?.()}\n```\n\n\n> [!info]\n> I avoided using @import with the original ASNA.com because of the potential performance implications of using `@import`. However, that was silly because I didn't understand how powerful the Sveltekit/Vite build process is. \n## Implementation details\n\nEach directory under `css` is a category of css that is assigned to a given layer. \n\n_global.css\n\n```css\n@import \"reset.css\";\n@import \"vars.css\";\n@import \"tags.css\";\n```\n\nreset.css\n\n```css\n*,\n*::before,\n*::after {\n  box-sizing: border-box;\n}\n\n/* Prevent font size inflation */\nhtml {\n  -moz-text-size-adjust: none;\n  -webkit-text-size-adjust: none;\n  text-size-adjust: none;\n}\n\n/* Remove default margin in favour of better control in authored CSS */\nbody, h1, h2, h3, h4, p,\nfigure, blockquote, dl, dd {\n  margin-block-end: 0;\n} \n\n/* Remove list styles on ul, ol elements with a list role, which suggests default styling will be removed */\nul[role='list'],\nol[role='list'] {\n  list-style: none;\n}\n\nu, ol {\n  font-size: inherit;\n}\n\n/* Set core body defaults */\nbody {\n  min-height: 100vh;\n  line-height: 1.5;\n}\n\n/* Set shorter line heights on headings and interactive elements */\nh1, h2, h3, h4,\nbutton, input, label {\n  line-height: 1.1;\n}\n\n/* Balance text wrapping on headings */\n/* h1, h2,\nh3, h4 {\n  text-wrap: balance;\n}  */\n\n/* A elements that don't have a class get default styles */\na:not([class]) {\n  text-decoration-skip-ink: auto;\n  color: currentColor;\n}\n\n/* Make images easier to work with */\nimg,\npicture {\n  max-width: 100%;\n  display: block;\n}\n\n/* Inherit fonts for inputs and buttons */\ninput, button,\ntextarea, select {\n  font-family: inherit;\n  font-size: inherit;\n}\n\n/* Make sure textareas without a rows attribute are not tiny */\ntextarea:not([rows]) {\n  min-height: 10em;\n}\n\n/* Anything that has been anchored to should have extra scroll margin */\n:target {\n  scroll-margin-block: 5ex;\n}\n```\n\n\nThe `?.` is the **optional chaining operator**. It safely handles cases where `children` might be `undefined` or `null`.\n\n## What it does\n\n```svelte\n{@render children?.()}\n```\n\nThis means:\n- **If `children` exists**: Render it\n- **If `children` is `undefined` or `null`: Do nothing (no error thrown)\n\n## Without optional chaining\n\n```svelte\n{@render children()}\n```\n\nThis would throw an error if `children` is `undefined`.\n\n## Why it's needed\n\nIn SvelteKit layouts, the `children` snippet represents the content of child routes. There might be edge cases during:\n- Initial rendering\n- Route transitions\n- Error states\n\nWhere `children` could temporarily be `undefined`.\n\n## Equivalent to\n\n```javascript\n// In regular JavaScript\nchildren ? children() : undefined\n```\n\nThe `?.()` is specifically for optional function/snippet calls. There's also:\n- `object?.property` - optional property access\n- `array?.[index]` - optional array indexing\n\nThis is a defensive programming practice to prevent runtime errors when the snippet might not be available."
        }
    },
    {
        "dirent": {
            "name": "svelte-forms-with-remote-functions.md",
            "path": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech",
            "parentPath": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech"
        },
        "markdownObject": {
            "frontMatter": {
                "title": "Svelte forms with remote functions are awesome!",
                "description": "This article explains how to use Svelte 5 forms with remote functions.",
                "date_updated": "2025-12-30T00:00:00.000Z",
                "date_created": "2025-02-02T00:00:00.000Z",
                "date_published": "2026-01-03T00:00:00.000Z",
                "pinned": false,
                "tags": [
                    "svelte",
                    "forms"
                ]
            },
            "content": "## Creating a form with Svelte 5 for use with a remote function\n\n### Create a form schema  \n\nYou have an interesting option we'll discuss later if you locate the form schema in a separate module (as opposed to defining it in the form's remote function). I put mine under the `src/lib/types` folder but you can put your schemas anywhere. \n\n`src/lib/types/form1.schema.ts`\n\nForm schema are defined with any standard schema compliant validation library (ie, Zod, Valibot, or Arktype). I'm using Zod in this example. Zod has a [rich set of schema definition options](https://zod.dev/api). \n\n```ts\nimport { z } from 'zod';\n\nexport const form1Schema = z.object({\n\ttitle: z.string().nonempty({ message: \"Hey, foo! Can't be empty\" }),\n\tcontent: z.string().nonempty()\n});\n```\n\nUsing a schema is part of the magic of Svelte 5 forms. It provides:\n- Strongly-typed, easy access to form fields\n- The ability to populate a form with the correct tag and attributes for every field.\n- Very effective form validation for the server- and client-side\n- Freedom from third-party forms libraries like SuperForms. \n## Create a remote function\n\nTo reduce learning friction, this example Svelte 5 form remote function doesn't do very much, but it does enough to understand how things work. There aren't any rules as to where a  remote function must reside, but it anywhere you want. While it may be prudent to co-locate with them their corresponding `+page.svelte`, I like the idea of knowing where all my remote functions are located and put them in `src\\lib\\remote-funcs`.  \n\n`src\\lib\\remote-funcs\\form.remote.ts`\n\n```\nimport { z } from 'zod';\nimport { form } from '$app/server';\nimport { error, redirect, invalid } from '@sveltejs/kit';\nimport { form1Schema } from '$lib/types/form1schema';\n\n// You cannot export a schema from a remote function. Put them in a shared module.\n\nexport const createPost = form(form1Schema, async (data, issue) => {\n\t// Descructure the form fields from the data object.\n\tconst { title, content } = data;\n\n\t// See a field value in the console.\n\tconsole.log(title);\n\n\tif (title == 'ROGER') {\n\t\t// use the issue object to throw custom error messages.\n\t\tinvalid(issue.title('Title cannot be ROGER'), issue.content('Title cannot be PENCE'));\n\t}\n\n\tredirect(303, `/results`);\n});\n```\n\n## Create a form \n\nInitially, the only unusual thing with the form declaration is its `<form>` tag. The `{...createPost}` populates the `type` and `action` attributes for you. In this case, the form fields were created manually. We'll see in a moment how the form schema helps with this.\n\n```\n<script lang=\"ts\">\n\timport { createPost } from '$lib/remote-funcs/form.remote';\n</script>\n\n<form {...createPost}>\n\t<label>\n\t\t<h2>Title</h2>\n\t\t<input type=\"text\" name=\"title\" />\t\n\t</label>\n\n\t<label>\n\t\t<h2>Write your post</h2>\n\t\t<textarea name=\"content\" id=\"content\"></textarea>\t\t\n\t</label>\n\n\t<button type=\"submit\">Submit</button>\n</form>\n```\n\nFor this example, Svelte5 rendered this  `<form>` tag\n\n```\n<form {...createPost}>\n```\n\nas \n\n```\n<form method=\"POST\" action=\"?/remote=1ig978e%2FcreatePost\">\n```\n\nIf you dig into the code Svelte 5 generates, you'll find that a remote function gets an alias assigned to it. For example, the remote function file provided above is assigned the `lig978e` alias. You won't ever know, or care, what this alias value is. \n\n```\ninit_remote_functions(m, \"src/lib/remote-funcs/form.remote.ts\", \"1ig978e\");\n```\n\nWith this alias, and the function name (`createPost`, in this case) your form has the info it needs to issue the correct HTTP POST. \n\nAn alternative manually creating the form fields yourself, is to use the schema and the spread operator to do the work for you:\n\n```\n<script lang=\"ts\">\n\timport { createPost } from '$lib/remote-funcs/form.remote';\n</script>\n\n<form {...createPost}>\n\t<label>\n\t\t<h2>Title</h2>\n\t\t<input {...createPost.fields.title.as('text')} />\n\t</label>\n\n\t<label>\n\t\t<h2>Write your post</h2>\n\t\t<textarea {...createPost.fields.content.as('text')}></textarea>\n\t</label>\n\n\t<button type=\"submit\">Submit</button>\n</form>\n```\n\nUsing the schema fields like this is mostly an easy way to populate the `name` attribute. This code:\n\n```\n<input {...createPost.fields.title.as('text')} />\n```\n\nRenders as:\n\n```\n<input name=\"title\">\n```\n\nI think the point of using the more verbose spread is to ensure the form field names correspond to the schema field names. If you use the schema spread, you can also add other attributes to the tag (eg, `id` or `class`). \n\npreflight\n\nerror messages"
        }
    },
    {
        "dirent": {
            "name": "podman-named-volumes.md",
            "path": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech",
            "parentPath": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech"
        },
        "markdownObject": {
            "frontMatter": {
                "title": "podman named volumes",
                "description": "podman named volumes",
                "date_created": "2025-05-18T00:00:00.000Z",
                "date_updated": "2025-11-03T00:00:00.000Z",
                "date_published": "2026-01-03T00:00:00.000Z",
                "pinned": false,
                "tags": [
                    "containers"
                ]
            },
            "content": "That's a great question, and it highlights a key difference between named volumes and bind mounts.\n\nThe data for a Podman **named volume** (like `postgres_data` you created) is **not stored directly** in an easily accessible folder like `C:\\Users\\thumb\\...` on your Windows host filesystem.\n\nInstead, it's stored:\n\n1.  **Inside the Podman Machine's Filesystem:** Podman on Windows typically runs using a lightweight Linux Virtual Machine (VM). This VM has its own virtual disk and filesystem.\n2.  **Managed by Podman:** Within that VM's filesystem, Podman manages a specific directory structure for its storage, including volumes. The typical path _inside the VM_ where volume data is stored is:\n    `/var/lib/containers/storage/volumes/<volume_name>/_data`\n\nSo, for your `postgres_data` volume, the actual PostgreSQL files would reside at this path _inside the Podman VM_:\n\n`/var/lib/containers/storage/volumes/postgres_data/_data`\n\n**How to Access or Interact with the Volume Data (If Needed):**\n\n-   **Via `podman machine ssh` (Recommended for exploration):** You can access the shell of the Podman VM and navigate to the path:\n\n    ```bash\n    # Start an SSH session into the default podman machine\n    podman machine ssh\n\n    # Once inside the VM's shell:\n    sudo ls -l /var/lib/containers/storage/volumes/postgres_data/_data\n    # Or:\n    cd /var/lib/containers/storage/volumes/postgres_data/_data\n    ls -la\n    exit\n    ```\n\n    _(You might need `sudo` depending on the VM's configuration)_. This lets you see the files as they exist within the Linux environment where Postgres is actually running.\n\n-   **Mounting into another container:** You can temporarily mount the volume into another utility container (like `alpine` or `ubuntu`) to inspect or copy files using `podman cp`.\n\n    ```bash\n    # Run an interactive alpine container with the volume mounted at /inspect\n    podman run -it --rm -v postgres_data:/inspect alpine sh\n\n    # Inside the temporary container's shell:\n    ls -l /inspect\n    # You could tar files here, etc.\n    exit\n    ```\n\n-   **Backups (Correct Way):** The **best practice** for backing up PostgreSQL data is **not** to copy files directly from the volume location (whether it's a named volume or bind mount), especially while the database is running, as this can lead to inconsistent or corrupt backups. Use standard PostgreSQL tools like `pg_dump` instead:\n    ```bash\n    # Example: Dump all databases to a file on your host\n    podman exec -t postgresdb pg_dumpall -U postgres > C:\\Users\\thumb\\Documents\\db_backup.sql\n    # (You might need to adjust the user '-U' if you configured a different one)\n    ```\n\n**Where is the VM itself stored?**\n\nThe VM's virtual disk file (which _contains_ the volume data) is stored on your Windows host, but its exact location depends on the virtualization provider Podman is using (usually WSL or Hyper-V by default):\n\n-   **WSL:** Managed within the WSL distribution's storage (often complex paths under `%LOCALAPPDATA%\\Packages`).\n-   **Hyper-V:** Typically in the path configured in Hyper-V Manager, often defaulting to somewhere like `C:\\ProgramData\\Microsoft\\Windows\\Hyper-V` or `C:\\Users\\Public\\Documents\\Hyper-V\\Virtual Hard Disks`.\n\n**In Summary:**\n\nNamed volumes offer better compatibility and performance by keeping the data within the Linux environment Podman controls. The trade-off is that the data isn't directly browsable via Windows File Explorer. Use `podman machine ssh` for inspection and standard database tools (`pg_dump`) for backups."
        }
    },
    {
        "dirent": {
            "name": "postgres-pooling-management.md",
            "path": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech",
            "parentPath": "C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\lib\\markdown\\tech"
        },
        "markdownObject": {
            "frontMatter": {
                "title": "Managing Postgres connection pooling",
                "description": "Managing Postgres connection pooling",
                "date_created": "2025-06-13T00:00:00.000Z",
                "date_updated": "2025-11-03T00:00:00.000Z",
                "date_published": "2026-01-03T00:00:00.000Z",
                "pinned": false,
                "tags": [
                    "postgres"
                ]
            },
            "content": "This discusses this Postgres error\n\n```MaxListenersExceededWarning: Possible EventEmitter memory leak detected.```\n\nencountered during dev work with Sveltekit.\n\nThis is a very common warning in Node.js applications, especially during development with tools like SvelteKit that have hot-reloading. Let's break it down.\n\n### The Short Answer (TL;DR)\n\nThis warning means your application is repeatedly setting up \"shutdown\" listeners for your database connection (or something similar) without ever cleaning up the old ones. This typically happens in a SvelteKit development environment because the server restarts on every file change (Hot Module Replacement), but the old database connection isn't being properly closed.\n\n**You are likely creating a new database connection pool on every server reload, and each one is adding a listener to know when the app shuts down. You're leaking these listeners.**\n\nThe fix is **not** to increase the listener limit. The fix is to ensure you only have **one** instance of your database client/pool for the entire application lifetime and to properly close it when the application terminates.\n\n\n### Detailed Breakdown of the Error Message\n\nLet's dissect the message itself: `(node:5232) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 SIGTERM listeners added to [process]. MaxListeners is 10. Use emitter.setMaxListeners() to increase limit`\n\n1.  **`EventEmitter`**: This is a core building block in Node.js. Many objects, like network connections, file streams, and the `process` object itself, \"emit\" events. You can listen for these events using methods like `.on()` or `.addListener()`.\n\n2.  **`process`**: This is a global Node.js object that represents the current running application. It's an `EventEmitter`.\n\n3.  **`SIGTERM`**: This is a \"signal\" that can be sent to a process to request its termination (a graceful shutdown). When your SvelteKit dev server restarts, or when a production environment manager (like Docker or PM2) stops your app, it often sends a `SIGTERM` signal. Your code can *listen* for this signal to perform cleanup tasks, like closing database connections.\n\n4.  **`MaxListenersExceededWarning`**: By default, Node.js will warn you if you add more than 10 listeners for the *same event* on the *same EventEmitter*. This is a safety feature because it's a strong indicator of a \"memory leak.\" You're allocating resources (the listeners) but never releasing them.\n\n### Why This Happens in a SvelteKit + Postgres App\n\nThe most common cause is how you're initializing your Postgres client (like `node-postgres` i.e., `pg`, or an ORM like `Drizzle ORM` or `Prisma`).\n\nLet's look at a **common incorrect pattern**:\n\n```javascript\n// src/routes/some-route/+server.js or a hook\nimport pg from 'pg';\n\n// ❌ ANTI-PATTERN: This code runs every time the server reloads in dev mode.\nconst pool = new pg.Pool({\n  connectionString: process.env.DATABASE_URL,\n});\n\n// Many database clients automatically add a SIGTERM listener\n// to gracefully close the pool on shutdown.\n// So, on every file save:\n// 1. Vite reloads the server module.\n// 2. A NEW pool is created.\n// 3. A NEW SIGTERM listener is added to the `process` object.\n// 4. The OLD pool and its listener are never cleaned up.\n// After 10 reloads, you get the warning.\n\nexport async function GET() {\n  const result = await pool.query('SELECT NOW()');\n  // ...\n}\n```\n\n### The Correct Solution: The Singleton Pattern\n\nYou need to ensure that only **one instance** of your database connection pool is ever created. A common way to do this is to create it in a dedicated module and cache it.\n\n**Step 1: Create a dedicated database client module.**\n\nSvelteKit's server-only modules are perfect for this. Create a file like `src/lib/server/db.js`.\n\n```javascript\n// src/lib/server/db.js\n\nimport pg from 'pg';\nimport { PG_DATABASE, PG_HOST, PG_PASSWORD, PG_PORT, PG_USER } from '$env/static/private';\n\n// This is the key: declare the pool variable outside the function.\nlet pool: pg.Pool | null = null;\n\n/**\n * Returns the singleton instance of the Postgres connection pool.\n */\nexport function getDbPool() {\n  if (!pool) {\n    console.log('Creating new PostgreSQL connection pool...');\n    pool = new pg.Pool({\n\t\thost: PG_HOST,\n\t\tport: Number(PG_PORT) || 5432, // Ensure port is a number\n\t\tuser: PG_USER,\n\t\tpassword: PG_PASSWORD,\n\t\tdatabase: PG_DATABASE,\n\t\tssl: false\n\t});\n\n    // This is the graceful shutdown logic.\n    // It's added ONCE when the pool is created.\n    process.on('SIGTERM', () => {\n      console.log('SIGTERM signal received. Closing DB pool.');\n      pool.end();\n    });\n\n    // Optional: Also listen for SIGINT (Ctrl+C in terminal)\n    process.on('SIGINT', () => {\n      console.log('SIGINT signal received. Closing DB pool.');\n      pool.end();\n    });\n  }\n  return pool;\n}\n```\n\n**Step 2: Use the singleton in your routes and hooks.**\n\nNow, instead of creating a new pool everywhere, you import and use your getter function.\n\n```javascript\n// src/routes/some-route/+server.js\n\nimport { getDbPool } from '$lib/server/db.js';\n\nconst pool = getDbPool(); // Gets the existing pool, or creates it if it's the first time.\n\nexport async function GET() {\n  const client = await pool.connect();\n  try {\n    const result = await client.query('SELECT NOW()');\n    return new Response(JSON.stringify(result.rows[0]));\n  } finally {\n    // Always release the client back to the pool\n    client.release();\n  }\n}\n```\n\n### Why This Works\n\n*   The `pool` variable is created only once because Node.js caches modules. The first time `getDbPool()` is called, it creates the pool. Subsequent calls will see that `pool` already exists and will just return the cached instance.\n*   Because the pool is only created once, the `process.on('SIGTERM', ...)` listener is also only attached **once**.\n*   This completely solves the listener leak, and you get the added benefit of efficient connection pooling across your entire application.\n\n### What About `emitter.setMaxListeners()`?\n\nThe error message suggests this as a \"solution.\" **Almost always, this is a bad idea.** It's like turning off a smoke alarm because you don't like the noise. It hides the symptom (the warning) but doesn't fix the underlying problem (the memory leak). Only use it if you are absolutely certain you have a valid reason to have more than 10 listeners for a single event. In this SvelteKit/DB scenario, you do not.\n\n## Fetching the pool object from locals\n\nThe code above says to call `getDbPool()` elsewhere in the app. I am putting the `pool` in the `locals` object and wanted to make sure that didn't defeat the purpose of the singleton object.\n\n**Answer**\n\nThe short answer is: **No, this does not defeat the purpose of the singleton pattern. In fact, what you are doing is the idiomatic, recommended way to handle shared resources like database connections in SvelteKit.**\n\nLet's break down why this works perfectly and is considered a best practice.\n\n### Why Your Pattern is Correct\n\nYour code is leveraging two separate concepts that work together beautifully:\n\n1.  **Module Scope (for the Singleton):** Node.js caches modules. When you do `import { getDbPool } from '$lib/server/db.js';` and then `const pool = getDbPool();` at the top level of your `hooks.server.js` file, that code runs **once** when your server process starts. The `pool` variable in your hooks file holds the single, shared instance of the database pool for the entire lifetime of the server. This correctly implements the singleton pattern.\n\n2.  **Request Scope (for `event.locals`):** The `handle` hook runs for **every incoming request**. The `event` object, including `event.locals`, is created fresh for each request and destroyed after the response is sent.\n\nWhen you do `event.locals.pool = pool;`, you are **not creating a new pool**. You are simply taking the single, shared `pool` object that already exists in your module's memory and attaching a *reference* to it to the current request's `locals` object.\n\nThink of it like this:\n*   You have **one** central library in a city (the singleton `pool` instance).\n*   Every person who visits the city (an incoming request) gets a library card (`event.locals`) that points them to the one and only library.\n*   You aren't building a new library for every person. You're just giving them a cheap, easy way to access the existing one.\n\n### The Advantages of Your Approach (Dependency Injection)\n\nThis pattern is a form of **Dependency Injection**, and it's highly beneficial:\n\n1.  **Decoupling:** Your route handlers (`+server.js`, `+page.server.js`) don't need to know *how* to create or get the database connection. They are \"dumb\" in that respect. They just know to expect it on `event.locals`. This makes your code cleaner and easier to reason about.\n\n2.  **Testability:** This pattern makes testing your endpoints much easier. You can create a mock database pool in your test setup and just place it on the `event.locals` object, without your endpoint code ever knowing the difference.\n\n3.  **Centralized Logic:** All the logic for initializing and managing the database connection is in one place (`hooks.server.js` and `lib/server/db.js`). If you need to add logging, monitoring, or other logic around the pool, you have a single place to do it.\n\n### Pro-Tip: Add Type-Safety\n\nTo make this pattern even better and get full autocompletion in your editor, you should declare the shape of `event.locals` in your `src/app.d.ts` file.\n\n**`src/app.d.ts`**\n```typescript\n// See https://kit.svelte.dev/docs/types#app\n// for information about these interfaces\nimport type { Pool } from 'pg'; // Or the type from your specific DB driver/ORM\n\ndeclare global {\n\tnamespace App {\n\t\t// interface Error {}\n\t\tinterface Locals {\n\t\t\tpool: Pool; // Add your property here\n\t\t}\n\t\t// interface PageData {}\n\t\t// interface Platform {}\n\t}\n}\n\nexport {};\n```\n\nNow, when you type `event.locals.` inside any of your server-side SvelteKit files, your editor will know that a `pool` property exists and what its type is.\n\n### Complete Example Flow\n\nHere's how your excellent pattern looks from start to finish:\n\n**1. The Singleton (`src/lib/server/db.js`)**\n```javascript\nimport pg from 'pg';\nimport { DATABASE_URL } from '$env/static/private';\n\nlet pool;\n\nexport function getDbPool() {\n  if (!pool) {\n    pool = new pg.Pool({ connectionString: DATABASE_URL });\n  }\n  return pool;\n}\n```\n\n**2. The Hook (`src/hooks.server.js`)**\n```javascript\nimport { getDbPool } from '$lib/server/db';\nimport type { Handle } from '@sveltejs/kit';\n\n// This runs ONCE when the server starts\nconst pool = getDbPool();\n\nexport const handle: Handle = async ({ event, resolve }) => {\n\t// This runs for EVERY request\n\t// It attaches a REFERENCE to the single pool instance\n\tevent.locals.pool = pool;\n\n\tconst response = await resolve(event);\n\treturn response;\n};\n```\n\n**3. The Endpoint (`src/routes/api/users/+server.js`)**\n```javascript\nimport { json } from '@sveltejs/kit';\n\nexport async function GET({ locals }) {\n\t// Get the pool from locals (no imports needed!)\n\tconst { pool } = locals;\n\n\tconst result = await pool.query('SELECT * FROM users');\n\n\treturn json(result.rows);\n}\n```\n\n**Conclusion:** Keep doing what you're doing. It is the correct, efficient, and maintainable way to manage shared resources like a database pool within a SvelteKit application. You have successfully avoided the listener leak while implementing a clean, testable architecture."
        }
    }
]