[
    {
        "title": "Postgres SQL",
        "description": "Postgres SQL",
        "date_created": "2025-06-14T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": "2026-01-03T00:00:00.000Z",
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "CREATE TABLE downloads (\n  id INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  date_requested timestamp,\n  name varchar(120),\n  company varchar(120),\n  country varchar(60),\n  email_address varchar(120),\n  product varchar(120),\n  family varchar(60)\n);\n\n-- To set the starting value for the auto-incrementing ID:\nALTER SEQUENCE downloads_id_seq RESTART WITH 1023;",
        "locale": "en",
        "slug": "tech/postgres-sql",
        "folder": ""
    },
    {
        "title": "Postgres on WSL 2 with Podman",
        "description": "How to install and configure Postres on WSL2 with Podman",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-29T00:00:00.000Z",
        "date_published": "2026-01-03T00:00:00.000Z",
        "pinned": false,
        "tags": [
            "containers",
            "podman",
            "postgres"
        ],
        "content": "\n[supabase-credentials#This is a test]]\n\n/home/user/postgres_data\n/home/user/postgres_storage\n\nSee 9:40 in Scott's video for this connection URL\n\n![[image-68.png]]\n\nThese are the commands that worked\n\npodman volume create pg_data_final\n\npodman run -d --name postgres-wsl  \n-e POSTGRES_PASSWORD=mysecretpassword -e PGDATA=/var/lib/postgresql/data  \n-v pg_data_final:/var/lib/postgresql/data:Z -p 5432:5432  \n--shm-size=1g postgres:latest  \n-c shared_buffers=512MB `  \n-c random_page_cost=1.1\n\n  It is great to hear you are up and running! This setup is a very good setup for running a database on Windows 11 via WSL 2 because it prioritizes native I/O speed while navigating the strict permission rules of rootless Podman. This setup's named volume keeps all DB data on the WSL 2-hosted VM. \n\nHere is a recap of the logic and steps that led to this successful, high-performance configuration:\n\n1. The Environment Reset\nWe started by clearing out \"phantom\" WSL distributions. When a WSL distro is deleted manually, Podman’s internal metadata gets out of sync. \nThe Fix: Running podman machine rm -f and podman machine init created a clean, fresh WSL 2 backend dedicated to Podman.\n\n2. Solving the \"Slow Disk\" Problem\nYou correctly identified that mounting Windows folders (like C:\\Users\\...) is extremely slow. This is because every database write has to cross the \"9P\" bridge between Linux and Windows.\nThe Fix: We used a Named Volume (pg_data_final). Unlike a direct folder mount, a named volume is managed entirely by Podman inside the WSL ext4 virtual disk. This allows Postgres to write to the disk at native SSD speeds.\n\n3. Overcoming Postgres 18 Permission Errors\nPostgres 18 introduced a new logic that tries to create a /18/data subdirectory inside your mount. In a \"rootless\" Podman environment, the container user doesn't have system permission to create new directories on a volume root, which caused the Permission Denied errors.\nThe Fix: We used -e PGDATA=/var/lib/postgresql/data. This told Postgres: \"Don't try to create a new versioned folder; use this exact path I've provided.\" This bypassed the failing mkdir command.\n\n4. Performance Tuning for Modern Hardware\nStandard Postgres images are configured conservatively to run on old servers with spinning hard drives.\nThe Fix: We injected high-performance flags directly into the run command:\n    --shm-size=1g: Gave Postgres enough shared memory to handle complex queries.\n    shared_buffers=512MB: Allocated 25% of your assigned RAM to the database cache.\n    random_page_cost=1.1: Told the Postgres query planner that you are using an SSD, making it much faster at choosing the right indexes.\n\nMaintenance Cheat Sheet\nNow that you have this working, here are the three commands you’ll likely need in the future:\n\nTo stop/start the database:\npodman stop postgres-wsl\npodman start postgres-wsl\n\nTo see if it's running healthy:\npodman ps\npodman logs postgres-wsl\n\nTo enter the database command line directly (psql):\npodman exec -it postgres-wsl psql -U postgres\n\nA note on Windows Restarts: \nIf you restart your computer, you will need to run podman machine start first, and then podman start postgres-wsl to get your database back online. Your data will be exactly where you left it inside that named volume!\n\nMore help\n\nWhen you run podman stop postgres-wsl, you are stopping only the container.\n\nTo understand exactly what is happening in your specific setup, it helps to look at the \"layers\" you have running. Think of it like a Russian nesting doll:\n\n1. The Container (postgres-wsl) \nThis is the \"process\" of PostgreSQL itself. When you run podman stop, you are telling the Postgres engine to perform a graceful shutdown. \nStatus: \"Exited.\"\nData: Safe. It stays in the pg_data_final volume.\nResources: It stops using CPU and RAM.\n\n2. The Podman Engine (The \"Machine\") \nBehind the scenes, you have a WSL 2 distribution (the podman-machine-default you saw earlier) that acts as the host for your containers. \nStatus: \"Running.\"\nNote: Even though the container is stopped, the Linux VM (WSL) is still powered on and idling in the background, waiting for you to start a container again.\n\n3. WSL 2/Windows 11 \nThe overall Windows subsystem for Linux is still active.\n\nWhy this distinction matters:\n\nIf you want to save RAM on your computer:\nStopping the container helps a little, but the Podman Machine (the Linux VM) usually reserves a chunk of your Windows RAM (you saw it was using 4GiB earlier). To get that RAM back, you have to stop the \"Machine\":\npodman machine stop\n\nIf you want to resume work:\nIf you only stopped the container, you only need one command to get back into DBeaver:\npodman start postgres-wsl\n\nIf you stopped the machine or restarted Windows, you need two steps:\npodman machine start (Turn on the Linux engine)\npodman start postgres-wsl (Turn on the Postgres process)\n\nSummary\npodman stop postgres-wsl = Turning off the App.\npodman machine stop = Turning off the Computer the app runs on.\n\nTo answer your question directly: No, it does not map to a folder on your Windows host.\n\nThis is exactly why your database is now performing so well. Here is the breakdown of where that data actually lives and how it works:\n\n1. Where is the data?\nThe data lives inside the WSL 2 Virtual Disk (.vhdx). \n\nWhen you created the named volume pg_data_final, Podman carved out a directory inside the Linux file system that belongs to your podman-machine-default instance. To Windows, that entire Linux environment is just one giant, high-performance file (usually located at %USERPROFILE%\\AppData\\Local\\Microsoft\\Windows\\WSL\\...).\n\n2. Does it contain the database data?\nYes. Everything that makes up your Postgres database is inside that volume:\nYour tables and actual row data.\nYour indexes and schemas.\nThe Postgres configuration files.\nThe transaction logs (WAL).\n\n3. Can I see the files from Windows?\nYou can actually \"peek\" into the volume from Windows Explorer, but you should never move or edit these files from Windows, or you will corrupt the database permissions.\n\nIf you want to see them, paste this into your Windows Explorer address bar:\n\\\\wsl$\\podman-machine-default\\home\\user\\.local\\share\\containers\\storage\\volumes\\pg_data_final\\_data\n\n(Note: The exact path might vary slightly depending on your Podman version, but it is always inside the \\\\wsl$ network share.)\n\n4. Why this is the \"Performance Secret\"\nNo \"Bridge\" Tax: When Postgres writes to a Windows folder (like C:\\Users), it has to use a translation layer called 9P. This layer acts like a slow network translator between Linux and Windows. It is death for database performance.\nNative Linux Speed: By using a Named Volume, Postgres talks directly to the ext4 filesystem. There is no translation. It’s as fast as running Postgres on a dedicated Linux server.\n\n5. How do I back it up?\nSince the files aren't in a standard Windows folder, you can't just \"Copy+Paste\" them to a USB drive easily. If you want to back up your data, you have two professional options:\n\nOption A: The SQL Dump (Safest)\nRun this from PowerShell to create a backup file on your Windows Desktop:\npodman exec postgres-wsl pg_dumpall -U postgres > C:\\Users\\YourUsername\\Desktop\\backup.sql\n\nOption B: The Podman Export\nYou can export the entire volume to a compressed file:\npodman volume export pg_data_final --output pg_backup.tar\n\nSummary\nYour data is stored in a high-speed virtual vault inside WSL 2. It is not on your \"Host\" Windows filesystem, which is exactly why you are no longer seeing the slowness you experienced before!\n\nSuccessful steps!\n\nThis is the \"Golden Path\" we discovered to get high-performance PostgreSQL running on Windows 11 via Podman and WSL 2. \n\nBy following these steps, you moved the database from the slow Windows filesystem into the native Linux environment, bypassing permission errors and performance bottlenecks.\n\nStep 1: Environment Cleanup & Reset\nWe first had to ensure Podman's background engine (the WSL \"machine\") was healthy and in sync.\nThe Problem: Old or manually deleted WSL distros leave \"phantom\" metadata that makes Podman fail to start.\nThe Fix: You removed the broken machine and initialized a fresh one with enough resources:\npodman machine rm -f podman-machine-default\npodman machine init --disk-size 50 --memory 4096\npodman machine start\n\nStep 2: Using a performant storage strategy\nYou originally had Postgres pointing to a folder on your Windows C:\\ drive, which was slow.\nThe Change: We switched from a \"Bind Mount\" (a Windows folder) to a Podman Named Volume.\nThe Result: This placed the data inside the WSL ext4 filesystem. This eliminates the 9P protocol overhead, allowing your SSD to perform at native Linux speeds.\npodman volume create pg_data_final\n\nStep 3: Resolving Postgres 18 Permission Errors\nPostgres 18 changed its storage logic to use versioned subdirectories (e.g., /var/lib/postgresql/18/data). In a rootless Podman environment, this caused a \"Permission Denied\" error because the container couldn't create new directories on the root of the volume.\nThe Solution: We used the PGDATA override. By explicitly setting the path, we told Postgres to skip the directory-creation logic and use the volume mount point directly.\n\nStep 4: The Final Performance-Tuned Run Command\nThis specific command combined the storage fix with engine tuning for modern SSDs:\n\npodman run -d `\n  --name postgres-wsl `\n  -e POSTGRES_PASSWORD=mysecretpassword `\n  -e PGDATA=/var/lib/postgresql/data `\n  -v pg_data_final:/var/lib/postgresql/data:Z `\n  -p 5432:5432 `\n  --shm-size=1g `\n  postgres:latest `\n  -c shared_buffers=512MB `\n  -c random_page_cost=1.1\n\nWhy these flags matter:\n-v ...:Z: Automatically handles the internal Linux security labels so Postgres can read/write to the volume.\n--shm-size=1g: Increases the shared memory limit. Without this, Postgres often crashes on complex queries in containers.\n-c random_page_cost=1.1: Tells Postgres you are on an SSD. By default, Postgres assumes you have a slow, spinning hard drive and avoids using certain indexes. This flag makes it much faster.\n\nStep 5: Connecting DBeaver via the Localhost Relay\nBecause Podman on Windows runs inside a specialized WSL VM, it uses a \"proxy\" to connect that VM to your Windows network.\nThe Result: You were able to connect DBeaver using localhost:5432 because Podman automatically forwarded the traffic from Windows into the WSL container.\n\nSummary of the \"Wins\":\nSpeed: Native ext4 storage via a Named Volume.\nStability: Overrode PGDATA to fix Postgres 18 directory permission issues.\nOptimization: Tuned for SSDs and allocated proper shared memory.\nCleanliness: Removed old, failed containers and volumes to keep the system tidy.\n\nYou now have a professional-grade PostgreSQL setup that is significantly faster and more reliable than a standard Windows-native installation or a basic Docker-on-Windows setup.\n-f flag\n\nYou don't need it for the database to function, but since your goal is high performance, it is a very good flag to have. \n\nHere is the breakdown of why you might care about it and whether it’s worth \"fixing\" your current container to include it.\n\nWhat does effective_io_concurrency do?\nThis setting tells PostgreSQL how many simultaneous disk I/O operations the underlying storage can handle at once. \nDefault (1): Assumes a single spinning hard drive platter. Postgres waits for one piece of data to be read before asking for the next.\nHigh Value (200-300): Optimized for SSDs. Since SSDs don't have a physical \"head\" that has to move, they can fetch dozens of different pieces of data simultaneously. \n\nBy setting this to 200, you are telling the Postgres query planner: \"Don't be shy; feel free to initiate up to 200 I/O requests at once.\" This dramatically speeds up Bitmap Heap Scans (a common way Postgres reads data when using indexes).\n\nDo you care?\nFor Development/Small Data: You won't notice the difference. The database is already very fast because it’s on the WSL native filesystem.\nFor Large Data/Complex Queries: You do care. It allows Postgres to fully saturate the speed of your NVMe/SSD.\n\nHow to add it \nBecause this is a startup flag, you cannot add it to a running container. You would have to recreate it. Since your data is safe in the Volume, this only takes 30 seconds:\n\nStop and Remove the current container:\n    podman stop postgres-wsl\n    podman rm postgres-wsl\n\nRun it again with the \"Ultimate\" performance flags:\npodman run -d `\n  --name postgres-wsl `\n  -e POSTGRES_PASSWORD=mysecretpassword `\n  -e PGDATA=/var/lib/postgresql/data `\n  -v pg_data_final:/var/lib/postgresql/data:Z `\n  -p 5432:5432 `\n  --shm-size=1g `\n  postgres:latest `\n  -c shared_buffers=512MB `\n  -c random_page_cost=1.1 `\n  -c effective_io_concurrency=200 `\n  -c work_mem=16MB\n\nSummary\nIf you are happy with the current speed in DBeaver, leave it as is. You’ve already fixed the biggest bottleneck (moving from Windows storage to WSL storage). \n\nHowever, if you want to brag that your Postgres is tuned exactly for modern SSD hardware, adding effective_io_concurrency=200 is the \"cherry on top\" for your configuration!\n\nDoes that named volume persist as long as the WSL distro is installed\n\nThe short answer is yes, the named volume persists as long as the Podman Machine (which is a WSL distribution) exists. \n\nHowever, because you are using Podman on Windows, there is a specific hierarchy of \"persistence\" you should understand to ensure you never accidentally lose your data.\n\n1. What IS safe (Data Persists)\nYour data in pg_data_final will survive all of the following:\nStopping/Starting the container: podman stop / podman start.\nDeleting the container: podman rm.\nRestarting Windows: When you reboot, the WSL virtual disk stays on your drive.\nStopping the Podman Machine: podman machine stop.\nUpdating the Postgres Image: If you pull a newer version of Postgres and recreate the container using the same volume, your data remains.\n\n2. What IS NOT safe (Data is Lost)\nYour data will be destroyed if you do any of the following:\npodman machine rm: If you delete the Podman Machine to \"start fresh,\" you are deleting the entire WSL virtual disk, including all volumes stored inside it.\npodman volume rm pg_data_final: This is the specific command to delete the virtual hard drive you created.\nUninstalling WSL or Podman Desktop: Depending on the settings, removing the application may trigger a cleanup of the associated WSL distributions.\nFactory Resetting Windows: This obviously wipes the WSL subsystem.\n\n3. Where exactly is the \"Risk\"?\nIn a standard Linux setup, the volume is just a folder on your physical disk. In your Windows setup, the volume is a folder inside a virtual disk file (.vhdx). \n\nIf that .vhdx file (the podman-machine-default distribution) is deleted, the volume goes with it.\n\nPro-Tip: The \"Golden Rule\" of Database Persistence\nWhile named volumes are great for performance and daily development, never rely on a container volume as your only backup for important data. \n\nBecause WSL distributions can occasionally become corrupted or need to be recreated (as we did in Step 1 of your setup), you should occasionally \"export\" your data to the Windows Host side where your standard PC backups (like OneDrive, Backblaze, or an external drive) can see it.\n\nThe \"Safety\" Command (Run this once a week):\npodman exec postgres-wsl pg_dumpall -U postgres > C:\\Users\\YourName\\Documents\\postgres_backups\\weekly_backup.sql\nThis takes the data *out* of the virtual WSL world and puts it into a standard Windows file that will survive even if you delete the Podman Machine.\n\nSummary\nAs long as you don't run podman machine rm, your Postgres data is as permanent as any other file on your computer. It is stored safely inside the WSL filesystem, optimized for the high performance you've now achieved.\n\nPodman command summary\n\nHere is a concise cheat sheet of the most useful Podman commands and flags for managing your new PostgreSQL environment and beyond.\n\n1. Essential Container Commands\n| Command | Purpose |\n| :--- | :--- |\n| podman ps | List running containers. |\n| podman ps -a | List all containers (including stopped/crashed ones). |\n| podman logs  | View the output of a container (add -f to follow live). |\n| podman stop  | Gracefully shut down a container. |\n| podman start  | Start an existing, stopped container. |\n| podman rm -f  | Forcefully delete a container. |\n| podman exec -it  bash | Enter a container's terminal while it is running. |\n| podman inspect  | View the full technical configuration (JSON format). |\n\n2. Common podman run Flags\nThese are the building blocks of the \"Run\" script you used:\n-d (Detached): Runs the container in the background.\n-p 5432:5432: Maps a host port to a container port (Host:Container).\n--name : Assigns a friendly name so you don't have to use long IDs.\n-e VAR=val: Sets environment variables (e.g., POSTGRES_PASSWORD).\n--rm: Automatically deletes the container when it stops (great for one-off tests).\n-it: Keeps the terminal interactive (used for bash or psql).\n--restart : Set to always or unless-stopped to auto-restart if it crashes.\n--shm-size=1g: Increases shared memory (crucial for databases like Postgres).\n\n3. Special Volume Suffixes\nIn Podman, the flags appended to your volume (-v volume_name:path:suffix) are critical for rootless environments:\n:z: (Lower-case) Tells SELinux the volume is shared between multiple containers.\n:Z: (Upper-case) Tells SELinux the volume is private to this container only.\n:U: Tells Podman to map the ownership of the host files to the container’s internal user (fixes \"Permission Denied\" errors in rootless mode).\n\n4. Image & Storage Management\n| Command | Purpose |\n| :--- | :--- |\n| podman images | List all downloaded images. |\n| podman pull  | Download the latest version of an image from a registry. |\n| podman rmi  | Delete an image from your drive. |\n| podman volume ls | List all named volumes (the virtual hard drives). |\n| podman volume prune | Danger: Deletes all volumes not currently attached to a container. |\n\n5. Windows/WSL \"Machine\" Commands\nSince you are on Windows, you are managing the Linux \"engine\" that runs Podman:\npodman machine ls: Check the status of your WSL backend (Running vs. Stopped).\npodman machine start: Power on the Linux VM.\npodman machine stop: Power off the VM (saves Windows RAM).\npodman machine ssh: Log directly into the Linux VM backend.\npodman machine set --cpus 4 --memory 4096: Resize the engine's resources.\n\n6. System Health & Maintenance\npodman stats: View live CPU and RAM usage of your containers.\npodman info: View system-wide settings, storage paths, and registry info.\npodman system prune -a: The \"Spring Cleaning\" command. Deletes all stopped containers, unused networks, and dangling images.\n\nShutdown and startup \n\nBecause your data is stored in a Named Volume (pg_data_final), the data is physically separated from the container's lifecycle. You can stop, start, or even delete the container without losing a single row of data.\n\nHere is the professional workflow for shutting down and starting back up.\n\nPart 1: Shutting Down Safely\nTo ensure your database files are closed correctly and your Windows RAM is freed up, follow these two steps:\n\n1. Stop the Postgres Container\nThis sends a signal to Postgres to flush all pending data to the disk and close connections gracefully.\npodman stop postgres-wsl\n\n2. Stop the Podman Machine (The RAM Saver)\nEven if the container is stopped, the WSL 2 Linux VM is still running in the background, consuming a few gigabytes of your Windows RAM. Run this to fully \"turn off\" the engine:\npodman machine stop\n\nPart 2: Starting Up (After a Reboot)\nWhen you turn your PC back on, the Podman engine does not start automatically. You must bring the environment back up in this order:\n\n1. Start the Podman Machine\nThis \"boots\" the hidden Linux environment that holds your Podman volumes and engine.\npodman machine start\n\n2. Start the Postgres Container\nOnce the machine is \"Started,\" run your container. You don't need the long run command with all the flags; Podman remembered them all when you first created it.\npodman start postgres-wsl\n\n3. Verify Health\nWait about 5 seconds for Postgres to finish its internal checks, then verify it's ready:\npodman logs postgres-wsl\nLook for: \"database system is ready to accept connections.\"\n\nBonus: The \"Easy Button\" (Automation)\nIf you don't want to type these every time you reboot, you can create a simple Startup Script on your Windows Desktop.\n\nRight-click your Desktop and select New > Text Document.\nPaste the following:\n @echo off\n  echo Starting Podman Machine...\n podman machine start\n echo Starting Postgres Container...\n podman start postgres-wsl\n echo.\n echo Done! Your database is ready for DBeaver.\n pause\n\nSave the file as Start-Postgres.bat (make sure it ends in .bat, not .txt).\n\nNow, after a reboot, you just double-click that file, and your high-performance Postgres environment will spin up automatically!\n\nWhy your data is safe:\nWhen you run podman stop or podman machine stop, the data inside the pg_data_final volume remains exactly where it is on the WSL virtual disk. It is exactly like turning off a computer; the files on the hard drive don't disappear just because the power is off.\n\nFull system shutdown\n\nTo fully restart the entire stack—from the database engine up to the Windows Subsystem for Linux itself—you should follow a \"top-down\" shutdown and a \"bottom-up\" startup. \n\nThis is the most reliable way to clear memory leaks, reset \"stuck\" network ports, and ensure your virtual disk is in a safe state.\n\nPhase 1: The Clean Shutdown (Top-Down)\nThis sequence ensures Postgres finishes its writes before the underlying \"computer\" is turned off.\n\nStop the Container:\n    podman stop postgres-wsl\nStop the Podman Machine:\n    podman machine stop\nShutdown WSL Entirely:\n    This command kills all running WSL distributions and the background WSL 2 lightweight utility VM. Use this if WSL feels sluggish or if you are having connection issues.\n    wsl --shutdown\n\nPhase 2: The Fresh Start (Bottom-Up)\nYou don't need a specific \"WSL start\" command; starting the Podman machine will automatically trigger the WSL 2 subsystem to wake up.\n\nStart the Podman Machine:\n    podman machine start\n    (If this hangs, it’s usually because Windows is still cleaning up the previous WSL shutdown; just wait 10 seconds and try again.)\n\nStart the Container:\n    podman start postgres-wsl\n\nWhy should you use wsl --shutdown?\nIn your specific environment, wsl --shutdown is a powerful tool for two reasons:\n\nMemory Reclamation: WSL 2 (and Podman by extension) is notorious for \"holding onto\" Windows RAM even after you stop a container. A full shutdown forces the vmmem process in Task Manager to release that memory back to Windows.\nNetworking Reset: If DBeaver suddenly stops connecting to localhost, a WSL shutdown resets the virtual network bridge between Windows and Linux, which usually fixes the issue.\n\nThe \"Full Restart\" Script\nIf you want to do this frequently to keep your system fast, you can put this into a .bat file:\n\n@echo off\necho --- SHUTTING DOWN ---\npodman stop postgres-wsl\npodman machine stop\nwsl --shutdown\necho.\necho --- RESTARTING ---\npodman machine start\npodman start postgres-wsl\necho.\necho System Restarted Successfully.\npause\n\nIs my data safe during wsl --shutdown?\nYes. As long as you ran podman stop postgres-wsl first, the database has finished writing its logs to the disk. The wsl --shutdown command is essentially like pulling the power plug on a PC that is already turned off—it is perfectly safe for your data.\n\nSSH into container\n\npodman machine ssh\n\nGet container info\n\ncat /etc/os-release\n\n[user@DESKTOP-FT1088C ~]$ cat /etc/os-release\nNAME=\"Fedora Linux\"\nVERSION=\"41 (Container Image)\"\nRELEASE_TYPE=stable\nID=fedora\nVERSION_ID=41\nVERSION_CODENAME=\"\"\nPLATFORM_ID=\"platform:f41\"\nPRETTY_NAME=\"Fedora Linux 41 (Container Image)\"\nANSI_COLOR=\"0;38;2;60;110;180\"\nLOGO=fedora-logo-icon\nCPE_NAME=\"cpe:/o:fedoraproject:fedora:41\"\nDEFAULT_HOSTNAME=\"fedora\"\nHOME_URL=\"https://fedoraproject.org/\"\nDOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f41/\"\nSUPPORT_URL=\"https://ask.fedoraproject.org/\"\nBUG_REPORT_URL=\"https://bugzilla.redhat.com/\"\nREDHAT_BUGZILLA_PRODUCT=\"Fedora\"\nREDHAT_BUGZILLA_PRODUCT_VERSION=41\nREDHAT_SUPPORT_PRODUCT=\"Fedora\"\nREDHAT_SUPPORT_PRODUCT_VERSION=41\nSUPPORT_END=2025-12-15\nVARIANT=\"Container Image\"\nVARIANT_ID=container\n\nPodman compose\n\nYes, a Podman Compose file would help significantly. While your current command works, it is \"imperative\"—meaning you have to remember and re-type (or copy-paste) that long string every time you want to recreate or update the container.\n\nUsing a compose.yaml file (the \"declarative\" way) offers several advantages for your specific setup:\n\n1. Benefits for your specific setup\nVersion Control: You can save the file in your project folder. If you change a setting (like shared_buffers), you just edit the text file instead of trying to edit a massive terminal command.\nAutomatic Restarts: You can add restart: always so the DB starts automatically when your machine/WSL2 starts.\nReadability: It’s much easier to see the relationship between your shared memory (shm_size), volumes, and Postgres flags.\nOne-Command Management: Instead of multiple steps (creating volume, then running), you just run podman-compose up -d.\n\n2. Your command converted to compose.yaml\nCreate a file named compose.yaml in your project directory and paste this:\n\nservices:\n  postgres:\n    image: postgres:latest\n    container_name: postgres-wsl\n    restart: always\n    environment:\n      POSTGRES_PASSWORD=mysecretpassword\n      PGDATA=/var/lib/postgresql/data\n    ports:\n      \"5432:5432\"\n    volumes:\nThe :Z ensures correct SELinux labels (useful if using RHEL/Fedora in WSL)\n      pg_data_final:/var/lib/postgresql/data:Z\n    shm_size: '1gb'\nThis translates your -c flags into the startup command\n    command: \n      postgres\n      -c\n      shared_buffers=512MB\n      -c\n      random_page_cost=1.1\n      -c \n      effective_io_concurrency=200        \n\nvolumes:\n  pg_data_final:\n    external: true  # Tells Podman to use the volume you already created manually\n\n3. How to use it\nInstall Podman Compose (if you haven't yet):\n    pip install podman-compose\nStart the container:\n    In the same folder as your file, run:\n    podman-compose up -d\nStop the container:\n    podman-compose down\n\nKey differences to note:\nshm_size: In your manual command, you used --shm-size=1g. In the Compose file, it’s a dedicated setting. This is critical for Postgres performance with large shared_buffers.\nexternal: true: I set the volume as \"external\" because you already created pg_data_final manually. If you want Compose to manage the volume lifecycle entirely, you would remove external: true.\nPostgres Flags: The command section cleanly breaks down your -c configurations, making them much easier to adjust later as your database grows.\n\nThe VHDX \n\npodman-machine-default is the standard, automatically assigned name for the Linux virtual machine (VM) that Podman creates on macOS and Windows to run containers, acting as the default target for Podman commands like podman machine init. It provides a Linux environment (Fedora CoreOS) for container operations, allowing Podman to function on non-Linux systems, and you use commands like podman machine start, stop, list, and init to manage it.\n\nUsing PowerShell to show  WSL distros\n\n(Get-ChildItem HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Lxss | ForEach-Object {Get-ItemProperty $_.PSPath}) | Select-Object DistributionName, BasePath\n\nDistributionName       BasePath\nUbuntu                 C:\\Users\\thumb\\AppData\\Local\\wsl\\{73929071-0e78-42fe-a25d-96580d570fe0}\npodman-machine-default \\\\?\\C:\\Users\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\n\nUsing the registry to see WSL distros\n\nHKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Lxss\n\nC:\\Users\\\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\\ext4.vhdx\n\nDistributionName       BasePath\nUbuntu                 C:\\Users\\thumb\\AppData\\Local\\wsl\\{73929071-0e78-42fe-a25d-96580d570fe0}\npodman-machine-default \\\\?\\C:\\Users\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\n\nThe Windows Extended Length Path Prefix\n\nThe path prefix for the podman-machine-default is a Windows \"[Extended Length Path\" prefix. \n\n\\\\?\\\n\nThis tells Windows to ignore the standard 260-character path limit.    \nIt's necessary with WSL and Podman because they often use deeply nested folders. Without this prefix, some Windows applications would crash or fail to find the file because the folder path is too long.",
        "locale": "en",
        "slug": "tech/postgres-on-wsl2-with-podman",
        "folder": ""
    },
    {
        "title": "Vanilla CSS for Sveltekit",
        "description": "Vanilla CSS for Sveltekit",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-15T00:00:00.000Z",
        "date_published": "2026-01-03T00:00:00.000Z",
        "pinned": false,
        "tags": [
            "sveltekit",
            "css"
        ],
        "content": "This document shows how to use vanilla CSS without an preprocessing. Given how much CSS has improved over the last couple of years, it's hard to make a good case for PostCSS. \nThe Sveltekit build step (with Vite's help) automatically compresses CSS.\nCSS's @import (which may, or may now, impede performance) runs at SvelteKit so it's performance is a moot point.\n\n└── src/\n    ├── css  /\n    │   ├── components/\n    │   │   ├── _components.css\n    │   │   └── button.css\n    │   ├── elements/\n    │   │   ├── _elements.css\n    │   │   └── select.css\n    │   ├── global/\n    │   │   ├── _global.css\n    │   │   ├── reset.css\n    │   │   ├── tags.css\n    │   │   └── vars.css\n    │   ├── layouts/\n    │   │   └── _layouts.css\n    │   ├── utilities/\n    │   │   └── _utilities.css\n    │   └── _style.css      \n    ├── lib/\n    │   ├── assets/\n    │   │   └── favicon.svg\n    │   └── routes/\n    │       ├── +layout.svelte\n    │       └── +page.svelte    \n    ├── routes/\n    │   └── ..      \n    └── style.css\n\n+layout.svelte\n\n\n\timport favicon from '$lib/assets/favicon.svg';\n\timport '../style.css';\n\n\tlet { children } = $props();\n\n\n\n\t\n\n\n{@render children?.()}\n\nsrc/css/style.css\n\n@layer global, layouts, elements, components, utilities;\n\n/* @import \"https://unpkg.com/open-props\" layer(global); */\n@import './global/_global.css' layer(global);\n@import './layouts/_layouts.css' layer(layouts);\n@import './elements/_elements.css' layer(elements);\n@import './components/_components.css' layer(components);\n@import './utilities/_utilities.css' layer(utiilties);\n\nsrc/style.css\n\n@import './css/_style.css';\n\nThis is the swindle that makes this all work. At build time (or runtime in dev mode), style.css imports the various CSS files. Then /css/style.css imports the rendered style.css contents. It's almost magic--CSS @imports are driving all of this. At build time, Sveltekit and Vite produce a concrete, compressed, version of /css/style.css. That single, static CSS file is produced at (where `` is the hash file name created.)\n\n.svelte-kit\\output\\client\\_app\\immutable\\assets\\.css\n\nBy the way, this CSS process occurs without any configuration! \n\nroutes/+layout.svelte\n\n\n\timport favicon from '$lib/assets/favicon.svg';\n\timport '../style.css';\n\n\tlet { children } = $props();\n\n\n\n\t\n\n\n{@render children?.()}\n\n[!info]\nI avoided using @import with the original ASNA.com because of the potential performance implications of using @import. However, that was silly because I didn't understand how powerful the Sveltekit/Vite build process is. \nImplementation details\n\nEach directory under css is a category of css that is assigned to a given layer. \n\n_global.css\n\n@import \"reset.css\";\n@import \"vars.css\";\n@import \"tags.css\";\n\nreset.css\n\n*,\n*::before,\n*::after {\n  box-sizing: border-box;\n}\n\n/* Prevent font size inflation */\nhtml {\n  -moz-text-size-adjust: none;\n  -webkit-text-size-adjust: none;\n  text-size-adjust: none;\n}\n\n/* Remove default margin in favour of better control in authored CSS */\nbody, h1, h2, h3, h4, p,\nfigure, blockquote, dl, dd {\n  margin-block-end: 0;\n} \n\n/* Remove list styles on ul, ol elements with a list role, which suggests default styling will be removed */\nul[role='list'],\nol[role='list'] {\n  list-style: none;\n}\n\nu, ol {\n  font-size: inherit;\n}\n\n/* Set core body defaults */\nbody {\n  min-height: 100vh;\n  line-height: 1.5;\n}\n\n/* Set shorter line heights on headings and interactive elements */\nh1, h2, h3, h4,\nbutton, input, label {\n  line-height: 1.1;\n}\n\n/* Balance text wrapping on headings */\n/* h1, h2,\nh3, h4 {\n  text-wrap: balance;\n}  */\n\n/* A elements that don't have a class get default styles */\na:not([class]) {\n  text-decoration-skip-ink: auto;\n  color: currentColor;\n}\n\n/* Make images easier to work with */\nimg,\npicture {\n  max-width: 100%;\n  display: block;\n}\n\n/* Inherit fonts for inputs and buttons */\ninput, button,\ntextarea, select {\n  font-family: inherit;\n  font-size: inherit;\n}\n\n/* Make sure textareas without a rows attribute are not tiny */\ntextarea:not([rows]) {\n  min-height: 10em;\n}\n\n/* Anything that has been anchored to should have extra scroll margin */\n:target {\n  scroll-margin-block: 5ex;\n}\n\n\nThe ?. is the optional chaining operator. It safely handles cases where children might be undefined or null.\n\nWhat it does\n\n{@render children?.()}\n\nThis means:\nIf children exists: Render it\n**If children is undefined or null: Do nothing (no error thrown)\n\nWithout optional chaining\n\n{@render children()}\n\nThis would throw an error if children is undefined.\n\nWhy it's needed\n\nIn SvelteKit layouts, the children snippet represents the content of child routes. There might be edge cases during:\nInitial rendering\nRoute transitions\nError states\n\nWhere children could temporarily be undefined.\n\nEquivalent to\n\n// In regular JavaScript\nchildren ? children() : undefined\n\nThe ?.() is specifically for optional function/snippet calls. There's also:\nobject?.property - optional property access\narray?.[index] - optional array indexing\n\nThis is a defensive programming practice to prevent runtime errors when the snippet might not be available.",
        "locale": "en",
        "slug": "tech/vanilla-css-for-sveltekit",
        "folder": ""
    },
    {
        "title": "Svelte forms with remote functions are awesome!",
        "description": "This article explains how to use Svelte 5 forms with remote functions.",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-30T00:00:00.000Z",
        "date_published": "2026-01-03T00:00:00.000Z",
        "pinned": false,
        "tags": [
            "svelte",
            "forms"
        ],
        "content": "Creating a form with Svelte 5 for use with a remote function\n\nCreate a form schema  \n\nYou have an interesting option we'll discuss later if you locate the form schema in a separate module (as opposed to defining it in the form's remote function). I put mine under the src/lib/types folder but you can put your schemas anywhere. \n\nsrc/lib/types/form1.schema.ts\n\nForm schema are defined with any standard schema compliant validation library (ie, Zod, Valibot, or Arktype). I'm using Zod in this example. Zod has a rich set of schema definition options. \n\nimport { z } from 'zod';\n\nexport const form1Schema = z.object({\n\ttitle: z.string().nonempty({ message: \"Hey, foo! Can't be empty\" }),\n\tcontent: z.string().nonempty()\n});\n\nUsing a schema is part of the magic of Svelte 5 forms. It provides:\nStrongly-typed, easy access to form fields\nThe ability to populate a form with the correct tag and attributes for every field.\nVery effective form validation for the server- and client-side\nFreedom from third-party forms libraries like SuperForms. \nCreate a remote function\n\nTo reduce learning friction, this example Svelte 5 form remote function doesn't do very much, but it does enough to understand how things work. There aren't any rules as to where a  remote function must reside, but it anywhere you want. While it may be prudent to co-locate with them their corresponding +page.svelte, I like the idea of knowing where all my remote functions are located and put them in src\\lib\\remote-funcs.  \n\nsrc\\lib\\remote-funcs\\form.remote.ts\n\nimport { z } from 'zod';\nimport { form } from '$app/server';\nimport { error, redirect, invalid } from '@sveltejs/kit';\nimport { form1Schema } from '$lib/types/form1schema';\n\n// You cannot export a schema from a remote function. Put them in a shared module.\n\nexport const createPost = form(form1Schema, async (data, issue) => {\n\t// Descructure the form fields from the data object.\n\tconst { title, content } = data;\n\n\t// See a field value in the console.\n\tconsole.log(title);\n\n\tif (title == 'ROGER') {\n\t\t// use the issue object to throw custom error messages.\n\t\tinvalid(issue.title('Title cannot be ROGER'), issue.content('Title cannot be PENCE'));\n\t}\n\n\tredirect(303, /results);\n});\n\nCreate a form \n\nInitially, the only unusual thing with the form declaration is its ` tag. The {...createPost} populates the type and action` attributes for you. In this case, the form fields were created manually. We'll see in a moment how the form schema helps with this.\n\n\n\timport { createPost } from '$lib/remote-funcs/form.remote';\n\n\n\n\t\n\t\tTitle\n\t\t\t\n\t\n\n\t\n\t\tWrite your post\n\t\t\t\t\n\t\n\n\tSubmit\n\n\nFor this example, Svelte5 rendered this  `` tag\n\n\n\nas \n\n\n\nIf you dig into the code Svelte 5 generates, you'll find that a remote function gets an alias assigned to it. For example, the remote function file provided above is assigned the lig978e alias. You won't ever know, or care, what this alias value is. \n\ninit_remote_functions(m, \"src/lib/remote-funcs/form.remote.ts\", \"1ig978e\");\n\nWith this alias, and the function name (createPost, in this case) your form has the info it needs to issue the correct HTTP POST. \n\nAn alternative manually creating the form fields yourself, is to use the schema and the spread operator to do the work for you:\n\n\n\timport { createPost } from '$lib/remote-funcs/form.remote';\n\n\n\n\t\n\t\tTitle\n\t\t\n\t\n\n\t\n\t\tWrite your post\n\t\t\n\t\n\n\tSubmit\n\n\nUsing the schema fields like this is mostly an easy way to populate the name attribute. This code:\n\n\n\nRenders as:\n\n\n\nI think the point of using the more verbose spread is to ensure the form field names correspond to the schema field names. If you use the schema spread, you can also add other attributes to the tag (eg, id or class). \n\npreflight\n\nerror messages",
        "locale": "en",
        "slug": "tech/svelte-forms-with-remote-functions",
        "folder": ""
    },
    {
        "title": "podman named volumes",
        "description": "podman named volumes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": "2026-01-03T00:00:00.000Z",
        "pinned": false,
        "tags": [
            "containers"
        ],
        "content": "That's a great question, and it highlights a key difference between named volumes and bind mounts.\n\nThe data for a Podman named volume (like postgres_data you created) is not stored directly in an easily accessible folder like C:\\Users\\thumb\\... on your Windows host filesystem.\n\nInstead, it's stored:\n\nInside the Podman Machine's Filesystem: Podman on Windows typically runs using a lightweight Linux Virtual Machine (VM). This VM has its own virtual disk and filesystem.\nManaged by Podman: Within that VM's filesystem, Podman manages a specific directory structure for its storage, including volumes. The typical path inside the VM where volume data is stored is:\n    /var/lib/containers/storage/volumes//_data\n\nSo, for your postgres_data volume, the actual PostgreSQL files would reside at this path inside the Podman VM:\n\n/var/lib/containers/storage/volumes/postgres_data/_data\n\nHow to Access or Interact with the Volume Data (If Needed):\n\nVia podman machine ssh (Recommended for exploration): You can access the shell of the Podman VM and navigate to the path:\n\nStart an SSH session into the default podman machine\n    podman machine ssh\n\nOnce inside the VM's shell:\n    sudo ls -l /var/lib/containers/storage/volumes/postgres_data/_data\nOr:\n    cd /var/lib/containers/storage/volumes/postgres_data/_data\n    ls -la\n    exit\n\n    (You might need sudo depending on the VM's configuration). This lets you see the files as they exist within the Linux environment where Postgres is actually running.\n\nMounting into another container: You can temporarily mount the volume into another utility container (like alpine or ubuntu) to inspect or copy files using podman cp.\n\nRun an interactive alpine container with the volume mounted at /inspect\n    podman run -it --rm -v postgres_data:/inspect alpine sh\n\nInside the temporary container's shell:\n    ls -l /inspect\nYou could tar files here, etc.\n    exit\n\nBackups (Correct Way): The best practice for backing up PostgreSQL data is not to copy files directly from the volume location (whether it's a named volume or bind mount), especially while the database is running, as this can lead to inconsistent or corrupt backups. Use standard PostgreSQL tools like pg_dump instead:\nExample: Dump all databases to a file on your host\n    podman exec -t postgresdb pg_dumpall -U postgres > C:\\Users\\thumb\\Documents\\db_backup.sql\n(You might need to adjust the user '-U' if you configured a different one)\n\nWhere is the VM itself stored?\n\nThe VM's virtual disk file (which contains the volume data) is stored on your Windows host, but its exact location depends on the virtualization provider Podman is using (usually WSL or Hyper-V by default):\n\nWSL: Managed within the WSL distribution's storage (often complex paths under %LOCALAPPDATA%\\Packages).\nHyper-V: Typically in the path configured in Hyper-V Manager, often defaulting to somewhere like C:\\ProgramData\\Microsoft\\Windows\\Hyper-V or C:\\Users\\Public\\Documents\\Hyper-V\\Virtual Hard Disks.\n\nIn Summary:\n\nNamed volumes offer better compatibility and performance by keeping the data within the Linux environment Podman controls. The trade-off is that the data isn't directly browsable via Windows File Explorer. Use podman machine ssh for inspection and standard database tools (pg_dump) for backups.",
        "locale": "en",
        "slug": "tech/podman-named-volumes",
        "folder": ""
    },
    {
        "title": "Managing Postgres connection pooling",
        "description": "Managing Postgres connection pooling",
        "date_created": "2025-06-13T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": "2026-01-03T00:00:00.000Z",
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "This discusses this Postgres error\n\nencountered during dev work with Sveltekit.\n\nThis is a very common warning in Node.js applications, especially during development with tools like SvelteKit that have hot-reloading. Let's break it down.\n\nThe Short Answer (TL;DR)\n\nThis warning means your application is repeatedly setting up \"shutdown\" listeners for your database connection (or something similar) without ever cleaning up the old ones. This typically happens in a SvelteKit development environment because the server restarts on every file change (Hot Module Replacement), but the old database connection isn't being properly closed.\n\nYou are likely creating a new database connection pool on every server reload, and each one is adding a listener to know when the app shuts down. You're leaking these listeners.\n\nThe fix is not to increase the listener limit. The fix is to ensure you only have one instance of your database client/pool for the entire application lifetime and to properly close it when the application terminates.\n\nDetailed Breakdown of the Error Message\n\nLet's dissect the message itself: (node:5232) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 SIGTERM listeners added to [process]. MaxListeners is 10. Use emitter.setMaxListeners() to increase limit\n\nEventEmitter: This is a core building block in Node.js. Many objects, like network connections, file streams, and the process object itself, \"emit\" events. You can listen for these events using methods like .on() or .addListener().\n\nprocess: This is a global Node.js object that represents the current running application. It's an EventEmitter.\n\nSIGTERM: This is a \"signal\" that can be sent to a process to request its termination (a graceful shutdown). When your SvelteKit dev server restarts, or when a production environment manager (like Docker or PM2) stops your app, it often sends a SIGTERM signal. Your code can listen for this signal to perform cleanup tasks, like closing database connections.\n\nMaxListenersExceededWarning: By default, Node.js will warn you if you add more than 10 listeners for the same event on the same EventEmitter. This is a safety feature because it's a strong indicator of a \"memory leak.\" You're allocating resources (the listeners) but never releasing them.\n\nWhy This Happens in a SvelteKit + Postgres App\n\nThe most common cause is how you're initializing your Postgres client (like node-postgres i.e., pg, or an ORM like Drizzle ORM or Prisma).\n\nLet's look at a common incorrect pattern:javascript\n// src/routes/some-route/+server.js or a hook\nimport pg from 'pg';\n\n// ❌ ANTI-PATTERN: This code runs every time the server reloads in dev mode.\nconst pool = new pg.Pool({\n  connectionString: process.env.DATABASE_URL,\n});\n\n// Many database clients automatically add a SIGTERM listener\n// to gracefully close the pool on shutdown.\n// So, on every file save:\n// 1. Vite reloads the server module.\n// 2. A NEW pool is created.\n// 3. A NEW SIGTERM listener is added to the process object.\n// 4. The OLD pool and its listener are never cleaned up.\n// After 10 reloads, you get the warning.\n\nexport async function GET() {\n  const result = await pool.query('SELECT NOW()');\n  // ...\n}\nThe Correct Solution: The Singleton Pattern\n\nYou need to ensure that only one instance of your database connection pool is ever created. A common way to do this is to create it in a dedicated module and cache it.\n\nStep 1: Create a dedicated database client module.\n\nSvelteKit's server-only modules are perfect for this. Create a file like src/lib/server/db.js.javascript\n// src/lib/server/db.js\n\nimport pg from 'pg';\nimport { PG_DATABASE, PG_HOST, PG_PASSWORD, PG_PORT, PG_USER } from '$env/static/private';\n\n// This is the key: declare the pool variable outside the function.\nlet pool: pg.Pool | null = null;\n\n/**\n Returns the singleton instance of the Postgres connection pool.\n */\nexport function getDbPool() {\n  if (!pool) {\n    console.log('Creating new PostgreSQL connection pool...');\n    pool = new pg.Pool({\n\t\thost: PG_HOST,\n\t\tport: Number(PG_PORT) || 5432, // Ensure port is a number\n\t\tuser: PG_USER,\n\t\tpassword: PG_PASSWORD,\n\t\tdatabase: PG_DATABASE,\n\t\tssl: false\n\t});\n\n    // This is the graceful shutdown logic.\n    // It's added ONCE when the pool is created.\n    process.on('SIGTERM', () => {\n      console.log('SIGTERM signal received. Closing DB pool.');\n      pool.end();\n    });\n\n    // Optional: Also listen for SIGINT (Ctrl+C in terminal)\n    process.on('SIGINT', () => {\n      console.log('SIGINT signal received. Closing DB pool.');\n      pool.end();\n    });\n  }\n  return pool;\n}\nStep 2: Use the singleton in your routes and hooks.\n\nNow, instead of creating a new pool everywhere, you import and use your getter function.javascript\n// src/routes/some-route/+server.js\n\nimport { getDbPool } from '$lib/server/db.js';\n\nconst pool = getDbPool(); // Gets the existing pool, or creates it if it's the first time.\n\nexport async function GET() {\n  const client = await pool.connect();\n  try {\n    const result = await client.query('SELECT NOW()');\n    return new Response(JSON.stringify(result.rows[0]));\n  } finally {\n    // Always release the client back to the pool\n    client.release();\n  }\n}\nWhy This Works\n\nThe pool variable is created only once because Node.js caches modules. The first time getDbPool() is called, it creates the pool. Subsequent calls will see that pool already exists and will just return the cached instance.\nBecause the pool is only created once, the process.on('SIGTERM', ...) listener is also only attached once.\nThis completely solves the listener leak, and you get the added benefit of efficient connection pooling across your entire application.\n\nWhat About emitter.setMaxListeners()?\n\nThe error message suggests this as a \"solution.\" Almost always, this is a bad idea. It's like turning off a smoke alarm because you don't like the noise. It hides the symptom (the warning) but doesn't fix the underlying problem (the memory leak). Only use it if you are absolutely certain you have a valid reason to have more than 10 listeners for a single event. In this SvelteKit/DB scenario, you do not.\n\nFetching the pool object from locals\n\nThe code above says to call getDbPool() elsewhere in the app. I am putting the pool in the locals object and wanted to make sure that didn't defeat the purpose of the singleton object.\n\nAnswer\n\nThe short answer is: No, this does not defeat the purpose of the singleton pattern. In fact, what you are doing is the idiomatic, recommended way to handle shared resources like database connections in SvelteKit.\n\nLet's break down why this works perfectly and is considered a best practice.\n\nWhy Your Pattern is Correct\n\nYour code is leveraging two separate concepts that work together beautifully:\n\nModule Scope (for the Singleton): Node.js caches modules. When you do import { getDbPool } from '$lib/server/db.js'; and then const pool = getDbPool(); at the top level of your hooks.server.js file, that code runs once when your server process starts. The pool variable in your hooks file holds the single, shared instance of the database pool for the entire lifetime of the server. This correctly implements the singleton pattern.\n\nRequest Scope (for event.locals): The handle hook runs for every incoming request. The event object, including event.locals, is created fresh for each request and destroyed after the response is sent.\n\nWhen you do event.locals.pool = pool;, you are not creating a new pool. You are simply taking the single, shared pool object that already exists in your module's memory and attaching a reference to it to the current request's locals object.\n\nThink of it like this:\nYou have one central library in a city (the singleton pool instance).\nEvery person who visits the city (an incoming request) gets a library card (event.locals) that points them to the one and only library.\nYou aren't building a new library for every person. You're just giving them a cheap, easy way to access the existing one.\n\nThe Advantages of Your Approach (Dependency Injection)\n\nThis pattern is a form of Dependency Injection, and it's highly beneficial:\n\nDecoupling: Your route handlers (+server.js, +page.server.js) don't need to know how to create or get the database connection. They are \"dumb\" in that respect. They just know to expect it on event.locals. This makes your code cleaner and easier to reason about.\n\nTestability: This pattern makes testing your endpoints much easier. You can create a mock database pool in your test setup and just place it on the event.locals object, without your endpoint code ever knowing the difference.\n\nCentralized Logic: All the logic for initializing and managing the database connection is in one place (hooks.server.js and lib/server/db.js). If you need to add logging, monitoring, or other logic around the pool, you have a single place to do it.\n\nPro-Tip: Add Type-Safety\n\nTo make this pattern even better and get full autocompletion in your editor, you should declare the shape of event.locals in your src/app.d.ts file.\n\nsrc/app.d.tstypescript\n// See https://kit.svelte.dev/docs/types#app\n// for information about these interfaces\nimport type { Pool } from 'pg'; // Or the type from your specific DB driver/ORM\n\ndeclare global {\n\tnamespace App {\n\t\t// interface Error {}\n\t\tinterface Locals {\n\t\t\tpool: Pool; // Add your property here\n\t\t}\n\t\t// interface PageData {}\n\t\t// interface Platform {}\n\t}\n}\n\nexport {};\nNow, when you type event.locals. inside any of your server-side SvelteKit files, your editor will know that a pool property exists and what its type is.\n\nComplete Example Flow\n\nHere's how your excellent pattern looks from start to finish:\n\n1. The Singleton (src/lib/server/db.js)javascript\nimport pg from 'pg';\nimport { DATABASE_URL } from '$env/static/private';\n\nlet pool;\n\nexport function getDbPool() {\n  if (!pool) {\n    pool = new pg.Pool({ connectionString: DATABASE_URL });\n  }\n  return pool;\n}\n2. The Hook (src/hooks.server.js)javascript\nimport { getDbPool } from '$lib/server/db';\nimport type { Handle } from '@sveltejs/kit';\n\n// This runs ONCE when the server starts\nconst pool = getDbPool();\n\nexport const handle: Handle = async ({ event, resolve }) => {\n\t// This runs for EVERY request\n\t// It attaches a REFERENCE to the single pool instance\n\tevent.locals.pool = pool;\n\n\tconst response = await resolve(event);\n\treturn response;\n};\n3. The Endpoint (src/routes/api/users/+server.js)javascript\nimport { json } from '@sveltejs/kit';\n\nexport async function GET({ locals }) {\n\t// Get the pool from locals (no imports needed!)\n\tconst { pool } = locals;\n\n\tconst result = await pool.query('SELECT * FROM users');\n\n\treturn json(result.rows);\n}\n`\n\nConclusion: Keep doing what you're doing. It is the correct, efficient, and maintainable way to manage shared resources like a database pool within a SvelteKit application. You have successfully avoided the listener leak while implementing a clean, testable architecture.",
        "locale": "en",
        "slug": "tech/postgres-pooling-management",
        "folder": ""
    }
]