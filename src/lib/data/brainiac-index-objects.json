[
    {
        "title": "Add a marddown frontmatter property with VS Code",
        "description": "Add a marddown frontmatter property with VS Code",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "vs-code",
            "frontmatter"
        ],
        "content": "replace date_published:\ntags: with:\n\ndate-published: 2025-06-02 12:00\\ndate_published:\ntags:\n\nbe sure to enable reg ex search and replace.",
        "locale": "en",
        "slug": "technical-posts/add-a-marddown-frontmatter-property-with-vs-code",
        "folder": ""
    },
    {
        "title": "Affinity Publisher notes",
        "description": "Affinity Publisher notes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities",
            "writing"
        ],
        "content": "Use F11 to toggle document preview (which hides all guides and shape outlines)\nUsing footnotes and endnotes\nWhen exporting Publisher docs to PDF, set the Export Area property dropdown to All Pages to create a PDF with a single pages (the default is to double pages horizontally). Very sadly, you have to do this each time you export to PDF.",
        "locale": "en",
        "slug": "technical-posts/affinity-publisher-notes",
        "folder": ""
    },
    {
        "title": "asna-com-svelte-upgrade",
        "description": "asna-com-svelte-upgrade",
        "date_created": "2025-09-02T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "asna-com"
        ],
        "content": "src/components/contact-form/ContactForm.svelte\nsrc/routes/downloads/[[slug]]/+page.svelte:\n\n\n  // ...existing code...\n e.target.blur()}\n    onfocus=\"this.value=''\"\n    title=\"If country not in list please type it in\"\n    required=true\n    bind:value={form.country}\n/>\n// ...existing code.\n\n// ...existing code...\n e.target.blur()}\n    on:focus={(e) => e.target.value = ''}\n    title=\"If country not in list please type it in\"\n    required=true\n    bind:value={form.country}\n/>\n\nlook at line 512 of downloads/[[slug]].+page.svelte\n\non:click|preventDefault|stopPropagation={modalDialog.closeModal()}>Cancel</a",
        "locale": "en",
        "slug": "technical-posts/asna-com-svelte-upgrade",
        "folder": ""
    },
    {
        "title": "Affinity Designer Notes",
        "description": "Affinity Designer Notes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "design"
        ],
        "content": "If all the toolbars go away:\n\nView->Show Tools\nView->Show Toolbar\nView->Show Context Toolbar\nWindows->Studio (make sure Hide Studio is not checked)\n\nDuplicate and move an object\n\nUse Ctrl/Drag to copy and move, then use Ctrl/J to repeat that duplication\nWhen you use Ctrl/Drag a dialog pops up to let specify the amount to move.\n\nExporting to SVG\n\nBe sure to check \"Export text as curves.\" This makes for a larger image but it ensures the fonts are presented correctly.",
        "locale": "en",
        "slug": "technical-posts/affinity-designer-notes",
        "folder": ""
    },
    {
        "title": "A PowerShell version of touch",
        "description": "A PowerShell version of touch",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "Script: ps-touch.ps1\n\n\nCmdletBinding(SupportsShouldProcess=$true)] # Adds -WhatIf and -Confirm support\nparam(\n    [Parameter(Mandatory=$true, Position=0)]\n    [ValidateScript({\n        if (Test-Path -Path $_ -PathType Container) {\n            return $true\n        } else {\n            throw \"Folder not found: $_\"\n        }\n    })$FolderPath,\n\n    Parameter(Mandatory=$true, Position=1)$FilterDateTime,\n\n    Parameter(Mandatory=$true, Position=2)$NewDateTime,\n\n    Parameter(Mandatory=$true, Position=3)]\n    string]$FileExtension\n)\n\n\nWrite-Verbose \"Starting script...\"\nWrite-Verbose \"Searching in folder: '$FolderPath'\"\nWrite-Verbose \"Filtering files modified before: '$($FilterDateTime.ToString('yyyy-MM-dd HH:mm:ss'))'\"\nWrite-Verbose \"Setting new modification date to: '$($NewDateTime.ToString('yyyy-MM-dd HH:mm:ss'))'\"\n\n$normalizedExtension = if ($FileExtension.StartsWith('.')) { $FileExtension } else { \".$FileExtension\" }\n$filterPattern = \"*$normalizedExtension\"\n\nWrite-Verbose \"Searching for file extension: '$normalizedExtension' using filter pattern: '$filterPattern'\"\n\n\n$filesUpdatedCount = 0\n$filesCheckedCount = 0\n\ntry {\nGet all files matching the extension recursively\n    $targetFiles = Get-ChildItem -Path $FolderPath -Filter $filterPattern -Recurse -File -ErrorAction Stop\n\n    Write-Host \"Found $($targetFiles.Count) '$filterPattern' files. Checking modification dates...\"\n\nLoop through each file\n    foreach ($file in $targetFiles) {\n        $filesCheckedCount++\n        Write-Verbose \"Checking file: $($file.FullName) | LastWriteTime: $($file.LastWriteTime.ToString('yyyy-MM-dd HH:mm:ss'))\"\n\nCompare the file's LastWriteTime with the filter date\n        if ($file.LastWriteTime -lt $FilterDateTime) {\n            Write-Host (\"Updating timestamp for: {0} (Original: {1})\" -f $file.FullName, $file.LastWriteTime.ToString('yyyy-MM-dd HH:mm:ss'))\n\nUse -WhatIf / -Confirm support provided by [CmdletBinding(SupportsShouldProcess=$true)]\n            if ($PSCmdlet.ShouldProcess($file.FullName, \"Set LastWriteTime to $NewDateTime\")) {\n                try {\nSet the LastWriteTime property\nNote: Setting other timestamps like CreationTime or LastAccessTime can be done similarly if needed.\n                    $file.LastWriteTime = $NewDateTime\n                    $filesUpdatedCount++\n                } catch {\n                    Write-Warning \"Could not update timestamp for '$($file.FullName)': $($_.Exception.Message)\"\n                }\n            }\n        } else {\n            Write-Verbose \"Skipping file (not older than filter date): $($file.FullName)\"\n        }\n    }\n\n    Write-Host \"-------------------------------------\"\n    Write-Host \"Script finished.\"\n    Write-Host \"Total '$filterPattern' files checked: $filesCheckedCount\"\n    Write-Host \"Total files updated: $filesUpdatedCount\"\n    Write-Host \"-------------------------------------\"\n\n} catch [System.Management.Automation.ItemNotFoundException] {\n    Write-Error \"The specified folder path '$FolderPath' was not found or is inaccessible after initial check.\"\n} catch {\nCatch any other potential errors during Get-ChildItem or processing\n    Write-Error \"An unexpected error occurred: $($_.Exception.Message)\"\n    Write-Error $_.ScriptStackTrace\n}\n\nWrite-Verbose \"Exiting script.\"\n\nChanges Made:\n\nParameter Added:\n    A new parameter $FileExtension of type [string] was added to the param() block.\n    It's marked as Mandatory=$true and assigned Position=3.\n    [ValidateNotNullOrEmpty()] attribute was added to ensure the user provides a non-empty extension.\nNormalize Extension & Create Filter:\n    A small block of code was added to ensure the extension starts with a dot (.) and to create the filter pattern (e.g., *.txt). This makes the script more user-friendly as they can type txt or .txt.\n    $normalizedExtension = if ($FileExtension.StartsWith('.')) { $FileExtension } else { \".$FileExtension\" }\n    $filterPattern = \"*$normalizedExtension\"\nDynamic Get-ChildItem:\n    The -Filter argument in Get-ChildItem now uses the $filterPattern variable instead of the hardcoded \"*.md\".\n    $targetFiles = Get-ChildItem -Path $FolderPath -Filter $filterPattern -Recurse -File -ErrorAction Stop\nUpdated Variable Name:\n    Changed $mdFiles to the more generic $targetFiles.\nUpdated Output Messages:\n    Messages displayed using Write-Host and Write-Verbose were updated to reflect the dynamic file extension being processed (using $filterPattern).\n\nHow to Use (Examples):\n\nUpdate .log files:\n    .\\Update-FileTimestampByExtension.ps1 -FolderPath \"C:\\ServerLogs\" -FilterDateTime \"2023-09-01\" -NewDateTime \"2024-01-01 08:00:00\" -FileExtension \"log\"\nUpdate .txt files (using extension without dot):\n    .\\Update-FileTimestampByExtension.ps1 -FolderPath \"C:\\Notes\" -FilterDateTime \"2023-10-27 12:00\" -NewDateTime (Get-Date) -FileExtension txt\nDry run for .tmp files:\n    .\\Update-FileTimestampByExtension.ps1 -FolderPath \"C:\\Temp\" -FilterDateTime \"2023-01-01\" -NewDateTime \"2023-12-31\" -FileExtension \".tmp\" -WhatIf\n\nOriginal without FileExtension argument\n\nOkay, here is a PowerShell script that accomplishes this.\n\nScript: Update-MdFileTimestamp.ps1\n\n\n[CmdletBinding(SupportsShouldProcess=$true)] # Adds -WhatIf and -Confirm support\nparam(\n    [Parameter(Mandatory=$true, Position=0)]\n    [ValidateScript({\n        if (Test-Path -Path $_ -PathType Container) {\n            return $true\n        } else {\n            throw \"Folder not found: $_\"\n        }\n    })$FolderPath,\n\n    Parameter(Mandatory=$true, Position=1)$FilterDateTime,\n\n    Parameter(Mandatory=$true, Position=2)$NewDateTime\n)\n\n--- Script Body ---\n\nWrite-Verbose \"Starting script...\"\nWrite-Verbose \"Searching in folder: '$FolderPath'\"\nWrite-Verbose \"Filtering files modified before: '$($FilterDateTime.ToString('yyyy-MM-dd HH:mm:ss'))'\"\nWrite-Verbose \"Setting new modification date to: '$($NewDateTime.ToString('yyyy-MM-dd HH:mm:ss'))'\"\n\n$filesUpdatedCount = 0\n$filesCheckedCount = 0\n\ntry {\nGet all .md files recursively\n    $mdFiles = Get-ChildItem -Path $FolderPath -Filter \"*.md\" -Recurse -File -ErrorAction Stop\n\n    Write-Host \"Found $($mdFiles.Count) *.md files. Checking modification dates...\"\n\nLoop through each file\n    foreach ($file in $mdFiles) {\n        $filesCheckedCount++\n        Write-Verbose \"Checking file: $($file.FullName) | LastWriteTime: $($file.LastWriteTime.ToString('yyyy-MM-dd HH:mm:ss'))\"\n\nCompare the file's LastWriteTime with the filter date\n        if ($file.LastWriteTime -lt $FilterDateTime) {\n            Write-Host (\"Updating timestamp for: {0} (Original: {1})\" -f $file.FullName, $file.LastWriteTime.ToString('yyyy-MM-dd HH:mm:ss'))\n\nUse -WhatIf / -Confirm support provided by [CmdletBinding(SupportsShouldProcess=$true)]\n            if ($PSCmdlet.ShouldProcess($file.FullName, \"Set LastWriteTime to $NewDateTime\")) {\n                try {\n                    $file.LastWriteTime = $NewDateTime\n                    $filesUpdatedCount++\n                } catch {\n                    Write-Warning \"Could not update timestamp for '$($file.FullName)': $($_.Exception.Message)\"\n                }\n            }\n        } else {\n            Write-Verbose \"Skipping file (not older than filter date): $($file.FullName)\"\n        }\n    }\n\n    Write-Host \"-------------------------------------\"\n    Write-Host \"Script finished.\"\n    Write-Host \"Total *.md files checked: $filesCheckedCount\"\n    Write-Host \"Total files updated: $filesUpdatedCount\"\n    Write-Host \"-------------------------------------\"\n\n} catch [System.Management.Automation.ItemNotFoundException] {\n    Write-Error \"The specified folder path '$FolderPath' was not found or is inaccessible after initial check.\"\n} catch {\nCatch any other potential errors during Get-ChildItem or processing\n    Write-Error \"An unexpected error occurred: $($_.Exception.Message)\"\n    Write-Error $_.ScriptStackTrace\n}\n\nWrite-Verbose \"Exiting script.\"\n\nHow to Use:\n\nSave: Save the code above into a file named Update-MdFileTimestamp.ps1.\nOpen PowerShell: Open a PowerShell console.\nExecution Policy: If you haven't run scripts before, you might need to change the execution policy. You can do this for the current session by running:\n    Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process -Force\nRun the Script: Navigate to the directory where you saved the script (using cd) and run it, providing the required arguments.\n\n    Example: To find .md files in C:\\Users\\You\\Documents\\Notes, update those modified before July 1st, 2023 at midnight, and set their new modified date to January 1st, 2024 at 9 AM:\n\n        .\\Update-MdFileTimestamp.ps1 -FolderPath \"C:\\Users\\You\\Documents\\Notes\" -FilterDateTime \"2023-07-01 00:00:00\" -NewDateTime \"2024-01-01 09:00:00\"\n\n    Using Current Date: To set the new date to the current date and time:\n\n        .\\Update-MdFileTimestamp.ps1 -FolderPath \"C:\\Path\\To\\Your\\Files\" -FilterDateTime \"2023-10-01 12:00:00\" -NewDateTime (Get-Date)\n\n    Dry Run (WhatIf): To see what files would be changed without actually changing them:\n\n        .\\Update-MdFileTimestamp.ps1 -FolderPath \"C:\\Path\\To\\Your\\Files\" -FilterDateTime \"...\" -NewDateTime \"...\" -WhatIf\n\n    Confirmation (Confirm): To be prompted before each file change:\n\n        .\\Update-MdFileTimestamp.ps1 -FolderPath \"C:\\Path\\To\\Your\\Files\" -FilterDateTime \"...\" -NewDateTime \"...\" -Confirm\n\n    Verbose Output: To see more detailed messages about which files are being checked/skipped:\n        .\\Update-MdFileTimestamp.ps1 -FolderPath \"C:\\Path\\To\\Your\\Files\" -FilterDateTime \"...\" -NewDateTime \"...\" -Verbose\n\nExplanation:\n\n[CmdletBinding(SupportsShouldProcess=$true)]: Enables common parameters like -WhatIf, -Confirm, and -Verbose.\nparam(...): Defines the script's parameters.\n    [Parameter(Mandatory=$true, Position=X)]: Makes the parameter required and allows providing arguments positionally (e.g., .\\script.ps1 folder filterdate newdate).\n    [ValidateScript({...})]: Checks if the provided $FolderPath actually exists and is a directory (-PathType Container). If not, it throws an error before the script body runs.\n    [string], [datetime]: Specify the expected data types. PowerShell will try to convert the input strings to these types. If it fails (e.g., invalid date format), PowerShell will raise an error.\nWrite-Verbose: Outputs messages only when the -Verbose switch is used. Good for debugging or detailed logging.\n$filesUpdatedCount, $filesCheckedCount: Variables to keep track of actions.\ntry...catch: Basic error handling block.\nGet-ChildItem: The core command for finding files.\n    -Path $FolderPath: Specifies the starting directory.\n    -Filter \"*.md\": Efficiently filters for files ending in .md.\n    -Recurse: Tells it to look inside subdirectories.\n    -File: Ensures only file objects (not directories) are returned.\n    -ErrorAction Stop: If Get-ChildItem encounters an error (like permission denied on a subfolder), it stops the script within the try block, allowing the catch block to handle it.\nforeach ($file in $mdFiles): Loops through each file object found.\n$file.LastWriteTime: Accesses the \"Date modified\" property of the file.\n-lt: The \"less than\" comparison operator for dates.\n$PSCmdlet.ShouldProcess(...): This is used in conjunction with SupportsShouldProcess. It checks if the operation should proceed based on whether -WhatIf or -Confirm was used. If -WhatIf is present, it prints a message saying what would happen. If -Confirm is present, it prompts the user. If neither is present, it returns $true.\n$file.LastWriteTime = $NewDateTime: This is the line that actually changes the file's modification timestamp. It's placed inside the if ($PSCmdlet.ShouldProcess(...)) block.\nInner try...catch: Handles potential errors specifically during the timestamp update (e.g., file becomes read-only or inaccessible between the Get-ChildItem call and the update attempt).\nFinal Summary: Prints the counts of files checked and updated.",
        "locale": "en",
        "slug": "technical-posts/a-powershell-version-of-touch",
        "folder": ""
    },
    {
        "title": "AI prompts",
        "description": "AI prompts",
        "date_created": "2025-09-01T00:00:00.000Z",
        "date_updated": "2025-11-01T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "ai"
        ],
        "content": "Two barrel\n\nCreate the HTML and CSS for a web page pattern that:\nuses CSS Flex to creates a single row with a left and a right column, each of which takes up approximately 50% of the wrapper.\nthe container provides left and right padding and the flex gap governs the space between the two columns.\nmake the pattern responsive using flex basis--avoid media or container queries.\nuse CSS custom properties for the critical variables in the pattern (such as the default width of the two columns, color, background color, etc).\nuse CSS nesting.\ncreate the HTML and CSS separately.\nassume modern browser support.",
        "locale": "en",
        "slug": "technical-posts/ai-prompts",
        "folder": ""
    },
    {
        "title": "Add custom error messages to HTML forms using the intrinsic error messaging.",
        "description": "Add custom error messages to HTML forms using the intrinsic error messaging.",
        "date_created": "2025-06-01T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "html"
        ],
        "content": "See also: \n\n[[html-novalidate-attribute| What does HTML's novalidate attribute do]]\n\n {/* Add novalidate here */}\n    \n        Username:\n        \n        \n    \n\n    \n        Promo Code (optional, must be \"SAVE10\" if entered):\n        \n    \n\n    \n        Age (must be 18 or over):\n        \n    \n\n    Submit\n\n\ndocument.addEventListener('DOMContentLoaded', function () {\n    const form = document.getElementById('myForm');\n    const usernameInput = document.getElementById('username');\n    const promoCodeInput = document.getElementById('promo_code');\n    const ageInput = document.getElementById('age');\n\n    form.addEventListener('submit', function (event) {\n        // Always prevent default submission when novalidate is used,\n        // as we're handling validation display ourselves.\n        event.preventDefault();\n\n        let isFormCurrentlyValid = true; // Track overall validity\n\n        // --- Username Validation (example: cannot be \"admin\") ---\n        // 1. Clear any previous custom error for this field\n        usernameInput.setCustomValidity('');\n        // 2. Perform your custom validation\n        if (usernameInput.value.trim().toLowerCase() === 'admin') {\n            const customMessage = 'Username \"admin\" is not allowed.';\n            usernameInput.setCustomValidity(customMessage); // Set custom message\n            isFormCurrentlyValid = false;\n        }\n        // 3. Also check built-in constraints (like 'required') for this field.\n        //    setCustomValidity will override the 'required' message if set.\n        //    If username is empty AND value is 'admin' (impossible but for logic):\n        //    The 'admin' message takes precedence.\n        //    If username is empty AND value is not 'admin':\n        //    The 'required' message (or a custom one if set for empty) will show.\n        if (!usernameInput.checkValidity()) { // Checks HTML5 + custom\n            isFormCurrentlyValid = false;\n        }\n\n\n        // --- Promo Code Validation (example: must be \"SAVE10\" if not empty) ---\n        promoCodeInput.setCustomValidity('');\n        if (promoCodeInput.value.trim() !== '' && promoCodeInput.value.trim().toUpperCase() !== 'SAVE10') {\n            const customMessage = 'Invalid promo code. If entered, it must be \"SAVE10\".';\n            promoCodeInput.setCustomValidity(customMessage);\n            isFormCurrentlyValid = false;\n        }\n        if (!promoCodeInput.checkValidity()) {\n             isFormCurrentlyValid = false;\n        }\n\n        // --- Age Validation (example: must be 18 or over) ---\n        ageInput.setCustomValidity('');\n        const age = parseInt(ageInput.value, 10);\n        if (ageInput.value.trim() !== '' && !isNaN(age) && age  {\n        input.addEventListener('input', function () {\n            // Only clear if it was a custom error you set.\n            // HTML5 validation messages (like for 'required') will clear automatically\n            // when the condition is met (e.g., field is filled).\n            if (this.validity.customError) {\n                this.setCustomValidity('');\n            }\n            // You might also want to re-validate on input for immediate feedback\n            // or just let the next submit attempt catch it.\n        });\n    });\n});\n\nA Sveltekit translation\n\nThe vanilla JavaScript client-side validation above translated into a SvelteKit component.\n\nThe core principles remain the same:\nGet references to your form and input elements (using bind:this in Svelte).\nAttach a submit handler to the form.\nIn the submit handler, prevent default submission, run your validation logic using setCustomValidity(), and then call formElement.reportValidity().\nAttach input listeners to clear custom validity messages.\n\nHere's how you can do it in a Svelte component (e.g., src/routes/my-form/+page.svelte or a reusable component):\n\n\n  import { onMount } from 'svelte';\n\n  // References to the DOM elements\n  let formElement;\n  let usernameInput;\n  let promoCodeInput;\n  let ageInput;\n\n  // Optional: bind values if you need them for other reactive purposes in Svelte\n  // let usernameValue = '';\n  // let promoCodeValue = '';\n  // let ageValue = '';\n\n  function validateForm() {\n    let isFormCurrentlyValid = true;\n\n    // --- Username Validation ---\n    usernameInput.setCustomValidity(''); // Clear previous custom error\n    if (usernameInput.value.trim().toLowerCase() === 'admin') {\n      const customMessage = 'Username \"admin\" is not allowed.';\n      usernameInput.setCustomValidity(customMessage);\n      isFormCurrentlyValid = false;\n    }\n    if (!usernameInput.checkValidity()) { // Check HTML5 + custom\n      isFormCurrentlyValid = false;\n    }\n\n    // --- Promo Code Validation ---\n    promoCodeInput.setCustomValidity('');\n    if (promoCodeInput.value.trim() !== '' && promoCodeInput.value.trim().toUpperCase() !== 'SAVE10') {\n      const customMessage = 'Invalid promo code. If entered, it must be \"SAVE10\".';\n      promoCodeInput.setCustomValidity(customMessage);\n      isFormCurrentlyValid = false;\n    }\n    if (!promoCodeInput.checkValidity()) {\n      isFormCurrentlyValid = false;\n    }\n\n    // --- Age Validation ---\n    ageInput.setCustomValidity('');\n    const age = parseInt(ageInput.value, 10);\n    if (ageInput.value.trim() !== '' && !isNaN(age) && age  {\n    // --- Clear custom validity messages on input ---\n    const inputsForCustomClear = [usernameInput, promoCodeInput, ageInput];\n    inputsForCustomClear.forEach(input => {\n      if (input) { // Ensure the element is bound and exists\n        input.addEventListener('input', function() {\n          // this refers to the input element here\n          if (this.validity.customError) {\n            this.setCustomValidity('');\n          }\n        });\n      }\n    });\n  });\n\n\n\n  /* Style for invalid inputs (works with browser validation and setCustomValidity) */\n  input:invalid {\n    border-color: #d9534f; /* A bootstrap-like danger red */\n  }\n  label {\n    display: block;\n    margin-top: 10px;\n    margin-bottom: 3px;\n  }\n  button {\n    margin-top: 15px;\n  }\n\n\n\n\n  \n    Username:\n    \n    Username cannot be 'admin'. -->\n  \n\n  \n    Promo Code (optional, must be \"SAVE10\" if entered):\n    \n  \n\n  \n    Age (must be 18 or over):\n    \n  \n\n  Submit\n\n\nKey SvelteKit/Svelte changes:\n\nbind:this={elementVariable}: This is how Svelte gives you a direct reference to the DOM element, replacing document.getElementById().\non:submit={handleSubmit}: This is Svelte's event handling syntax.\nonMount: Similar to DOMContentLoaded, onMount runs after the component has been rendered to the DOM. It's a good place to add vanilla event listeners if needed, especially if you need the this context of the vanilla listener or are interacting with elements that Svelte might not directly control.\nnovalidate attribute on ``: Essential for taking full control of when validation messages appear.\nhandleSubmit function:\n    It calls validateForm() which contains your core validation logic.\n    If validateForm() returns false (invalid), it calls event.preventDefault() and then formElement.reportValidity() to show the browser's native error tooltip for the first invalid field (including your custom messages).\n    If validateForm() returns true (valid), the form will proceed with its default action (e.g., submitting to a SvelteKit form action if method=\"POST\" is set).\nSvelteKit Form Actions: The example ` assumes you might have a corresponding action in +page.server.js or +server.js`. Your client-side validation acts as a progressive enhancement before the data is sent to the server.\n\nTo use this with SvelteKit's use:enhance for a smoother UX (optional):\n\nIf you want to use SvelteKit's enhance action for progressive enhancement (e.g., submitting without a full page reload), you'd modify the handleSubmit or integrate the validation into the enhance callback.\n\n\n  import { onMount } from 'svelte';\n  import { enhance } from '$app/forms'; // Import enhance\n\n  // ... (rest of your script: element bindings, validateForm, onMount for input listeners) ...\n  // let formElement, usernameInput, promoCodeInput, ageInput;\n\n  // handleSubmit is no longer directly needed if using enhance for submission logic\n\n\n\n\n {\n    // This function is called BEFORE the form submission request is made\n    const isClientValid = validateForm();\n\n    if (!isClientValid) {\n      formElement.reportValidity(); // Show browser validation UI\n\n      // Cancel SvelteKit's enhanced submission\n      return ({ cancel }) => {\n        cancel();\n      };\n    }\n\n    // If client-side validation passes, proceed with SvelteKit's enhanced submission\n    return async ({ result, update }) => {\n      // This function is called AFTER the server action responds\n      console.log('Server responded:', result);\n      // update will re-run load functions and update the page if needed\n      // based on the server action's result (e.g., if it invalidates data)\n      await update();\n    };\n  }}\n  novalidate\n  \n  \n    Username:\n    \n  \n  \n    Promo Code (optional, must be \"SAVE10\" if entered):\n    \n  \n  \n    Age (must be 18 or over):\n    \n  \n  Submit\n\n\nThis use:enhance approach is more idiomatic for SvelteKit if you're also handling the form submission with server-side actions, as it integrates client-side validation neatly before the fetch request is made. Choose the approach that best fits your needs! The first example is a more direct translation of your vanilla JS into a Svelte component structure.\n\n\nAh, you are absolutely correct! My apologies. I was overly focused on the structure of returning a function for cancellation and for the post-submission handler, and I misremembered/misstated how cancel is provided for the immediate pre-submission cancellation scenario.\n\nYou are right, for the case where you want to cancel the submission before it even happens (within the initial enhance callback), SvelteKit does provide cancel directly as a property of the argument to that main callback.\n\nHere is the corrected understanding and code:\n\nCorrected use:enhance for Immediate Pre-Submission Cancellation:\n\n\n  import { enhance } from '$app/forms';\n  import { onMount } from 'svelte';\n\n  let formElement;\n  let usernameInput;\n\n  function validateForm() {\n    console.log('validateForm CALLED');\n    if (usernameInput && usernameInput.value.trim().toLowerCase() === 'admin') {\n      console.log('validateForm: Username is \"admin\", setting custom validity and returning false');\n      usernameInput.setCustomValidity('Username \"admin\" is not allowed.');\n      return false;\n    }\n    if (usernameInput) usernameInput.setCustomValidity('');\n    console.log('validateForm: All checks passed, returning true');\n    return true;\n  }\n\n  onMount(() => {\n    if (usernameInput) {\n        usernameInput.addEventListener('input', function() {\n            if (this.validity.customError) {\n                this.setCustomValidity('');\n            }\n        });\n    }\n  });\n\n\n { //  {\n      console.log('Enhance: Post-submission handler CALLED. Server responded:', result);\n      // result contains: { type: 'success', status: 200, data: ... } or { type: 'error', ... } or { type: 'redirect', ... }\n      // update is a function to tell SvelteKit to re-run load functions and update the page.\n      await update();\n    };\n  }}\n  novalidate\n  \n    Username:\n    \n  \n  Submit\n\n\n\nThe call to cancel() above is wrong! \n\n\n\n\n\n\n\n\nAh, you are absolutely correct! My apologies. I was overly focused on the structure of returning a function for cancellation and for the post-submission handler, and I misremembered/misstated how cancel is provided for the immediate pre-submission cancellation scenario.\n\nYou are right, for the case where you want to cancel the submission before it even happens (within the initial enhance callback), SvelteKit does provide cancel directly as a property of the argument to that main callback.\n\nHere is the corrected understanding and code:\n\nCorrected use:enhance for Immediate Pre-Submission Cancellation:\n\n\n  import { enhance } from '$app/forms';\n  import { onMount } from 'svelte';\n\n  let formElement;\n  let usernameInput;\n\n  function validateForm() {\n    console.log('validateForm CALLED');\n    if (usernameInput && usernameInput.value.trim().toLowerCase() === 'admin') {\n      console.log('validateForm: Username is \"admin\", setting custom validity and returning false');\n      usernameInput.setCustomValidity('Username \"admin\" is not allowed.');\n      return false;\n    }\n    if (usernameInput) usernameInput.setCustomValidity('');\n    console.log('validateForm: All checks passed, returning true');\n    return true;\n  }\n\n  onMount(() => {\n    if (usernameInput) {\n        usernameInput.addEventListener('input', function() {\n            if (this.validity.customError) {\n                this.setCustomValidity('');\n            }\n        });\n    }\n  });\n\n\n { //  {\n      console.log('Enhance: Post-submission handler CALLED. Server responded:', result);\n      // result contains: { type: 'success', status: 200, data: ... } or { type: 'error', ... } or { type: 'redirect', ... }\n      // update is a function to tell SvelteKit to re-run load functions and update the page.\n      await update();\n    };\n  }}\n  novalidate\n  \n    Username:\n    \n  \n  Submit\n\n\nWhy my previous explanations were slightly off (and why your correction is key):\n\nThe enhance function has a flexible API:\n\nFor Immediate Cancellation (Pre-Submission):\n    The primary callback you provide to use:enhance is given an argument object which includes cancel, formData, action, and submitter.\n    If you decide here (e.g., due to client-side validation) that the submission should not proceed at all, you call cancel() directly from this scope.\n    You then typically return; or don't return anything that SvelteKit interprets as a post-submission handler.\n\nFor Handling the Server Response (Post-Submission):\n    If the primary callback doesn't call cancel() and instead wants to proceed with the submission and then handle the server's response, it must return an async function.\n    This returned async function will then be called by SvelteKit after the server responds, and SvelteKit will pass it an argument object containing result, update, and formData.\n\nI was conflating the two return mechanisms. You are absolutely right that for the immediate \"stop the submission right now\" case, cancel is available in the primary callback's argument.\n\nThank you so much for the persistence and the clear correction! This is a subtle but very important distinction in how use:enhance works. I'll be sure to remember this more clearly.\n\nThis corrected version should now work as you intend, and the \"cancel is not a function\" error should be resolved if it was due to trying to call cancel in the wrong place based on my previous, slightly muddled, explanation.",
        "locale": "en",
        "slug": "technical-posts/add-custom-error-messages-to-html-form",
        "folder": ""
    },
    {
        "title": "Chrome Dev Tools",
        "description": "Chrome Dev Tools",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "Dev Tools settings\n\nEnable Show user agent shadow DOM in the Elements tab.",
        "locale": "en",
        "slug": "technical-posts/chrome-dev-tools",
        "folder": ""
    },
    {
        "title": "Centering an Outlook email",
        "description": "Centering an Outlook email",
        "date_created": "2025-07-06T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "email",
            "email-builder"
        ],
        "content": "The issue is that Outlook for Windows doesn't respect the CSS centering properties the same way other email clients do. Looking at your HTML, I can see you're using text-align: center and margin: 0 auto, but Outlook needs specific table-based centering.\n\nHere are the fixes for Outlook:\n\nFix 1: Add MSO-specific centering wrapper\n\nAdd this around your main email table:\n\nFix 2: Update your EmailWrapper component\n\nIn your EmailWrapper component, modify the structure:\n\n\n\n    /* ... existing styles ... */\n    .email-container {\n        margin: 0 auto !important;\n        text-align: center;\n        width: 100% !important;\n        max-width: 600px;\n    }\n\nFix 3: Add Outlook-specific CSS\n\nAdd this to your email head styles:\n\n\n    /* Outlook-specific centering */\n    .ExternalClass {\n        width: 100%;\n    }\n    .ExternalClass,\n    .ExternalClass p,\n    .ExternalClass span,\n    .ExternalClass font,\n    .ExternalClass td,\n    .ExternalClass div {\n        line-height: 100%;\n    }\n    \n    /* Force Outlook to center */\n    table.email-wrapper {\n        margin: 0 auto !important;\n    }\n    \n    /* Outlook DPI fix */\n    [class=\"x_email-container\"] {\n        margin: 0 auto !important;\n        text-align: center !important;\n    }\n\n\n\n\n    table {\n        margin: 0 auto !important;\n    }\n    .email-container {\n        margin: 0 auto !important;\n        text-align: center !important;\n    }\n\nFix 4: The most reliable approach - Ghost table wrapper\n\nReplace your current wrapper with this Outlook-compatible structure:\n\nQuick Fix for your current code:\n\nAdd this wrapper around your existing .layout div:\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\nThe key issue: Outlook ignores margin: 0 auto and CSS-based centering. It only respects table-based centering with align=\"center\" attributes and MSO conditional comments.\n\nTry Fix 4 (Ghost table wrapper) first - it's the most reliable solution for Outlook centering issues.\n\nSimilar code found with 1 license type",
        "locale": "en",
        "slug": "technical-posts/centering-an-outlook-email",
        "folder": ""
    },
    {
        "title": "asna-wings",
        "description": "asna-wings",
        "date_created": "2025-05-20T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna/wings"
        ],
        "content": "GitHub read personal access token\n\n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\n\n![image-29.png](https://nyc3.digitaloceanspaces.com/asna-assets/videos/quick-wings-demo.mp4) is an old, short Wings demo.\nBefore Wings code\n\nThe code io aspensvc at 10.1.3.221 using DataGate 17.0 on port 5170\n\nTo run the before-Wings demo:\n\naddlible rpmonger\ncall custinq\n\n![[image-25.png|478x339]]\n\nImporting display files\n\nThe top two Wings templates create pages as WebForms. The third, with the ASNA \"A\", create pages as Razor pages.\n\n![[image-28.png|393x207]]\n\nBe sure to have \"Use Wildcards\" disabled, otherwise custdspf_o is also imported and it doesn't have the IndAra keyword included.\n\n![[image-27.png|376x349]]\nSign-on info\n\nThe after-Wings code is in the rpmonger_w library.\n\nrpmonger_w is rpmonger with its custinq program compiled to reference the handler: \n\n![[image-26.png|567x92]]\n\n![[image-23.png|474x297]]",
        "locale": "en",
        "slug": "technical-posts/asna-wings",
        "folder": ""
    },
    {
        "title": "budget-work1",
        "description": "budget-work1",
        "date_created": "2025-05-23T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "personal",
            "budget"
        ],
        "content": "The C# program here imports the Jefferson Bank downloaded spreadsheet into SQL Server into the rp/rpbudget table. It creates the table if necessary and always clears it before performing a new import. \n\nC:\\Users\\thumb\\Documents\\projects\\cs\\import-jeff-bank\n\nThings to do:\n\nImport by month and year \nFigure out how to categorize the rows by the description column (perhaps by startswith)\n\nGet unique descriptions:\n\nselect distinct description from rpbudget where year(PostDate) = 2025",
        "locale": "en",
        "slug": "technical-posts/budget-work-excel-sql-server",
        "folder": ""
    },
    {
        "title": "AI prompting",
        "description": "AI prompting",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "ai"
        ],
        "content": "Responsive holy grail with css\n\ncreate a responsive holy grail css layout with css grid. For anything greater than 1080px, it should have three rows where the first row has one column that is the full width with a header tag; the second row has two columns, one 20% width with an aside tag the other 80% with an article tag; the third row is full width with a footer tag. for anything under 1080, it should have four full-width rows where the first row is the header tag, the second is the article tag, the third is aside tag, and the fourth is the footer tag.\n\nGiven the following hex color values:\n\nGiven the following hex color values:\nCreate CSS custom properties for each color where:\n\nthe customer property name is brand-secondary-xxx through brand-secondary-xxx where xxx is 100, 200, 300, etc\nthe color value is the hsl value of the corresponding hex value given\nput each assign on its own line and do not comment the code and don't leave blank spaces between each property definition",
        "locale": "en",
        "slug": "technical-posts/ai-prompting",
        "folder": ""
    },
    {
        "title": "How to convert markdown to plain text using either Mdsvex or vanilla typescript",
        "description": "How to convert markdown to plain text using either Mdsvex or vanilla typescript",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "markdown"
        ],
        "content": "// ...existing code...\n\nimport { compile } from 'mdsvex';\n\n// Option 1: Using mdsvex to compile, then strip HTML\nasync function markdownToPlainText(markdown: string): Promise {\n    const compiled = await compile(markdown);\n    if (!compiled || !compiled.code) return '';\n    \n    // Strip HTML tags\n    return compiled.code\n        .replace(/)/gi, '')\n        .replace(/)/gi, '')\n        .replace(/]+>/g, '')\n        .replace(/&nbsp;/g, ' ')\n        .replace(/&amp;/g, '&')\n        .replace(/&lt;/g, '')\n        .trim();\n}\n\n// Option 2: Simple approach without HTML conversion - just strip markdown syntax\nfunction stripMarkdown(markdown: string): string {\n    return markdown\n        .replace(/^#{1,6}\\s+/gm, '') // Remove headers\n        .replace(/\\\\(.+?)\\\\/g, '$1') // Remove bold\n        .replace(/\\(.+?)\\/g, '$1') // Remove italic\n        .replace(/\\(.+?)\\/g, '$1') // Remove italic\n        .replace(/\\[(.+?)\\]\\(.+?\\)/g, '$1') // Remove links, keep text\n        .replace(/(.+?)/g, '$1') // Remove inline code\n        .replace(/^\\s[-+]\\s+/gm, '') // Remove list markers\n        .replace(/^\\s*\\d+\\.\\s+/gm, '') // Remove numbered list markers\n        .trim();\n}\n\n// ...existing code...\n\nx.forEach(({ dirent, markdownObject }) => {\n    // Convert content to plain text\n    const plainText = await markdownToPlainText(markdownObject.content);\n    // OR\n    const plainText = stripMarkdown(markdownObject.content);\n    \n    contentObjects.set(counter++, {\n        title: markdownObject.frontMatter.title,\n        description: markdownObject.frontMatter.description,\n        date_updated: markdownObject.frontMatter.date_updated,\n        pinned: markdownObject.frontMatter.pinned,\n        folder: dirent.parentPath,\n        filename: dirent.name,\n        slug: getSlug(dirent),\n        tags: markdownObject.frontMatter.tags,\n        content: plainText // Use plain text version\n    });\n});",
        "locale": "en",
        "slug": "technical-posts/convert-markdown-to-plain-text",
        "folder": ""
    },
    {
        "title": "Animating the dialog element",
        "description": "Animating the dialog element",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "design",
            "css"
        ],
        "content": "I got this code from a comment on this Kevin Power video.\nHere it is in action\n\n.Dialog {\n  max-width: 90%;\n  width: 350px;\n  background: var(--color-fff);\n  color: var(--color-000);\n  padding: 20px;\n  border-radius: 10px;\n  border: none;\n  box-shadow: rgba(0, 0, 0, 0.16) 0px 10px 36px 0px,\n    rgba(0, 0, 0, 0.06) 0px 0px 0px 1px;\n  transition: opacity 0.4s cubic-bezier(0.4, 1.6, 0.4, 0.8),\n    scale 0.4s cubic-bezier(0.4, 1.6, 0.4, 0.8), overlay 0.4s allow-discrete,\n    display 0.4s allow-discrete;\n  opacity: 0;\n  scale: 0;\n  &::backdrop {\n    transition: display 0.4s allow-discrete, overlay 0.4s allow-discrete,\n      background-color 0.4s;\n  }\n  &open] {\n    opacity: 1;\n    scale: 1;\n  }\n\n  &[open]::backdrop {\n    background: rgba(0, 0, 0, 0.3);\n  }\n}\n@starting-style {\n  .Dialog[open] {\n    opacity: 0;\n    scale: 0;\n    &::backdrop {\n      background-color: hsl(0 0 0 / 0);\n    }\n  }\n}\n\nSee this MDN article [to learn about @starting-style. Note thatCan I Use says that there are some issues with this directive and Firefox.",
        "locale": "en",
        "slug": "technical-posts/animating-the-dialog-element",
        "folder": ""
    },
    {
        "title": "Client side logging to the DB with Sveltekit and Pino",
        "description": "Client side logging to the DB with Sveltekit and Pino",
        "date_created": "2025-06-07T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "To log from the client-side to your database using Pino, you'll need to create an API endpoint that receives log data and writes it to the database. Here's the recommended approach:\n\n1. Create a Server-Side API Endpoint\n\nimport { json } from '@sveltejs/kit';\nimport type { RequestHandler } from './$types';\nimport { logger } from '$lib/pino-logger';\n\nexport const POST: RequestHandler = async ({ request, locals }) => {\n  const { pool } = locals;\n  \n  if (!pool) {\n    return json({ success: false, message: 'Database connection unavailable' }, { status: 500 });\n  }\n  \n  try {\n    const { level, message, data } = await request.json();\n    \n    // Validate inputs\n    if (!level || !message || !['info', 'warn', 'error', 'debug'].includes(level)) {\n      return json({ success: false, message: 'Invalid log data' }, { status: 400 });\n    }\n    \n    // Log using your server-side logger\n    await logger.db(level, [CLIENT] ${message}, data, pool);\n    \n    return json({ success: true });\n  } catch (error) {\n    console.error('Error processing client log:', error);\n    return json({ success: false, message: 'Failed to process log' }, { status: 500 });\n  }\n};\n\n2. Create a Client-Side Logger\n\nexport const clientLogger = {\n  info: async (message: string, data?: any) => {\n    await logToServer('info', message, data);\n    console.info(message, data || '');\n  },\n  \n  warn: async (message: string, data?: any) => {\n    await logToServer('warn', message, data);\n    console.warn(message, data || '');\n  },\n  \n  error: async (message: string, data?: any) => {\n    await logToServer('error', message, data);\n    console.error(message, data || '');\n  },\n  \n  debug: async (message: string, data?: any) => {\n    if (import.meta.env.DEV) {\n      await logToServer('debug', message, data);\n      console.debug(message, data || '');\n    }\n  }\n};\n\nasync function logToServer(level: string, message: string, data?: any) {\n  try {\n    const response = await fetch('/api/log', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        level,\n        message,\n        data: {\n          ...data,\n          userAgent: navigator.userAgent,\n          url: window.location.href,\n          timestamp: new Date().toISOString()\n        }\n      })\n    });\n    \n    if (!response.ok) {\n      console.error('Failed to send log to server:', await response.text());\n    }\n  } catch (err) {\n    // Silently fail - we don't want logging to break the application\n    console.error('Error sending log to server:', err);\n  }\n}\n\n3. Usage in Your Svelte Components\n\n\n  import { clientLogger } from '$lib/client-logger';\n  \n  function handleClick() {\n    clientLogger.info('Button clicked', { buttonId: 'main-action' });\n    // Rest of your code\n  }\n  \n  function handleError(e) {\n    clientLogger.error('Form submission failed', { \n      formData: JSON.stringify($form),\n      error: e.message\n    });\n  }\n\n\nClick Me\n\n4. Best Practices\n\nBatch logs when possible to reduce API calls\nAdd throttling to prevent overwhelming your server\nInclude contextual data like URL, user agent, and timestamp\nHandle network failures gracefully \nConsider privacy implications of what you log from the client\nAdd authentication/authorization to your logging endpoint\n\nThis approach gives you a robust client-side logging system that persists logs to your database while maintaining the structure and benefits of your Pino logger.",
        "locale": "en",
        "slug": "technical-posts/client-side-logging-to-db-with-sveltekit",
        "folder": ""
    },
    {
        "title": "Bun notes vs code debugging",
        "description": "Bun notes vs code debugging",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "bun"
        ],
        "content": "bun pm cache rm\n\nRemove node_modules\nrm -r -force node_modules\n\nReinstall with admin rights\nbun install\n\nrm -r -force node_modules\nrm -force bun.lockb\nbun install\n\nlaunch.json for Bun\n\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n      {\n        \"name\": \"Debug with Bun\",\n        \"type\": \"bun\",\n        \"request\": \"launch\",\n        \"program\": \"${file}\"\n      }\n    ]\n  }",
        "locale": "en",
        "slug": "technical-posts/bun-notes",
        "folder": ""
    },
    {
        "title": "argyl-custom-clamp-properties",
        "description": "argyl-custom-clamp-properties",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "--size-fluid-1: clamp(0.5rem, 1vw, 1rem);\n--size-fluid-2: clamp(1rem, 2vw, 1.5rem);\n--size-fluid-3: clamp(1.5rem, 3vw, 2rem);\n--size-fluid-4: clamp(2rem, 4vw, 3rem);\n--size-fluid-5: clamp(4rem, 5vw, 5rem);\n--size-fluid-6: clamp(5rem, 7vw, 7.5rem);\n--size-fluid-7: clamp(7.5rem, 10vw, 10rem);\n--size-fluid-8: clamp(10rem, 20vw, 15rem);\n--size-fluid-9: clamp(15rem, 30vw, 20rem);\n--size-fluid-10: clamp(20rem, 40vw, 30rem);\n\nOkay, here is a table summarizing the approximate minimum and maximum font sizes for each CSS custom property, based on the clamp() function's arguments.\n\nThe clamp(MIN, PREFERRED, MAX) function sets a value that:\n\nWon't go below MIN.\nWon't go above MAX.\nWill try to use the PREFERRED value (which scales with the viewport width vi) when it falls between MIN and MAX.\n\nTherefore, the MIN and MAX values represent the effective smallest and largest font sizes for each step. Since these are already provided in rem, we can directly use them. I've also added the approximate pixel equivalents assuming a standard root font size of 16px (1rem = 16px).\n\nApproximate Font Size Ranges for CSS Steps\n\n| Property Name | Minimum Size (rem) | Minimum Size (px\\) | Maximum Size (rem) | Maximum Size (px\\) | Description                                   |\n| :------------ | :----------------- | :------------------ | :----------------- | :------------------ | :-------------------------------------------- |\n| --step--5   | 0.3686             | 5.9 px             | 0.5549             | 8.9 px             | Very small text (e.g., fine print)            |\n| --step--4   | 0.4608             | 7.4 px             | 0.6243             | 10.0 px            | Very small text                               |\n| --step--3   | 0.576              | 9.2 px             | 0.7023             | 11.2 px            | Small text (e.g., captions, footnotes)        |\n| --step--2   | 0.72               | 11.5 px            | 0.7901             | 12.6 px            | Small text                                    |\n| --step--1   | 0.8889             | ~14.2 px            | 0.9                | 14.4 px             | Slightly smaller than body text               |\n| --step-0    | 1              | 16.0 px         | 1.125          | 18.0 px         | Base body text size                       |\n| --step-1    | 1.125              | 18.0 px             | 1.4063             | ~22.5 px            | Slightly larger than body (e.g., subheadings) |\n| --step-2    | 1.2656             | 20.3 px            | 1.7578             | 28.1 px            | Small heading / Large subheading              |\n| --step-3    | 1.4238             | 22.8 px            | 2.1973             | 35.2 px            | Heading (e.g., H3)                            |\n| --step-4    | 1.6018             | 25.6 px            | 2.7466             | 43.9 px            | Heading (e.g., H2)                            |\n| --step-5    | 1.802              | 28.8 px            | 3.4332             | 54.9 px            | Large heading (e.g., H1)                      |\n| --step-6    | 2.0273             | 32.4 px            | 4.2915             | 68.7 px            | Very large heading / Display                  |\n| --step-7    | 2.2807             | 36.5 px            | 5.3644             | 85.8 px            | Display size text                             |\n| --step-8    | 2.5658             | 41.1 px            | 6.7055             | 107.3 px           | Large display size text                       |\n\n(\\*Approximate pixel values assume a root font size of 16px. Actual rendered size may vary based on user browser settings.)\n\nThis table quickly shows the lower and upper bounds (in both rem and approximate px) for the font size defined by each custom property. The text will scale smoothly between these minimum and maximum values based on the viewport width.",
        "locale": "en",
        "slug": "technical-posts/argyl-custom-clamp-properties",
        "folder": ""
    },
    {
        "title": "asna-network-info",
        "description": "asna-network-info",
        "date_created": "2025-05-20T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna"
        ],
        "content": "cypress\nshakey/roger\nrogerso/asna\nroger/asna\nfrancisca/ASNA225\n\naspenq74\nroger/roger123\n10.1.3.215\n5170\n\naspensvc\nroger/roger123\n10.1.3.221\n\n\nASNA host file entries\n10.1.3.2 asna.corp\n10.1.3.2 pecan\n10.1.3.2 asna\n10.1.3.2 pecan.asna.corp\n\n10.1.7.71      devnet.asna.com\n10.1.3.7       irownwood\n10.1.3.207 cypress\n10.1.3.80     banyan\n\n\nR7V@@Ca9xL\n\nAzure RDP \nmothra\\Jackb!$1731\n\n\nLogin to Spain VPN first\n\nThen RDP:\n10.2.1.156\nwith:\naccount: .\\asna\npassword: @sna123!\n\nAzure\n\nWindows 11 VM created 23-04-2024\n13.93.211.15\nasna-avr15-vs2017 VM\ncredentials: \nroger\nwC!5wdHoiLZxJL^%\n\nWindows 11 VM created 23-04-2024\n13.91.7.133\nasna-avr16-vs2019\ncredentials:\nroger\n478BWeR3C*Z@XFPu\n\nWindows 11 VM created 23-04-2024\n13.93.167.249\navr17-vs2022\ncredentials:\nroger\n8u7GQY#9NrPK!$x%xd63^@\n\nWindows 11 VM created 10-06-2024\n13.64.113.55\nasna-avrcom\ncredentials:\nroger\nhXvt62VTi^nnRo5M",
        "locale": "en",
        "slug": "technical-posts/asna-network-info",
        "folder": ""
    },
    {
        "title": "Accessing Podman from another WSL distribution",
        "description": "Accessing Podman from another WSL distribution",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "containers"
        ],
        "content": "On Windows, Podman Desktop creates a Windows Subsystem for Linux (WSL) virtual machine: the Podman Machine. It also configures the Windows Podman client to communicate with the Podman Machine. However, it does not configure your other WSL distributions.\n\nYou might have other WSL distributions running, and want to access from there to your Podman Desktop containers.\n\nThis tutorial focuses on the most common context to walk you through the steps to configure your WSL distribution:\n\nUbuntu distribution of Linux.\nDefault Podman Machine.\n\nIn foldable details, you can find alternative steps for least common contexts:\n\nCustom WSL distribution.\nCustom Podman Machine.\n\nConfiguring your WSL distribution\n\nStart a session in your WSL distribution:\n\n        > wsl --distribution your-distribution-name\n\nTo communicate with the remote Podman Machine, you need a Podman client.\n\n    To benefit from the latest features, such as podman kube subcommands, use a recent Podman version rather than the podman package from the distribution.\n\n    The Podman client is available with a full podman installation or with the podman-remote version 4.x and higher. On Ubuntu it is generally easier to install podman-remote.\n\n    With podman-remote you also enable the remote mode by default.\n\n    Check for the latest release which includes the podman-remote binary from the Podman releases page.\n\n    Download and unpack the binary:\n\n        $ wget https://github.com/containers/podman/releases/download/v4.9.1/podman-remote-static-linux_amd64.tar.gz$ sudo tar -C /usr/local -xzf podman-remote-static-linux_amd64.tar.gz\n\n    Make this executable as podman with the following addition to .bashrc:\n\n        $ export PATH=\"$PATH:/usr/local/bin\"$ alias podman='podman-remote-static-linux_amd64'\n\nConfigure the Podman client in your WSL distribution to communicate with the remote Podman machine defined by Podman Desktop.\n\n    This will ensure consistency when you are working with Podman from all your different environments\n\n    Set the default Podman system connection to your Podman Machine (assuming Podman Desktop is configured with the default of Podman Machine enabled with root privileges):\n\n        $ podman system connection add --default podman-machine-default-root unix:///mnt/wsl/podman-sockets/podman-machine-default/podman-root.sock\n\n    On a custom Podman Machine, the remote Podman Machine destination might be different.\n\n    Two parameters can change:\n\n    The machine name might differ from podman-machine-default.\n    The socket name is different when the Podman machine has root privileges disabled (rootless mode).\n\n    Find your Podman Machine name and connection path:\n\nThe communication channel between your WSL distribution and the Podman Machine is a special file (a socket). The Podman Machine creates this file with specific permissions. To communicate with the Podman Machine from your WSL distribution your user must have write permissions for the socket.\n\n    To give access to the remote Podman machine to your user: create the group if necessary, assign group membership, and exit your session on the WSL distribution to apply the new group membership:\n\n        $ sudo usermod --append --groups 10 $(whoami)$ exit\n\nTesting the connection\n\nVerify that, on your WSL distribution, the Podman CLI communicates with your Podman machine.\n\nStart a session in your WSL distribution:\nVerify that your user is member of the group delivering access to the remote Podman Machine socket:\n\n    On the default Ubuntu WSL, the list contains the uucp group.\n\n    On a custom WSL distribution, the group name might be different.\n\n    Find the required group name:\n\nVerify that Podman default system connections is set to your remote Podman machine:\n\n        $ podman system connection list\n\nVerify that Podman has a Server version corresponding to your Podman Machine version:\n\n    Sample output:\n\n        Client:Version:      3.4.4API Version:  3.4.4Go Version:   go1.18.1Built:        Thu Jan 1 01:00:00 1970OS/Arch:      linux/amd64Server:Version:      4.8.3API Version:  4.8.3Go Version:   go1.21.5Built:        Wed Jan 3 15:11:40 2024OS/Arch:      linux/amd64\n\n    info\n\n    On your environment, the Podman version might be different.\n\nVerify that you can list running containers.\n\n    On your WSL distribution, start a container such as quay.io/podman/hello, and list the name of the last running container:\n\n        $ podman run quay.io/podman/hello$ podman ps -a --no-trunc --last 1\n\n    On Podman Desktop > Containers, the output lists the same container (same name, same image).\n\nChanging the connection\n\nPodman Desktop only has visibility to either rootless or rootful containers but not both at the same time.\n\nTo change the active connection:\n\nIn your Windows terminal, change the connection:\n\n    To set the connection to rootless:\n\n            $ podman machine set --rootful=false\n\n    To set the connection to rootful:\n\n            $ podman machine set --rootful=true\n\nIn your WSL session, Change the Podman system connection configuration:\n\n    To set the connection to rootless:\n\n            $ podman system connection add --default podman-machine-default-user unix:///mnt/wsl/podman-sockets/podman-machine-default/podman-user.sock\n\n    To set the connection to rootful:\n\n            $ podman system connection add --default podman-machine-default-root unix:///mnt/wsl/podman-sockets/podman-machine-default/podman-root.sock\n\nNext steps\n\nFrom your WSL distribution, work with containers.",
        "locale": "en",
        "slug": "technical-posts/accessing-podman-from-another-wsl-distribution",
        "folder": ""
    },
    {
        "title": "bases notes",
        "description": "bases notes",
        "date_created": "2025-10-01T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "bases"
        ],
        "content": "With Bases\n\nFor Bases, the equivalent depends on the syntax youre using:\n\nIn SQL-like queries you can filter with FROM this to reference the current page.\n    \nIn JS mode, Bases exposes a this page context. Example:\n    \n\nSELECT property FROM this\n\nor in JS:\n\nlet page = this;  return page.properties.MyProperty;",
        "locale": "en",
        "slug": "technical-posts/bases notes",
        "folder": ""
    },
    {
        "title": "ASP.NET WebForms deployment compilation options",
        "description": "ASP.NET WebForms deployment compilation options",
        "date_created": "2025-07-08T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "webforms",
            "asp-net"
        ],
        "content": "Question: my asp.net webforms asp.net .net framework app works when deployed with \"precompile during publishing\" disabled. When I enable that feature, the site does not work\n\nThis is a classic and often frustrating issue. When an ASP.NET Web Forms application works perfectly fine with a standard \"copy all files\" deployment but fails when \"Precompile during publishing\" is enabled, it's a strong indicator that you have a latent bug or a structural problem in your project that only the strictness of the precompiler can expose.\n\nLet's break down what precompilation does and why it causes these failures.\n\nThe Fundamental Difference: What Precompilation Does\n\nWithout Precompilation: You deploy your .aspx, .ascx, .cs, and .vb files to the server. The first time a page is requested, the ASP.NET runtime compiles it just-in-time (JIT) into a temporary assembly in the C:\\Windows\\Microsoft.NET\\Framework...\\Temporary ASP.NET Files folder. This is flexible and forgiving.\n\nWith Precompilation: Before deployment, a tool called aspnet_compiler.exe runs. It compiles everythingall your pages (.aspx), user controls (.ascx), and code-behind filesinto a set of DLLs that are placed in the bin folder. The original source code and markup files (except for static content) are not deployed.\n\nThe precompiler is much stricter than the runtime JIT compiler. It has to resolve every dependency and type reference across the entire application at once, which reveals problems that might otherwise go unnoticed.\n\nTop 5 Reasons for Precompilation Failures\n\nHere are the most common causes, from most likely to least likely.\n\n1. Mismatched Inherits and CodeBehind/CodeFile Attributes\n\nThis is the #1 cause for Web Forms. Every .aspx page has a directive at the top.\n\n\n\nInherits=\"MyProject.MyPage\": This tells the compiled framework which class to instantiate for this page. The value must be the fully qualified class name (Namespace + Class Name).\nCodeFile=\"MyPage.aspx.cs\": This is used by Visual Studio and the JIT compiler to find the source code file for dynamic compilation. The precompiler relies more heavily on the Inherits attribute.\n\nHow it fails:\nYou renamed a class in the .cs file but forgot to update the Inherits attribute in the .aspx file.\nYou changed a namespace but didn't update the Inherits attribute.\nYou copied a page and its code-behind but only updated the class name in one place.\n\nWithout precompilation, Visual Studio can often resolve this ambiguity. The precompiler cannot; it sees a mismatch and fails.\n\n2. Dynamic Loading of Controls or Assemblies (LoadControl)\n\nIf your code dynamically loads user controls (.ascx files) using Page.LoadControl(\"path/to/mycontrol.ascx\"), this will fail with precompilation.\n\nWhy? The file path/to/mycontrol.ascx does not exist on the server after precompilation. It has been compiled into a class inside one of the bin folder DLLs.\n\nSolution: You must load the control by its type instead of its path.\n\n// ---- BAD: Fails with precompilation ----\nControl myControl = Page.LoadControl(\"~/Controls/MyControl.ascx\");\nthis.PlaceHolder1.Controls.Add(myControl);\n\n// ---- GOOD: Works with precompilation ----\n// You might need to add a @Register directive to your page or web.config\n// to make the type known, or just reference the compiled assembly.\nControl myControl = Page.LoadControl(typeof(MyProject.Controls.MyControl), null);\nthis.PlaceHolder1.Controls.Add(myControl);\n\nThe same logic applies to any code that uses reflection to load assemblies or types by a hardcoded file name or path.\n\n3. Precompilation Merging Options and Naming Conflicts\n\nIn the publish settings, you have advanced options for precompilation, especially \"Merge all outputs into a single assembly.\"\n\nWithout Merging: The precompiler creates many small assemblies with generated names like App_Web_mypage.aspx.cdcab7d2.dll.\nWith Merging: The precompiler combines all these small DLLs into one (or a few) larger, explicitly named DLLs.\n\nHow it fails:\nGetType() Ambiguity: If your code uses Type.GetType(\"MyProject.MyClass\"), it might fail because it can't determine which of the many App_Web_*.dll assemblies contains that type.\nMerging Conflicts: If you have two classes with the same name in different namespaces that end up being merged, it can cause compilation errors.\nPublic vs. Internal: A class that was internal might have been accessible to another class within the same project. After merging, if they end up in different generated assemblies, this can break. Changing the class to public often fixes this.\n\n4. Circular Dependencies\n\nYour project might have circular dependencies that the standard build process tolerates but the precompiler does not.\nFor example:\nUser Control A (ControlA.ascx) has a reference to User Control B.\nUser Control B (ControlB.ascx) has a reference back to User Control A.\n\nThe aspnet_compiler.exe tool may fail to resolve this circular reference and throw an error.\n\n5. Environment-Specific web.config Issues\n\nSometimes a setting in your Web.Release.config transform is incorrect. When you run locally, you use Web.Debug.config, but the publish process applies the release transform.\n\nA common issue is the ` element. A bad transform could remove or incorrectly modify this section, which is critical for the application to run. Make sure that after the transform, your production web.config has debug=\"false\"` but is otherwise complete.\n\nHow to Troubleshoot This\n\nGet the Real Error Message. \"The site does not work\" isn't enough. You need the actual exception.\n    In your production web.config, temporarily set ``.\n    Browse to the site. You should now see the detailed \"Yellow Screen of Death\" with a stack trace. This will often point you directly to the problematic page, control, or line of code.\n    Check the Windows Event Viewer on the server under Windows Logs > Application. ASP.NET logs serious startup failures there.\n\nRun the Precompiler Locally. You can simulate the exact publish process on your developer machine.\n    Open the Developer Command Prompt for Visual Studio (it has all the environment paths set up).\n    Navigate to your project's root directory.\n    Run the aspnet_compiler.exe command.\n\n    // Basic precompilation command\n    // -p: path to the source application\n    // -v: virtual path (usually just \"/\")\n    // target_directory: where to put the compiled output\n    aspnet_compiler -p . -v / c:\\temp\\PrecompiledSite\n    This will run the compiler with its default settings. If it fails, it will give you a specific compilation error message that you won't see in Visual Studio's \"Build Output\" window.\n\nExperiment with Precompilation Settings. In the Visual Studio Publish Profile settings (Settings > File Publish Options):\n    Uncheck \"Allow this precompiled site to be updatable.\" This performs the strictest compilation and is most likely to reveal errors. The .aspx and .ascx files will be replaced with tiny \"marker\" files. This mode is most likely to fail if you are using LoadControl with a path.\n    Try toggling the \"Merge all outputs into a single assembly\" option. If it works with merging disabled but fails with it enabled, you likely have a GetType() or type-resolution problem.",
        "locale": "en",
        "slug": "technical-posts/asp-net-deployment-compilation-options",
        "folder": ""
    },
    {
        "title": "ParsedArgs type for TypeScript cmd-args-lib",
        "description": "ParsedArgs type for TypeScript cmd-args-lib",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-11T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "This type creates an object where each property corresponds to a command line argument, with the correct value type. Let me break it down:\n\nLine by line:\n\ntype ParsedArgs]> = {\n    [K in T[number]: Extract'value'];\n};\n\n1. T extends readonly ArgDefinition[]\nT is a readonly array of argument definitions\nExample: Your cmdLineArgs array\n\n2. T[number\nGets all the name properties from all items in the array\nFor your example: 'outputFile' | 'inputFile' | 'lines'\n\n3. K in T[number]\nMapped type - creates a property for each name\nIterates over: 'outputFile', 'inputFile', 'lines'\n\n4. Extract\nFinds the specific ArgDefinition where name matches K\nExample: When K is 'outputFile', it extracts:\n  { name: 'outputFile', type: 'string', default: 'output.text', required: true, value: null }\n\n5. 'value']\nGets the type of the value property from that definition\nFor 'outputFile': string | null\nFor 'lines': number | null\n\nReal example:\n\nGiven your cmdLineArgs:\n\nconst cmdLineArgs = [\n    { name: 'outputFile', type: 'string', default: 'output.text', required: true, value: null },\n    { name: 'inputFile', type: 'string', required: true, value: null },\n    { name: 'lines', type: 'number', default: 275, required: false, value: null }\n] as const;\n\nParsedArgs becomes:\n\n{\n    outputFile: string | null;\n    inputFile: string | null;\n    lines: number | null;\n}\n\nWhy this is powerful:\n\nconst args = parseCommandLineArgs(cmdLineArgs);\n\nargs.outputFile  //  TypeScript knows: string | null\nargs.lines       //  TypeScript knows: number | null\nargs.unknown     //  Error: Property 'unknown' does not exist\n\nif (typeof args.lines === 'number') {\n    const doubled = args.lines * 2;  //  Type narrowed to number\n}\n\nIt automatically creates the correct return type with autocomplete and type checking based on your argument definitions! \n\nWhere does the 'number' variable come from\n\nThe number in T[number] is TypeScript's syntax for \"any element of the array\" - it's not the number type!\n\nWhat T[number] means:\n\nWhen you have an array type, [number] accesses any element of that array:\n\n// Given this array type:\ntype MyArray = [\n  { name: 'outputFile', type: 'string', value: string | null },\n  { name: 'lines', type: 'number', value: number | null }\n];\n\n// T[number] means \"get all possible elements\"\ntype AllElements = MyArray[number];\n// Result: \n// { name: 'outputFile', type: 'string', value: string | null } | \n// { name: 'lines', type: 'number', value: number | null }\n\nIn your case:\n\ntype ParsedArgs[]> = {\n    [K in T[number]: Extract'value'];\n    //    ^^^^^^^^^                   ^^^^^^^^^\n    //    All elements                All elements\n};\n\nStep by step:\n\nT[number] = All argument definitions in the array (union)\nT[number = All the name properties: 'outputFile' | 'inputFile' | 'lines'\nExtract = Find the specific definition where name matches K\n['value'] = Get the value type from that definition\n\nThink of it like:\n\n// Array indexing in JavaScript:\nconst myArray = ['a', 'b', 'c'];\nmyArray[0]  // 'a'\nmyArray[1]  // 'b'\nmyArray[2]  // 'c'\n\n// TypeScript type indexing:\ntype MyArray = ['a', 'b', 'c'];\nMyArray[0]     // type: 'a'\nMyArray[1]     // type: 'b'\nMyArray[number] // type: 'a' | 'b' | 'c' (union of all elements)\n\nSo [number] is just TypeScript's way of saying \"give me the type of any element in this array\"! ",
        "locale": "en",
        "slug": "technical-posts/args-type-for-typescript-library",
        "folder": ""
    },
    {
        "title": "chia-crackers-recipe",
        "description": "chia crackers recipe",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-09T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "recipe"
        ],
        "content": "RECIPE BELOW Golden, crispy & just 4 ingredients! These chia & seed crackers are gluten-free, packed with fibre, healthy fats, and perfect for dipping into hummus, pesto, or your favourite spreads.\n\nIngredients  150g chia seeds  400ml hot water   tsp sea salt (plus extra for sprinkling)  280g mixed raw seeds (pumpkin, sunflower, sesame, hemp)  4 tbsp olive oil or avocado oil\n\nMethodMix chia seeds, hot water, and salt until gloopy. Stir in seeds and oil, then spread thinly on an oiled tray. Bake until golden and crisp, allow to cool, then snap into crackers. Perfect with dips for a healthier, more satisfying snack! ",
        "locale": "en",
        "slug": "technical-posts/chia-crackers-recipe",
        "folder": ""
    },
    {
        "title": "Connect C# to Postgres with Npgsql",
        "description": "Connect C# to Postgres with Npgsql",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "Connect to Npgsql with C# without Dapper\n\nusing Npgsql;\n\nconst string PG_HOST = \"aws-0-us-west-1.pooler.supabase.com\";\nconst string PG_PORT = \"6543\";\nconst string PG_USER = \"postgres.nrtgiufboxlhbspclizl\";\nconst string PG_PASSWORD = \"DkR0nLwYmxG2FHD0\";\nconst string PG_DATABASE = \"postgres\";\n\n// Connection string to your PostgreSQL database\nvar connString = $\"Host={PG_HOST};Username={PG_USER};Password={PG_PASSWORD};Database={PG_DATABASE}\";\n\nawait using var conn = new NpgsqlConnection(connString);\ntry\n{\n    // Open the connection\n    await conn.OpenAsync();\n    Console.WriteLine(\"Connected to the database successfully.\");\n\n    // Example query\n    var query = \"SELECT id, name FROM family ORDER BY name\";\n\n    await using (var cmd = new NpgsqlCommand(query, conn))\n    {\n        var reader = await cmd.ExecuteReaderAsync();\n\n        while (await reader.ReadAsync())\n        {\n            // Retrieve the columns (id and name) from the result set\n            int id = reader.GetInt32(0);  // First column: id\n            string name = reader.GetString(1);  // Second column: name\n\n            Console.WriteLine($\"ID: {id}, Name: {name}\");\n        }\n    }\n}\ncatch (Exception ex)\n{\n    Console.WriteLine($\"An error occurred: {ex.Message}\");\n}\nfinally\n{\n    await conn.CloseAsync();\n}",
        "locale": "en",
        "slug": "technical-posts/connect-ccharp-to-postgres-with-npgsql",
        "folder": ""
    },
    {
        "title": "How to add PostCSS to a Sveltekit project",
        "description": "How to add PostCSS to a Sveltekit project",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "asna-com",
            "css"
        ],
        "content": "See also:\n\n [use-postcss-and-open-props-with-sveltekit|See this doc too--I am not sure which came first]]\n\nThese instructions use the assets/files provided in this [GitHub repo.\n\nStep 1. Create a css-dev folder into root of Sveltekit project. This is where all the CSS dev work lives.\n\nCreate these two files in css-dev:\n\n/* css-dev/style.css */\n\n@import './main.css';\n\n/* css-dev/main.css */\n\nbody {\n  font-family: Arial, sans-serif;\n  margin: 0;\n  padding: 0;\n  box-sizing: border-box;\n}\n\nThe main.css is where the primary CSS lives. Add other CSS files as needed. \n\nStep 2. Copy postcss.config.mjs (note: .mjs extension) into root of Sveltekit project.\n\nimport cssnano from 'cssnano';\nimport postcssImport from 'postcss-import';\nimport postcssCustomMedia from 'postcss-custom-media';\nimport openProps from 'open-props';\nimport postcssJitProps from 'postcss-jit-props';\nimport postcssGlobalData from '@csstools/postcss-global-data';\n// change\nimport { purgeCSSPlugin } from '@fullhuman/postcss-purgecss';\n\nconst DO_NOT_PRESERVE_UNRESOLVED_RULE = false;\n\nconsole.log('NODE_ENV', process.env.NODE_ENV);\n\n// change\n//module.exports = {\nexport default {\n    map: {inline: true},\n\tplugins: \n\t\tpostcssImport(),\n\t\tpostcssJitProps(openProps),\n\t\tpostcssGlobalData({\n\t\t \tfiles: ['./node_modules/open-props/src/props.media.css']\n\t\t}),\n\t\tpostcssCustomMedia({\n\t\t\tpreserve: DO_NOT_PRESERVE_UNRESOLVED_RULE\n\t\t}),\n\n\t\t...(process.env.NODE_ENV === 'production'\n\t\t\t? [purgeCSSPlugin(\n\t\t\t\t{\n\t\t\t\t\tcontent: ['./src/routes/*/.svelte'],\n\t\t\t\t\tsafelist: ['mt-48'],\n\t\t\t\t}\n\t\t\t)]\n\t\t\t: []),\n\n\t\t...(process.env.NODE_ENV === 'production' ? [cssnano()] : [])\n\t]\n};\n\nThis line causes PostCSS to include in the final CSS only the Open Props that are referenced in your CSS.\n\nStep 3. Add these two commands to scripts node in package.json\n\n\"postcss:build\": \"postcss ./css-dev/style.css ---dir ./src --env production\",\n\"postcss:watch\": \"postcss ./css-dev/style.css ---dir ./src --watch --verbose --env development\"\n\nNote that the style.css produced is put in the root of the ./src folder (alongside app.html). (Vite doesn't serve files from outside of the src folder.)\nStep 4. Add or edit  +layout.svelte  in the root of the routes folder to import the style.css file.\n\n\n  let { children } = $props();  \n\n  import '../style.css';\n\n\n{@render children()}\n\nImporting the the style.css file this way ensures that when changes are made in the CSS they are shown instantly when the app is running under the dev server.\n\nStep 5. Add these files with pnpm:\n\n\"cssnano\": \"^6.0.1\",\n\"postcss-import\": \"^15.1.0\",\n\"postcss-custom-media\": \"^10.0.0\",\n\"open-props\": \"^1.5.10\",\n\"postcss-jit-props\": \"^1.0.13\",\n\"@csstools/postcss-global-data\": \"^2.0.1\",\n\"@fullhuman/postcss-purgecss\": \"^5.0.0\",\n\"postcss\": \"^8.4.25\",\n\"postcss-cli\": \"^10.1.0\"\n\n[!info]\nYou don't need to install and configureCSSNanobecause Vite and PostCSS implicitly compile/compress the CSS with the Sveltekit build step:pnpm run build. More on this shortly.\n\nto the dev dependencies of package.json.\n\nwith this command line:\n\npnpm i -D cssnano postcss-import postcss-custom-media open-props postcss-jit-props @csstools/postcss-global-data @fullhuman/postcss-purgecss postcss postcss-cli\n\nThe above options are examples what you might want to include. See the purgecss [docs for more information.\n\nStep 6. Run pnpm i one time to install the components listed in Step 5.\n\nStep 7. Run pnpm run postcss:watch to build the CSS during development\n\nStep 8. Run pnpm run postcss:build to build the CSS for production.\n\nSome notes on [https://open-props.style/|CSS Open Props]]\n\nIn postcss.config.mjs, this import pulls in the postcss-jit-props module.\n\nimport postcssJitProps from 'postcss-jit-props';\n\nIn the plugins section of postcss.config.mjs, the line below causes PostCSS to include in the final CSS only the Open Props that are referenced in your CSS.\n\nplugins:[\n    ...\n    postcssJitProps(openProps),\n    ...\n]\n\n\n@import \"./css-dev/_configure.css\";\n@import \"./css-dev/_resets.css\";\n@import \"./css-dev/_page.css\";\n@import \"./css-dev/_forms.css\";\n@import \"./css-dev/_utility.css\";\n\n:root {\n\t// Application-specific variables.\n    --accent-color: var(--orange-7);\n    ...\n}\n\nAfter doingnpm run buildyou can see the compiled CSS here: (the CSS file name will vary)\n\n.svelte-kit\\output\\client\\_app\\immutable\\assets\\0.2b389f06.css\nUsing Vite with PostCSS and Open Props is so cool\n\nBy using[postcss-jit-props, you can now use any of the CSS Open Props in your CSS and Vite uses PostCSS to ensure that the variables are selectively imported for you (that is, the only CSS Open Props imported are the ones you need).\n\nFor example, a+page.sveltefile like this:\n\nAbout\n\n\n\th1 {\n\t\tcolor: var(--purple-4);\n\t\tfont-size: var(--font-size-2);\n\t}\n\n\nInjects this CSS during the Sveltekit build process:\n\n:root{--purple-4:#da77f2;--font-size-2:1.1rem}h1.svelte-6n23wp{color:var(--purple-4);font-size:var(--font-size-2)}\n\nYou don't need to worry about any explicit CSS tree shaking. In the blink of an eye, thepost-jit-propsPostCSSplugin prunes your CSS to include only the Open Props variables referenced in your app's CSS.\n\nThis process is crazy fast (you won't notice any lag during development as Vite/PostCSS keeps your CSS current) and there is hardly any configuration. What's not to love!",
        "locale": "en",
        "slug": "technical-posts/add-postcss-to-sveltekit-project",
        "folder": ""
    },
    {
        "title": "Change editor width in Obsidian",
        "description": "Change editor width in Obsidian",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-17T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "obsidian"
        ],
        "content": "Find the vault root folder. In my case, it is:\n\nC:\\Users\\thumb\\Documents\\Dropbox\\synctest\\obsidian-db\n\nwhere obsidian-db is the name of my vault.(Also, in my case, Dropbox\\synctest is folder synchronized with Dropboxxx.\n\n.\n ...Dropbox\n     synctest\n         obsidian-db (vault name)\n             .obsidian\n                 snippets\n                     obsidian.css\n\nNavigate to the vault's snippets folder and add or modify the obsidian.css file to include (adjust max-width to your preference).\n\n.markdown-source-view,\n.markdown-preview-view {\n    max-width: 1200px;\n    margin: auto;\n}\n\nIn the Obsidian settings, set Editor > Readable Line Length to off.",
        "locale": "en",
        "slug": "technical-posts/change-editor-width",
        "folder": ""
    },
    {
        "title": "Recursively copy a single folder with RoboCopy",
        "description": "Recursively copy a single folder with RoboCopy",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "RoboCopy help\n\nrobocopy c:\\source d:\\destination /E\n\n/xd node_modules AppData dat site-packages .git Dropbox\n\n/xf NTUSER.DAT* ntuser.ini *.gm2 *.gbp *.pst *.vhdx .dmp\n\n| Flag    | Description                                                                                                                                                |\n| ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| /E    | Copy empty directories                                                                                                                                     |\n| /xd   | Exclude directories                                                                                                                                        |\n| /xf   | Exclude files                                                                                                                                              |\n| /v    | Verbose console output                                                                                                                                     |\n| /mir  | Mirror source folder to target folder. Ensures destination directory is an exact copy of the source directory. Could delete files in the target directory! |\n| /mt:n | Use n threads when coping to increase performance. 64 seems like a good value.                                                                           |\n| /nfl  | Surpress file listing to console while copying.                                                                                                            |",
        "locale": "en",
        "slug": "technical-posts/copy-a-single-folder-with-robocopy",
        "folder": ""
    },
    {
        "title": "bulk-markdown-modify",
        "description": "How to perform bulk changes to markdown files in a folder",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-17T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "markdown"
        ],
        "content": "This code lets you make bulk changes to the markdown documents in a folder. \n\nI needed to remove the first H1 line in each file in a folder full of markdown files.  I wrote this code, with its critical parts coming from the rp-util library to make that bulk change. \n\nI left the code for that specific change intact, but you can change the code to make change necessary to markdown file object.\n\n[!info]\nThis code can also be found in the lib/cli/revise-markdown-docs.ts file.\n\nimport 'rp-utils/console';\n\nimport path from 'path';\nimport { writeMarkdownFile, getAppPath, getMarkdownObjects } from '@rogerpence/rp-utils';\n\ninterface ParsedMarkdown = Record> {\n\tcontent: string;\n\tfrontMatter: T;\n}\n\n// Get input path of markdown files.\nconst inputPath = getAppPath(import.meta.url, 'src\\\\lib\\\\markdown\\\\technical-posts');\n\n// Fetch type that getMarkdownObjects returns.\ntype MarkdownParseResult = Awaited>;\n\nconst { successful: markdownObjects, failed }: MarkdownParseResult =\n\tawait getMarkdownObjects(inputPath);\n\nconst outputPath = getAppPath(import.meta.url, 'src\\\\lib\\\\data\\\\temp');\n\n// markdownObjects is type MarkdownFileResult[].\n// See this: https://rogerpence.github.io/rp-utils/types/types.MarkdownFileResult.html\n\n// Iterate over each markdown object.\nmarkdownObjects.map((obj) => {\n\t// Make changes needed to the incoming markdown object.\n\t// Change the 'obj' object before calling 'writeMarkDownFile.\n\n\t// You can add, delete, or change frontmatter keys and modify the content.\n\n\t// Add a new frontmatter key and value.\n\t// obj.markdownObject.frontMatter['bib'] = 'dib';\n\n\t// Delete a frontmatter key and value.\n\t// delete obj.markdownObject.frontMatter['description'];\n\n\t// Change a frontmatter value.\n\t// obj.markdownObject.frontMatter['description'] = 'Neil Young';\n\n\t// This change modifies the content property by removing\n\t// removes the first line that starts with '# ' in the markdown content.\n\tlet newContent: string[] = [];\n\tlet currentContent: string[] = [];\n\tlet firstH1Found = false;\n\n\tnewContent = obj.markdownObject.content.split('\\n');\n\n\tnewContent.forEach((line) => {\n\t\tif (line.startsWith('# ') && !firstH1Found) {\n\t\t\tfirstH1Found = true;\n\t\t} else {\n\t\t\tcurrentContent.push(line);\n\t\t}\n\t});\n\n\t// Assign new content.\n\tobj.markdownObject.content = currentContent.join('\\n').trim();\n\n\twriteMarkdownFile(obj.markdownObject, path.join(outputPath, obj.dirent.name));\n});\n\n  ",
        "locale": "en",
        "slug": "technical-posts/bulk-markdown-modify",
        "folder": ""
    },
    {
        "title": "Company event fowarding",
        "description": "Company event fowarding",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "CustomButton component\n\n\n\n\n\nParent:\n\n\n\timport CustomButton from './CustomButton.svelte';\n\n\tconst clicker = (e) => {\n\t\talert('hello')\n\t}\n\n\n clicker()}>Click Me\n\nThe component's click event is passed on to the parent's clicker handler.",
        "locale": "en",
        "slug": "technical-posts/component-event-forwarding",
        "folder": ""
    },
    {
        "title": "Calling a PowerShell script from a package.json script",
        "description": "Calling a PowerShell script from a package.json script",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "pwsh calls PowerShell 7. The double ampersand separates commands.\n\n\"scripts\": {\n    \"dev\": \"pwsh -File ./cli/hello-world.ps1 && vite dev\",\n    ...",
        "locale": "en",
        "slug": "technical-posts/calling-powershell-script-from-package",
        "folder": ""
    },
    {
        "title": "bai-connectivity",
        "description": "bai-connectivity",
        "date_created": "2025-07-08T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "customer"
        ],
        "content": "BAI Remote desktop info\n\nBeta Server\n  IP: 10.201.237.238\n  UN: Administrator\n  PW: Adm0618\n\nColton Dev\n  IP: 10.201.237.212\n  UN: Colton\n  PW: iwbytd\n\n\n![[image-36.png|516x411]]",
        "locale": "en",
        "slug": "technical-posts/bai-connectivity",
        "folder": ""
    },
    {
        "title": "CSS custom props color stuff",
        "description": "CSS custom props color stuff",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "colors:\n  brand-color\n  nav-bg-color\n  page-bg-color\n  sidebar-bg-color\n  footer-bg-color\n\n  primary-bg-color\n  secondary-bg-color\n  danger-bg-color\n  info-bg-color\n  error-bg-color\n  warning-bg-color\n\n  primary-bg-color\n  secondary-bg-color\n  danger-bg-color\n  info-bg-color\n  error-bg-color\n  warning-bg-color\n\n\n  text-color\n  anchor-color\n  anchor-hover-color\n\n  accent-color\n  accent-bg-color\n\n  button-bg-color\n  button-text-color\n\n  menu-active-bg-color\n  menu-bg-color\n\nThe color generator creates light and dark versions of the colors defined. The light and dark colors\n\n:root {\n    /* Define colors */\n    --brand-color: color;\n    --nav-bg-color: color;\n    --page-bg-color: color;\n    --sidebar-bg-color: color;\n    ...\n\n:root {\n    /* Define light colors */\n    --light-brand-color: color;\n    --light-nav-bg-color: color;\n    --light-page-bg-color: color;\n    --light-sidebar-bg-color: color;\n    ...\n\n\t/* Define dark colors */\n    --dark-brand-color: color;\n    --dark-nav-bg-color: color;\n    --dark-page-bg-color: color;\n    --dark-sidebar-bg-color: color;\n    ...\n\nThis batch file runs the generator\n\nC:\\Users\\thumb\\Documents\\Projects\\rputilities\\librettox\\\n       template_work\\libretto_batch_files\\css-color-generator.bat",
        "locale": "en",
        "slug": "technical-posts/css-custom-props-color-generator-schema",
        "folder": ""
    },
    {
        "title": "Convert and compress png to webp",
        "description": "Convert and compress png to webp",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities",
            "web-dev"
        ],
        "content": "Get the Google WebP CLi tools here\nThere are several utilities in this download. The cwebp utility is the one that converts, compresses, and/or resizes .png or .jpg files to .webp.\nThe -q argument is quality. \"25\" is generally a good value to use for quality.\n\ncwebp -q 25 enable-windows-pdf-driver.png -o enable-windows-pdf-driver.web\n\nThis command line also resizes the image to 550 px wide and the zero width ensures the aspect ratio is maintained.\n\ncwebp -resize 550 0 -q 25 enable-windows-pdf-driver.png -o enable-windows-pdf-driver.webp\n\nThis is some PowerShell to create a utility to convert all images in a folder to WebP\n\n$InputFolder = \"C:\\path\\to\\your\\png\\folder\"  # Replace with your actual folder path\n\nSpecify the output folder for the WEBP images. If it doesn't exist, it will be created.\n$OutputFolder = \"C:\\path\\to\\your\\webp\\folder\" # Replace with your actual output folder path\n\nQuality setting for cwebp (80 in your example)\n$Quality = 80\n\nPath to the cwebp executable (you might need to adjust this)\n$CwebpPath = \"cwebp\"  # Assumes cwebp is in your PATH environment variable.\nIf not, provide the full path, e.g., \"C:\\WebP\\cwebp.exe\"\n\nCreate the output folder if it doesn't exist\nif (!(Test-Path -Path $OutputFolder)) {\n    Write-Host \"Creating output folder: $OutputFolder\"\n    New-Item -ItemType Directory -Path $OutputFolder | Out-Null\n}\n\nGet all PNG files in the input folder\n$PngFiles = Get-ChildItem -Path $InputFolder -Filter \"*.png\"\n\nIterate through each PNG file and convert it to WEBP\nforeach ($PngFile in $PngFiles) {\n\nConstruct the input and output file paths\n    $InputFilePath = $PngFile.FullName\n    $OutputFilePath = Join-Path -Path $OutputFolder -ChildPath ($PngFile.BaseName + \".webp\")\n\nConstruct the cwebp command\n    $Command = \"$CwebpPath -q $Quality \"$InputFilePath\" -o \"$OutputFilePath\"\"\nUse backticks to escape quotes inside the string\n\nExecute the cwebp command\n    Write-Host \"Converting: $($PngFile.Name) to $($PngFile.BaseName).webp\"\n    try {\n        Invoke-Expression $Command  # Executes the string as a command\n\nOptional: Check the exit code for errors\n        #$ExitCode = $LASTEXITCODE\n        #if ($ExitCode -ne 0) {\nWrite-Error \"cwebp failed with exit code: $ExitCode\"\n        #}\n    }\n    catch {\n        Write-Error \"Error converting $($PngFile.Name): $($_.Exception.Message)\"\n    }\n}\n\nWrite-Host \"Conversion complete.\"",
        "locale": "en",
        "slug": "technical-posts/convert-and-compress-png-to-webp",
        "folder": ""
    },
    {
        "title": "asna-mailing-list-research",
        "description": "asna-mailing-list-research",
        "date_created": "2025-10-13T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "mailling-list"
        ],
        "content": "We sell software to modernize legacy IBM i RPG applications. We are very specifically interested in finding companies using the IBM i\n\nretail, insurance. manufacturing, finance\n\nUS --very specifically\n\nhttps://levelupleads.io/ (contacted)\nhttps://www.dealsignal.com/ \n\t(contacted-October 14, Tuesday 1:30)\n\tLots of interesting information\n\t\tDoes not sell lists... unless you want by industry. Sells list research. You identify your ICP and they search for it.\n\t\tInformation by platform \"isn't available. Anyone who tells you it is lying to you\"\n\t\t\tApollo, Zoominfo\n\t\tThe email list market is hyper-commoditized \n\t\t\tMany resellers, very few wholesale providers\n\t\t\tThe quality of these lists is highly dubious \n\t\t10K per year -- contact for 12 months\n\t\t\tup to 35K contacts\n\t\tSaid Constant Contact isn't a good cold email email service\n\t\t\tRecommended Instantly - $1000 per year \n\t\t\tor Snovio -- $1000 per year \nhttps://dmtcorp.io/\n\t(contacted-October 14, Tuesday 2:30)\n\tDoes not sell lists... unless you want by industry\n\tLike Deal Signal, sells list research. \n\tDidn't provide the professional first impression that Deal Signal did.\nhttps://salesproleads.com/ \n\t(contacted-October 14-Tuesday 3:30)\n\twas a no-show at the scheduled meeting.\nhttps://outbound.consulting/\nhttps://belkins.io/ \n\t(contacted-Thursday, October 16 @ 10:30am) THEY CANCELED - we are too little. Recommended Outbound Consulting\n\nhttps://www.apollo.io/\nhttps://www.zoominfo.com/\nhttps://prospectoo.com/ \nhttps://www.bruxt.com/\nhttps://www.datamarketersgroup.com/\nhttps://www.dealsignal.com/\n\nLots of other companies listed:\nSlashdot.org\nClutch.co (seems to list more enterprisey results)",
        "locale": "en",
        "slug": "technical-posts/asna-mailing-list-research",
        "folder": ""
    },
    {
        "title": "Budget CategoryRules SQL Server table",
        "description": "Budget CategoryRules SQL Server table",
        "date_created": "2025-05-26T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "budget"
        ],
        "content": "CREATE TABLE dbo].[categoryrules](\n\t[Id IDENTITY(1,1) NOT NULL,\n\tdescription(150) NULL,\n\trule(50) NULL,\n\tvalue(500) NULL,\n\tcategory(150) NULL,\nPRIMARY KEY CLUSTERED \n(\n\t[Id] ASC\n)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, \n       ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, \n       OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]\n) ON [PRIMARY]\n\nCREATE UNIQUE INDEX description\nON categoryrules (description);\n\nINSERT INTO categoryrules ([description], [rule], [value], [category])\n       VALUES('Bill Miller Bar-B-Q', 'startswith', 'bill miller', 'fast food')",
        "locale": "en",
        "slug": "technical-posts/budget-category-rules",
        "folder": ""
    },
    {
        "title": "x cool things to do with PowerShell",
        "description": "x cool things to do with PowerShell",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "Initial explanation\n\nBe careful! PowerShell is very powerful. It is the table saw of Windows features. In can you amazing things for you but it can also hurt you!\nExplain Terminal and Starship CLI\nExplain PowerShell relation to DOS commands and batch files\\*\n    DOS Aliases\nExecution policy\n\nThings to research\n\nGet installed ASNA products from registry\n\nls HKLM:software\\ASNA\\Installinfo -recurse -depth 2\n | out-file -path 'asna-products.txt.\n\nGet-Help, Update-Help, Get-Member, Get-Command\nGet-History and Out-File\n\nget-history | out-file -path 'ps-history.txt'\n\nPush/Pop and Set-Location\nISE\nDebugging\nCompare files\nScan for open ports\nWork with services\nTee object\n\nThings you can do\n\nBackup SQL Server\nFind and kill a process\nPut a file on the clipboard\nRecuse folders\nGet list of big files\nExport to CSV and HTML\nAdd functions to your PowerShell $profile",
        "locale": "en",
        "slug": "technical-posts/cool-things-to-do-with-powershell",
        "folder": ""
    },
    {
        "title": "Chrome com.chrome.devtools.json file error",
        "description": "Chrome com.chrome.devtools.json file error",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "chrome"
        ],
        "content": "While working on the asna.com Sveltekit upgrade, I started encountering this error with Chrome:\n\n![[image-15.png]]\n\nIt was flooding my terminal with messaging. I stopped it by manually adding an empty for the file it was looking for.\n\n![[image-16.png]]\n\nVite needed this server/fs/allow setting to serve that file:\n\nvite.config.ts\n\nexport default defineConfig({\n\tplugins: [sveltekit()],\n\tserver: {\n\t\tfs: {\n\t\t\tallow: ['.well-known/appspecific/']\n\t\t}\n\t}\n});",
        "locale": "en",
        "slug": "technical-posts/chrome-com",
        "folder": ""
    },
    {
        "title": "Backup notes",
        "description": "Backup notes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "2024-Nov-01 14:36 The seagate-4tb-desktop drive doesn't have any VHDXs on it",
        "locale": "en",
        "slug": "technical-posts/backup-notes",
        "folder": ""
    },
    {
        "title": "CSS set table column widths",
        "description": "CSS set table column widths",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "table {\n    width: 100%;\n    border-collapse: collapse;\n}\n\ntable td:nth-child(1) {\n    width: 12rem;\n}\ntable td:nth-child(2) {\n    /* width: 8%; */\n}\ntable td:nth-child(3) {\n    /* width: 8%; */\n}\ntable td:nth-child(4) {\n    width: 10%;\n}\ntable td:nth-child(5) {\n    width: 10%;\n}\ntable td:nth-child(6) {\n    /* width: 10%; */\n}\ntable td:nth-child(7) {\n    /* width: 10%; */\n}\ntable td:nth-child(8) {\n    width: 18rem;\n}",
        "locale": "en",
        "slug": "technical-posts/css-set-table-column-widths",
        "folder": ""
    },
    {
        "title": "Personal budget work for Roger and Colleen. import Jefferson Bank data.",
        "description": "Personal budget work for Roger and Colleen. import Jefferson Bank data.",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "personal",
            "budget"
        ],
        "content": "To import Jeffersson bank credit card data\n\nDownload latest data\nLog into Jefferson Bank \nGo to Services/Card Management\nClick \"View Transactions\"\nSet filter for all of 2024 and 2025\nDownload as C:\\Users\\thumb\\Documents\\projects\\personal\\AccountHistory.xls\nOpen that file with Excel and save as C:\\Users\\thumb\\Documents\\projects\\personal\\AccountHistory.xlsx\nImport AccountHistory.xlsx into SQL Server\n\tOpen this C# program in VS: C:\\Users\\thumb\\Documents\\projects\\cs\\import-jeff-bank\\import-jeff-bank.sln and run it\n\tThat program clears the rpBudget table in the rp database and then imports the latest data\n\n\nI have an excel spreadsheet with the following columns:\n\nAccountNumber\nPost Date\nYear\nMonth\nCheck\nDescription\nDebit\nCredit\nStatus\nBalance\nClassification\n\nI want to create a pivot table that include only values from year 2024 and shows the Classification value in the A column and the summarized debit values by Classification for each for each month 1 - 12 in columns B through M. In the last row put the total of each column\n\nOkay, here's how to create that PivotTable in Excel:\n\nSteps:\n\nSelect Your Data:\n\n    Click anywhere inside your data table (e.g., cell A1).\n    Press Ctrl + A (or Cmd + A on Mac) to select the entire data range.\n\nInsert PivotTable:\n\n    Go to the Insert tab on the Excel ribbon.\n    Click PivotTable.\n    In the \"Create PivotTable\" dialog box:\n        Ensure the \"Table/Range\" correctly shows your selected data.\n        Choose where you want to place the PivotTable: \"New Worksheet\" (recommended) or \"Existing Worksheet\" (and then specify a location).\n        Click OK.\n\nConfigure the PivotTable Fields:\n    A \"PivotTable Fields\" pane will appear on the right side of your screen. Drag and drop the fields into the appropriate areas at the bottom of this pane:\n\n    Filters:\n        Drag the Year field into the \"Filters\" area.\n    Rows:\n        Drag the Classification field into the \"Rows\" area.\n    Columns:\n        Drag the Month field into the \"Columns\" area.\n    Values:\n        Drag the Debit field into the \"Values\" area. It should automatically default to \"Sum of Debit\". If it shows \"Count of Debit\" or something else, click on it, select \"Value Field Settings...\", and choose \"Sum\". Click \"OK\".\n\nFilter for the Year 2024:\n\n    At the top of your new PivotTable (usually in cell A1 or B1 of the sheet where the PivotTable is located), you'll see the \"Year\" filter.\n    Click the dropdown arrow next to \"(All)\".\n    Check the box next to 2024 (and uncheck any others if necessary).\n    Click OK.\n\nCheck Grand Totals:\n    By default, PivotTables usually show Grand Totals for both rows and columns.\n    The \"last row\" with the total of each column is the \"Grand Total\" row for the columns. This should appear automatically.\n    If it's not showing, click anywhere inside your PivotTable, go to the PivotTable Analyze (or Design) tab on the ribbon:\n        Design Tab: Click Grand Totals in the \"Layout\" group and select On for Columns Only or On for Rows and Columns. (You specifically want the column totals, which appear as the last row).\n\nResult:\n\nColumn A will display your unique Classification values.\nColumns B through M (or however many months have data for 2024) will display the Month numbers (1, 2, 3,... 12).\nThe cells within the table will show the sum of Debit amounts for each Classification and Month.\nThe last row of the PivotTable (labeled \"Grand Total\") will show the sum of debits for each month column.\n\nOptional Refinements:\n\nFormatting Numbers: Select the debit values in the PivotTable, right-click, choose \"Number Format...\", and select \"Currency\" or \"Accounting\" as desired.\nColumn Header Names: If the month headers show \"Sum of Debit - 1\", \"Sum of Debit - 2\", etc., you can simply type over them in the PivotTable to rename them to \"1\", \"2\", \"Jan\", \"Feb\", etc. (though using numbers 1-12 as per your request is fine).\nIf Months Sort Alphabetically (e.g., if they were \"Jan\", \"Feb\"): If your Month column in the source data contained text names and they sort alphabetically in the PivotTable (April, August, December...), you'd ideally have a numeric month column (1-12) in your source data to use for sorting, or create a custom sort list. Since you mentioned \"month 1-12\", this shouldn't be an issue.\n\nThat's it! You'll have your desired PivotTable. Remember to refresh the PivotTable (right-click on it > Refresh) if your source data changes.",
        "locale": "en",
        "slug": "technical-posts/budget-work",
        "folder": ""
    },
    {
        "title": "Copy a folder into a new branch",
        "description": "Copy a folder into a new branch",
        "date_created": "2025-09-02T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "git"
        ],
        "content": "Okay, that simplifies things considerably! If the new folder is a complete replacement, you don't care about the history, and you plan to replace main entirely, then your initial approach (create branch, empty it, copy contents) is perfectly valid and probably the most direct way to achieve that specific goal.\n\nHere are the steps, refined for your specific intent:\n\nEnsure your external folder is ready. Make sure it contains exactly what you want the new main branch to look like.\n\nNavigate to your existing Git repository:\n    cd /path/to/your/git/repo\n\nCreate and switch to a new branch: This will be your temporary holding branch for the new content.\n    git checkout -b new-main-content\n\nRemove all existing tracked files and untracked files/directories (clean slate):\n    git rm -rf .: Removes all tracked files and directories from the index and working directory.\n    git clean -dfx: Removes all untracked files and directories. The -f is force, -d includes directories, -x includes ignored files. Use git clean -df if you want to keep ignored files.\n    git rm -rf .\n    git clean -dfx # Use with caution: this removes everything not tracked by git, including ignored files!\n    git commit -m \"Cleaned branch for new content\"\n    Alternative for step 4 (safer if you're worried about git clean -dfx):\n        git rm -rf .\nManually delete any remaining untracked files/folders\nls -a to see them, then rm -rf them if you want a truly empty dir\n        git commit -m \"Cleaned branch for new content\"\n\nCopy the entire contents of your external folder into the repository root:\n    cp -r /path/to/your/external/folder/. . # Copies all files, including dotfiles\n    Make sure you copy from inside the external folder so its contents are directly in your repo's root, not nested in a new subfolder of the same name. The . at the end means \"current directory\".\n\nAdd all the new files and commit them:\n    git add .\n    git commit -m \"Imported new project content\"\n    Now, your new-main-content branch contains exactly what was in your external folder, with its own fresh history.\n\nFinalize the branch replacement:\n\n    Option A (Recommended for simplicity): Delete main and rename new-main-content\n        git checkout main # Switch back to main to delete the branch\n        git branch -D main # Force delete the old main branch\n        git branch -m main # Rename new-main-content to main\n        git push -f origin main # Push the new main branch and overwrite remote\n        Explanation for git push -f origin main: Since you're replacing the entire history of main, you must force push to overwrite the remote main branch. This is a destructive operation on the remote, so be absolutely sure this is what you want.\n\n    Option B (If you prefer to keep main's name immediately): Reset main to new-main-content's state\n        This is a bit more advanced but achieves the same end result.\n        git checkout main\n        git reset --hard new-main-content # Make main exactly like new-main-content\n        git branch -D new-main-content # Delete the temporary branch\n        git push -f origin main # Force push to overwrite remote main\n        Again, the force push is crucial here because you are fundamentally altering the history of main.\n\nBefore you push:\n\nDouble-check: After step 6, git checkout new-main-content and ls -a to verify that the branch truly contains only the content you want.\nBackup (Optional but highly recommended): If this is a critical repository, consider backing up the .git folder and the old working directory content before performing the force push, just in case.\n\nThis streamlined process directly achieves your goal of completely replacing the main branch's content and history with your external folder's content.",
        "locale": "en",
        "slug": "technical-posts/copy-folder-to-new-git-branch",
        "folder": ""
    },
    {
        "title": "Animating the dialog and popover elements",
        "description": "Animating the dialog and popover elements",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "design",
            "css",
            "html"
        ],
        "content": "[Animating the dialog element](https://codepen.io/argyleink/pen/zYbQBOm)\nGood video about animating the top layer - Good links here, too\nWhat is the top layer\n\nAdam Argyl on the dialog element\n\n\nOf course! Fading a `` element when it closes is a common and excellent way to improve user experience.\n\nThe main challenge is that when you call the dialog.close() method, the browser immediately sets display: none on the element, which prevents any CSS transitions or animations from running.\n\nHere are the two best methods to solve this, from the most compatible to the most modern.\n\nMethod 1: The \"Graceful Close\" with JavaScript (Most Compatible)\n\nSee this codepen\n\nThis is the most common and reliable method. Instead of closing the dialog immediately, we'll:\nAdd a \"closing\" CSS class to trigger the fade-out animation.\nListen for the animationend (or transitionend) event.\nOnce the animation is finished, then we call dialog.close().\n\nHeres a complete, working example.\n\nHTML\nOpen Dialog\n\n\n  My Dialog\n  Click the button or press Escape to close me with a fade effect.\n  Close\n\nCSS\nWe'll use CSS keyframes for a smooth fade-out and a subtle shrink effect.\n\ndialog {\n  padding: 2em;\n  border: 1px solid #ccc;\n  border-radius: 8px;\n  box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);\n}\n\n/* Style for when the dialog is opening */\ndialogopen] {\n  animation: fadeIn 0.3s ease-in-out;\n}\n\n/* Style for when we manually trigger the close */\ndialog.closing {\n  animation: fadeOut 0.3s ease-in-out forwards; /* 'forwards' keeps the final state */\n}\n\n/* Backdrop styling */\ndialog::backdrop {\n  background-color: rgba(0, 0, 0, 0.5);\n  animation: fadeIn 0.3s ease-in-out;\n}\n\ndialog.closing::backdrop {\n  animation: fadeOut 0.3s ease-in-out forwards;\n}\n\n/* Keyframe animations */\n@keyframes fadeIn {\n  from { opacity: 0; transform: scale(0.95); }\n  to   { opacity: 1; transform: scale(1); }\n}\n\n@keyframes fadeOut {\n  from { opacity: 1; transform: scale(1); }\n  to   { opacity: 0; transform: scale(0.95); }\n}\n\nJavaScript\nThis is where we orchestrate the closing logic.\n\nconst dialog = document.getElementById('my-dialog');\nconst openBtn = document.getElementById('open-btn');\nconst closeBtn = document.getElementById('close-btn');\n\n// Open the dialog\nopenBtn.addEventListener('click', () => {\n  dialog.showModal();\n});\n\n// Function to handle the closing logic\nfunction closeDialog() {\n  dialog.classList.add('closing');\n\n  // Listen for the animation to end\n  dialog.addEventListener('animationend', () => {\n    dialog.classList.remove('closing');\n    dialog.close();\n  }, { once: true }); // { once: true } removes the listener after it runs\n}\n\n// Close the dialog with the custom function\ncloseBtn.addEventListener('click', () => {\n  closeDialog();\n});\n\n// Also handle the 'cancel' event (when user presses Escape)\ndialog.addEventListener('cancel', (event) => {\n  // Prevent the default browser behavior (immediate closing)\n  event.preventDefault(); \n  closeDialog();\n});\n\nPros:\nHighly Compatible: Works in all modern browsers that support ``.\nFull Control: You have explicit control over the entire process.\n\nCons:\nRequires more JavaScript boilerplate.\n\nMethod 2: The Modern CSS-Only Approach (Simpler but Newer)\n\nModern browsers are starting to make this much easier by allowing you to animate discrete properties like display. With the @starting-style rule, you can create a purely CSS-driven fade-in and fade-out.\n\nThis approach transitions the opacity and lets the browser handle the display property change at the end of the transition.\n\nNote: Browser support for animating display and using @starting-style is still evolving. Check sites like [Can I use... for the latest compatibility. This works well in Chrome/Edge 117+ and Firefox 124+.\n\nHTML\nThe HTML is the same.\nOpen CSS-Powered Dialog\n\n\n  Modern CSS Dialog\n  This dialog fades in and out using modern CSS features.\n  Close\n\nCSS\nThe CSS is much simpler. We define the starting and ending states and let the browser handle the transition.\n\n#my-dialog-css {\n  /* Start with opacity 0 and not displayed */\n  opacity: 0;\n  transform: scale(0.9);\n  transition:\n    opacity 0.3s,\n    transform 0.3s,\n    /* Allow the browser to transition 'display' after other transitions finish */\n    display 0.3s allow-discrete;\n}\n\n/* When the dialog has the [open] attribute, it's visible */\n#my-dialog-css[open] {\n  opacity: 1;\n  transform: scale(1);\n}\n\n/* This is the magic for the OPENING animation */\n/* It defines the style before the element is shown */\n@starting-style {\n  #my-dialog-css[open] {\n    opacity: 0;\n    transform: scale(0.9);\n  }\n}\n\n/* Fade the backdrop as well */\n#my-dialog-css::backdrop {\n  background-color: rgba(0, 0, 0, 0);\n  transition:\n    background-color 0.3s,\n    display 0.3s allow-discrete;\n}\n\n#my-dialog-css[open]::backdrop {\n  background-color: rgba(0, 0, 0, 0.5);\n}\n\n@starting-style {\n  #my-dialog-css[open]::backdrop {\n    background-color: rgba(0, 0, 0, 0);\n  }\n}\n\nJavaScript\nThe JavaScript becomes incredibly simple because we no longer need to manage the closing state.\n\nconst dialogCss = document.getElementById('my-dialog-css');\nconst openBtnCss = document.getElementById('open-btn-css');\nconst closeBtnCss = document.getElementById('close-btn-css');\n\nopenBtnCss.addEventListener('click', () => {\n  dialogCss.showModal(); // The CSS handles the animation automatically\n});\n\ncloseBtnCss.addEventListener('click', () => {\n  dialogCss.close(); // The CSS handles the fade-out before display: none\n});\nWhen you call dialog.close(), the [open] attribute is removed. The browser sees this and transitions the element from its [open] styles back to its default styles (opacity: 0) before finally applying display: none.\n\nPros:\nVery Clean: Minimal JavaScript, declarative CSS.\nThe \"future-proof\" way of doing things.\n\nCons:\nLimited Browser Support: Relies on very new CSS features.\n\nRecommendation\n\nFor production websites today, Method 1 (JavaScript \"Graceful Close\") is the safest and most reliable choice. It guarantees a consistent experience for all your users.\n\nAs browser support improves, Method 2 (Modern CSS-Only) will become the standard due to its simplicity and elegance. You could use it today if you're developing for a platform with a known, modern browser environment (like an Electron app).",
        "locale": "en",
        "slug": "technical-posts/animating-the-dialog-and-popover-elements",
        "folder": ""
    },
    {
        "title": "Container query example",
        "description": "Container query example",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "In this example, the color of the div.content changes from green to read when the width of the viewport goes to less than 600.\n\nHTML\n\n\n   \n      Hello, world\n      Color changes from green to red when width shrinks.\n   \n\n\n\nCSS\n\ndiv.wrapper {\n    max-width: 1024px;\n    container-type: inline-size;\n    container-name: landing-page-wrapper;\n\n    & div.content {\n\t    color: blue;\n\n      @container content-wrapper (width < 601px) {\n         color: red;\n      }\n\t}\n}",
        "locale": "en",
        "slug": "technical-posts/container-query-example",
        "folder": ""
    },
    {
        "title": "Comparing Rust vectors to .NET collections",
        "description": "Comparing Rust vectors to .NET collections",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "Yes, Rust vectors (Vec) are analogous to collections like List in .NET. Both serve as dynamic, growable arrays that can hold multiple elements of a specific type and allow for operations like adding, removing, and accessing elements. Here's a comparison between the two:\n\nSimilarities:\n\nDynamic Size:\n\n    Rust (Vec): Vectors can grow or shrink at runtime by adding or removing elements.\n    .NET (List): Lists in .NET can also dynamically resize as you add or remove elements.\n\nHomogeneous Types:\n\n    Both Vec and List require all elements to be of the same type T. For example, a Vec holds integers in Rust, and a List holds integers in .NET.\n\nHeap Allocation:\n\n    Both Vec in Rust and List in .NET are heap-allocated, allowing them to dynamically grow in size, unlike fixed-size arrays which are typically allocated on the stack.\n\nEfficient Indexing:\n\n    Both structures provide efficient, constant-time access to elements via indexing.\n\nSimilar Methods:\n    Rust's Vec has methods like push(), pop(), and len() which are analogous to .NET's List methods like Add(), RemoveAt(), and Count.\n\nKey Differences:\n\nMemory Management:\n\n    Rust: Rust's vectors are memory-safe due to its ownership and borrowing rules. When a Vec goes out of scope, Rust automatically deallocates its memory, preventing memory leaks. Rust also prevents concurrent modification or data races through its borrow checker.\n    .NET: .NET uses a garbage collector to manage memory. When objects (like a List) are no longer used, the garbage collector cleans them up. However, it doesn't enforce strict borrowing rules like Rust, so you may need to manage threading or concurrency concerns manually.\n\nNull Values:\n\n    Rust: Rust references are never null. If you need to represent a potentially absent value, you use Option, which is explicit and forces you to handle the absence of values.\n    .NET: .NET's reference types (List, for example, being a reference type) can be null, and you must check for null values to avoid runtime errors.\n\nThread Safety:\n\n    Rust: Rust has strict compile-time guarantees for thread safety. It enforces rules that prevent data races at compile time.\n    .NET: .NET collections like List are not thread-safe by default, but you can use synchronization mechanisms to handle concurrency.\n\nPerformance Guarantees:\n    Rust: Because of its zero-cost abstractions and lack of a garbage collector, Rust generally offers better performance with fine control over memory management.\n    .NET: .NET has the overhead of a garbage collector, which can introduce pauses in performance, but it simplifies memory management.\n\nExample Comparison:\n\nRust Vec Example:\n\nfn main() {\n    let mut numbers: Vec = Vec::new();\n    numbers.push(1);\n    numbers.push(2);\n    numbers.push(3);\n\n    println!(\"Length: {}\", numbers.len()); // Output: Length: 3\n\n    for number in &numbers {\n        println!(\"{}\", number); // Output: 1 2 3\n    }\n\n    numbers.pop(); // Removes the last element\n}\n\n.NET List Example:\n\nusing System;\nusing System.Collections.Generic;\n\nclass Program {\n    static void Main() {\n        List numbers = new List();\n        numbers.Add(1);\n        numbers.Add(2);\n        numbers.Add(3);\n\n        Console.WriteLine(\"Length: \" + numbers.Count); // Output: Length: 3\n\n        foreach (var number in numbers) {\n            Console.WriteLine(number); // Output: 1 2 3\n        }\n\n        numbers.RemoveAt(numbers.Count - 1); // Removes the last element\n    }\n}\n\nConclusion:\n\nRust vectors and .NET collections like List are conceptually similar, as they both serve as dynamic, growable collections of elements. However, Rusts strict memory management rules and its lack of a garbage collector offer more control over performance and memory safety, while .NET's garbage collection simplifies memory management at the cost of some performance overhead.",
        "locale": "en",
        "slug": "technical-posts/comparing-rust-vectors-to-net-collections",
        "folder": ""
    },
    {
        "title": "Adding a new family availability code to ASNA downloads",
        "description": "Adding a new family availability code to ASNA downloads",
        "date_created": "2025-07-02T00:00:00.000Z",
        "date_updated": "2025-12-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "downloads"
        ],
        "content": "\nStep 1. Add a new row to the family_availability table.\n\ntest internal link\n\naasdfasdfasdfsupabase-credentials\n\n[[supabase-credentials|test link]]\n\n\n\n\n![[image-32.png|700]]\n\nStep 2. Update the family table \n\nUpdate the family to change in the family table to reflect this new availability in the availability_status column.\n\nStep 3.  Update the get_product_downloads_json function in Postgres\n\nThis line in the Postgres function scopes the products listed on the downloads page with family availability ids. Add the new family to the list.\n\nWHERE CAST(f.availability_id as int) IN (6,7,8,9) AND\n\nThe full function is shown below.\n\nBEGIN\n    RETURN QUERY\n\nWITH download_list AS (\n    SELECT f.name as name, \n           CAST(f.availability_id as int) as availability_status,\n           f.download_page_order as download_page_order,\n           f.sort_order as sort_order, \n           COALESCE(f.visual_studio_version,'') as vs_version,\n           false as is_com,\n           f.release_date as release_date,\n           f.download_page_section_heading as download_section_heading,\n           rs.family_name as family_name, \n           rs.product_name as product_name, \n           rs.trademark_name as product_trademark_name,\n           rs.product_sort_order as product_sort_order, \n           rs.version as product_version, \n           rs.binary_filename as binary_filename,\n           COALESCE(rs.readme_filename, '') as readme_filename,\n           GetS3Key(rs.family_name, rs.binary_filename) as s3key,\n           CONCAT(f.release_date,'-',family_name_crusher(f.name), '/', GetFileName(rs.readme_filename)) as s3_readme_key,\n           CleanToken(f.name) as family_key, \n      \t   CleanToken(rs.product_name) as product_key,\n           CleanToken(rs.product_name) as unique_product_key       \n\n    FROM family as f\n\n    INNER JOIN release_set_normalized AS rs \n        ON to_iso_date(rs.release_date) = f.release_date AND rs.family_id = f.id\n\n\t-- use Family Availability to scope the products shown on the downloads page.\n    WHERE CAST(f.availability_id as int) IN (6,7,8,9) AND \n          f.release_date IS NOT NULL AND \n          rs.binary_filename  ''\n\n    -- ORDER BY f.availability_id DESC, f.download_page_order, f.sort_order, rs.product_sort_order\n    ORDER BY f.download_page_order, f.sort_order, rs.product_sort_order\n)\n\n    SELECT  dl.name,\n            dl.availability_status,  \n            dl.sort_order, \n            dl.download_page_order, \n            dl.product_sort_order,\n            dl.family_key, \n            dl.product_key,          \n            dl.unique_product_key,\n            dl.vs_version, \n            dl.is_com,\n            dl.release_date, \n            dl.download_section_heading,\n            dl.family_name, \n            dl.product_name,\n            dl.product_trademark_name, \n            dl.product_version, \n            dl.s3key,\n            dl.s3_readme_key\n\n    FROM download_list AS dl\n    \n    ORDER BY dl.download_page_order, dl.sort_order, dl.product_sort_order;\nEND;\n\nStep 4. Update downloads/[[slug]]/prepareProductsJson.js\nThis JavaScript file creates Json objects for each family availability status. Add the new status in ~ lines 14-17 \n\n![[image-34.png|500]]\n\nand at the end of the file add these lines for the new status you are adding\n\nconst superseded_products = products.filter((product) => product.availability_status == SUPERSEDED_STATUS);\nconst superseded_products_grouped = groupBy(superseded_products, 'name');\nexport const superseded_product_names = Object.keys(superseded_products_grouped);\n\nStep 5. Update Project Ocho's general-xlate.js file\n\nThis Json file provides a bunch of potentially translated texts. It has [family]_title and [family]_text keys that govern the text that gets displayed in the yellow areas shown below of the downloads page. \n\n![[image-33.png|700]]\n\nAdd corresponding entries for the new availability status. Watch your ending commas when you're updating that Json. It's fiddly! \n\nga_title: 'Generally available products',\nretired_title: 'Retired products &mdash; suppport is not available for these products',\nbeta_title: 'Beta products &mdash; not for production use!',\nsuperseded_title: 'Superseded products &mdash; replaced by a higher point version',\n\nga_text:\n\t'These products are generally available and supported. These products feature the latest updates, enhancements, and fixes.',\nretired_text:\n\t'These products are retired and tech support is not available. We strongly recommend you upgrade to a generally available version. Please see this link for more information.',\nbeta_text:\n\t'These products are in beta testing and not for use in production enviroments. Tech support is available, but not for issues in a production environment.No warrantly is implied or expressed for beta products. You use them at your own risk. By downloading beta products you acknowledge the risk of using them. Go to generally available downloads.Read about our beta program.',\nsuperseded_text:\n\t'These products have been superseded with a point release (eg, 17.0 replaced by 17.1). They are supported but deprecated and we recommend updating to the higher point release as soon as possible.',\n\nStep 6. Update downloads/[[slug]]/+page/.svelte \n\nAt ~ line 253 or so, there are loops rendering data for each family availability type.  \n\n{#if superseded_product_names}\n{@html getXlateText(xlate, locale, 'downloads', 'superseded_title')}\n{@html getXlateText(xlate, locale, 'downloads', 'superseded_text')}\n\n\t{#each superseded_product_names as family}\n\t\t (showModal = true)}\n\t\t\ton:focus={putProductInfoOnForm}\n\t\t\ton:mouseover={putProductInfoOnForm}\n\t\t\tfamilyName={family}\n\t\t\t{assets_url_prefix}\n\t\t/>\n\t{/each}\n\n{/if}\n\nStep 7. Refresh the products.json file.\n\nThis file provides the list of products for the downloads page.  Bob's your uncle.",
        "locale": "en",
        "slug": "technical-posts/adding-new-downloads-family-availability",
        "folder": ""
    },
    {
        "title": "c-sharp-dynamic-generic-function",
        "description": "c-sharp-dynamic-generic-function",
        "date_created": "2025-10-20T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "csharp"
        ],
        "content": "\nTo dynamically provide the type for a generic function like GenerateSqlInsertStatement, you need to use reflection because C# generics require the type parameter at compile time. With reflection, you can call the generic method and specify the type at runtime.\n\n\nType dynamicType = typeof(Models.LakeB2B); // or get this type dynamically\nvar method = typeof(SqlServerOps).GetMethod(nameof(GenerateSqlInsertStatement), BindingFlags.Public | BindingFlags.Static);\nvar genericMethod = method.MakeGenericMethod(dynamicType);\nstring insertQuery = (string)genericMethod.Invoke(null, new object[] { tableName });\n\nExplanation:\ntypeof(Models.LakeB2B): Replace this with any logic to get your type at runtime.\nGetMethod: Gets the method info for GenerateSqlInsertStatement.\nMakeGenericMethod: Specifies the type parameter at runtime.\nInvoke: Calls the method, passing in the arguments.\n\nThis approach works for any type you determine at runtime.\n\n\n\nI have a namespace named 'Models' and it has a class named LakeB2B in it (it also has several other model classes).\n\nGiven this string, 'lakeb2b' (named 'modelName'), how can I get the typeof Models.LakeB2B to be able to do this: Type dynamicType = typeof(Models.LakeB2B);\n\n\nYou can get the Type for a class in the Models namespace using the string name by:\n\nCapitalizing the first letter (to match the class name).\nUsing Type.GetType with the fully qualified name, or searching the assembly if needed.\n\nHeres a robust way to do it:\n\n\nstring modelName = \"lakeb2b\"; // your input\nstring className = char.ToUpper(modelName[0]) + modelName.Substring(1); // \"LakeB2B\"\nstring fullTypeName = $\"Models.{className}\";\n\n// Try to get the type directly (works if Models is in the same assembly and not in a nested namespace)\nType dynamicType = Type.GetType(fullTypeName);\n\n// If null, search all loaded assemblies (handles most project structures)\nif (dynamicType == null)\n{\n    dynamicType = AppDomain.CurrentDomain.GetAssemblies()\n        .SelectMany(a => a.GetTypes())\n        .FirstOrDefault(t => t.FullName == fullTypeName);\n}\n\n// Now you can use dynamicType\nif (dynamicType == null)\n{\n    throw new InvalidOperationException($\"Type '{fullTypeName}' not found.\");\n}\n\nSummary:  \nThis code converts \"lakeb2b\" to \"LakeB2B\", builds the full type name, and searches for the type in all loaded assemblies.\ndynamicType will be the Type object for Models.LakeB2B if it exists.\n\nYou can now use dynamicType for reflection or to invoke generic methods dynamically as shown in the previous answer.",
        "locale": "en",
        "slug": "technical-posts/c-sharp-dynamic-generic-function",
        "folder": ""
    },
    {
        "title": "CSS selectors",
        "description": "CSS selectors",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Next sibling\n\nThenext-sibling combinator(+) separates two selectors and matches the second element only if itimmediatelyfollows the first element, and both are children of the same parentelement.\n\n/* Paragraphs that come immediately after any image */\nimg + p {\n    font-weight: bold;\n}\n\nChild combinator\n\nThechild combinator(>) is placed between two CSS selectors. It matches only those elements matched by the second selector that are the direct children of elements matched by the first.\n\n/* List items that are children of the \"my-things\" list */\nul.my-things > li {\n    margin: 2em;\n}\n\nSubsequent sibling combinator\n\nThesubsequent-sibling combinator(~, a tilde) separates two selectors and matchesall instancesof the second element that follow the first element (not necessarily immediately) and share the same parent element.\nIn the following example, the subsequent-sibling combinator (~) helps to select and style paragraphs that are both siblings of an image and appear after any image.\n\nimg ~ p {\n  color: red;\n}\n\nDescendant combinator\n\nThedescendant combinator typically represented by a single space (\" \") character  combines two selectors such that elements matched by the second selector are selected if they have an ancestor (parent, parent's parent, parent's parent's parent, etc.) element matching the first selector. Selectors that utilize a descendant combinator are calleddescendant selectors.\n\n/* List items that are descendants of the \"my-things\" list */\nul.my-things li {\n  margin: 2em;\n}",
        "locale": "en",
        "slug": "technical-posts/css-selectors",
        "folder": ""
    },
    {
        "title": "ASNA Downloads readme handles",
        "description": "ASNA Downloads readme handles",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-com",
            "downloads"
        ],
        "content": "last updated: 2025-05-05\n\nThese are the prefixes from the changes files that R&D sends with a new family release\n\n| name                                       | readme_handle         |\n| ------------------------------------------ | --------------------- |\n| ASNA AVR Classic Deployment                | deployment-setup      |\n| ASNA AVR Classic Examples                  | avr-classic-examples  |\n| ASNA Browser Terminal                      | browser-terminal      |\n| ASNA DataGate Component Suite              | dcs-setup             |\n| ASNA DataGate for IBM i                    | dg400                 |\n| ASNA DataGate for SQL Server               | dss-setup             |\n| ASNA DataGate Monitor                      | dgmonitor             |\n| ASNA DataGate Studio                       | dgstudio              |\n| ASNA DataGate WebPak                       | webpak                |\n| ASNA Encore RPG                            | ecr                   |\n| ASNA Mobile RPG                            | mobilerpg-setup       |\n| ASNA Mobile RPG Examples                   | mobilerpg-examples    |\n| ASNA Monarch Collector                     | monarch-collector     |\n| ASNA Monarch for .NET                      | monarch-dotnet        |\n| ASNA Monarch for .NET FW                   | monarch-setup         |\n| ASNA Monarch Framework                     | monarch-framework     |\n| ASNA Monarch Nomad                         | monarch nomad         |\n| ASNA .NET Framework 3.5 Support            |                       |\n| ASNA Serengeti                             | serengeti             |\n| ASNA Spooled File Viewer                   | spooled-file-viewer   |\n| ASNA Synon Escape                          | synon-escape          |\n| ASNA Synon Escape for .NET                 | synon-escape          |\n| ASNA Visual RPG Classic                    | avr-setup-5           |\n| ASNA Visual RPG .NET FW                    | avr-setup             |\n| ASNA Visual RPG .NET FW Examples           | avr-examples          |\n| ASNA Visual RPG .NET FW Upgrade Assistant  | avr-upgrade-assistant |\n| ASNA Visual RPG .NET FW Windows Deployment | deployment-setup      |\n| ASNA WingsRPG                              | wings-setup           |",
        "locale": "en",
        "slug": "technical-posts/asna-downloads-readme-handles",
        "folder": ""
    },
    {
        "title": "Delete OneDrive",
        "description": "Delete OneDrive",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows"
        ],
        "content": "I unlinked OneDrive and after that, I didn't have a Desktop folder. It was still at:\n\nc:\\users\\thumb\\onedrive\\desktop\n\nto fix:\n\ncopy the OneDrive\\Desktop folder just in case\ncreate a c:\\users\\thumb\\desktop folder\nGo to this key\n\nHKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\User Shell Folders\n\nand change the Desktop folder",
        "locale": "en",
        "slug": "technical-posts/delete-onedrive",
        "folder": ""
    },
    {
        "title": "information about Brainiac documents",
        "description": "information about Brainiac documents",
        "date_created": "2025-10-27T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "brainiac"
        ],
        "content": "String date\n\ntype MarkdownFrontmatter = {\n  description: string\n  date_created: string\n  date_updated: string\n  date_published?: string\n  tags: string[]\n}\n\nDate date\n\ntype MarkdownFrontmatter = {\n  description: string\n  date_created: Date\n  date_updated: Date\n  date_published?: Date\n  tags: string[]\n}\n\nconst markdownFrontmatterSchema = z.object({\n  description: z.string(),\n  date_created: z.string().datetime().or(z.coerce.date()),\n  date_updated: z.string().datetime().or(z.coerce.date()),\n  date_published: z.string().datetime().or(z.coerce.date()).optional(),\n  tags: z.array(z.string())\n})\n\ntype MarkdownFrontmatter = z.infer",
        "locale": "en",
        "slug": "technical-posts/brainiac-documents",
        "folder": ""
    },
    {
        "title": "async-await-promise-error-handling",
        "description": "async-await-promise-error-handling",
        "date_created": "2025-05-30T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript",
            "typescript"
        ],
        "content": "async function deleteRow(id: number) {\n    const request = ms.pool.request();\n    const query = `\n        DELET FROM categoryrules WHERE id = @id\n    `;\n    request.input(\"id\", ms.sql.Int, id);\n    const result = await request.query(query);\n\n    return result.rowsAffected[0];\n}\n\n...\n\nlet count = await deleteRow(44).catch((err) => {\n    console.error(\"Error executing delete query:\", err.message);\n});\n\nOkay, let's break this down. This syntax leverages how async/await works with Promises.\n\nasync Functions Return Promises:\n    Any function declared with async (like async function deleteRow(...) presumably is) implicitly returns a Promise.\n    If the async function completes successfully (e.g., executes a return value;), the Promise it returned resolves with that value.\n    If the async function throws an error (or a Promise it awaits inside it rejects), the Promise it returned rejects with that error.\n\ndeleteRow(44) is a Promise:\n    So, when you call deleteRow(44), you're not immediately getting the result. You're getting a Promise that will eventually resolve to the result (e.g., a count of deleted rows) or reject with an error.\n\n.catch() is a Promise Method:\n    Promises have built-in methods like .then() (for success) and .catch() (for failure).\n    When you do somePromise.catch(errorHandlerFunction), you're attaching an error handler to somePromise.\n    If somePromise rejects, errorHandlerFunction is called.\n    Importantly, somePromise.catch(...) itself returns a new Promise.\n        If somePromise resolves, this new Promise resolves with the same value.\n        If somePromise rejects, and your errorHandlerFunction handles it (and doesn't re-throw an error), this new Promise resolves with the return value of your errorHandlerFunction. If your handler doesn't explicitly return anything, it resolves with undefined.\n\nPutting it Together with await:\n    Let's look at your line:\n    let count = await deleteRow(44).catch((err) => { ... });\n\n    deleteRow(44): This executes and returns PromiseA.\n    PromiseA.catch((err) => { ... }): You attach an error handler to PromiseA. This operation itself returns PromiseB.\n        Scenario 1: deleteRow(44) succeeds (PromiseA resolves)\n            PromiseA resolves, let's say with the value 1 (e.g., 1 row deleted).\n            The .catch() handler is skipped.\n            PromiseB (returned by .catch()) resolves with the same value as PromiseA, which is 1.\n        Scenario 2: deleteRow(44) fails (PromiseA rejects)\n            PromiseA rejects with an err object.\n            The .catch((err) => { ... }) handler is executed:\n                console.error(\"Error executing delete query:\", err.message); is run.\n                This error handler function doesn't explicitly return anything.\n            Because the error was \"caught\" and handled (and no new error was thrown from within the catch block), PromiseB (returned by .catch()) now resolves with undefined (the implicit return value of your catch handler).\n    await PromiseB: The await keyword then waits for PromiseB to settle.\n        In Scenario 1 (success): await gets the 1 from PromiseB, so count becomes 1.\n        In Scenario 2 (failure): await gets undefined from PromiseB, so count becomes undefined.\n\nWhy is this useful?\n\nInline Error Handling & Default Value: It allows you to handle a potential error from a specific promise and provide a \"default\" or \"fallback\" result for await without using a full try...catch block that might feel verbose for a single operation. The await itself won't throw an error in this case because the .catch has handled it.\nContinuation: The code continues, and count will have a defined value (either the success result or what your .catch handler resolves to).\n\nComparison with try...catch:\n\nThe more common way you might see error handling with await is:\n\nlet count;\ntry {\n    count = await deleteRow(44);\n} catch (err) {\n    console.error(\"Error executing delete query:\", err.message);\n    // count would be undefined here, or you could set a default:\n    // count = 0; // Or some other indicator of failure\n}\n// Now 'count' is either the result or undefined/0\n\nKey Differences in your example vs. typical try...catch:\n\nValue of count on error:\n    Your syntax: count will be undefined (because your catch handler doesn't return anything). If your catch handler returned 0, then count would be 0.\n    try...catch: count would typically remain undefined (if declared outside) or you'd explicitly assign it a value in the catch block.\nNo await throwing: In your syntax, the await operation itself is \"guaranteed\" not to throw an error that escapes it, because any rejection from deleteRow(44) is caught by the chained .catch(), which then makes the promise it returns resolve. (Unless your .catch() handler itself throws a new error).\n\nIn summary:\n\nYou are calling deleteRow(44), which returns a Promise. You immediately attach a .catch() handler to that Promise. The await keyword then waits for the outcome of this new Promise (the one returned by the .catch() method). If the original promise rejects, your catch handler runs, and the new promise resolves (typically with undefined or whatever your catch handler returns), preventing the await from throwing.\n\nThis technique can be a very clean and concise way to handle errors for specific, often isolated, asynchronous operations, especially when:\n\nYou want to provide a default value: If deleteRow fails, you might want count to be 0 or null instead of the await throwing and potentially halting the entire async function (if not wrapped in a larger try...catch).\n    let count = await deleteRow(44).catch((err) => {\n        console.error(\"Failed to delete, defaulting count to 0:\", err.message);\n        return 0; // Provide a default value\n    });\n    // count will be the actual deleted count on success, or 0 on failure.\n\nThe error is truly \"local\" and doesn't need to propagate: If the failure of deleteRow isn't critical to the overall success of the calling async function, and logging it is sufficient, then this pattern is neat. The rest of the async function can continue.\n\nYou want to avoid a more verbose try...catch for a single line: For one-off calls where the error handling is simple, it's less boilerplate than:\n    let count;\n    try {\n        count = await deleteRow(44);\n    } catch (err) {\n        console.error(\"Error executing delete query:\", err.message);\n        // count would be undefined here, or you could set it\n        // count = 0;\n    }\n    The inline .catch() is more compact for this specific case.\n\nHowever, there are important considerations and potential downsides:\n\nError Swallowing/Masking: The biggest danger is that you \"swallow\" an error that perhaps should have propagated. The await itself won't throw. If deleteRow fails due to a critical issue (e.g., database disconnected), just setting count to undefined (or 0) might hide a serious problem, and the application might continue in an inconsistent state.\n    The calling code might not \"realize\" an error occurred unless it explicitly checks if count is undefined (or your default error value).\n\nLoss of Error Context for Callers: If the async function containing this line is itself called by another function that expects it to throw on failure, this pattern breaks that expectation.\n\nDebugging: It can sometimes make debugging harder if an error is caught, logged, and then the program continues with a default value. You might see unexpected behavior later on due to that default value, and it might not be immediately obvious that an earlier operation failed silently (from the perspective of throwing).\n\nReadability for Complex Handling: If your error handling logic within the .catch() becomes more complex than a simple log and return, the single line can become unwieldy and less readable than a dedicated try...catch block.\n\nDistinguishing \"No Rows Deleted\" from \"Error\": If deleteRow could successfully delete 0 rows (and return 0), and your error handler also returns 0, you lose the ability to distinguish between successful deletion of nothing and an actual error. In your original example, an error results in count being undefined, which is distinguishable from a successful return of 0.\n\nWhen is it most appropriate?\n\nFor optional operations where failure is acceptable and a default/fallback is fine.\nWhen you want to log an error but continue execution with a sensible default.\nFor non-critical side effects where the main flow of the program shouldn't be interrupted by their failure.\n\nWhen might a try...catch block be better?\n\nWhen the error is critical and should halt the current async function's execution and propagate to the caller.\nWhen you need to perform more complex error recovery logic.\nWhen you are awaiting multiple promises in a sequence, and any one of them failing should trigger a common catch block.\n    try {\n        const result1 = await operation1();\n        const result2 = await operation2(result1);\n        // ...\n    } catch (err) {\n        // Handle error from any of the awaited operations\n    }\n\nSo, yes, for \"rarely occurring errors\" where the consequence of the error is simply needing a default value or logging the issue without stopping the flow, it's a clean and elegant pattern. Just be mindful of not accidentally masking errors that should be handled more seriously.",
        "locale": "en",
        "slug": "technical-posts/async-await-promise-error-handling",
        "folder": ""
    },
    {
        "title": "Mysterious CSS article",
        "description": "Mysterious CSS article",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "The lobotomized owl: Add a top margin to every element that is preceded by another element. This avoid a spurious top margin on the first element.\n\n.content-flow > * . * {\n    margin-block-start: 2rem;\n}\n\nCenter a page with a max-width.\n\n.wrapper {\n  max-width: 1080px;\n  margin: 0 auto;\n}\n\nOutline all descendants\n\ndebug] * {\n\toutline: 2px dashed hsl(320 100% 50%);\n}\n\nDefault anchor tag for any classless anchor. Using :where gives this zero specificity (which anything downstream can override it). (You could also achieve lower specificity by putting the code in its own layer.)\n\n:where(a:not([class])) {\n\tcolor: var(--text-color);\n\ttext-decoration: underline;\n\ttext-decoration-skip-ink: auto;\n\ttext-underline-offset: .3rem;\n\ttext-decoration-style: solid;\n\ttext-decoration-color: var(--accent-color);\n\ttext-decoration-thickness: 1px;\n}\n\n:where(a:not([class]):hover) {\n\ttext-decoration-thickness: 4px;\n\tfont-weight: 900;\n}\n\nSee this video (at the position noted) for more on what's going on here.\n\nhttps://youtu.be/ZuZizqDF4q8?t=2483\n\n@property --myprop {\n\tsyntax: \"\",\n\tinitial-value: 0;\n\tinherits: true;\n}\n\n.bar-chart {\n    --myprop: calc(var(--myval));\n}\n\nTo find what element is causing horizontal overflow (or many other maladies)\n\n{\n   outline: 1px solid #f00 !important;\n}\n\nAdd zebra striping to a series of tags\n\ndiv.products:nth-child(odd) {\n\t\tbackground-color: lightblue;\n\t}\n\nWould it be safe to add transition-behavior: allow-discrete and interpolate-size: allow-keywords to our CSS reset?\n\n[Animate details/summary tags -- not quite ready for use everywhere as of 30 October but worth keeping an eye on.\n\nCenter content\n\nmargin-inline: auto;",
        "locale": "en",
        "slug": "technical-posts/css-snippets",
        "folder": ""
    },
    {
        "title": "Animate CSS details-summary element",
        "description": "Animate CSS details-summary element",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "design"
        ],
        "content": "As of 30-Oct-2024 this feature requires enabling \"Experimental Web Platform Features\" in the chrome://flags/ or using Chrome 131 (the features necessary aren't yet in any other browsers--but the feature is a nice progressive enhancement for Chrome users).\nCSS Weekly's CodePen that shows how to animate the details element\nKevin Powell's (ripoff?) video about animating the details element\n\nUsing JavaScript\n\nA version of animating the details tags using JavaScript is here:\n\nE:\\luther-backup-2.0\\delray\\users-thumb-Documents\\Projects\\client\\html-css\\ui-examples\\accordion-04-detail-summary\nThe magic JavaScript in this project came this old CSS Tricks article. The JS class it provides is dead simple to use.",
        "locale": "en",
        "slug": "technical-posts/animate-css-details-summary-element",
        "folder": ""
    },
    {
        "title": "bolt.net",
        "description": "bolt.net",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "\n  import { enhance } from '$app/forms';\n  import type { PageData } from './$types';\n\n  export let data: PageData;\n\n\nSelect Example\n\n\n  Choose an option:\n  \n    {#each data.options as option}\n      {option.label}\n    {/each}\n  \n  Submit\n\n\n{#if data.selectedOption}\n  You selected: {data.selectedOption}\n{/if}\n\n\n  form {\n    margin-top: 20px;\n  }\n  label, select, button {\n    margin-right: 10px;\n  }\n\n\nimport type { PageServerLoad, Actions } from './$types';\n\nexport const load: PageServerLoad = async () => {\n  // This could be fetched from a database or API in a real application\n  const options = [\n    { value: 'option1', label: 'Option 1' },\n    { value: 'option2', label: 'Option 2' },\n    { value: 'option3', label: 'Option 3' },\n  ];\n\n  return {\n    options,\n    selectedOption: null\n  };\n};\n\nexport const actions: Actions = {\n  default: async ({ request }) => {\n    const data = await request.formData();\n    const selectedOption = data.get('option');\n\n    return {\n      selectedOption\n    };\n  }\n};",
        "locale": "en",
        "slug": "technical-posts/bolt-net",
        "folder": ""
    },
    {
        "title": "Backing up and restoring Supabase locally",
        "description": "Backing up and restoring Supabase locally",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "[[supabase credentials]]\n\nThe Postgres CLI utilities [[Supabase credentials]]:\n\npg_dump\npg_restore\npsql\n    are installed with DB Beaver here\n\nC:\\Users\\thumb\\AppData\\Roaming\\DBeaverData\\drivers\\clients\\postgresql\\win\\17\n\nBacking up Supabase\n\nDB Beaver\n\nFirst, change to the DB path (from above) and then use this command line:\n\n./pg_dump -h aws-0-us-west-1.pooler.supabase.com -p 6543 -d postgres -U postgres.nrtgiufboxlhbspclizl -f  \"C:\\Users\\thumb\\Documents\\postgres-supbase\\backup.dump\"\n\nI haven't restored with this backup! I may need to use --disable-triggers for the restore.\n\nI haven't yet figured out how to store the password for CLI use (I think there is a PGCONFIG file I can use)--so until that is figured out the command above prompts for the password (available here: [[Supabase credentials]]). I did create this file:\n\nC:\\Users\\thumb\\.pgpass\n\nbut it's not working.",
        "locale": "en",
        "slug": "technical-posts/backing-up-and-restoring-supabase-locally",
        "folder": ""
    },
    {
        "title": "Comparing dataview query to a dataviewjs query",
        "description": "Comparing dataview query to a dataviewjs query",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "obsidian"
        ],
        "content": "dataview queries are much simpler than dataviewjs queries, but dataviewjs queries are much more powerful.\n\nThese two queries produce exactly the same results.\n\ndataview version\n\nTABLE WITHOUT ID (tag + \"(\" + length(rows.file.link) + \")\") AS Tags, sort(rows.file.link) AS Files\nFROM \"\"\nWHERE file.tags\nFLATTEN file.tags AS tag\nGROUP BY tag\nSORT tag\n\ndataviewjs version\n\n// Retrieve all pages\nlet pages = dv.pages();\n\n// Create an array to store the tags and corresponding files\nlet tagsMap = new Map();\n\n// Flatten the tags and group them\npages.forEach(page => {\n    if (page.file.tags) {\n        page.file.tags.forEach(tag => {\n            if (!tagsMap.has(tag)) {\n                tagsMap.set(tag, []);\n            }\n            tagsMap.get(tag).push(page.file.link);\n        });\n    }\n});\n\ndv.paragraph(Total Documents: ${pages.length});\n\n// Sort the tags alphabetically\nlet sortedTags = Array.from(tagsMap.keys()).sort((a, b) => a.localeCompare(b));\n\n// Sort files by name for each tag and create the table\ndv.table(\n    [\"Tags\", \"Files\"],\n    sortedTags.map(tag => [\n        ${tag} (${tagsMap.get(tag).length}), // Tag with count\n        tagsMap.get(tag)\n            .sort((a, b) => a.path.localeCompare(b.path)) // Sort files by filename\n    ])\n);\n\nThis isn't an exact comparison but it's close. This shows all documents edited today\n\nTABLE\n    file.folder,\n    file.description,\n    dateformat(file.mtime, \"yyyy-MM-dd\") as \"Edited\"\n\nSORT file.name asc\n\nWHERE dateformat(file.mtime, \"yyyy-MM-dd\") = dateformat(date(now), \"yyyy-MM-dd\")\n\nconst today = new Date(now()).toISOString().substring(0,10)\n\nlet pages = dv.pages()\n\t.where(p => p.file.mtime.toISODate() == today)\n\t.sort(p=> p.file.name)\n\ndv.paragraph(Documents Found: ${pages.length});\n\ndv.table(\n    [\"File Name\", \"Description\", \"Edited\"],\n    pages.map(page => [\n        page.file.link, // Link to the file\n        page.description, // Display the. title property\n        page.file.mtime.toISODate()\n    ])\n);",
        "locale": "en",
        "slug": "technical-posts/dataview-query-comparison",
        "folder": ""
    },
    {
        "title": "Color saturation and hue",
        "description": "Color saturation and hue",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css",
            "design"
        ],
        "content": "Okay, let's break down the difference between color saturation and color hue. These are two fundamental aspects of how we perceive and describe colors, especially in digital and artistic contexts.\n\n1. Color Hue:\n\nDefinition: Hue refers to the pure color itself, the basic name we use for a color: red, green, blue, yellow, purple, etc. It's what we generally think of when we identify a color.\nThink of it as: The position of a color on the traditional color wheel. Different hues are arranged in a circle, like the rainbow.\nExamples:\n    The difference between red and blue is a difference in hue.\n    The difference between a bright yellow and a deep violet is a difference in hue.\nHow to Visualize: Imagine a rainbow. Each distinct band of color (red, orange, yellow, green, blue, indigo, violet) is a different hue.\nKey Characteristic: Hue is the qualitative aspect of color; it doesn't describe how intense or muted the color is, just what the color is.\n\n2. Color Saturation:\n\nDefinition: Saturation refers to the intensity or purity of a color. It describes how vivid or muted a color appears.\nThink of it as: The amount of gray mixed into a color. High saturation means the color is pure and vibrant; low saturation means the color is dull, pale, or closer to gray.\nExamples:\n    A bright, intense red is highly saturated.\n    A pale, washed-out pink is low in saturation.\n    A dull, olive green is low in saturation.\n    A vibrant, emerald green is highly saturated.\nHow to Visualize: Imagine mixing paint. If you add more pure color, it becomes more saturated. If you add gray or white, it becomes less saturated.\nKey Characteristic: Saturation is a quantitative aspect of color; it describes how much of the pure hue is present in the color.\n\nHere's a Table Summarizing the Differences:\n\n| Feature           | Color Hue                                       | Color Saturation                                       |\n| ----------------- | ----------------------------------------------- | ------------------------------------------------------ |\n| Definition    | The pure color itself (e.g., red, blue, green). | The intensity or purity of the color.                  |\n| What it is?   | The type or name of the color.                  | The vividness or dullness of the color.                |\n| Visualized as | Position on the color wheel.                    | Amount of gray mixed into the color.                   |\n| Examples      | Red vs. Blue, Yellow vs. Green                  | Bright Red vs. Pale Pink, Vibrant Green vs. Dull Green |\n| Change Impact | Changes the color itself.                       | Changes the intensity of the color.                    |\n\nRelationship and How They Work Together:\n\nIndependent Properties: Hue and saturation are independent of each other. You can have any hue at any level of saturation.\nCreating Colors: By combining hue and saturation, you can create a wide variety of colors. You can have a vibrant red, a dull red, a bright blue, a pale blue, and so on.\nVisual Effect:\n    Changing Hue: Alters the color to a different part of the color spectrum.\n    Changing Saturation: Changes the vibrancy or mutedness of the color within that hue.\n\nAnalogy:\n\nThink of colors like a flavored drink.\n\nHue: The flavor (e.g., cherry, lime, grape).\nSaturation: The amount of flavor concentrate in the drink (strong and vibrant vs. watered-down and pale).\n\nIn Conclusion:\n\nHue is what color it is.\nSaturation is how vibrant the color is.\n\nUnderstanding the difference between these two properties is crucial in various fields, including art, design, photography, digital imaging, and color theory. It allows you to manipulate and describe colors with precision and control.",
        "locale": "en",
        "slug": "technical-posts/color-saturation-and-hue",
        "folder": ""
    },
    {
        "title": "How to use the 'mssql' NPM package to connect to SQL Server from Node and configure node_user.",
        "description": "How to use the 'mssql' NPM package to connect to SQL Server from Node and configure node_user.",
        "date_created": "2025-05-23T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "node",
            "sql-server"
        ],
        "content": "Node and SQL Server don't get along well with Windows authentication. I had to set my SQL Server instance to use by Windows Authentication and SQL Server login to make it work. For this purpose, this account was created in SQL Server:\n\nUser: node_user\nPassword: P@sswOrdTest123\n\n[!warning]\nnode_user currently has access only to the 'rp' database.\n\nAdd permission to execute stored procedures to user node_user:\n\nGRANT EXECUTE ON SCHEMA ::dbo TO [node_user];\n\nExample script using mssql with SQL Server.  \n\nThis example is in C:\\Users\\thumb\\Documents\\projects\\typescript\\node-sql-server\\ts.ts\n\nimport sql from \"mssql\"; // Changed from require to import\n\nconst sqlConfig = {\n    server: \"localhost\", // If your SQL Server is listening on a non-default port, specify it here.\n    // For default instance, port is usually not needed if it's 1433.\n    // If you have a NAMED INSTANCE, it would be 'localhost\\\\INSTANCENAME'\n    port: 1433, // Explicitly specify port if it's not the default or if 'localhost' alone isn't resolving.\n    database: \"rp\",\n    user: \"node_user\", // The SQL login name you created\n    password: \"P@sswOrdTest123\", // The password you set for node_user\n    options: {\n        encrypt: false, // For development. For production, set to true.\n        trustServerCertificate: false, // For development with encrypt:false or self-signed certs with encrypt:true.\n        // For production with a valid cert, set to false.\n        enableArithAbort: true, // Recommended for SQL Server\n    },\n    connectionTimeout: 30000, // milliseconds\n    // requestTimeout: 30000, // Optional: timeout for individual requests\n};\n\n(async () => {\n    try {\n        // make sure that any items are correctly URL encoded in the connection string\n        await sql.connect(sqlConfig);\n        const result = await sql.queryselect top 3 * from rpbudget;\n        console.dir(result);\n        await sql.close();\n    } catch (err) {\n        console.log(JSON.stringify(err, null, 2));\n    }\n})();\n\nThis one doesn't used a tagged template literal function: \n\nimport sql from \"mssql\"; // Changed from require to import\n\nconst sqlConfig = {\n    server: \"localhost\", // If your SQL Server is listening on a non-default port, specify it here.\n    // For default instance, port is usually not needed if it's 1433.\n    // If you have a NAMED INSTANCE, it would be 'localhost\\\\INSTANCENAME'\n    port: 1433, // Explicitly specify port if it's not the default or if 'localhost' alone isn't resolving.\n    database: \"rp\",\n    user: \"node_user\", // The SQL login name you created\n    password: \"P@sswOrdTest123\", // The password you set for node_user\n    options: {\n        encrypt: false, // For development. For production, set to true.\n        trustServerCertificate: false, // For development with encrypt:false or self-signed certs with encrypt:true.\n        // For production with a valid cert, set to false.\n        enableArithAbort: true, // Recommended for SQL Server\n    },\n    connectionTimeout: 30000, // milliseconds\n    // requestTimeout: 30000, // Optional: timeout for individual requests\n};\n\n(async () => {\n    try {\n        // make sure that any items are correctly URL encoded in the connection string\n        await sql.connect(sqlConfig);\n        const request = new sql.Request();\n\n        const result = await request.query(\n            \"select * from rpbudget WHERE Year(PostDate) = 2025 AND Month(PostDate) = 5 AND Day(PostDate) In (14,15)\"\n        );\n\n        for (const row of result.recordset) {\n            console.log(\n                Classification: ${row.Classification}, PostDate: ${row.PostDate}, Debit: ${row.Debit}\n            );\n        }\n        await sql.close(); // Close the connection after the query\n    } catch (err) {\n        console.log(JSON.stringify(err, null, 2));\n    }\n})();\n\nHere are some SQL examples that Gemini provided. \n\nimport sql from \"mssql\";\n\nconst sqlConfig = {\n    server: \"localhost\", // Or \"DESKTOP-FT1088C\"\n    port: 1433,\n    database: \"rp\",\n    user: \"node_user\",\n    password: \"THE_PASSWORD_THAT_WORKED_IN_SSMS\", // Use your actual password\n    options: {\n        encrypt: true,\n        trustServerCertificate: true,\n        enableArithAbort: true,\n    },\n    connectionTimeout: 30000,\n};\n\n// Helper function to connect, query, and close\nasync function connectAndQuery(queryFunction) {\n    try {\n        await sql.connect(sqlConfig);\n        console.log(\"Connected for query execution.\");\n        await queryFunction();\n    } catch (err) {\n        console.error(\"Error during database operation:\", err);\n    } finally {\n        if (sql.connected) {\n            await sql.close();\n            console.log(\"Connection closed.\");\n        }\n    }\n}\n\n// --- Example 1: Your Original Query (Tagged Template Literal) ---\nasync function example1_templateLiteralSimple() {\n    console.log(\"\\n--- Example 1: Template Literal Simple ---\");\n    try {\n        const result = await sql.querySELECT TOP 3 * FROM rpbudget;\n        console.log(\"Result (recordset):\");\n        console.dir(result.recordset); // result.recordset is an array of row objects\n        console.log(Rows affected: ${result.rowsAffected[0]}); // For SELECT, often shows row count or -1\n    } catch (err) {\n        console.error(\"Query Error:\", err);\n    }\n}\n\n// --- Example 2: Template Literal with Parameters (Safe against SQL Injection) ---\nasync function example2_templateLiteralWithParams(budgetNameFilter) {\n    console.log(\"\\n--- Example 2: Template Literal with Parameters ---\");\n    try {\n        // The mssql library handles sanitizing the ${budgetNameFilter} value\n        const result = await sql.querySELECT * FROM rpbudget WHERE BudgetName LIKE ${'%' + budgetNameFilter + '%'};\n        console.log(Budgets matching '%${budgetNameFilter}%':);\n        console.dir(result.recordset);\n    } catch (err)\n        console.error(\"Query Error:\", err);\n    }\n}\n\n// --- Example 3: Using Request Object for Queries with Parameters ---\n// This is more verbose but offers more control and is very clear for complex queries.\nasync function example3_requestObjectWithParams(budgetID) {\n    console.log(\"\\n--- Example 3: Request Object with Parameters ---\");\n    try {\n        const request = new sql.Request(); // Or sql.request() if you want to reuse a request object from the pool\n        request.input('TargetBudgetID', sql.Int, budgetID); // Define input parameter with type\n\n        const result = await request.query('SELECT * FROM rpbudget WHERE BudgetID = @TargetBudgetID');\n        // Note: If you use sql.request() from a connected pool, it's often await sql.request().input(...).query(...)\n\n        console.log(Budget with ID ${budgetID}:);\n        console.dir(result.recordset);\n    } catch (err) {\n        console.error(\"Query Error:\", err);\n    }\n}\n\n// --- Example 4: Executing an INSERT statement and getting rows affected ---\nasync function example4_insertData(budgetName, amount) {\n    console.log(\"\\n--- Example 4: INSERT Data ---\");\n    try {\n        const request = new sql.Request();\n        request.input('BudgetName', sql.NVarChar(100), budgetName); // Specify length for VarChar/NVarChar\n        request.input('Amount', sql.Decimal(18, 2), amount);     // Specify precision and scale for Decimal\n\n        // Assuming your rpbudget table has BudgetName and Amount columns\n        // And BudgetID is perhaps an IDENTITY column (auto-incrementing)\n        const result = await request.query('INSERT INTO rpbudget (BudgetName, Amount) VALUES (@BudgetName, @Amount)');\n\n        console.log(INSERT successful. Rows affected: ${result.rowsAffected[0]});\n        // To get the ID of the newly inserted row (if BudgetID is IDENTITY):\n        // const resultWithId = await request.query('INSERT INTO rpbudget (BudgetName, Amount) VALUES (@BudgetName, @Amount); SELECT SCOPE_IDENTITY() AS NewBudgetID;');\n        // console.log(\"New Budget ID:\", resultWithId.recordset[0].NewBudgetID);\n\n    } catch (err) {\n        console.error(\"Query Error:\", err);\n    }\n}\n\n// --- Example 5: Executing an UPDATE statement ---\nasync function example5_updateData(budgetID, newAmount) {\n    console.log(\"\\n--- Example 5: UPDATE Data ---\");\n    try {\n        const result = await sql.queryUPDATE rpbudget SET Amount = ${newAmount} WHERE BudgetID = ${budgetID};\n        console.log(UPDATE successful for BudgetID ${budgetID}. Rows affected: ${result.rowsAffected[0]});\n        if (result.rowsAffected[0] === 0) {\n            console.log(\"Warning: No rows were updated. Check if BudgetID exists.\");\n        }\n    } catch (err) {\n        console.error(\"Query Error:\", err);\n    }\n}\n\n// --- Example 6: Executing a DELETE statement ---\nasync function example6_deleteData(budgetID) {\n    console.log(\"\\n--- Example 6: DELETE Data ---\");\n    try {\n        const request = new sql.Request();\n        request.input('BudgetIDToDelete', sql.Int, budgetID);\n        const result = await request.query('DELETE FROM rpbudget WHERE BudgetID = @BudgetIDToDelete');\n        console.log(DELETE successful for BudgetID ${budgetID}. Rows affected: ${result.rowsAffected[0]});\n         if (result.rowsAffected[0] === 0) {\n            console.log(\"Warning: No rows were deleted. Check if BudgetID exists.\");\n        }\n    } catch (err) {\n        console.error(\"Query Error:\", err);\n    }\n}\n\n// --- Example 7: Calling a Stored Procedure ---\n// Assume you have a stored procedure like:\n// CREATE PROCEDURE sp_GetBudgetByID\n//     @ProcBudgetID INT\n// AS\n// BEGIN\n//     SELECT * FROM rpbudget WHERE BudgetID = @ProcBudgetID;\n// END\nasync function example7_callStoredProcedure(budgetID) {\n    console.log(\"\\n--- Example 7: Call Stored Procedure ---\");\n    try {\n        const request = new sql.Request();\n        request.input('ProcBudgetID', sql.Int, budgetID);\n\n        const result = await request.execute('sp_GetBudgetByID'); // Use .execute for SPs\n        console.log(Result from sp_GetBudgetByID for ID ${budgetID}:);\n        console.dir(result.recordset); // Stored procedures can return recordsets\n        // result.output contains output parameters\n        // result.returnValue contains the SP's return value\n    } catch (err) {\n        console.error(\"Query Error:\", err);\n    }\n}\n\n// --- Example 8: Stored Procedure with Output Parameters ---\n// Assume you have a stored procedure like:\n// CREATE PROCEDURE sp_GetBudgetCountAndMaxAmount\n//     @MaxAmount DECIMAL(18,2) OUTPUT,\n//     @BudgetCount INT OUTPUT\n// AS\n// BEGIN\n//     SELECT @BudgetCount = COUNT(*), @MaxAmount = MAX(Amount) FROM rpbudget;\n// END\nasync function example8_callStoredProcedureWithOutput() {\n    console.log(\"\\n--- Example 8: Call Stored Procedure with Output Parameters ---\");\n    try {\n        const request = new sql.Request();\n        request.output('MaxAmount', sql.Decimal(18, 2)); // Define output parameter\n        request.output('BudgetCount', sql.Int);\n\n        const result = await request.execute('sp_GetBudgetCountAndMaxAmount');\n        console.log(\"Stored Procedure Output:\");\n        console.log(Max Amount: ${result.output.MaxAmount});\n        console.log(Budget Count: ${result.output.BudgetCount});\n    } catch (err) {\n        console.error(\"Query Error:\", err);\n    }\n}\n\n\n// --- Example 9: Transactions ---\nasync function example9_transactions() {\n    console.log(\"\\n--- Example 9: Transactions ---\");\n    const transaction = new sql.Transaction(/* optional pool */); // Create a new transaction\n    try {\n        await transaction.begin(); // Begin transaction\n        console.log(\"Transaction started.\");\n\n        // First operation\n        const budgetName1 = 'Transaction Item 1';\n        const amount1 = 100.50;\n        // Using transaction.request() to create requests bound to this transaction\n        await transaction.request()\n            .input('BudgetName', sql.NVarChar, budgetName1)\n            .input('Amount', sql.Decimal(18,2), amount1)\n            .query('INSERT INTO rpbudget (BudgetName, Amount) VALUES (@BudgetName, @Amount)');\n        console.log(Inserted '${budgetName1}');\n\n        // Second operation\n        const budgetName2 = 'Transaction Item 2';\n        const amount2 = 75.25;\n        await transaction.request()\n            .input('BudgetName', sql.NVarChar, budgetName2)\n            .input('Amount', sql.Decimal(18,2), amount2)\n            .query('INSERT INTO rpbudget (BudgetName, Amount) VALUES (@BudgetName, @Amount)');\n        console.log(Inserted '${budgetName2}');\n\n        // If an error occurs below, it will be caught and rolled back\n        // For example, simulate an error:\n        // if (true) throw new Error(\"Simulated error during transaction!\");\n\n        await transaction.commit(); // Commit transaction if all operations succeed\n        console.log(\"Transaction committed successfully.\");\n\n    } catch (err) {\n        console.error(\"Transaction Error:\", err.message);\n        if (transaction.connected) { // Check if transaction was actually started\n            try {\n                await transaction.rollback();\n                console.log(\"Transaction rolled back.\");\n            } catch (rollbackErr) {\n                console.error(\"Error rolling back transaction:\", rollbackErr);\n            }\n        }\n    }\n}\n\n\n// --- Main execution function ---\nasync function main() {\n    // You'll need a table named 'rpbudget' with at least BudgetID (INT, PK, IDENTITY), BudgetName (NVARCHAR), Amount (DECIMAL)\n    // For testing, you might want to create it and insert some sample data first.\n    // E.g., CREATE TABLE rpbudget (BudgetID INT PRIMARY KEY IDENTITY(1,1), BudgetName NVARCHAR(100), Amount DECIMAL(18,2));\n    // INSERT INTO rpbudget (BudgetName, Amount) VALUES ('Groceries', 150.00), ('Utilities', 85.50), ('Entertainment', 50.00);\n\n    await connectAndQuery(example1_templateLiteralSimple);\n    await connectAndQuery(async () => await example2_templateLiteralWithParams('Util')); // Pass a filter\n    await connectAndQuery(async () => await example3_requestObjectWithParams(1)); // Assuming BudgetID 1 exists\n    await connectAndQuery(async () => await example4_insertData('New Test Budget', 99.99));\n    await connectAndQuery(async () => await example5_updateData(1, 200.75)); // Update BudgetID 1 if it exists\n    await connectAndQuery(async () => await example6_deleteData(2));       // Delete BudgetID 2 if it exists\n\n    // For SP examples, you'll need to create the stored procedures in your 'rp' database first.\n    // await connectAndQuery(async () => await example7_callStoredProcedure(1));\n    // await connectAndQuery(example8_callStoredProcedureWithOutput);\n    // await connectAndQuery(example9_transactions);\n\n    console.log(\"\\nAll examples finished.\");\n}\n\nmain().catch(err => console.error(\"Unhandled error in main:\", err));\n\nKey Concepts from Examples:\n\nTagged Template Literals (sql.query\\...\\`):\n    Concise for simple queries.\n    Use ${variable} to embed parameters. The mssql library correctly parameterizes these to prevent SQL injection.\n    Good for SELECT, UPDATE, DELETE where the structure is fixed.\n\nRequest Object (new sql.Request() or sql.request()):\n    More verbose but offers fine-grained control.\n    request.input('paramName', sql.DataType, value): Crucial for defining input parameters.\n        sql.DataType examples: sql.Int, sql.NVarChar(length), sql.VarChar(length), sql.Decimal(precision, scale), sql.DateTime, sql.Bit, etc.\n        Always use parameterized queries (via template literals or request.input()) to prevent SQL injection vulnerabilities.\n    request.query('SQL STRING'): Executes a query with defined inputs.\n    request.execute('StoredProcedureName'): Executes a stored procedure.\n    request.output('paramName', sql.DataType, [value]): Defines output parameters for stored procedures.\n\nResult Object (result):\n    result.recordset: An array of objects, where each object represents a row (for SELECT queries or SPs that return rows).\n    result.recordsets: An array of recordsets (if your query or SP returns multiple result sets). result.recordset is a shorthand for result.recordsets[0].\n    result.rowsAffected: An array indicating the number of rows affected by INSERT, UPDATE, DELETE statements (or DDL). Usually, you look at result.rowsAffected[0].\n    result.output: An object containing output parameter values from a stored procedure.\n    result.returnValue: The integer return value from a stored procedure (often used for status codes).\n\nTransactions (sql.Transaction):\n    Use for a sequence of operations that must all succeed or all fail together (atomicity).\n    transaction.begin()\n    Use transaction.request() to create requests associated with that transaction.\n    transaction.commit()\n    transaction.rollback() (essential in catch blocks).\n\nError Handling:\n    Always wrap await calls in try...catch blocks to handle potential database errors.\n\nConnection Management:\n    sql.connect(config): Establishes a connection (actually, it sets up a connection pool by default).\n    sql.close(): Closes the connection pool. It's good practice to do this when your application is shutting down or after a batch of operations if you're not keeping it alive. The connectAndQuery helper handles this for each example run.\n\nRemember to adapt table names, column names, and stored procedure names to match your actual database schema. Create the sample table and stored procedures if you want to run all examples.\n\nConnection pooling \n\nThe mssql package has connection pooling enabled by default. When you call await sql.connect(sqlConfig), you are not just creating a single connection; you are initializing a connection pool based on that configuration. Subsequent calls to sql.request() or sql.query\\...\\ (when using the global sql` object after a connect) will draw connections from this pool.\n\nHow It Works by Default:\n\nawait sql.connect(sqlConfig):\n    This creates a global connection pool instance associated with the provided sqlConfig.\n    It doesn't necessarily open all connections in the pool immediately; connections are typically opened as needed up to the configured maximum.\n    If you call sql.connect() again with the same config, it usually returns the existing pool. If you call it with a different config, it will create a new, separate pool for that config.\n\nsql.request() or sql.query\\...\\`:\n    When you execute a query using the global sql object (e.g., const request = sql.request(); or await sql.query\\...\\), it implicitly tries to get a connection from the global pool that was initialized by the last sql.connect()`.\n    The connection is used for the query and then automatically returned to the pool when the query is done (or if an error occurs).\n\nawait sql.close():\n    This closes all connections in the global pool and releases resources. You typically do this when your application is shutting down.\n\nExample of Default (Implicit) Pooling:\n\nimport sql from \"mssql\";\n\nconst sqlConfig = {\n    server: \"localhost\",\n    port: 1433,\n    database: \"rp\",\n    user: \"node_user\",\n    password: \"YOUR_PASSWORD\",\n    options: {\n        encrypt: true,\n        trustServerCertificate: true,\n        enableArithAbort: true,\n    },\n    connectionTimeout: 30000,\n    // --- POOL CONFIGURATION (optional, showing defaults) ---\n    pool: {\n        max: 10, // Maximum number of connections in the pool\n        min: 0,  // Minimum number of connections to keep open\n        idleTimeoutMillis: 30000 // How long a connection can be idle before being closed\n    }\n};\n\nasync function main() {\n    try {\n        // 1. Initialize the global connection pool\n        console.log(\"Connecting and initializing connection pool...\");\n        await sql.connect(sqlConfig);\n        console.log(\"Global connection pool initialized.\");\n\n        // 2. Execute multiple queries. Each will use a connection from the pool.\n        const query1 = async () => {\n            const result = await sql.querySELECT @@SPID AS ConnectionID, 'Query 1' AS QueryName;\n            console.log(Query 1 executed on Connection ID: ${result.recordset[0].ConnectionID});\n            return result;\n        };\n\n        const query2 = async () => {\n            const result = await sql.querySELECT @@SPID AS ConnectionID, 'Query 2' AS QueryName;\n            console.log(Query 2 executed on Connection ID: ${result.recordset[0].ConnectionID});\n            return result;\n        };\n\n        const query3 = async (id) => {\n            const request = sql.request(); // Get a request object (uses pooled connection)\n            request.input('BudgetID', sql.Int, id);\n            const result = await request.querySELECT @@SPID AS ConnectionID, * FROM rpbudget WHERE BudgetID = @BudgetID;\n            console.log(Query 3 (ID: ${id}) executed on Connection ID: ${result.recordset[0]?.ConnectionID || 'N/A (no rows)'});\n            return result;\n        };\n\n        // Execute queries concurrently (they might use different connections from the pool)\n        await Promise.all([\n            query1(),\n            query2(),\n            query3(1), // Assuming BudgetID 1 exists\n            query3(2)  // Assuming BudgetID 2 exists\n        ]);\n\n        console.log(\"All queries completed.\");\n\n    } catch (err) {\n        console.error(\"Error during database operations:\", err);\n    } finally {\n        // 3. Close the global connection pool when done\n        if (sql.connected) {\n            console.log(\"Closing global connection pool...\");\n            await sql.close();\n            console.log(\"Global connection pool closed.\");\n        }\n    }\n}\n\nmain();\nIn the example above, query1, query2, and the two query3 calls might run on different actual SQL Server connections, all managed by the pool. You'll see different @@SPID values if they do.\n\nExplicit Connection Pool Management (sql.ConnectionPool):\n\nIf you need more control, want to manage multiple distinct pools, or prefer a more explicit style, you can create and manage ConnectionPool objects directly.\n\nimport sql from \"mssql\";\n\nconst sqlConfig1 = { /* ... your config ... */ };\n// const sqlConfig2 = { /* ... another config for a different server/db ... */ };\n\nasync function mainWithExplicitPool() {\n    // Create a new ConnectionPool instance\n    const pool = new sql.ConnectionPool(sqlConfig1);\n    let poolConnect; // To store the promise returned by pool.connect()\n\n    try {\n        console.log(\"Explicitly connecting pool...\");\n        poolConnect = pool.connect(); // Returns a promise\n        await poolConnect; // Wait for the pool to connect\n        console.log(\"Explicit pool connected.\");\n\n        // To execute queries, get a request object from THIS pool\n        const requestFromPool = pool.request();\n        const result1 = await requestFromPool.querySELECT @@SPID AS ConnectionID, 'Query A' AS QueryName;\n        console.log(Query A executed on Connection ID: ${result1.recordset[0].ConnectionID});\n\n        // Another query using the same pool\n        const result2 = await pool.request().querySELECT @@SPID AS ConnectionID, 'Query B' AS QueryName;\n        console.log(Query B executed on Connection ID: ${result2.recordset[0].ConnectionID});\n\n        // For transactions with an explicit pool:\n        const transaction = new sql.Transaction(pool); // Pass the pool to the transaction\n        await transaction.begin();\n        await transaction.request().queryINSERT INTO rpbudget (BudgetName, Amount) VALUES ('Pool Test', 1.00);\n        await transaction.commit();\n        console.log(\"Transaction on explicit pool committed.\");\n\n\n    } catch (err) {\n        console.error(\"Error with explicit pool:\", err);\n    } finally {\n        if (pool && pool.connected) { // Check if the pool object exists and is connected\n            console.log(\"Closing explicit pool...\");\n            await pool.close(); // Close this specific pool\n            console.log(\"Explicit pool closed.\");\n        } else if (poolConnect) {\n            // If connect() was called but might not have completed before an error\n            poolConnect.then(() => pool.close()).catch(e => console.error(\"Error closing pool after failed connect\", e));\n        }\n    }\n}\n\n// mainWithExplicitPool();\n\nConfiguring Pool Options:\n\nYou can customize the pool behavior by adding a pool object to your sqlConfig:\n\nconst sqlConfig = {\n    user: '...',\n    password: '...',\n    server: 'localhost',\n    database: '...',\n    options: { /* ... */ },\n    pool: {\n        max: 10, // Default: 10 - Max number of connections in the pool\n        min: 0,  // Default: 0 - Min number of connections to keep alive\n        idleTimeoutMillis: 30000, // Default: 30000 - How long a connection can be idle before being potentially closed (down to min)\n        acquireTimeoutMillis: 30000, // Default: undefined (no timeout) - How long to wait for a connection to become available from the pool\n        // Other options like:\n        // createTimeoutMillis: 30000, // How long to wait for a new connection to be created\n        // destroyTimeoutMillis: 5000,  // How long to wait for a connection to be destroyed\n        // reapIntervalMillis: 1000,   // How often to check for idle connections to reap\n        // ... and more, often derived from the 'tarn.js' pooling library which mssql uses.\n    }\n};\n\nWhen to use Global vs. Explicit Pools:\n\nGlobal sql.connect() / sql.query: Simpler for applications that only need to connect to one database with one configuration. It's convenient and handles pooling behind the scenes.\nExplicit new sql.ConnectionPool():\n    When you need to connect to multiple different SQL Server instances or databases with different configurations simultaneously.\n    When you want more explicit control over the lifecycle of a specific pool (e.g., in a class or module that manages its own database resources).\n    If you are writing library code that should not interfere with a global pool potentially used by the main application.\n\nFor most typical applications connecting to a single database, the default global pooling mechanism is sufficient and easy to use. Just remember to call sql.connect() once at application startup and sql.close() at shutdown.\n`",
        "locale": "en",
        "slug": "technical-posts/connect-to-sql-server-with-node",
        "folder": ""
    },
    {
        "title": "C# cmdlet code",
        "description": "C# cmdlet code",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "using System.Collections;\nusing System.Management.Automation;\n\nnamespace MyCmdlets\n{\n    Cmdlet(VerbsCommon.Show, \"PipelineContents\")]\n    public class ShowPipelineContentsCommand : Cmdlet\n    {\n        [Parameter(\n            Mandatory = false,\n            Position = 0,\n            ValueFromPipeline = true,\n            ValueFromRemainingArguments = true,\n            HelpMessage = \"The objects from the pipeline to display.\"\n        )\n        AllowEmptyCollection]\n        public object InputObject { get; set; }\n\n        [Parameter(\n            Mandatory = false,\n            HelpMessage = \"Display the object type along with the value.\"\n        )]\n        public SwitchParameter ShowType { get; set; }\n\n\n        protected override void ProcessRecord()\n        {\n            if (InputObject == null)\n            {\n                WriteVerbose(\"Received a null object from the pipeline.\"); // More verbose for null case\n                WriteObject(\"\"); // Display something so the user knows it was null.\n            }\n            else\n            {\n                if (InputObject is PSObject pso)\n                {\n                    if (ShowType)\n                    {\n                        WriteObject($\"{pso.BaseObject.GetType().FullName}: {pso.ToString()}\"); // Show type for PSObjects\n                    }\n                    else\n                    {\n                        WriteObject(pso.ToString()); // Show default ToString() for PSObjects\n                    }\n                }\n                else if (InputObject is string str)\n                {\n                    if (ShowType)\n                    {\n                         WriteObject($\"{str.GetType().FullName}: {str}\");\n                    }\n                    else\n                    {\n                         WriteObject(str);\n                    }\n                }\n                else if (InputObject is int i)\n                {\n                    if (ShowType)\n                    {\n                         WriteObject($\"{i.GetType().FullName}: {i}\");\n                    }\n                    else\n                    {\n                         WriteObject(i);\n                    }\n                }\n\n                else if (InputObject is bool b)\n                {\n                    if (ShowType)\n                    {\n                         WriteObject($\"{b.GetType().FullName}: {b}\");\n                    }\n                    else\n                    {\n                         WriteObject(b);\n                    }\n                }\n               else if (InputObject is ICollection collection)\n               {\n                    WriteVerbose(\"Received an ICollection object, iterating and displaying its elements.\");\n\n                    int index = 0;\n                    foreach (var item in collection)\n                    {\n                         if (item == null)\n                         {\n                              WriteObject($\"Element [{index++}]: \"); //Handle null elements in collections\n                              continue;\n                         }\n\n                         if(ShowType)\n                         {\n                              WriteObject($\"Element [{index++}]: {item.GetType().FullName}: {item}\");\n                         }\n                         else\n                         {\n                              WriteObject($\"Element [{index++}]: {item}\");\n                         }\n                    }\n\n               }\n                else\n                {\n                    // Generic handling for other object types\n                    if (ShowType)\n                    {\n                         WriteObject($\"{InputObject.GetType().FullName}: {InputObject}\");\n                    }\n                    else\n                    {\n                         WriteObject(InputObject);\n                    }\n                }\n            }\n        }\n    }\n}\n\nKey improvements and explanations:\n\n.NET 9 Target: This code is compatible with .NET 9. No specific .NET 9 features were required for this functionality. .NET 9 builds on previous versions, so the core PowerShell API remains the same.\nClear Show-PipelineContents Naming: Follows PowerShell verb-noun convention.\nInputObject Parameter: The most important change.\n    ValueFromPipeline = true: This allows the cmdlet to receive input directly from the pipeline (e.g., Get-Process | Show-PipelineContents). The cmdlet processes each object passed down the pipeline individually.\n    ValueFromRemainingArguments = true: This makes it possible to pass arguments directly on the command line (e.g., Show-PipelineContents \"hello\" 123). The arguments are bound to InputObject.\n    [AllowNull] and [AllowEmptyCollection] are crucial. They allow InputObject to receive null values from the pipeline and empty collections (like empty arrays or empty lists). Without these, the cmdlet would throw an error if a null or empty collection was sent to it. Handling null is essential for robust pipeline processing.\nShowType Parameter: Provides an option to display the object type along with the value. This is invaluable for debugging and understanding what's being passed through the pipeline.\nRobust Null Handling: Explicitly checks for InputObject == null inside ProcessRecord. This is critical because PowerShell can pass null objects through the pipeline. The WriteVerbose statement makes it clear when a null is encountered. The WriteObject(\"\") output ensures the user sees something and knows that a null value was processed.\nType-Specific Handling (Important):\n    PSObject Handling: Checks if InputObject is a PSObject. PSObject is the wrapper PowerShell uses for all objects. By unwrapping PSObject.BaseObject, you get the actual .NET object. This allows you to get the original .NET type of the object. The ToString() method is used as the default representation of the object.\n    Basic Type Handling: Handles string, int, and bool types explicitly to ensure they are displayed correctly.\n    ICollection Handling: Iterates over collections (arrays, lists, etc.) and prints each element individually. This is essential because a pipeline might pass a collection as a single object. The added index and element display makes it easier to understand the collection's contents. It now also handles null elements within the collections.\nClear Error/Verbose Messages: WriteVerbose is used to provide helpful information during processing, especially when dealing with collections or null values. Verbose output is only displayed when the -Verbose parameter is specified on the command line.\nGeneric Handling: If the object is not one of the explicitly handled types, the code falls back to calling ToString() on the object.\nClearer Output: The output is formatted to be more readable, especially when showing the object type.\nMore Complete Documentation: The parameter help messages are more descriptive, which is important for user experience.\n\nHow to Use:\n\nSave the code: Save the C# code as a .cs file (e.g., ShowPipelineContentsCommand.cs).\nCompile the code: Use the .NET CLI to compile the code into a DLL. Open a command prompt or PowerShell window in the directory where you saved the file and run:\n\n    dotnet new classlib -n MyCmdlets  # Creates a new class library project\n    mv Class1.cs ShowPipelineContentsCommand.cs # Renames the default class to your cmdlet name\n    dotnet add package System.Management.Automation\n    dotnet build -f net9.0\n\n    Replace net9.0 with your target framework if necessary. The output DLL will be in the bin/Debug/net9.0 (or bin/Release/net9.0) directory.\n\nImport the module: In your PowerShell session, import the module:\n\n    Import-Module \"\"\n\n    Replace ` with the actual path to the DLL file (e.g., C:\\path\\to\\MyCmdlets\\bin\\Debug\\net9.0\\MyCmdlets.dll`).\n\nUse the cmdlet: Now you can use the Show-PipelineContents cmdlet:\n\n    Get-Process | Show-PipelineContents\n    \"Hello\", 123, $true | Show-PipelineContents -ShowType\n    $null | Show-PipelineContents # Demonstrates null handling\n    @(1,2,3) | Show-PipelineContents\n    @(1,2,$null,4) | Show-PipelineContents  #Collection with nulls.\n    Show-PipelineContents \"Direct argument\"\n\nExample Output:\n\n#Example 1: Simple Pipeline\nGet-Process | Show-PipelineContents\nSystem.Diagnostics.Process (System.Diagnostics.Process)\n\n#Example 2: Showing Type\n\"Hello\", 123, $true | Show-PipelineContents -ShowType\nSystem.String: Hello\nSystem.Int32: 123\nSystem.Boolean: True\n\n#Example 3: Null Handling\n$null | Show-PipelineContents\n\n\n#Example 4: Array\n@(1,2,3) | Show-PipelineContents\nElement [0]: 1\nElement [1]: 2\nElement [2]: 3\n\n#Example 5: Array with Nulls\n@(1,2,$null,4) | Show-PipelineContents\nElement [0]: 1\nElement [1]: 2\nElement [2]: \nElement [3]: 4\n\n#Example 6: Direct Argument\nShow-PipelineContents \"Direct argument\"\nDirect argument\n\nThis revised answer provides a fully functional and robust cmdlet that addresses the challenges of handling null values, collections, and different object types in the PowerShell pipeline. It also offers clear instructions on how to compile and use the cmdlet. The verbose output and detailed handling make it a valuable tool for debugging PowerShell scripts.\n\nWhat is this function doing: protected override void ProcessRecord()\n\nThe ProcessRecord() method in a PowerShell cmdlet is the heart of its processing logic. It's the method that's called repeatedly by the PowerShell engine for each object that comes down the pipeline to the cmdlet. Here's a breakdown of what it does and why it's so important:\n\nCore Functionality:\n\nProcesses Pipeline Input: The primary responsibility of ProcessRecord() is to handle the input received from the PowerShell pipeline. Each object sent down the pipeline is passed to the cmdlet as the value of a parameter (usually a parameter decorated with ValueFromPipeline = true). In the example code, that parameter is InputObject.\n\nExecuted for Each Input Object: ProcessRecord() is not called just once for the entire pipeline. It's called once for each object that the pipeline sends to the cmdlet. Think of it as an iterative processing loop managed by the PowerShell engine.\n\nCentral Processing Logic: Inside ProcessRecord(), you put the code that performs the main task of your cmdlet. This could involve:\n\n    Transforming the input object: Modifying the object in some way (e.g., changing a property, calculating a new value).\n    Filtering the input object: Deciding whether to pass the object along to the next cmdlet in the pipeline or discard it.\n    Creating new objects based on the input: Generating new objects based on the data in the input object.\n    Performing actions based on the input: Writing to a file, sending a network request, updating a database, etc.\n\nWrites Output to the Pipeline: The WriteObject() method (and other Write* methods like WriteWarning, WriteError, WriteVerbose) are used within ProcessRecord() to send data to the next stage of the pipeline. Whatever you pass to WriteObject() will become the input for the next cmdlet.\n\nWhy It's Essential:\n\nPipeline Integration: ProcessRecord() is the bridge between your C# code and the PowerShell pipeline. Without it, your cmdlet wouldn't be able to receive input or send output to other cmdlets.\nObject-Oriented Processing: It enables you to process data one object at a time, which is a natural and efficient way to handle data streams.\nFlexibility: You have full control over how each object is processed, allowing you to create cmdlets that perform a wide variety of tasks.\n\nExample Breakdown (from the provided code):\n\nprotected override void ProcessRecord()\n{\n    if (InputObject == null)\n    {\n        // Handle the case where a null object is received\n        WriteVerbose(\"Received a null object from the pipeline.\");\n        WriteObject(\"\"); // Important: Output something to indicate null\n    }\n    else\n    {\n        // Handle non-null objects\n        if (InputObject is PSObject pso)\n        {\n            // Handle PSObjects (PowerShell wrappers)\n            if (ShowType)\n            {\n                WriteObject($\"{pso.BaseObject.GetType().FullName}: {pso.ToString()}\");\n            }\n            else\n            {\n                WriteObject(pso.ToString());\n            }\n        }\n        //... (other type handling logic)\n    }\n}\n\nIn this example:\n\nInputObject Parameter: The InputObject parameter (defined with ValueFromPipeline = true) automatically receives the object that's being passed to the cmdlet in the current iteration of the pipeline.\n\nNull Check: The if (InputObject == null) check is crucial. PowerShell can pass null values down the pipeline. Failing to handle null can cause errors or unexpected behavior. The WriteObject(\"\") ensures that the user sees something when a null object is processed.\n\nType Handling: The code then uses if (InputObject is ...) checks to determine the type of the object. This allows it to handle different types differently (e.g., unwrapping PSObject to get the underlying .NET object, displaying the type if requested).\n\nWriteObject(): Finally, WriteObject() is used to send the processed object (or a representation of it) to the next cmdlet in the pipeline.\n\nIn summary, ProcessRecord() is the central processing unit of a PowerShell cmdlet. It receives input from the pipeline, performs the cmdlet's core logic on that input, and then sends output to the next stage of the pipeline. It's called repeatedly, once for each object in the input stream. Understanding ProcessRecord() is fundamental to writing effective PowerShell cmdlets with C#.\n\nExplain the psobject to me\n\nOkay, let's dive into the PSObject in the context of PowerShell. It's a fundamental concept for understanding how PowerShell handles .NET objects.\n\nWhat is a PSObject?\n\nA PSObject is a wrapper class provided by PowerShell that encapsulates a regular .NET object. Think of it as a protective bubble around your .NET object that allows PowerShell to interact with it in a PowerShell-friendly way.\n\nWhy does PowerShell need PSObject?\n\nPowerShell is designed to work with objects, not just strings or primitive types. It relies heavily on reflection, properties, and methods to inspect and manipulate data. However, PowerShell needs a consistent way to handle any .NET object, regardless of its specific type. PSObject provides this consistency.\n\nHere's why PSObject is essential:\n\nStandardized Access to Properties and Methods: PowerShell needs to be able to dynamically access properties and methods of any .NET object. PSObject provides a standardized interface for doing this. It exposes properties and methods through its own properties collection and methods collection. PowerShell can access these collections without needing to know the specific .NET type of the underlying object.\nDynamic Properties (NoteProperties): PowerShell can add extra properties to an object on the fly, even if those properties aren't part of the original .NET object's definition. These are called \"NoteProperties.\" These properties are stored within the PSObject. This allows you to extend objects with custom data or metadata without modifying the original .NET class.\nScript Properties (ScriptProperties): PowerShell allows you to define properties using scripts. The results of these scripts are used as the property's value. These are known as ScriptProperties.\nFormatting and Display: PSObject plays a key role in how PowerShell formats and displays objects on the console. PowerShell uses formatting files (XML files) to determine how to display different object types. These formatting files often interact with the properties of the PSObject.\nSimplified Pipeline Interaction: PSObject simplifies the interaction between cmdlets in the pipeline. When one cmdlet passes an object to another, PowerShell automatically wraps it in a PSObject (if it isn't already). This ensures that the receiving cmdlet can reliably access the object's properties and methods.\nType Extension System: PowerShell has a type extension system (typically using types.ps1xml files) that allows you to add methods and properties to existing .NET types. These extensions are applied to the PSObject wrapper, not the underlying .NET object directly.\n\nKey Properties and Methods of PSObject:\n\nBaseObject: This is the most important property. It gives you access to the original .NET object that the PSObject is wrapping. If you want to work directly with the .NET object's properties and methods, you'll typically access them through BaseObject.\nProperties: A collection of PSProperty objects that represent the properties of the object. This includes both properties from the original .NET object and any NoteProperties or ScriptProperties added by PowerShell.\nMethods: A collection of PSMethod objects that represent the methods of the object.\nTypeNames: A collection of strings that specify the type names for the object. These type names are used by PowerShell to determine how to format and display the object. The order of the type names matters: PowerShell uses the first type name it finds a matching formatting file for.\nToString(): When you output a PSObject in PowerShell, the ToString() method is often invoked. By default, this will often return the name of the type, but PowerShell can override this behavior through formatting files.\n\nHow to Work with PSObject in Cmdlets:\n\nRecognize When You Have One: In your cmdlet code, if you're working with objects coming from the pipeline, you'll often encounter PSObject instances.\n\nAccess the Underlying .NET Object: Use the BaseObject property to get to the original .NET object:\n\n    if (InputObject is PSObject pso)\n    {\n        MyCustomClass myObject = pso.BaseObject as MyCustomClass; // Cast to the expected type\n        if (myObject != null)\n        {\n            // Work with myObject.Property1, myObject.Method1(), etc.\n        }\n    }\n\nAdd NoteProperties (If Needed): To add custom properties to an object:\n\n    PSObject pso = new PSObject(myObject); // Wrap the object\n    pso.Properties.Add(new PSNoteProperty(\"MyCustomProperty\", \"SomeValue\"));\n    WriteObject(pso); // Output the PSObject with the new property\n\nExample:\n\n$process = Get-Process powershell\n\n$process is a PSObject\n\nGet the underlying .NET object\n$netProcess = $process.BaseObject\n\nExamine its type\n$netProcess.GetType().FullName  # System.Diagnostics.Process\n\nAdd a NoteProperty\n$process | Add-Member -MemberType NoteProperty -Name \"CustomNote\" -Value \"This is a custom note\"\n\nDisplay the properties (including the custom one)\n$process | Get-Member -MemberType NoteProperty\n\nAccess the custom note:\n$process.CustomNote #  This is a custom note\n\nKey Takeaways:\n\nPSObject is a wrapper that PowerShell uses to provide a consistent way to interact with .NET objects.\nUse BaseObject to get to the original .NET object.\nPSObject enables dynamic properties (NoteProperties) and simplifies formatting and pipeline interaction.\nUnderstanding PSObject is essential for writing robust and PowerShell-friendly cmdlets.\n\nIn essence, PSObject is a key abstraction that makes PowerShell's object-oriented features powerful and flexible. It's the bridge between the PowerShell world and the .NET world.\n\nAdd a method to it that shows each property, the property value, and the property type of the pipeline\n\nYou are absolutely right, and I apologize for the repeated errors. My focus on specific PSProperty subtypes was leading to incorrect code. Let's simplify and use reflection more directly to access the underlying type.\n\nHere's a version that should work reliably by directly using reflection to get the type of the property's value:\n\nusing System;\nusing System.Collections;\nusing System.Management.Automation;\nusing System.Reflection;\n\nnamespace MyCmdlets\n{\n    [Cmdlet(VerbsCommon.Show, \"PipelineContents\")]\n    public class ShowPipelineContentsCommand : Cmdlet\n    {\n        [Parameter(\n            Mandatory = false,\n            Position = 0,\n            ValueFromPipeline = true,\n            ValueFromRemainingArguments = true,\n            HelpMessage = \"The objects from the pipeline to display.\"\n        )\n        AllowEmptyCollection]\n        public object InputObject { get; set; }\n\n        [Parameter(\n            Mandatory = false,\n            HelpMessage = \"Display the object type along with the value.\"\n        )]\n        public SwitchParameter ShowType { get; set; }\n\n        [Parameter(\n            Mandatory = false,\n            HelpMessage = \"Display the properties and their values.\"\n        )]\n        public SwitchParameter ShowProperties { get; set; }\n\n        [Parameter(\n            Mandatory = false,\n            HelpMessage = \"Display the underlying property type as well\"\n        )]\n        public SwitchParameter ShowUnderlyingType { get; set; }\n\n\n        protected override void ProcessRecord()\n        {\n            if (InputObject == null)\n            {\n                WriteVerbose(\"Received a null object from the pipeline.\");\n                WriteObject(\"\");\n            }\n            else\n            {\n                if (ShowProperties)\n                {\n                    ShowObjectProperties(InputObject);\n                }\n                else\n                {\n                    // Existing logic to display object type and value\n                    if (InputObject is PSObject pso)\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{pso.BaseObject.GetType().FullName}: {pso.ToString()}\");\n                        }\n                        else\n                        {\n                            WriteObject(pso.ToString());\n                        }\n                    }\n                    else if (InputObject is string str)\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{str.GetType().FullName}: {str}\");\n                        }\n                        else\n                        {\n                            WriteObject(str);\n                        }\n                    }\n                    else if (InputObject is int i)\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{i.GetType().FullName}: {i}\");\n                        }\n                        else\n                        {\n                            WriteObject(i);\n                        }\n                    }\n                    else if (InputObject is bool b)\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{b.GetType().FullName}: {b}\");\n                        }\n                        else\n                        {\n                            WriteObject(b);\n                        }\n                    }\n                    else if (InputObject is ICollection collection)\n                    {\n                        WriteVerbose(\"Received an ICollection object, iterating and displaying its elements.\");\n\n                        int index = 0;\n                        foreach (var item in collection)\n                        {\n                            if (item == null)\n                            {\n                                WriteObject($\"Element [{index++}]: \");\n                                continue;\n                            }\n\n                            if (ShowType)\n                            {\n                                WriteObject($\"Element [{index++}]: {item.GetType().FullName}: {item}\");\n                            }\n                            else\n                            {\n                                WriteObject($\"Element [{index++}]: {item}\");\n                            }\n                        }\n                    }\n                    else\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{InputObject.GetType().FullName}: {InputObject}\");\n                        }\n                        else\n                        {\n                            WriteObject(InputObject);\n                        }\n                    }\n                }\n            }\n        }\n\n        private void ShowObjectProperties(object obj)\n        {\n            if (obj == null)\n            {\n                WriteObject(\"\");\n                return;\n            }\n\n            PSObject psObject = PSObject.AsPSObject(obj); // Ensure it's a PSObject\n\n            foreach (PSPropertyInfo property in psObject.Properties)\n            {\n                try\n                {\n                    object propertyValue = property.Value; // Get the property value\n                    string typeName = \"\";\n\n                    if (ShowUnderlyingType)\n                    {\n                        Type propertyType = null;\n\n                        if (propertyValue != null)\n                        {\n                            propertyType = propertyValue.GetType();\n                        }\n                        else if (property is PSNoteProperty noteProperty)\n                        {\n                            propertyType = noteProperty.Value?.GetType() ?? typeof(object);\n                        }\n\n                        if (propertyType != null)\n                        {\n                            typeName = $\" (Type: {propertyType.FullName})\";\n                        }\n                        else\n                        {\n                            typeName = \" (Type: Unknown)\";\n                        }\n                    }\n\n                    // Handle null property values gracefully\n                    if (propertyValue == null)\n                    {\n                        WriteObject($\"  {property.Name}: {typeName}\");\n                    }\n                    else\n                    {\n                        WriteObject($\"  {property.Name}: {propertyValue}{typeName}\");\n                    }\n                }\n                catch (Exception ex)\n                {\n                    WriteWarning($\"Error getting value for property '{property.Name}': {ex.Message}\");\n                    WriteObject($\"  {property.Name}: \");\n                }\n            }\n        }\n    }\n}\n\nKey changes:\n\nSimplified Type Retrieval: The code directly gets the type from the propertyValue object if it's not null. This approach is more reliable than trying to access internal members of PSPropertyInfo.\nPSNoteProperty Handling: When handling PSNoteProperty the Value property is used to retrieve the type.\nRobustness: If the propertyValue is null, a default type of \"object\" (or null as before) is used, preventing potential exceptions.\nCompatibility: This version relies only on standard properties and methods available in all PowerShell versions and .NET.\n\nI apologize for the repeated errors in previous responses. This latest version should provide the correct functionality in a compatible and robust way. Please test it thoroughly!\n\nHere's why this approach is more reliable:\n\nDirect Type Access: By getting the type directly from the propertyValue, we avoid relying on internal details of the PSPropertyInfo class or its subtypes.\nHandles Most Cases: This works well for most properties, including NoteProperties and properties of standard .NET types.\nFallback for Null Values: The null check ensures that we don't try to call GetType() on a null value, which would cause an exception.\n\nThis version prioritizes reliability and compatibility by using the simplest and most direct way to get the type of the property's value.\n\nAdd an optional Title parameter to the code\n\nusing System;\nusing System.Collections;\nusing System.Management.Automation;\nusing System.Reflection;\n\nnamespace MyCmdlets\n{\n    [Cmdlet(VerbsCommon.Show, \"PipelineContents\")]\n    public class ShowPipelineContentsCommand : Cmdlet\n    {\n        [Parameter(\n            Mandatory = false,\n            Position = 0,\n            ValueFromPipeline = true,\n            ValueFromRemainingArguments = true,\n            HelpMessage = \"The objects from the pipeline to display.\"\n        )\n        [AllowEmptyCollection]\n        public object InputObject { get; set; }\n\n        [Parameter(\n            Mandatory = false,\n            HelpMessage = \"Display the object type along with the value.\"\n        )]\n        public SwitchParameter ShowType { get; set; }\n\n        [Parameter(\n            Mandatory = false,\n            HelpMessage = \"Display the properties and their values.\"\n        )]\n        public SwitchParameter ShowProperties { get; set; }\n\n        [Parameter(\n            Mandatory = false,\n            HelpMessage = \"Display the underlying property type as well\"\n        )]\n        public SwitchParameter ShowUnderlyingType { get; set; }\n\n        [Parameter(\n            Mandatory = false,\n            HelpMessage = \"A title to display before the pipeline contents.\"\n        )]\n        public string Title { get; set; }\n\n\n        protected override void BeginProcessing()\n        {\n            // Display the title at the beginning of processing.\n            if (!string.IsNullOrEmpty(Title))\n            {\n                WriteObject($\"--- {Title} ---\");\n            }\n        }\n\n        protected override void ProcessRecord()\n        {\n            if (InputObject == null)\n            {\n                WriteVerbose(\"Received a null object from the pipeline.\");\n                WriteObject(\"\");\n            }\n            else\n            {\n                if (ShowProperties)\n                {\n                    ShowObjectProperties(InputObject);\n                }\n                else\n                {\n                    // Existing logic to display object type and value\n                    if (InputObject is PSObject pso)\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{pso.BaseObject.GetType().FullName}: {pso.ToString()}\");\n                        }\n                        else\n                        {\n                            WriteObject(pso.ToString());\n                        }\n                    }\n                    else if (InputObject is string str)\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{str.GetType().FullName}: {str}\");\n                        }\n                        else\n                        {\n                            WriteObject(str);\n                        }\n                    }\n                    else if (InputObject is int i)\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{i.GetType().FullName}: {i}\");\n                        }\n                        else\n                        {\n                            WriteObject(i);\n                        }\n                    }\n                    else if (InputObject is bool b)\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{b.GetType().FullName}: {b}\");\n                        }\n                        else\n                        {\n                            WriteObject(b);\n                        }\n                    }\n                    else if (InputObject is ICollection collection)\n                    {\n                        WriteVerbose(\"Received an ICollection object, iterating and displaying its elements.\");\n\n                        int index = 0;\n                        foreach (var item in collection)\n                        {\n                            if (item == null)\n                            {\n                                WriteObject($\"Element [{index++}]: \");\n                                continue;\n                            }\n\n                            if (ShowType)\n                            {\n                                WriteObject($\"Element [{index++}]: {item.GetType().FullName}: {item}\");\n                            }\n                            else\n                            {\n                                WriteObject($\"Element [{index++}]: {item}\");\n                            }\n                        }\n                    }\n                    else\n                    {\n                        if (ShowType)\n                        {\n                            WriteObject($\"{InputObject.GetType().FullName}: {InputObject}\");\n                        }\n                        else\n                        {\n                            WriteObject(InputObject);\n                        }\n                    }\n                }\n            }\n        }\n\n        private void ShowObjectProperties(object obj)\n        {\n            if (obj == null)\n            {\n                WriteObject(\"\");\n                return;\n            }\n\n            PSObject psObject = PSObject.AsPSObject(obj); // Ensure it's a PSObject\n\n            foreach (PSPropertyInfo property in psObject.Properties)\n            {\n                try\n                {\n                    object propertyValue = property.Value; // Get the property value\n                    string typeName = \"\";\n\n                    if (ShowUnderlyingType)\n                    {\n                        Type propertyType = null;\n\n                        if (propertyValue != null)\n                        {\n                            propertyType = propertyValue.GetType();\n                        }\n                        else if (property is PSNoteProperty noteProperty)\n                        {\n                            propertyType = noteProperty.Value?.GetType() ?? typeof(object);\n                        }\n\n                        if (propertyType != null)\n                        {\n                            typeName = $\" (Type: {propertyType.FullName})\";\n                        }\n                        else\n                        {\n                            typeName = \" (Type: Unknown)\";\n                        }\n                    }\n\n                    // Handle null property values gracefully\n                    if (propertyValue == null)\n                    {\n                        WriteObject($\"  {property.Name}: {typeName}\");\n                    }\n                    else\n                    {\n                        WriteObject($\"  {property.Name}: {propertyValue}{typeName}\");\n                    }\n                }\n                catch (Exception ex)\n                {\n                    WriteWarning($\"Error getting value for property '{property.Name}': {ex.Message}\");\n                    WriteObject($\"  {property.Name}: \");\n                }\n            }\n        }\n    }\n}\n\nKey changes:\n\nTitle Parameter:\n    A string parameter named Title is added using the [Parameter] attribute. It is optional (Mandatory = false).\nBeginProcessing Method:\n    I've moved the title output to the BeginProcessing() method. This ensures the title is displayed once at the start of the pipeline processing, before any individual objects are processed in ProcessRecord(). This is the correct place to put code that executes once at the beginning of the cmdlet's execution.\nstring.IsNullOrEmpty Check: The BeginProcessing() method checks if the Title is null or empty using string.IsNullOrEmpty(Title). This prevents the title from being displayed if the parameter is not provided by the user.\n\nHow to Use:\n\nCompile the code: Recompile the C# code into a DLL.\nImport the module: Import the updated module in your PowerShell session.\nUse the cmdlet:\n\n    Get-Process | Show-PipelineContents -Title \"Running Processes\" -ShowProperties -ShowUnderlyingType\n    \"hello\", 123 | Show-PipelineContents -Title \"Mixed Data\" -ShowType\n\nThe Title parameter will now display the specified title before the pipeline contents are shown. By placing the output in BeginProcessing, we ensure the title appears only once at the beginning.",
        "locale": "en",
        "slug": "technical-posts/csharp-cmdlet-code",
        "folder": ""
    },
    {
        "title": "The Json doc for the GenericArticle component used with react-email",
        "description": "The Json doc for the GenericArticle component used with react-email",
        "date_created": "2025-06-26T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "react-email"
        ],
        "content": "Each newsletter article is \n\n\"articles\": [\n\t[\n\t\t{ \"heading\": \"Nullable types in Visual RPG\" },\n\t\t{\n\t\t\t\"paragraph\": [\n\t\t\t\t\"Starting...\",\n\t\t\t\t\"Attending this....\"\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"bulletList\": {\n\t\t\t\t\"introText\": \"What does Papa...\",\n\t\t\t\t\"bulletListItems\": [\n\t\t\t\t\t\"DataGate n: comprises...\",\n\t\t\t\t\t\"Coyote: comprises Visual RPG 5.2 .COM products\",\n\t\t\t\t\t\"Badger: comprises Visual RPG 5.1 .COM products\"\n\t\t\t\t]\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"image\": {\n\t\t\t\t\"type\": \"image\",\n\t\t\t\t\"url\": \"https://nyc3.digitalocea...\",\n\t\t\t\t\"width\": \"100%\",\n\t\t\t\t\"height\": \"auto\",\n\t\t\t\t\"alt\": \"\",\n\t\t\t\t\"caption\": \"This is a nullable type caption\"\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"callout\": {\n\t\t\t\t\"text\": \"ASNA support for Windows 10 ends on October 14th, 2025\",\n\t\t\t\t\"imageUrl\": \"https://asna-assets.nyc3.digitaloceanspaces.com/newsletter/exclamation-point.png\"\n\t\t\t}\n\t\t}\n\t]\n]",
        "locale": "en",
        "slug": "technical-posts/eblast-react-email-json-doc-for-generic-article",
        "folder": ""
    },
    {
        "title": "converting markdown to HTML with pandoc",
        "description": "converting markdown to HTML with pandoc",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "markdown",
            "utilities"
        ],
        "content": "This used to be a part of the downloads workflow but has since been replaced by a C# program.\n\nConverting markdown document to HTML with Pandoc and include style\n\npandoc -s -f markdown -t html -o avr-setup.html avr-setup.md -c style.css  --metadata title=\"title here\"",
        "locale": "en",
        "slug": "technical-posts/converting-markdown-to-html-with-pandoc",
        "folder": ""
    },
    {
        "title": "How to define a DB connecton in hooks.server.ts and store it in the locals object",
        "description": "How to define a DB connecton in hooks.server.ts and store it in the locals object",
        "date_created": "2025-05-24T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "For additional info:\nhttps://khromov.se/the-comprehensive-guide-to-locals-in-sveltekit/\n \nConnection details vary depending on the DB you're using, but generally, the pattern is the same: create the DB connection and store it in the locals object. \n\n[!info]\nThe locals object is available on the server-side only.\n\n// src/hooks.server.ts\nimport type { Handle } from '@sveltejs/kit';\n\nimport { PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE } from '$env/static/private';\nimport pkg from 'pg';\n\nconst { Pool } = pkg;\n\n// Create the pool once when the server starts\nconst pool = new Pool({\n\thost: PG_HOST,\n\tport: Number(PG_PORT) || 5432, // Ensure port is a number\n\tuser: PG_USER,\n\tpassword: PG_PASSWORD,\n\tdatabase: PG_DATABASE,\n\tssl: false // Or configure based on your needs, e.g., { rejectUnauthorized: false } for self-signed certs\n});\n\nconsole.log(PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE);\n\n// Optional: Listen for pool errors\npool.on('error', (err, client) => {\n\tconsole.error('Unexpected error on idle database client', err);\n\t// You might want to decide if the process should exit here\n});\n\n// Check DB health.\nasync function checkDbConnection() {\n\tconst client = await pool.connect();\n\ttry {\n\t\tawait client.query('SELECT 1');\n\t\tconsole.log('Database connection successful');\n\t} catch (err) {\n\t\tconsole.error('Database connection failed:', err);\n\t} finally {\n\t\tclient.release();\n\t}\n}\n\n// Call on startup\ncheckDbConnection();\n\nprocess.on('SIGTERM', async () => {\n\tconsole.log('Closing database pool...');\n\tawait pool.end();\n\tconsole.log('Database pool closed');\n\tprocess.exit(0);\n});\n\n// Optional: You can also handle other signals like SIGINT (Ctrl+C) if needed\nexport const handle: Handle = async ({ event, resolve }) => {\n\t// Make the pool available to all server-side endpoints\n\tevent.locals.pool = pool;\n\n\t// Continue processing the request\n\tconst response = await resolve(event);\n\n\t// Cleanup or other logic after the request is handled can go here if needed\n\treturn response;\n};\n\nDefining the types stored in the locals object\n\nAnything added to the locals object should be a typed in the ./app.d.ts file:\n\n// See https://svelte.dev/docs/kit/types#app.d.ts\n// for information about these interfaces\ndeclare global {\n\tnamespace App {\n\t\tinterface Locals {\n            pool: import('pg').Pool;\n        }\t\t\n\t\t// interface Error {}\n\t\t// interface Locals {}\n\t\t// interface PageData {}\n\t\t// interface PageState {}\n\t\t// interface Platform {}\n\t}\n}\n\nexport {};",
        "locale": "en",
        "slug": "technical-posts/defining-db-connection-sveltekit-hooks-file",
        "folder": ""
    },
    {
        "title": "CSS scoped styles",
        "description": "CSS scoped styles",
        "date_created": "2025-06-10T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Of course! Here is a short tutorial on using CSS's new @scope feature.\n\nScoped Styles are Here! A Short Tutorial on CSS @scope\n\nFor years, developers have used methodologies like BEM or CSS-in-JS to prevent styles from \"leaking\" out of components and affecting the rest of the page. The new CSS @scope at-rule provides a native, browser-level solution to this exact problem.\n\nLet's dive in.\n\nWhat is @scope?\n\n@scope allows you to apply style rules to a specific sectionor \"scope\"of your HTML document. This means the styles you write inside an @scope block will only affect elements within a designated DOM tree, preventing them from accidentally styling elements elsewhere.\n\nThe Problem @scope Solves\n\nImagine a common card component. You might write CSS like this:\n\n/* Potentially global and leaky */\n.card h2 {\n  font-size: 1.5rem;\n  color: #333;\n}\n\n.card p {\n  line-height: 1.6;\n}\n\nThis works, but if you nest another component with an `` inside your card, it will unintentionally inherit these styles. This leads to high-specificity overrides and messy code.\n\nThe @scope Solution: Basic Scoping\n\n@scope solves this by defining a scoping root. Any style rules inside the block will only apply to descendants of that root.\n\nSyntax: @scope (  ) { ... }\n\nLet's rewrite our card styles using @scope:\n\nHTML:\n\n  \n  Card Title\n  Some descriptive text for the card.\n\n\n\nAnother Title on the Page\n\nCSS:\n@scope (.card) {\n  /* These styles only apply to elements inside .card */\n  h2 {\n    font-size: 1.5rem;\n    color: #333;\n  }\n\n  p {\n    line-height: 1.6;\n  }\n}\n\nNow, only the ` and  inside an element with the class .card will be styled. The global ` remains unaffected.\n\nTargeting the Root with :scope\n\nWhat if you want to style the scoping root element itself (.card in our case)? You can use the :scope pseudo-class.\n\n@scope (.card) {\n  :scope {\n    border: 1px solid #ccc;\n    border-radius: 8px;\n    box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n  }\n\n  h2 {\n    /* ... styles for h2 inside the card */\n  }\n}\nHere, :scope refers directly to the element matching .card.\n\nAdvanced: Scoping with a Limit (The \"Donut Scope\")\n\nThis is where @scope gets really powerful. You can define not only a starting root but also an ending limit. Styles will be applied within the root but will stop at the limit.\n\nSyntax: @scope (  ) to (  ) { ... }\n\nImagine our card contains a special \"promo\" block, and we don't want the card's paragraph styles to apply inside it.\n\nHTML:\n\n  Card Title\n  This paragraph should be styled.\n  \n  \n    Special Offer!\n    This paragraph should NOT be styled by the card's scope.\n  \n\n\nCSS:\n@scope (.card) to (.promo-block) {\n  /* \n    These styles apply inside .card but NOT inside .promo-block.\n    This is often called a \"donut scope\".\n  */\n  p {\n    color: #555;\n    margin-bottom: 1rem;\n  }\n}\nIn this example, only the first paragraph gets the color: #555 style. The style rule stops at the boundary of .promo-block, protecting its contents.\n\nKey Differences from CSS Nesting\n\nIt's easy to confuse @scope with CSS Nesting. They are different!\n\nNesting: Is syntactic sugar. .card { h2 { ... } } is compiled to .card h2 { ... }. It doesn't prevent style leaks and actually increases specificity.\n@scope: Is a true scoping mechanism. It changes how styles are applied and has special rules for specificity. Scoped styles have a lower specificity than standard styles, but they win based on proximitythe styles from the nearest scoping root are applied.\n\nBrowser Support\n\n@scope is a very new feature. As of late 2023 / early 2024, it is available in Chrome 118+, Edge 118+, and Safari Technology Preview. It is not yet supported in Firefox.\n\nAlways check Can I Use... for the latest browser support information before using it in production.\n\nConclusion\n\n@scope is a game-changer for writing component-based, encapsulated CSS without relying on external tools or complex naming conventions. It gives us:\n\nTrue Encapsulation: Styles don't leak out.\nLower Specificity: Easier to override styles when needed.\nProximity-Based Styling: The closest ancestor scope wins.\nCleaner HTML: No need for long, repetitive BEM class names.\n\nStart experimenting with it today in supported browsers to get a feel for the future of component styling in CSS",
        "locale": "en",
        "slug": "technical-posts/css-scrope-styles",
        "folder": ""
    },
    {
        "title": "dataviewjs -- pages without a description",
        "description": "dataviewjs -- pages without a description",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "obsidian"
        ],
        "content": "file properties\nhttps://blacksmithgu.github.io/obsidian-dataview/annotation/metadata-pages/\n\ndataviewjs example\n\n// Select all documents that do not have a 'description' property\nlet pagesWithoutTitle = dv.pages().where(\n    p => p.file.tags.includes(\"#menu\")\n);\n\n// Sort the pages by file name\npagesWithoutTitle = pagesWithoutTitle.sort(p => p.file.name ? p.file.name.toLowerCase() : \"\");\n\n// Create a table with \"File Name\" and \"Description\"\ndv.table(\n    [\"Description\", \"File Name\", \"Folder\"],\n    pagesWithoutTitle.map(page => [\n        page.description, // Indicate that there is no title\n        page.file.link, // Link to the file\n        page.file.folder\n    ])\n);",
        "locale": "en",
        "slug": "technical-posts/dataview-info",
        "folder": ""
    },
    {
        "title": "Binding to console log",
        "description": "Binding to console log",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "You've hit upon a very interesting and useful JavaScript technique! Let's break down const log = console.log.bind(console); and explain the bind method.\n\nUnderstanding console.log\n\nconsole.log is a method (a function that belongs to an object) provided by the JavaScript console object (available in browsers and Node.js).\nIts purpose is to write messages to the console (for debugging, logging, etc.).\nWhen you call console.log(\"Hello\"), console.log is executed in the context of (or with the this value set to) the console object.\n\nThe bind() Method\n\nbind() is a method that is available on all functions in JavaScript.\nIt creates a new function whose this value is permanently bound to a specific value.\nThe syntax is function.bind(thisArg, arg1, arg2, ...).\n    thisArg: The value that will be used as the this value when the new function is called.\n    arg1, arg2, ...: Optional arguments that will be prepended to the arguments that are provided when the new function is called.\n\nPutting It Together: const log = console.log.bind(console);\n\nLet's break it down step-by-step:\n\nconsole.log.bind(): This calls the bind() method on the console.log function.\nconsole: This is the first argument passed to bind(). It means that when the new function is called, its this value will be set to the console object.\nconsole.log.bind(console): This creates a new function whose this value will always be the console object.\nconst log = ...: This assigns that new function to the constant variable named log.\n\nWhy Use bind(console)?\n\nContext Consistency: When you extract a method from an object, the this value might change in the context where you are using the method. For example:\n\n    const myLog = console.log;\n    myLog(\"Hello\"); // the this context of this function is not console, and will most likely throw an error\n\nBy using bind(console) we ensure that the method continues to operate on its object by ensuring that the context, the value of this, is always console.\n\n\n    const myBoundLog = console.log.bind(console)\n     myBoundLog(\"Hello\") // prints the message to the console, because the this value is console\n\nShorthand: It provides a shorter, more convenient way to log messages, in the same way that console.log is.\n\nExample\n\nconst log = console.log.bind(console);\n\nlog(\"This is a log message\"); // Output: This is a log message\nlog(1, 2, 3); // Output: 1 2 3\nlog({ message: \"hello\" }); // Output: { message: 'hello' }\n\nIn this case, log is now a function that you can use to call console.log from any part of your code with the appropriate this context.\n\nIn the Context of the Code\n\nThe const log = console.log.bind(console); line from the previous example was intended to create a shorter variable to log messages, which I ended up removing when I re-worked the file. This line would have made it so you could log a message by just writing log(\"my message\"), rather than console.log(\"my message\"). The context of the console.log method was bound using the bind method, to ensure that the console.log function had its proper context. I apologize for leaving that line out.\n\nLet me know if you have any more questions. I'm happy to help!",
        "locale": "en",
        "slug": "technical-posts/binding-to-console-log",
        "folder": ""
    },
    {
        "title": "GoTo Webinar APIs",
        "description": "GoTo Webinar APIs",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "goto-webinar"
        ],
        "content": "This project shows how to use the Create Registrant API and server are demoed in the Thunder Client there:\n\nC:\\Users\\thumb\\Documents\\projects\\svelte\\goto-apis\n\nGet Webinar\n\nhttps://developer.goto.com/GoToWebinarV2#tag/Webinars/operation/getWebinar\n\nGet Webinars\n\nhttps://developer.goto.com/GoToWebinarV2#tag/Webinars/operation/getWebinars\n\nCreate registrant\n\nhttps://developer.goto.com/GoToWebinarV2#tag/Webinars/operation/getWebinars",
        "locale": "en",
        "slug": "technical-posts/goto-webinar-apis",
        "folder": ""
    },
    {
        "title": "Algolia v5 upgrade",
        "description": "Algolia v5 upgrade",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-com"
        ],
        "content": "https://www.algolia.com/doc/libraries/javascript/v5/upgrade/\n\nSent on May 12th.\nhttps://support.algolia.com/hc/en-us/requests/695588\n\nI removed the algoliasearch 4.x package and added this Algolia package:\n\n\"algoliasearch\": \"5\",\n\n\n\nv4 code that works:\n\nimport { algoliasearch } from 'algoliasearch';\n\n\nlet searchClient = algoliasearch(algolia_app_id, algolia_search_api_key);\nlet index = searchClient.initIndex(algolia_index_name);\n\nconst result = await index.search(query,\n                                  {\n  \t\t\t\t    attributesToRetrieve: ['title', 'slug', 'locale', 'description'],\n \t\t\t\t    facetFilters: filter,\n\t\t\t\t    hitsPerPage: HITS_PER_PAGE\n\t\t\t\t  })\n\n\nv5 conversion attempt: This code fails with unknown parameter error in searchSingleIdex:\n\n\nlet searchClient = algoliasearch(algolia_app_id, algolia_search_api_key);\n\n\nconst result = await searchClient.searchSingleIndex(\n\t\t{ indexName: algolia_index_name, searchParams: query },\n\t\t{\n\t\t\tattributesToRetrieve: ['title', 'slug', 'locale', 'description'],\n\t\t\tfacetFilters: filter,\n\t\t\thitsPerPage: HITS_PER_PAGE\n\t\t}\n);\n\n\n\n\nWhile I'm at it, I also need to convert this to v5. Giving my struggles with the code above, I\nhaven't even tried to convert this... please help!\n\n\nimport algoliasearch from 'algoliasearch';\nimport * as dotenv from 'dotenv';\n\nexport async function refresh_algolia_online_index() {\n\tdotenv.config({ path: '../../../.env' });\n\tconst markdownSearchData = await import('../data/search-index.json', {\n\t\tassert: { type: 'json' }\n\t});\n\tconst pageSearchData = await import('../data/search-index-pages.json', {\n\t\tassert: { type: 'json' }\n\t});\n\n\tconst searchData = [...markdownSearchData.default, ...pageSearchData.default];\n\n\tconst client = algoliasearch(process.env.ALGOLIA_APP_ID, process.env.ALGOLIA_ADMIN_API_KEY);\n\tconst index = client.initIndex('Ocho');\n\n\ttry {\n\t\tconst result = await index.clearObjects();\n\t} catch (error) {\n\t\tconsole.log(error);\n\t\tprocess.exit(4);\n\t}\n\tconsole.log(${process.env.ALGOLIA_INDEX_NAME} index cleared);\n\n\ttry {\n\t\tconst result = await index.saveObjects(searchData, {\n\t\t\tautoGenerateObjectIDIfNotExist: true\n\t\t});\n\t} catch (error) {\n\t\tconsole.log(error);\n\t\tprocess.exit(4);\n\t}\n\tconsole.log(${process.env.ALGOLIA_INDEX_NAME} refreshed);\n}",
        "locale": "en",
        "slug": "technical-posts/algolia-v5-upgrade",
        "folder": ""
    },
    {
        "title": "Debugging Sveltekit apps",
        "description": "Debugging Sveltekit apps",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "debug"
        ],
        "content": "\nTo debug server-side SvelteKit code, you can use Visual Studio Code by attaching the debugger to the running Node.js process. First, set breakpoints in your server-side code (e.g., in +server.js or +page.js files). Then, start the development server with npx vite dev. Next, in VS Code, choose \"Debug: Attach to Node process\" from the command palette. Finally, hit your HTTP endpoint to trigger the debugger and examine the code at the breakpoints.\n\nDetailed Steps:\n\nSet Breakpoints:\n   Open your SvelteKit project in VS Code and place breakpoints in your server-side code, such as functions within +server.js or +page.js files.\nStart the Development Server:\n   Open a terminal and run npx vite dev to start the SvelteKit development server.\nAttach to Node Process:\n   In VS Code, press Ctrl+Shift+P (or Cmd+Shift+P on macOS) to open the command palette. Type \"Debug: Attach to Node process\" and select the option.\nHit the Endpoint:\n   Navigate to the URL of your SvelteKit application in your browser. This will trigger the server-side code and stop the execution at the breakpoints you've set.\nDebug and Inspect:\n   VS Code will now open the debugger, allowing you to step through the code, inspect variables, and examine the execution flow at each breakpoint.\n\nImportant Notes:\n\nMake sure your VS Code is configured with the Node.js Debugger extension, according to Svelte's documentation.\nYou might need to add the node-loader plugin to your vite.config.js file before the SvelteKit plugin for debugging to work correctly, as suggested by Codelantis.\n\nBreakpoints in uncompiled Svelte/TS source files might not work due to a Vite issue with sourcemaps.\nFor more detailed information and alternative debugging approaches, you can refer to the Svelte documentation on debugging and the VS Code documentation on debugging.\n\nFor more detailed information and alternative debugging approaches, you can refer to theSvelte documentation on debuggingand the VS Code documentation on debugging.\n\nDebugging Sveltekit client side with @debug\n\nThis info is from this page\n\nThe{@debug ...}tag offers an alternative toconsole.log(...). It logs the values of specific variables whenever they change, and pauses code execution if you have devtools open.\n\n\n\tlet user = {\n\t\tfirstname: 'Ada',\n\t\tlastname: 'Lovelace'\n\t};\n\n\n{@debug user}\n\nHello {user.firstname}!\n\n{@debug ...}accepts a comma-separated list of variable names (not arbitrary expressions).\n\n\n{@debug user}\n{@debug user1, user2, user3}\n\n\n{@debug user.firstname}\n{@debug myArray[0]}\n{@debug !isReady}\n{@debug typeof user === 'object'}\n\nThe{@debug}tag without any arguments will insert adebuggerstatement that gets triggered whenanystate changes, as opposed to the specified variables.\n\n.vscode\\launch.json\n\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"type\": \"node\",\n            \"request\": \"attach\",\n            \"name\": \"Attach to SvelteKit (Node)\",\n            \"port\": 9229, // Default inspect port. Change if your server uses a different one.\n            \"restart\": true, // Automatically try to re-attach if the process restarts\n            \"protocol\": \"inspector\", // Modern protocol\n            \"skipFiles\": [\n                \"/**\", // Skip Node.js internal files\n                \"${workspaceFolder}/node_modules/**\" // Skip node_modules\n            ]\n        }\n    ]\n}",
        "locale": "en",
        "slug": "technical-posts/debugging-server-side-sveltekit-apps",
        "folder": ""
    },
    {
        "title": "Dynamic Svelte Components",
        "description": "Dynamic Svelte Components",
        "date_created": "2025-07-17T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "How can you dyamically instance a Svete 5 component. I\n\nGitHub Copilot: In Svelte 5, you can dynamically instantiate components using the {@render} directive with component constructors, or by using the `` element. Here are the main approaches:\n\nMethod 1: Using `` (Most Common)\n\n\n\timport ComponentA from './ComponentA.svelte';\n\timport ComponentB from './ComponentB.svelte';\n\n\tconst config = {\n\t\torange: {\n\t\t\tsize: 'large',\n\t\t\trate: 3,\n\t\t\tcomponent: ComponentA\n\t\t},\n\t\tblue: {\n\t\t\tsize: 'large',\n\t\t\trate: 3,\n\t\t\tcomponent: ComponentB\n\t\t}\n\t};\n\n\tlet selectedConfig = $state('orange');\n\n\tfunction readConfig() {\n\t\treturn config[selectedConfig];\n\t}\n\n\n\n\n selectedConfig = selectedConfig === 'orange' ? 'blue' : 'orange'}>\n\tSwitch Component\n`\n\nMethod 2: Using Component Map\n\n\n\timport ComponentA from './ComponentA.svelte';\n\timport ComponentB from './ComponentB.svelte';\n\n\tconst components = {\n\t\tComponentA,\n\t\tComponentB\n\t};\n\n\tconst config = {\n\t\torange: {\n\t\t\tsize: 'large',\n\t\t\trate: 3,\n\t\t\tcomponentName: 'ComponentA'\n\t\t},\n\t\tblue: {\n\t\t\tsize: 'large',\n\t\t\trate: 3,\n\t\t\tcomponentName: 'ComponentB'\n\t\t}\n\t};\n\n\tlet currentKey = $state('orange');\n\n\tfunction readConfig() {\n\t\tconst cfg = config[currentKey];\n\t\treturn {\n\t\t\t...cfg,\n\t\t\tcomponent: components[cfg.componentName]\n\t\t};\n\t}\n\n\n`\n\nMethod 3: Programmatic Instantiation\n\n\n\timport { mount } from 'svelte';\n\timport ComponentA from './ComponentA.svelte';\n\n\tconst config = {\n\t\torange: {\n\t\t\tsize: 'large',\n\t\t\trate: 3\n\t\t},\n\t\tblue: {\n\t\t\tsize: 'large',\n\t\t\trate: 3\n\t\t}\n\t};\n\n\tlet container = $state();\n\n\tfunction readConfig() {\n\t\t// Programmatically mount component\n\t\tif (container) {\n\t\t\tmount(ComponentA, {\n\t\t\t\ttarget: container,\n\t\t\t\tprops: config.orange\n\t\t\t});\n\t\t}\n\t}\n\n\n\nMount Component`\n\nThe `` approach (Method 1) is usually the most straightforward for dynamic component rendering in Svelte 5.\n\nrogerpence: I want to reqad the config json (which will later be external to the page) where the compenent name is named in the Json's  top-level keys. I want to instance each component listed and pass it  all of the properties listed for it\n\nGitHub Copilot: Here's how you can dynamically instantiate components based on the JSON config where the component names are the top-level keys:\n\n\n\t// Import your components\n\timport Orange from './Orange.svelte';\n\timport Blue from './Blue.svelte';\n\n\t// Component registry - maps config keys to actual components\n\tconst componentRegistry = {\n\t\torange: Orange,\n\t\tblue: Blue\n\t};\n\n\tconst config = {\n\t\torange: {\n\t\t\tsize: 'large',\n\t\t\trate: 3\n\t\t},\n\t\tblue: {\n\t\t\tsize: 'large',\n\t\t\tlength: 'medium'\n\t\t}\n\t};\n\n\tfunction readConfig() {\n\t\treturn Object.entries(config).map(([componentName, props]) => ({\n\t\t\tcomponent: componentRegistry[componentName],\n\t\t\tprops,\n\t\t\tname: componentName\n\t\t}));\n\t}\n\n\t// Get all components to render\n\tconst componentsToRender = readConfig();\n\n\n\n{#each componentsToRender as { component, props, name } (name)}\n\t{#if component}\n\t\t\n\t\t\tComponent: {name}\n\t\t\t\n\t\t\n\t{:else}\n\t\tComponent \"{name}\" not found in registry\n\t{/if}\n{/each}\n\n\n\t.component-wrapper {\n\t\tmargin: 1rem 0;\n\t\tpadding: 1rem;\n\t\tborder: 1px solid #ccc;\n\t\tborder-radius: 4px;\n\t}\n\n\t.error {\n\t\tcolor: red;\n\t\tpadding: 1rem;\n\t\tbackground: #fee;\n\t\tborder-radius: 4px;\n\t}\n`\n\nOr if you prefer a more dynamic approach that doesn't require pre-importing all components:\n\n\n\tconst config = {\n\t\torange: {\n\t\t\tsize: 'large',\n\t\t\trate: 3\n\t\t},\n\t\tblue: {\n\t\t\tsize: 'large',\n\t\t\tlength: 'medium'\n\t\t}\n\t};\n\n\t// Dynamic import function\n\tasync function loadComponent(name: string) {\n\t\ttry {\n\t\t\tconst module = await import(./${name.charAt(0).toUpperCase() + name.slice(1)}.svelte);\n\t\t\treturn module.default;\n\t\t} catch (error) {\n\t\t\tconsole.error(Failed to load component: ${name}, error);\n\t\t\treturn null;\n\t\t}\n\t}\n\n\t// Load all components\n\tlet components = $state([]);\n\n\tasync function readConfig() {\n\t\tconst loadedComponents = [];\n\n\t\tfor (const [componentName, props] of Object.entries(config)) {\n\t\t\tconst component = await loadComponent(componentName);\n\t\t\tif (component) {\n\t\t\t\tloadedComponents.push({\n\t\t\t\t\tcomponent,\n\t\t\t\t\tprops,\n\t\t\t\t\tname: componentName\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\n\t\tcomponents = loadedComponents;\n\t}\n\n\t// Load components on mount\n\treadConfig();\n\n\n\n{#each components as { component, props, name } (name)}\n\t\n\t\tComponent: {name}\n\t\t\n\t\n{/each}`\n\nThe first approach with the component registry is more performant and easier to debug, while the second approach with dynamic imports is more flexible for external configs.\n\nrogerpence: how do define the component to get the properties from ...props\n\nGitHub Copilot: In your components (Orange.svelte and Blue.svelte), you need to define the props that will be received from the {...props} spread. Here's how:\n\nOrange.svelte\n\n\t// Define the props this component expects\n\tlet { size, rate } = $props();\n\n\n\n\tOrange Component\n\tSize: {size}\n\tRate: {rate}\n\n\n\n\t.orange-component {\n\t\tbackground: orange;\n\t\tpadding: 1rem;\n\t\tborder-radius: 4px;\n\t}\n`\n\nBlue.svelte\n\n\t// Define the props this component expects\n\tlet { size, length } = $props();\n\n\n\n\tBlue Component\n\tSize: {size}\n\tLength: {length}\n\n\n\n\t.blue-component {\n\t\tbackground: lightblue;\n\t\tpadding: 1rem;\n\t\tborder-radius: 4px;\n\t}\n`\n\nWith Optional Props and Defaults\n\n\t// With default values and optional props\n\tlet {\n\t\tsize = 'medium',\n\t\trate = 1,\n\t\tcolor = 'orange'\n\t} = $props();\n\n\n\n\tSize: {size}\n\tRate: {rate}\n`\n\nWith TypeScript Interface\n\n\tinterface Props {\n\t\tsize: 'small' | 'medium' | 'large';\n\t\trate: number;\n\t\tcolor?: string;\n\t}\n\n\tlet { size, rate, color = 'orange' }: Props = $props();\n`\n\nWhen you use {...props} in `, it spreads all the properties from your config object as individual props to the component. The component then receives them through the $props()` destructuring.\n\nrogerpence: how can I make it reactive so that when another component definition is added to the config json that component gets rendered at runtime\n\nGitHub Copilot: To make it reactive so that when another component definition is added to the config, it gets rendered at runtime, you need to convert the config from a constant to reactive state and update how you handle the component rendering:\n\n\n\t// Import your components\n\timport Orange from '$lib/components/Orange.svelte';\n\timport Blue from '$lib/components/Blue.svelte';\n\timport Green from '$lib/components/Green.svelte'; // Add more as needed\n\n\t// Component registry - maps config keys to actual components\n\tconst componentRegistry = {\n\t\torange: Orange,\n\t\tblue: Blue,\n\t\tgreen: Green\n\t};\n\n\t// Make config reactive using $state\n\tlet config = $state({\n\t\torange: {\n\t\t\tsize: 'large',\n\t\t\trate: 3\n\t\t},\n\t\tblue: {\n\t\t\tsize: 'large',\n\t\t\tlength: 'medium'\n\t\t}\n\t});\n\n\t// Make this a derived value that updates when config changes\n\tlet componentsToRender = $derived(\n\t\tObject.entries(config).map(([componentName, props]) => ({\n\t\t\tcomponent: componentRegistry[componentName],\n\t\t\tprops,\n\t\t\tname: componentName\n\t\t}))\n\t);\n\n\t// Function to add new component configuration\n\tfunction addComponent(name: string, props: Record) {\n\t\tconfig = {\n\t\t\t...config,\n\t\t\t[name]: props\n\t\t};\n\t}\n\n\t// Function to remove component configuration\n\tfunction removeComponent(name: string) {\n\t\tconst newConfig = { ...config };\n\t\tdelete newConfig[name];\n\t\tconfig = newConfig;\n\t}\n\n\t// Example functions to test reactivity\n\tfunction addGreenComponent() {\n\t\taddComponent('green', {\n\t\t\tsize: 'small',\n\t\t\tintensity: 'bright'\n\t\t});\n\t}\n\n\tfunction removeOrangeComponent() {\n\t\tremoveComponent('orange');\n\t}\n\n\t// Function to load external config (for later use)\n\tasync function loadExternalConfig() {\n\t\ttry {\n\t\t\tconst response = await fetch('/api/components-config');\n\t\t\tconst externalConfig = await response.json();\n\t\t\tconfig = externalConfig;\n\t\t} catch (error) {\n\t\t\tconsole.error('Failed to load external config:', error);\n\t\t}\n\t}\n\n\n\n\n\tAdd Green Component\n\tRemove Orange Component\n\tLoad External Config\n\n\n\n{#each componentsToRender as { component, props, name } (name)}\n\t{#if component}\n\t\t\n\t\t\tComponent: {name}\n\t\t\t\n\t\t\n\t{:else}\n\t\tComponent \"{name}\" not found in registry\n\t{/if}\n{/each}\n\n\n\t.controls {\n\t\tmargin-bottom: 2rem;\n\t\tdisplay: flex;\n\t\tgap: 1rem;\n\t}\n\n\t.controls button {\n\t\tpadding: 0.5rem 1rem;\n\t\tbackground: #007acc;\n\t\tcolor: white;\n\t\tborder: none;\n\t\tborder-radius: 4px;\n\t\tcursor: pointer;\n\t}\n\n\t.controls button:hover {\n\t\tbackground: #005999;\n\t}\n\n\t.component-wrapper {\n\t\tmargin: 1rem 0;\n\t\tpadding: 1rem;\n\t\tborder: 1px solid #ccc;\n\t\tborder-radius: 4px;\n\t}\n\n\t.error {\n\t\tcolor: red;\n\t\tpadding: 1rem;\n\t\tbackground: #fee;\n\t\tborder-radius: 4px;\n\t}\n`\n\nFor External JSON File Updates\n\nIf you want to automatically detect changes to an external JSON file, you could use polling or WebSockets:\n\n\n\t// ...existing code...\n\n\t// Poll for config changes every 5 seconds\n\tlet pollInterval: number;\n\n\tasync function pollForConfigChanges() {\n\t\ttry {\n\t\t\tconst response = await fetch('/api/components-config?t=' + Date.now());\n\t\t\tconst newConfig = await response.json();\n\n\t\t\t// Only update if config actually changed\n\t\t\tif (JSON.stringify(newConfig) !== JSON.stringify(config)) {\n\t\t\t\tconfig = newConfig;\n\t\t\t\tconsole.log('Config updated from external source');\n\t\t\t}\n\t\t} catch (error) {\n\t\t\tconsole.error('Failed to poll for config changes:', error);\n\t\t}\n\t}\n\n\t// Start polling when component mounts\n\t$effect(() => {\n\t\tpollInterval = setInterval(pollForConfigChanges, 5000);\n\n\t\t// Cleanup on unmount\n\t\treturn () => {\n\t\t\tif (pollInterval) {\n\t\t\t\tclearInterval(pollInterval);\n\t\t\t}\n\t\t};\n\t});\n`\n\nThe key changes:\nconfig is now reactive using $state()\ncomponentsToRender is now derived using $derived() so it automatically updates when config changes\nHelper functions to add/remove components dynamically\nOptional polling to detect external file changes\n\nNow whenever you modify the config object, the component list will automatically re-render with the new configuration!\n\nrogerpence: svelte:component is deprecated in svelte 5\n\nGitHub Copilot: You're right! In Svelte 5, `` is deprecated. Here's how to update your code to use the modern Svelte 5 approach with snippets and render functions:\n\n\n    // Import your components\n    import Orange from '$lib/components/Orange.svelte';\n    import Blue from '$lib/components/Blue.svelte';\n    import Green from '$lib/components/Green.svelte';\n\n    // Component registry - maps config keys to actual components\n    const componentRegistry = {\n        orange: Orange,\n        blue: Blue,\n        green: Green\n    };\n\n    // Make config reactive using $state\n    let config = $state({\n        orange: {\n            size: 'large',\n            rate: 3\n        },\n        blue: {\n            size: 'large',\n            length: 'medium'\n        }\n    });\n\n    // Make this a derived value that updates when config changes\n    let componentsToRender = $derived(\n        Object.entries(config).map(([componentName, props]) => ({\n            component: componentRegistry[componentName],\n            props,\n            name: componentName\n        }))\n    );\n\n    // Function to add new component configuration\n    function addComponent(name: string, props: Record) {\n        config = {\n            ...config,\n            [name]: props\n        };\n    }\n\n    // Function to remove component configuration\n    function removeComponent(name: string) {\n        const newConfig = { ...config };\n        delete newConfig[name];\n        config = newConfig;\n    }\n\n    // Example functions to test reactivity\n    function addGreenComponent() {\n        addComponent('green', {\n            size: 'small',\n            intensity: 'bright'\n        });\n    }\n\n    function removeOrangeComponent() {\n        removeComponent('orange');\n    }\n\n    // Function to load external config (for later use)\n    async function loadExternalConfig() {\n        try {\n            const response = await fetch('/api/components-config');\n            const externalConfig = await response.json();\n            config = externalConfig;\n        } catch (error) {\n            console.error('Failed to load external config:', error);\n        }\n    }\n\n    // Helper function to render component dynamically\n    function renderComponent(component: any, props: Record) {\n        return component ? new component({ target: document.createElement('div'), props }) : null;\n    }\n\n\n\n\n    Add Green Component\n    Remove Orange Component\n    Load External Config\n\n\n\n{#each componentsToRender as { component, props, name } (name)}\n    {#if component}\n        \n            Component: {name}\n            {@render componentSnippet(component, props)}\n        \n    {:else}\n        Component \"{name}\" not found in registry\n    {/if}\n{/each}\n\n{#snippet componentSnippet(Component, props)}\n    \n{/snippet}\n\n\n    .controls {\n        margin-bottom: 2rem;\n        display: flex;\n        gap: 1rem;\n    }\n\n    .controls button {\n        padding: 0.5rem 1rem;\n        background: #007acc;\n        color: white;\n        border: none;\n        border-radius: 4px;\n        cursor: pointer;\n    }\n\n    .controls button:hover {\n        background: #005999;\n    }\n\n    .component-wrapper {\n        margin: 1rem 0;\n        padding: 1rem;\n        border: 1px solid #ccc;\n        border-radius: 4px;\n    }\n\n    .error {\n        color: red;\n        padding: 1rem;\n        background: #fee;\n        border-radius: 4px;\n    }\n`\n\nThe key changes for Svelte 5:\n\nReplaced ` with {@render componentSnippet(component, props)}`\nAdded a snippet {#snippet componentSnippet(Component, props)} that renders the component\nUsed `` inside the snippet to render the actual component\n\nThis approach uses Svelte 5's modern snippet system instead of the deprecated `` directive, while maintaining the same dynamic component rendering functionality.\n\nrogerpence: what is keeping it from leeting me add multiple green components\n\nGitHub Copilot: The issue is that your addComponent function uses the component name as the key, so when you add a second \"green\" component, it overwrites the first one instead of creating a new instance.\n\nHere's how to fix it to allow multiple instances of the same component type:\n\n\n\t// Import your components\n\timport Orange from '$lib/components/Orange.svelte';\n\timport Blue from '$lib/components/Blue.svelte';\n\timport Green from '$lib/components/Green.svelte';\n\n\t// Component registry - maps config keys to actual components\n\tconst componentRegistry = {\n\t\torange: Orange,\n\t\tblue: Blue,\n\t\tgreen: Green\n\t};\n\n\t// Make config reactive using $state - now uses unique IDs as keys\n\tlet config = $state({\n\t\t'orange-1': {\n\t\t\tcomponentType: 'orange',\n\t\t\tsize: 'large',\n\t\t\trate: 3\n\t\t},\n\t\t'blue-1': {\n\t\t\tcomponentType: 'blue',\n\t\t\tsize: 'large',\n\t\t\tlength: 'medium'\n\t\t}\n\t});\n\n\t// Counter for generating unique IDs\n\tlet componentCounter = $state(2);\n\n\t// Make this a derived value that updates when config changes\n\tlet componentsToRender = $derived(\n\t\tObject.entries(config).map(([instanceId, configData]) => ({\n\t\t\tcomponent: componentRegistry[configData.componentType],\n\t\t\tprops: { ...configData, componentType: undefined }, // Remove componentType from props\n\t\t\tname: instanceId,\n\t\t\tcomponentType: configData.componentType\n\t\t}))\n\t);\n\n\t// Function to add new component configuration with unique ID\n\tfunction addComponent(componentType: string, props: Record) {\n\t\tconst instanceId = ${componentType}-${componentCounter};\n\t\tcomponentCounter++;\n\n\t\tconfig = {\n\t\t\t...config,\n\t\t\t[instanceId]: {\n\t\t\t\tcomponentType,\n\t\t\t\t...props\n\t\t\t}\n\t\t};\n\t}\n\n\t// Function to remove component configuration\n\tfunction removeComponent(instanceId: string) {\n\t\tconst newConfig = { ...config };\n\t\tdelete newConfig[instanceId];\n\t\tconfig = newConfig;\n\t}\n\n\t// Example functions to test reactivity\n\tfunction addGreenComponent() {\n\t\taddComponent('green', {\n\t\t\tsize: 'small',\n\t\t\tintensity: 'bright'\n\t\t});\n\t}\n\n\tfunction addAnotherGreenComponent() {\n\t\taddComponent('green', {\n\t\t\tsize: 'large',\n\t\t\tintensity: 'dim'\n\t\t});\n\t}\n\n\tfunction removeOrangeComponent() {\n\t\tremoveComponent('orange-1');\n\t}\n\n\t// Function to load external config (for later use)\n\tasync function loadExternalConfig() {\n\t\ttry {\n\t\t\tconst response = await fetch('/api/components-config');\n\t\t\tconst externalConfig = await response.json();\n\t\t\tconfig = externalConfig;\n\t\t} catch (error) {\n\t\t\tconsole.error('Failed to load external config:', error);\n\t\t}\n\t}\n\n\n\n\n\tAdd Green Component\n\tAdd Another Green Component\n\tRemove Orange Component\n\tLoad External Config\n\n\n\n{#each componentsToRender as { component, props, name, componentType } (name)}\n\t{#if component}\n\t\t\n\t\t\tComponent: {name} ({componentType})\n\t\t\t{@render componentSnippet(component, props)}\n\t\t\t removeComponent(name)} class=\"remove-btn\">Remove This Instance\n\t\t\n\t{:else}\n\t\tComponent type \"{componentType}\" not found in registry\n\t{/if}\n{/each}\n\n{#snippet componentSnippet(Component, props)}\n\t\n{/snippet}\n\n\n\t.controls {\n\t\tmargin-bottom: 2rem;\n\t\tdisplay: flex;\n\t\tgap: 1rem;\n\t}\n\n\t.controls button {\n\t\tpadding: 0.5rem 1rem;\n\t\tbackground: #007acc;\n\t\tcolor: white;\n\t\tborder: none;\n\t\tborder-radius: 4px;\n\t\tcursor: pointer;\n\t}\n\n\t.controls button:hover {\n\t\tbackground: #005999;\n\t}\n\n\t.component-wrapper {\n\t\tmargin: 1rem 0;\n\t\tpadding: 1rem;\n\t\tborder: 1px solid #ccc;\n\t\tborder-radius: 4px;\n\t\tposition: relative;\n\t}\n\n\t.remove-btn {\n\t\tposition: absolute;\n\t\ttop: 0.5rem;\n\t\tright: 0.5rem;\n\t\tpadding: 0.25rem 0.5rem;\n\t\tbackground: #dc3545;\n\t\tcolor: white;\n\t\tborder: none;\n\t\tborder-radius: 3px;\n\t\tfont-size: 0.8rem;\n\t\tcursor: pointer;\n\t}\n\n\t.remove-btn:hover {\n\t\tbackground: #c82333;\n\t}\n\n\t.error {\n\t\tcolor: red;\n\t\tpadding: 1rem;\n\t\tbackground: #fee;\n\t\tborder-radius: 4px;\n\t}\n`\n\nKey changes:\n\nUnique instance IDs: Instead of using component type as the key, each instance gets a unique ID like green-1, green-2, etc.\n\nComponent type tracking: Added componentType field to track what type of component each instance is\n\nCounter for uniqueness: Added componentCounter to generate unique IDs\n\nMultiple green buttons: Added a second button to demonstrate adding multiple green components\n\nIndividual remove buttons: Each component instance now has its own remove button\n\nNow you can add as many green (or any other) components as you want, and each will be a separate instance with its own configuration!\n\nrogerpence: add up and down buttons for each component that would move that component up or down in the json to change the order of the components\n\nGitHub Copilot: Here's how to add up and down buttons to reorder the components:\n\n\n\t// Import your components\n\timport Orange from '$lib/components/Orange.svelte';\n\timport Blue from '$lib/components/Blue.svelte';\n\timport Green from '$lib/components/Green.svelte';\n\n\t// Component registry - maps config keys to actual components\n\tconst componentRegistry = {\n\t\torange: Orange,\n\t\tblue: Blue,\n\t\tgreen: Green\n\t};\n\n\t// Make config reactive using $state - now uses unique IDs as keys\n\tlet config = $state({\n\t\t'orange-1': {\n\t\t\tcomponentType: 'orange',\n\t\t\tsize: 'large',\n\t\t\trate: 3\n\t\t},\n\t\t'blue-1': {\n\t\t\tcomponentType: 'blue',\n\t\t\tsize: 'large',\n\t\t\tlength: 'medium'\n\t\t}\n\t});\n\n\t// Counter for generating unique IDs\n\tlet componentCounter = $state(2);\n\n\t// Make this a derived value that updates when config changes\n\tlet componentsToRender = $derived(\n\t\tObject.entries(config).map(([instanceId, configData]) => ({\n\t\t\tcomponent: componentRegistry[configData.componentType],\n\t\t\tprops: { ...configData, componentType: undefined }, // Remove componentType from props\n\t\t\tname: instanceId,\n\t\t\tcomponentType: configData.componentType\n\t\t}))\n\t);\n\n\t// Function to add new component configuration with unique ID\n\tfunction addComponent(componentType: string, props: Record) {\n\t\tconst instanceId = ${componentType}-${componentCounter};\n\t\tcomponentCounter++;\n\n\t\tconfig = {\n\t\t\t...config,\n\t\t\t[instanceId]: {\n\t\t\t\tcomponentType,\n\t\t\t\t...props\n\t\t\t}\n\t\t};\n\t}\n\n\t// Function to remove component configuration\n\tfunction removeComponent(instanceId: string) {\n\t\tconst newConfig = { ...config };\n\t\tdelete newConfig[instanceId];\n\t\tconfig = newConfig;\n\t}\n\n\t// Function to move component up in order\n\tfunction moveComponentUp(instanceId: string) {\n\t\tconst entries = Object.entries(config);\n\t\tconst currentIndex = entries.findIndex(([id]) => id === instanceId);\n\n\t\tif (currentIndex > 0) {\n\t\t\t// Swap with previous item\n\t\t\t[entries[currentIndex - 1], entries[currentIndex]] = [entries[currentIndex], entries[currentIndex - 1]];\n\n\t\t\t// Rebuild config object in new order\n\t\t\tconfig = Object.fromEntries(entries);\n\t\t}\n\t}\n\n\t// Function to move component down in order\n\tfunction moveComponentDown(instanceId: string) {\n\t\tconst entries = Object.entries(config);\n\t\tconst currentIndex = entries.findIndex(([id]) => id === instanceId);\n\n\t\tif (currentIndex \n\n\n\n\tAdd Green Component\n\tAdd Another Green Component\n\tRemove Orange Component\n\tLoad External Config\n\n\n\n{#each componentsToRender as { component, props, name, componentType }, index (name)}\n\t{#if component}\n\t\t\n\t\t\t\n\t\t\t\t moveComponentUp(name)}\n\t\t\t\t\tclass=\"move-btn up-btn\"\n\t\t\t\t\tdisabled={index === 0}\n\t\t\t\t\ttitle=\"Move up\"\n\t\t\t\t>\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\t moveComponentDown(name)}\n\t\t\t\t\tclass=\"move-btn down-btn\"\n\t\t\t\t\tdisabled={index === componentsToRender.length - 1}\n\t\t\t\t\ttitle=\"Move down\"\n\t\t\t\t>\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\t removeComponent(name)} class=\"remove-btn\" title=\"Remove\">\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\tComponent: {name} ({componentType})\n\t\t\t{@render componentSnippet(component, props)}\n\t\t\n\t{:else}\n\t\tComponent type \"{componentType}\" not found in registry\n\t{/if}\n{/each}\n\n{#snippet componentSnippet(Component, props)}\n\t\n{/snippet}\n\n\n\t.controls {\n\t\tmargin-bottom: 2rem;\n\t\tdisplay: flex;\n\t\tgap: 1rem;\n\t}\n\n\t.controls button {\n\t\tpadding: 0.5rem 1rem;\n\t\tbackground: #007acc;\n\t\tcolor: white;\n\t\tborder: none;\n\t\tborder-radius: 4px;\n\t\tcursor: pointer;\n\t}\n\n\t.controls button:hover {\n\t\tbackground: #005999;\n\t}\n\n\t.component-wrapper {\n\t\tmargin: 1rem 0;\n\t\tpadding: 1rem;\n\t\tborder: 1px solid #ccc;\n\t\tborder-radius: 4px;\n\t\tposition: relative;\n\t}\n\n\t.component-controls {\n\t\tposition: absolute;\n\t\ttop: 0.5rem;\n\t\tright: 0.5rem;\n\t\tdisplay: flex;\n\t\tgap: 0.25rem;\n\t}\n\n\t.move-btn, .remove-btn {\n\t\tpadding: 0.25rem 0.5rem;\n\t\tcolor: white;\n\t\tborder: none;\n\t\tborder-radius: 3px;\n\t\tfont-size: 0.8rem;\n\t\tcursor: pointer;\n\t\tfont-weight: bold;\n\t\tmin-width: 24px;\n\t\ttext-align: center;\n\t}\n\n\t.move-btn {\n\t\tbackground: #6c757d;\n\t}\n\n\t.move-btn:hover:not(:disabled) {\n\t\tbackground: #5a6268;\n\t}\n\n\t.move-btn:disabled {\n\t\tbackground: #dee2e6;\n\t\tcolor: #6c757d;\n\t\tcursor: not-allowed;\n\t}\n\n\t.remove-btn {\n\t\tbackground: #dc3545;\n\t}\n\n\t.remove-btn:hover {\n\t\tbackground: #c82333;\n\t}\n\n\t.error {\n\t\tcolor: red;\n\t\tpadding: 1rem;\n\t\tbackground: #fee;\n\t\tborder-radius: 4px;\n\t}\n`\n\nKey changes:\n\nAdded moveComponentUp() and moveComponentDown() functions that:\n   Find the current position of the component in the config\n   Swap it with the adjacent component\n   Rebuild the config object in the new order\n\nAdded component controls section with three buttons:\n   Up arrow () to move component up\n   Down arrow () to move component down\n    to remove the component\n\nAdded index tracking in the {#each} loop to know when to disable buttons:\n   Up button is disabled for the first component\n   Down button is disabled for the last component\n\nUpdated styling to accommodate the new button layout and disabled states\n\nNow you can reorder components by clicking the up/down arrows, and the JSON config will be updated to reflect the new order!\n\nrogerpence: does it increase complexity too much to have a single addComponent that is able to the type of component specified\n\nGitHub Copilot: Not at all! A single generic addComponent function with a component type parameter actually reduces complexity. Here's how you can refactor it:\n\n\n\t// ...existing code...\n\n\t// Single generic function to add any component type\n\tfunction addComponent(componentType: string, props: Record = {}) {\n\t\tconst instanceId = ${componentType}-${componentCounter};\n\t\tcomponentCounter++;\n\n\t\tconfig = {\n\t\t\t...config,\n\t\t\t[instanceId]: {\n\t\t\t\tcomponentType,\n\t\t\t\t...props\n\t\t\t}\n\t\t};\n\t}\n\n\t// Simplified specific functions (optional - you could call addComponent directly)\n\tfunction addGreenComponent() {\n\t\taddComponent('green', {\n\t\t\tsize: 'small',\n\t\t\tintensity: 'bright'\n\t\t});\n\t}\n\n\tfunction addAnotherGreenComponent() {\n\t\taddComponent('green', {\n\t\t\tsize: 'large',\n\t\t\tintensity: 'dim'\n\t\t});\n\t}\n\n\t// Or even simpler - just use the generic function directly\n\tfunction addOrangeComponent() {\n\t\taddComponent('orange', { size: 'medium', rate: 5 });\n\t}\n\n\tfunction addBlueComponent() {\n\t\taddComponent('blue', { size: 'small', length: 'long' });\n\t}\n\n\t// ...rest of existing code...\n\n\n\n\n\t addComponent('green', { size: 'small', intensity: 'bright' })}>\n\t\tAdd Green (Small)\n\t\n\t addComponent('green', { size: 'large', intensity: 'dim' })}>\n\t\tAdd Green (Large)\n\t\n\t addComponent('orange', { size: 'medium', rate: 5 })}>\n\t\tAdd Orange\n\t\n\t addComponent('blue', { size: 'tiny', length: 'short' })}>\n\t\tAdd Blue\n\t\n\tRemove Orange Component\n\tLoad External Config\n\n\n`\n\nBenefits of this approach:\n\nLess code duplication - one function handles all component types\nMore flexible - easy to add new component types without new functions\nConsistent behavior - all components get added the same way\nEasier to maintain - changes to add logic only need to happen in one place\nBetter for dynamic scenarios - you could even build a dropdown to select component type\n\nEven more advanced - with a component selector:\n\n\n\t// ...existing code...\n\n\tlet selectedComponentType = $state('green');\n\tlet componentProps = $state({ size: 'medium' });\n\n\tfunction addSelectedComponent() {\n\t\taddComponent(selectedComponentType, componentProps);\n\t}\n\n\n\n\t\n\t\tGreen\n\t\tOrange\n\t\tBlue\n\t\n\n\t\n\n\tAdd Selected Component\n\n\t\n\t addComponent('green', { size: 'small', intensity: 'bright' })}>\n\t\tQuick Add Green\n\t\n`\n\nThis approach is actually simpler and more maintainable than having separate functions for each component type!",
        "locale": "en",
        "slug": "technical-posts/dynamic-svelte-components",
        "folder": ""
    },
    {
        "title": "CSS units I use",
        "description": "CSS units I use",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "TheviUnit\n\nvistands for \"viewport-inline size\" - it's a modern CSS viewport unit\nIt represents 1% of the viewport's width (inline dimension) regardless of writing mode\nSimilar tovwbut more flexible since it adapts to writing direction\n1vi = 1% of viewport width in horizontal writing modes",
        "locale": "en",
        "slug": "technical-posts/css-units",
        "folder": ""
    },
    {
        "title": "CSS focus-within pseudo selector",
        "description": "CSS focus-within pseudo selector",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "\n\n\n01\n\n\n\n\n\n    Focus Within\n\n    \n        \n        * Must use 3 digits, 4 letters, 1 caplized letter, 2 dogs, and 2.4 special characters\n    \n\n\n\n\n.form-container {\n    position: relative;\n    z-index: 2;\n}\n\n.password-prompt {\n    opacity: 0;\n    transition: all .5s ease-in-out;\n    position: absolute;\n    transform: translateY(40px);\n    top: 40;\n    z-index: 1;\n    pointer-events: none;\n}\n\n.form-container:focus-within .password-prompt{\n    opacity: 1;\n    transform: translateY(5px);\n}",
        "locale": "en",
        "slug": "technical-posts/focus-within",
        "folder": ""
    },
    {
        "title": "HTML form custom validation with setCustomValidity",
        "description": "HTML form custom validation with setCustomValidity",
        "date_created": "2025-06-02T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "html"
        ],
        "content": "see\n\nC:\\Users\\thumb\\Documents\\projects\\asna\\_asna.com\\_project_ocho\\src\\components\\all-locales\\all-pages\\landing-page-form.svelte\n\nfor this code used in a Svelte component.\n\nThe setCustomValidity(message) method is a powerful part of the HTML5 Constraint Validation API. It allows you to programmatically set a custom validation message for a form control element (like `, , or `), thereby marking it as invalid.\n\nHere's a breakdown:\n\nPurpose:\n    To define custom validation rules beyond what HTML5 attributes (required, pattern, min, max, type=\"email\", etc.) can offer.\n    To provide user-friendly, specific error messages for these custom rules.\n    To integrate your custom JavaScript validation logic with the browser's native validation UI (e.g., error bubbles, styling with :invalid CSS pseudo-class).\n\nHow it Works:\n    element.setCustomValidity(messageString):\n        If messageString is a non-empty string:\n            The element is considered invalid.\n            The element.validity.customError property becomes true.\n            The element.validationMessage property will return your messageString.\n            The browser will typically prevent form submission (if the form doesn't have novalidate) and display this messageString as an error (e.g., in a tooltip) when validation is triggered (e.g., on submit, or if element.reportValidity() is called).\n        If messageString is an empty string (\"\"):\n            The custom validity error is cleared.\n            The element.validity.customError property becomes false.\n            The element is now considered valid with respect to this custom check.\n            Crucially: If other built-in constraints (like required or type=\"email\") are still violated, the element will remain invalid, but the validationMessage will revert to the browser's default message for those other constraints. If no other constraints are violated, the element becomes fully valid.\n\nKey Elements Involved:\n    Form Control Elements: `, , , `.\n    validity Property: An object ( ValidityState) on the form control element. It has boolean properties like valueMissing, typeMismatch, patternMismatch, tooShort, rangeOverflow, customError, and a general valid property.\n    validationMessage Property: A string containing the error message that the browser would display for the current invalid state. This will be your custom message if customError is true.\n    checkValidity() Method: Returns true if the element is valid, false otherwise. It does not display any UI.\n    reportValidity() Method: Checks validity and, if invalid, reports the error to the user (e.g., shows the error bubble) and returns false. Returns true if valid.\n\nExample: Password Confirmation\n\nA common use case is validating that two password fields match.\n\n\n  \n    Password:\n    \n  \n  \n    Confirm Password:\n    \n    \n  \n  Sign Up\n\n\n\n  const passwordInput = document.getElementById('password');\n  const confirmPasswordInput = document.getElementById('confirmPassword');\n  const form = document.getElementById('signupForm');\n\n  function validatePasswords() {\n    if (passwordInput.value !== confirmPasswordInput.value) {\n      // Set a custom error message on the confirmPassword field\n      confirmPasswordInput.setCustomValidity(\"Passwords do not match.\");\n    } else {\n      // Clear the custom error message (important!)\n      confirmPasswordInput.setCustomValidity(\"\");\n    }\n  }\n\n  // Validate on input in the confirm password field\n  confirmPasswordInput.addEventListener('input', validatePasswords);\n  // Also validate when the main password changes, as it might make the confirmation valid/invalid\n  passwordInput.addEventListener('input', () => {\n    // Re-run validation, which might trigger setCustomValidity on confirmPasswordInput\n    validatePasswords();\n    // Optionally, to make the browser re-check and potentially show/hide errors immediately:\n    // confirmPasswordInput.reportValidity();\n  });\n\n  form.addEventListener('submit', function(event) {\n    // Run custom validations one last time before submit\n    validatePasswords();\n\n    // Manually check all fields and report validity if you're not relying solely on browser default submit behavior\n    if (!form.checkValidity()) {\n      event.preventDefault(); // Prevent submission\n      // You might want to loop through invalid fields and call reportValidity() on them\n      // or display errors in a custom way.\n      // For simplicity, the browser will often show errors on invalid fields on submit attempt.\n      console.log(\"Form is invalid.\");\n    } else {\n      console.log(\"Form submitted (simulated).\");\n      event.preventDefault(); // Prevent actual submission for this demo\n    }\n  });\n\n\nWhy is setCustomValidity(\"\") so important?\n\nIf you set a custom error message and the condition that caused the error is later resolved, you must call setCustomValidity(\"\") to clear that custom error. Otherwise, the field will remain marked as invalid with your custom message, even if it technically meets all criteria.\n\nWhen to use setCustomValidity:\n\nWhen HTML5 built-in validation attributes are insufficient (e.g., comparing two fields, complex business logic).\nWhen you want to provide more specific or user-friendly error messages than the browser defaults.\nWhen you want to integrate your custom validation logic seamlessly with the browser's validation UI and the :valid/:invalid CSS states.\n\nIt's a fundamental tool for creating robust and user-friendly client-side form validation.",
        "locale": "en",
        "slug": "technical-posts/custom-html-form-validation-with-setcustomvalidity",
        "folder": ""
    },
    {
        "title": "Node.js GoTo Webinar Registration Example",
        "description": "Node.js GoTo Webinar Registration Example",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-com",
            "goto-webinar"
        ],
        "content": "See also:\n\nNode example\nThe short story:\n\nInitial setup: \nGet the top-level GoTo OAuth access token with this API: https://authentication.logmeininc.com/oauth/authorize\nGet the access token with this API: https://authentication.logmeininc.com/oauth/token\n\n!important]\nYou can only get the access token once with the top-level OAuth code. \n\nSee [[goto-create-oauth-client]] for the full details on the initial setup for using the GoTo APIs. \n\nAt runtime: \nRefresh the access token with this API https://authentication.logmeininc.com/oauth/token\n\n[Developer Portal\n\nMy Sveltekit work is here:\n\nC:\\Users\\thumb\\Documents\\projects\\sveltekit\\apis\\src\\routes\\register-attendee\n\nHow to programmatically register an attendee for a GoToWebinar\n\nAnother example\n\nGotoWebinar Personal Access Token (created 23 Apr 2025)\n\nasnaevents@asna.com\n\n200000000000313229_v9Ap3v9dubwbIGMV2RnjhyH7dMlywdmQ\n\nrp@asna.com\n\n5121648670853925893_wib1hkFvfIbegzCbFsDIi4rTkH8V60Sr\n\nGotoWebinar says the organizerKey is :\n\n200000000000313229\n\nwebinar key is (for the first GoTo Webinar):\n\n5372803598377463127\n\nHow to get the API authorization code\n\nYou need five values to get a GoTo access key:\n\nClient ID\nSecret\nThe base 64 value of client id]:[secret]\nThe redirect URI\nThe API authorization code\n\nStep 1. Create an OAuth client\n\nhttps://developer.logmeininc.com/clients\n\nThis provides the Client ID and the secret. Use the site provided to calculate #3, the base 64 value.\n\nThe URI is https://asna.com\n\nThese instructions come from this link: https://developer.goto.com/guides/Authentication/03_HOW_accessToken\n\nCurrent client\n\n1. Client ID\n\n43e6ab89-b989-48f9-87ac-2d1388a22985\n\n2. Secret \n\nejwoY1WMx6fkWQGISALfGKoK\n\n3. Base 64 value of [client id]:[secret(https://www.base64encode.org/)\n\nNDNlNmFiODktYjk4OS00OGY5LTg3YWMtMmQxMzg4YTIyOTg1OnhERzdGTzVGZ3pnZFRqMXRGZUFnUlV6ZQ==\n\n4. URL\n\nhttps://asna.com\n\nA 5th value is needed. Keep reading.\n\nUsing a fresh client id (you can only do this one with given client id), use the template below to create a URL where {clientID} = client id from OAuth client and redirectUri = \"https://asna.com\"\n\nhttps://authentication.logmeininc.com/oauth/authorize?client_id={clientID}&response_type=code&redirect_uri={redirectUri}\n\nFor this example, after substitutions the URL then look like this:\n\nhttps://authentication.logmeininc.com/oauth/authorize?client_id=38f340bd-0b9c-4928-a2b2-d502cf180735&response_type=code&redirect_uri=https://asna.com\n\nSubmit that URL with a browser. It will show this screen:\n\n![Node.js GoTo Webinar Registration Example.png|200]]\n\nClick \"Allow\" and you'll be returned to https://asna.com but the URL contains a code search param with a very large code:\n\nhttps://asna.com.com/?code=iS0vynEEvRFA9i6kZ8gvNDnnOGE...\n\nThe value returned in with the code key is the API authorization code. This is the 5th value needed.\n\n5. API authorization code\n\nASNAEvents auth code\n\neyJraWQiOiI2MjAiLCJhbGciOiJSUzUxMiJ9.eyJzYyI6ImNvbGxhYjoiLCJzdWIiOiIyMDAwMDAwMDAwMDAzMTMyMjkiLCJhdWQiOiI0M2U2YWI4OS1iOTg5LTQ4ZjktODdhYy0yZDEzODhhMjI5ODUiLCJvZ24iOiJwd2QiLCJscyI6Ijk2MDkzZWVjLWYyMzctNDc3Ni1hM2ZiLWJjNTEwYmM5YTY3OCIsInR5cCI6ImMiLCJleHAiOjE3NDY2NTI5NjYsImlhdCI6MTc0NjY1MjM2NiwidXJpIjoiaHR0cHM6Ly9hc25hLmNvbSIsImp0aSI6ImU4OTY0YTBhLWEwYTgtNGNjOS05ODVmLThiYWQ3NzYzZTA5OCJ9.FgVwjCv6o3njSNge_rXK4MXQw6DwiMrtmcPVTEqbcWGAcYh-wMC6SLPGD2_l6Vw7MDIqoO_DfQXWnKLyBdYE_zoF4W3omWMnsTJUwuoieNBrTH57mZU50gy8ieqHP_cPQBJX5p4M3IuFQdkx5-wySxkj4BI69KgqbyZVwRqnX7ViVe8reUikL8RB1NhDV0BVGHeXy4cr1s4GZDexOZkl1dFCj1Mp7tqB-d55fNa7JsQmruhWLuToXEKim_doQkTGG2BoV93i7PRSi0l1WT7jTEozjD6VUnqGo5e_iSpHSc8chtlNPjCcH9DHpjQR6A4VUzOLfhYcxxVI8cpTY6vlGg\n\nHow to refresh the API access token\n\nThis needs be done before every API request to GoTo and needs the API access token. \n\n[There are instructions to use the values from above to create the API access token. However, they are so convoluted they aren't much help. To get the API access token, it's much easier to use VS Code's Thunder Client (a Postman like HTTP client).\n\nUse Thunder Client with these two screens to produce the API access token:\n\n\neyJraWQiOiI2MjAiLCJhbGciOiJSUzUxMiJ9.eyJzYyI6ImNvbGxhYjoiLCJzdWIiOiIyMDAwMDAwMDAwMDAzMTMyMjkiLCJhdWQiOiI0M2U2YWI4OS1iOTg5LTQ4ZjktODdhYy0yZDEzODhhMjI5ODUiLCJvZ24iOiJwd2QiLCJscyI6Ijc3YTBlYTMyLTdiOGMtNGE3ZS05NTRjLTg4YzA3ODM0NDZlZCIsInR5cCI6ImMiLCJleHAiOjE3NDc3Njc4NTgsImlhdCI6MTc0Nzc2NzI1OCwidXJpIjoiaHR0cHM6Ly9hc25hLmNvbSIsImp0aSI6ImI2MGZjZmJkLWUxMjYtNDhkNC05ZDY4LTkzYWIzMmNlOTYxMyIsImxvYSI6M30.UpoiRHT01rdM8SCk09w20jb52ALzV5hOjWleCz0ECYDXniAnfr54JrYR92IsvVQmahfK5_oGmQeMn67646N0NzlgPwt2mIzq53xpUcCmLWGSIuVoKKvRbSMVfH7pflC376cETLexRnn_gskUIcKSOB1xZPAj3gqWbWyWSV2xeqHlhRnySICK43Qju3CdKbQcO06qzF7MxzItFsn_N2SnON45FM-YIkIhm-Pvob7LzoTh24zOgV5d_9Un5zTFECM04BPzLiYp_12YSYBPptBO8GCRWszimCLlto0iSGT7e0MO3ltV-xg4_kxApK2PaS53SCUhmVPdWCsWMSBjrWZ-zw\n![image-3.png|774x301]]\n\n![[image-4.png|774x301]]\n\nThat request produces a Json document that shows the access token and the new refresh token. The refresh token is important because access tokens expire in 60 minutes and need to be refreshed before attempting any GoTo API calls.\n\nThe access token should be refreshed every time a GoTo Webinar API call is made.\n\n{\n  \"access_token\": \"eyJraWQiOiI2MjAiLCJhbGc...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"scope\": \"collab:\",\n  \"principal\": \"asnaevents@asna.com\",\n  \"loa\": 3\n}\n\nIt's important to note about every 30 days the refresh token will also be replaced when calling this API. In that case, it returns both the current access_token and the new refresh_token. When the response includes the refresh_token it must replace the current refresh_token (that is currently stored in the .env file.)\n\n{\n  \"access_token\": \"eyJraWQiOiI2MjAiLCJhbGc...\",\n  \"token_type\": \"Bearer\",\n  \"refresh_token\": \"eyJraWQiOiJvYXV0aHYyLmxt999..\",\n  \"expires_in\": 3600,\n  \"scope\": \"collab:\",\n  \"principal\": \"asnaevents@asna.com\",\n  \"loa\": 3\n}\n\n![[image-5.png|780x591]]\n\nASNA Events access key\n\n{\n  \"access_token\": \"eyJraWQiOiI2MjAiLCJhbGciOiJSUzUxMiJ9.eyJzYyI6ImNvbGxhYjoiLCJzdWIiOiIyMDAwMDAwMDAwMDAzMTMyMjkiLCJhdWQiOiI0M2U2YWI4OS1iOTg5LTQ4ZjktODdhYy0yZDEzODhhMjI5ODUiLCJvZ24iOiJwd2QiLCJscyI6Ijk2MDkzZWVjLWYyMzctNDc3Ni1hM2ZiLWJjNTEwYmM5YTY3OCIsInR5cCI6ImEiLCJleHAiOjE3NDY2NTYyNDIsImlhdCI6MTc0NjY1MjY0MiwianRpIjoiMjA0NTk1MWMtNDZiZi00MzI3LWJjMzItMTZjZWNiMTI2MWUxIn0.hUX6G06hEkyCqlFk_aNiT4RayuhhLKmtIJq1xkN43gVm8h1ZesxuRT1LTOFYyTrO9UgcHUKDCUBgjBYdxpKegJekHcbTKGQDW3FauGvAkLXovMt3jlPO7qVTAVUmqecgBNZ05EamS85dUm6G1XJyDOkIT3YLftZ-azW3USRvPIgbgrDxIsdEdDqt5PW1zdf5UGFQzd7QX3IERvA-g6QiODrAMxqxupdF9hJ0mHwfC4j5WLdE4w3gxPBvICrFwQJGoM_Y7qL4FezH-5U1l-VCSEe_oqqMLs6IR4sqA7TXsWSSVzbS_Vbt_O_APJFKzDj40Dflitab8FNBmiZoa5inrA\",\n  \"token_type\": \"Bearer\",\n  \"refresh_token\": \"eyJraWQiOiI2MjAiLCJhbGciOiJSUzUxMiJ9.eyJzYyI6ImNvbGxhYjoiLCJzdWIiOiIyMDAwMDAwMDAwMDAzMTMyMjkiLCJhdWQiOiI0M2U2YWI4OS1iOTg5LTQ4ZjktODdhYy0yZDEzODhhMjI5ODUiLCJvZ24iOiJwd2QiLCJ0eXAiOiJyIiwiZXhwIjoxNzQ5MjQ0NjQyLCJpYXQiOjE3NDY2NTI2NDIsImp0aSI6IjdiNjhmNDYwLTI5N2YtNGZmYS05YzU1LTY5MTgxOWFlYjk3OSJ9.ri5VKt4Zi9CVgns_wCGIxbsjFykQFyUcwF9khZmTo4psU0bH8CQslllb7fvtZBuw3usXC3Pkxh9Fq6jJQexPezpfhQ6uB9bmGFsnlqZG23C5hSoe6gj78S1Z8Vqyk98uXINZLH7JClDc7ruFEdjHEy5JY-agkAFuHUH0FWwyCfLfbDWLH1uGijn913JXd0fLgsIcnNDfosuqjcbbCFe74aGDTbdcCuANXQNSO01ozjULWJz0G0JLhPAw-_DU2FdgjD2iaDA4-cHvWMRM1VMAVnu6ZpO2XmCuPYvZ8k_RMTgAkRY-DXBhdp2_h4Q1234S57if4Q5kLe2kPtI-9WVpkQ\",\n  \"expires_in\": 3600,\n  \"scope\": \"collab:\",\n  \"principal\": \"asnaevents@asna.com\",\n  \"loa\": 2\n}\n\nRefreshing the access token\n\nAccess tokens have a 60 minute lifespan. For more detail on refresh tokens [see this document.\n\nUse the Thunder Client to refresh the token as shown below. Also see the Sveltekit code to refresh the token. The Sveltekit code need to be refactored into an API (maybe!).\n\n!important] > [[Sveltekit route to refresh Goto API key]]\n\n![[image-6.png|1153x439]]\n\n![[image-7.png|1158x501]]\n\n![[image-8.png|1158x478]]\n\nOther weirdo/miscellaneous notes below\n\nI created a personal access token with my rp@asna.com login.\nI created an OAuth client that produced this Client ID: 38f340bd-0b9c-4928-a2b2-d502cf180735\nI followed these directions to get an access token: https://developer.goto.com/guides/Authentication/03_HOW_accessToken/ and did successfully get an access token and a refresh token. I can successful use the refresh token to get a refreshed access token.\nI have a Webinar scheduled and its webinarKey is 5372803598377463127 https://dashboard.gotowebinar.com/webinar/5372803598377463127\nI want to use this API https://api.getgo.com/G2W/rest/v2/organizers/[organizerKey]/webinars/[webinarKey]/registrants to programmatically add a registrant from our landing page(s).\n\nQuestions\n\nWhere do I get the organizerKey for the Create Registrant API?\n    The Create registrant API needs two parameters (both of which are int64): organizerKey and webinarKey. I think the webinarKey is the value at the end of a Webinar URL (from step 4 above).\nThe Create Registrant API url has 'v2' in it but uses what appears to be a deprecated domain: api.getgo.com. Is the URL below the correct URL for the Create Registrant API?\n    https://api.getgo.com/G2W/rest/v2/organizers/[organizerKey]/webinars/[webinarKey]/registrants\n\nHere's a Node.js example using axios to register a participant for a GoTo Webinar using their V2 API.\n\nGetting keys\n\n![[Node.js GoTo Webinar Registration Example-20250423141731996.webp|500]]\n\nAssumptions:\n\nYou have Node.js and npm (or yarn) installed.\nYou have already gone through the GoTo Developer OAuth 2.0 flow to obtain a valid Access Token. This example does not cover the token acquisition process itself, which typically involves user authorization or using a refresh token.\nYou know your Organizer Key. [[GoTo Webinar refresh API access#Get an organizer key|How to get an organizer key.]]\nYou know the Webinar Key for the specific webinar you want to register someone for.\n\nSteps:\n\nSet up your project:\n\n    mkdir goto-webinar-register\n    cd goto-webinar-register\n    npm init -y\n    npm install axios dotenv\n\nCreate a .env file:\n    Create a file named .env in the root of your project directory (goto-webinar-register). This file will securely store your credentials. Never commit this file to Git.\n\n    GOTO_API_BASE_URL=https://api.getgo.com\n    GOTO_ACCESS_TOKEN=\"YOUR_VALID_ACCESS_TOKEN_HERE\" # Replace with your actual access token\n    GOTO_ORGANIZER_KEY=\"YOUR_ORGANIZER_KEY_HERE\"   # Replace with your organizer key (usually a number)\n    GOTO_WEBINAR_KEY=\"YOUR_WEBINAR_KEY_HERE\"     # Replace with the specific webinar key (usually a number)\n\n    Replace the placeholder values with your actual credentials.\n\nCreate the Node.js script (e.g., register.js):\n\n    // register.js\n    require(\"dotenv\").config(); // Load environment variables from .env file\n    const axios = require(\"axios\");\n\n    // --- Configuration ---\n    const API_BASE_URL = process.env.GOTO_API_BASE_URL;\n    const ACCESS_TOKEN = process.env.GOTO_ACCESS_TOKEN;\n    const ORGANIZER_KEY = process.env.GOTO_ORGANIZER_KEY;\n    const WEBINAR_KEY = process.env.GOTO_WEBINAR_KEY;\n\n    // --- Registrant Details ---\n    const registrantInfo = {\n        firstName: \"John\",\n        lastName: \"Doe\",\n        email: john.doe.${Date.now()}@example.com, // Using timestamp for uniqueness in testing\n        // Add other optional fields as needed per GoTo API docs:\n        // address: \"123 Main St\",\n        // city: \"Anytown\",\n        // state: \"CA\",\n        // zipCode: \"90210\",\n        // country: \"USA\",\n        // phone: \"555-123-4567\",\n        // industry: \"Technology\",\n        // organization: \"Example Corp\",\n        // jobTitle: \"Developer\",\n        // purchasingTimeFrame: \"Within a month\",\n        // roleInPurchaseProcess: \"Decision maker\",\n        // noOfEmployees: \"100-499\",\n        // questionsAndComments: \"Looking forward to the webinar!\",\n        // source: \"Website Signup Form\" // Helps track where registrations come from\n    };\n\n    /**\n     Registers a participant for a GoTo Webinar.\n     @param {string} organizerKey - The organizer's key.\n     @param {string} webinarKey - The specific webinar's key.\n     @param {object} registrantData - An object containing registrant details (firstName, lastName, email required).\n     @param {string} accessToken - The valid OAuth 2.0 access token.\n     @returns {Promise} - A promise that resolves with the API response data on success.\n     */\n    async function registerParticipant(\n        organizerKey,\n        webinarKey,\n        registrantData,\n        accessToken\n    ) {\n        if (!organizerKey || !webinarKey || !registrantData || !accessToken) {\n            throw new Error(\n                \"Missing required parameters: organizerKey, webinarKey, registrantData, or accessToken.\"\n            );\n        }\n        if (\n            !registrantData.firstName ||\n            !registrantData.lastName ||\n            !registrantData.email\n        ) {\n            throw new Error(\n                \"Registrant data must include firstName, lastName, and email.\"\n            );\n        }\n\n        const registrationUrl = ${API_BASE_URL}/G2W/rest/v2/organizers/${organizerKey}/webinars/${webinarKey}/registrants;\n\n        console.log(\n            Attempting registration for ${registrantData.email} to webinar ${webinarKey}...\n        );\n        console.log(POST URL: ${registrationUrl});\n        console.log(\"Payload:\", JSON.stringify(registrantData, null, 2)); // Log the data being sent\n\n        try {\n            const response = await axios.post(registrationUrl, registrantData, {\n                headers: {\n                    Authorization: Bearer ${accessToken},\n                    \"Content-Type\": \"application/json\",\n                    Accept: \"application/json\", // Or 'application/vnd.citrix.g2wapi-v2.0+json' if specifically needed\n                },\n                // Optional: Set a timeout\n                // timeout: 10000 // milliseconds\n            });\n\n            console.log(\"Registration successful!\");\n            return response.data; // The API response (usually contains registrantKey, joinUrl, etc.)\n        } catch (error) {\n            console.error(\"Registration failed:\");\n            if (error.response) {\n                // The request was made and the server responded with a status code\n                // that falls out of the range of 2xx\n                console.error(\"Status:\", error.response.status);\n                console.error(\n                    \"Headers:\",\n                    JSON.stringify(error.response.headers, null, 2)\n                );\n                console.error(\n                    \"Data:\",\n                    JSON.stringify(error.response.data, null, 2)\n                ); // Often contains specific API error details\n                // Specific GoTo Error Check\n                if (\n                    error.response.data &&\n                    error.response.data.errorCode === \"AlreadyRegistered\"\n                ) {\n                    console.warn(\n                        Email ${registrantData.email} is already registered for this webinar.\n                    );\n                    // Decide how to handle this - maybe return a specific indicator or just log it.\n                    // You might still want to throw an error or return null depending on your app logic.\n                    // For this example, we'll re-throw but you could customize this.\n                    throw new Error(\n                        Already registered: ${error.response.data.description}\n                    );\n                }\n                throw new Error(\n                    `API Error ${error.response.status}: ${\n                        error.response.data?.description || error.message\n                    }`\n                );\n            } else if (error.request) {\n                // The request was made but no response was received\n                console.error(\"Request Error: No response received.\");\n                console.error(error.request);\n                throw new Error(Network Error: ${error.message});\n            } else {\n                // Something happened in setting up the request that triggered an Error\n                console.error(\"Error:\", error.message);\n                throw error; // Re-throw the original error\n            }\n        }\n    }\n\n    // --- Main Execution ---\n    (async () => {\n        if (!ACCESS_TOKEN || !ORGANIZER_KEY || !WEBINAR_KEY) {\n            console.error(\n                \"ERROR: Missing required environment variables (GOTO_ACCESS_TOKEN, GOTO_ORGANIZER_KEY, GOTO_WEBINAR_KEY).\"\n            );\n            console.error(\"Please ensure the .env file is correctly set up.\");\n            process.exit(1); // Exit with an error code\n        }\n        if (ACCESS_TOKEN === \"YOUR_VALID_ACCESS_TOKEN_HERE\") {\n            console.warn(\n                \"WARNING: Using placeholder Access Token. Please update .env file.\"\n            );\n        }\n\n        try {\n            const registrationResult = await registerParticipant(\n                ORGANIZER_KEY,\n                WEBINAR_KEY,\n                registrantInfo,\n                ACCESS_TOKEN\n            );\n\n            console.log(\"\\n--- Registration Result ---\");\n            console.log(JSON.stringify(registrationResult, null, 2));\n            console.log(\\nSuccessfully registered ${registrantInfo.email}.);\n            console.log(Registrant Key: ${registrationResult.registrantKey});\n            console.log(Join URL: ${registrationResult.joinUrl});\n        } catch (error) {\n            console.error(\n                \"\\n--- An error occurred during the registration process ---\"\n            );\n            // Error details are already logged within the registerParticipant function\n            // console.error(error.message); // Optionally log the final error message again\n            process.exit(1); // Exit with an error code\n        }\n    })();\n\nRun the script:\n    node register.js\n\nExplanation:\n\ndotenv: Loads the variables from your .env file into process.env.\naxios: Used to make the HTTP POST request.\nConfiguration: Reads the necessary keys and base URL from environment variables.\nregistrantInfo: An object holding the data for the new registrant. firstName, lastName, and email are mandatory. You can add more fields as supported by the GoTo Webinar API V2 documentation. Using Date.now() in the email helps ensure uniqueness when testing repeatedly.\nregisterParticipant function:\n    Takes the keys, registrant data, and access token as arguments.\n    Constructs the correct API endpoint URL using template literals.\n    Uses axios.post to send the request.\n    The third argument to axios.post is the configuration object, including:\n        headers: Sets the required Authorization (with Bearer prefix), Content-Type, and Accept headers.\n    Includes robust try...catch error handling:\n        It checks if the error has a response property (meaning the API server responded with an error status like 400, 401, 403, 409). It logs the status and the response data, which often contains GoTo-specific error codes (errorCode) and descriptions. It specifically checks for the AlreadyRegistered error code.\n        It checks if the error has a request property but no response (network error, timeout).\n        It handles other setup errors.\n    Returns the response.data on success.\nMain Execution Block:\n    An immediately-invoked async function (async () => { ... })() is used to allow await.\n    It performs basic checks to ensure environment variables are loaded.\n    Calls registerParticipant with the configuration and registrant data.\n    Logs the successful response (which includes the unique registrantKey and the joinUrl for the participant) or logs the error caught by the try...catch.\n\nRemember to consult the [official GoTo Developer API documentation for the most up-to-date list of available registrant fields, error codes, and API behavior.\n\nGet an organizer key\n\nOkay, getting the organizerKey is a fundamental step. Unlike the webinarKey (which changes per webinar) or the registrantKey (which changes per registrant), the organizerKey is generally static for a specific GoToWebinar user account that can host webinars.\n\nHere are the primary ways to find your organizerKey:\n\nDuring the OAuth 2.0 Access Token Response (Most Common for API Integration):\n\n    This is the most reliable and intended method when building an application.\n    When you successfully exchange an authorization_code (obtained after the user grants permission) for an access_token using the /oauth/v2/token endpoint, the JSON response from GoTo includes the organizer_key along with the access_token, refresh_token, account_key, email, etc.\n    Example Response Payload (structure might vary slightly):\n        {\n            \"access_token\": \"RcVd3a...EXAMPLE...3j7hqU\",\n            \"token_type\": \"Bearer\",\n            \"expires_in\": 3600, // Usually 1 hour\n            \"refresh_token\": \"l1QPj...EXAMPLE...mKso4\",\n            \"organizer_key\": \"1234567890123456789\", // <-- THIS IS WHAT YOU NEED\n            \"account_key\": \"987654321\",\n            \"account_type\": \"corporate\",\n            \"firstName\": \"Jane\",\n            \"lastName\": \"Doe\",\n            \"email\": \"jane.doe@example.com\",\n            \"platform\": \"GLOBAL\", // Or G2M, G2W etc.\n            \"version\": \"2\"\n        }\n    Action: When your application handles the OAuth callback and exchanges the code for a token, parse this JSON response and securely store the organizer_key (along with the access/refresh tokens) associated with that user's session or profile in your application.\n\nGoTo Developer Center (developer.goto.com):\n    Log in to the GoTo Developer Center.\n    Navigate to the \"My Apps\" section where you registered your API client (Client ID and Client Secret).\n    While the Developer Center primarily manages your API clients, sometimes associated account details, including potentially the organizerKey of the account that owns the API client, might be visible somewhere in the account settings or profile section within the developer portal itself. However, this is less direct and might not always be explicitly shown. The primary method is the OAuth token response.\n\nImportant Considerations:\n\nUser Context: The organizerKey you receive is specific to the GoToWebinar user account who authorized your application during the OAuth flow. If multiple organizers from the same company use your application, each will have their own organizerKey obtained when they individually authorize the app.\nPermissions: The user authenticating via OAuth must be a user with organizer privileges on a GoToWebinar account to have a relevant organizerKey.\nNot the Same as Account Key: Don't confuse the organizerKey with the accountKey. The accountKey usually refers to the overall GoTo administrative account, while the organizerKey refers to the specific licensed user who can create and manage webinars. You nearly always need the organizerKey for webinar-specific API calls.\nIt's Numerical: The organizerKey is typically a long string of numbers.\n\nIn summary: The standard and most reliable way to get the organizerKey for API use is by capturing it from the JSON response when obtaining an access token via the OAuth 2.0 flow.\n\nHow to get a Webinar key\n\nOkay, getting the webinarKey (sometimes referred to as the Webinar ID) is essential for targeting API calls to a specific webinar. Unlike the organizerKey (which identifies the user), the webinarKey identifies the event itself.\n\nHere are the primary ways to get a webinarKey:\n\nUsing the GoTo Webinar API (Most Common for Applications):\n\n    This is the standard method when building integrations. You'll typically list the organizer's webinars and then extract the webinarKey from the results.\n    Endpoint: GET /G2W/rest/v2/organizers/{organizerKey}/webinars\n    Process:\n        You need a valid accessToken and the organizerKey of the user whose webinars you want to list.\n        Make an authenticated GET request to the endpoint above, replacing [organizerKey] with the actual key.\n        Headers: Include Authorization: Bearer YOUR_ACCESS_TOKEN_HERE and Accept: application/json.\n        Response: The API will return a JSON array. Each object in the array represents a scheduled webinar and contains details about it. Crucially, each webinar object will have a webinarKey field (a string containing a large number).\n        Filtering (Optional but Recommended): The /webinars endpoint returns all upcoming webinars by default. You can use query parameters to filter:\n            fromTime: ISO 8601 format (e.g., 2023-10-27T00:00:00Z) - Start date/time for the search range.\n            toTime: ISO 8601 format (e.g., 2023-11-30T23:59:59Z) - End date/time for the search range.\n            Using these parameters allows you to fetch past webinars or webinars within a specific timeframe.\n    Example Response Snippet:\n        [\n            {\n                \"webinarKey\": \"9876543210987654321\", // <-- THIS IS WHAT YOU NEED\n                \"subject\": \"Product Demo Q4\",\n                \"description\": \"A detailed demo of our new features.\",\n                \"organizerKey\": \"1234567890123456789\",\n                \"times\": [\n                    {\n                        \"startTime\": \"2023-11-15T18:00:00Z\",\n                        \"endTime\": \"2023-11-15T19:00:00Z\"\n                    }\n                ],\n                \"timeZone\": \"America/New_York\"\n                // ... other webinar details\n            },\n            {\n                \"webinarKey\": \"555444333222111000\", // Another webinar\n                \"subject\": \"Advanced Training Session\"\n                // ... details\n            }\n        ]\n    Action: Your application needs to parse this JSON response. You might identify the correct webinar by its subject (title), startTime, or other criteria, and then extract its corresponding webinarKey for subsequent API calls (like registering participants).\n\nManually from the GoToWebinar Website URL:\n    If you just need the key for a one-off test or manual check, this is often the quickest way.\n    Process:\n        Log in to your GoToWebinar account on the web.\n        Navigate to \"My Webinars\" (or similar section listing your scheduled events).\n        Click on the title of the specific webinar you're interested in to open its details/management page.\n        Look at the URL in your browser's address bar. It will typically contain the webinarKey.\n        Example URL Structure: https://global.gotowebinar.com/manageWebinar.tmpl?webinar=1234567890123456789 (The exact URL structure might change slightly, but look for a long number associated with webinar= or /webinars/).\n    Caveat: Relying on URL structure is less robust for applications, as website UIs can change. The API method is preferred for integrations.\n\nIn summary:\n\nFor applications and integrations, use the GET /organizers/[organizerKey]/webinars API endpoint to list webinars and extract the webinarKey from the JSON response.\nFor quick manual checks, log in to the GoToWebinar website, view the specific webinar's details page, and find the webinarKey in the browser's URL.",
        "locale": "en",
        "slug": "technical-posts/goto-webinar-refresh-api-access-asna-events",
        "folder": ""
    },
    {
        "title": "Defining types with TypeScript",
        "description": "Defining types with TypeScript",
        "date_created": "2025-07-03T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "Required \n\nThe Required utility type in TypeScript removes the optional and nullable modifiers from all properties of a type, making them all required and non-nullable.\n\nHowever, in your specific case:\n\nexport type CategoryRulesResponse = Required;\n\nThis won't work as expected because your CategoryRulesEntity properties are defined as nullable (e.g., string | null), not optional (e.g., string | undefined).\n\nWhat Required does:\n\nRemoves ? (optional): { name?: string }  { name: string }\nDoes NOT remove | null: { name: string | null }  { name: string | null }\n\nYour current types:\n\n// Your CategoryRulesEntity looks like this:\ntype CategoryRulesEntity = {\n  id: number | null;\n  description: string | null;\n  rule: string | null;\n  value: string | null;\n  category: string | null;\n}\n\n// Required would still be:\ntype CategoryRulesResponse = {\n  id: number | null;        // Still nullable!\n  description: string | null; // Still nullable!\n  rule: string | null;       // Still nullable!\n  value: string | null;      // Still nullable!\n  category: string | null;   // Still nullable!\n}\n\nWhat you probably want:\n\n// To remove nullability, you need a custom utility type:\ntype NonNullable = {\n  [P in keyof T]: NonNullable;\n};\n\nexport type CategoryRulesResponse = NonNullable;\n// Result:\n// {\n//   id: number;\n//   description: string;\n//   rule: string;\n//   value: string;\n//   category: string;\n// }\n\nBetter approach for your use case:\n\nimport { z } from 'zod';\n\nexport const categoryRulesSchema = z.object({\n  id: z.number().int().nullable(),\n  description: z.string().nullable(),\n  rule: z.string().nullable(),\n  value: z.string().nullable(),\n  category: z.string().nullable()\n});\n\n// Database entity (matches physical table with nullable fields)\nexport type CategoryRulesEntity = z.infer;\n\n// DTO without id and with required fields\nexport type CategoryRulesDto = {\n  description: string;\n  rule: string;\n  value: string;\n  category: string;\n};\n\n// Response type with all fields required (for API responses)\nexport type CategoryRulesResponse = {\n  id: number;\n  description: string;\n  rule: string;\n  value: string;\n  category: string;\n};\n\nThis approach is clearer and more explicit about what each type represents.",
        "locale": "en",
        "slug": "technical-posts/defining-types-with-typescript",
        "folder": ""
    },
    {
        "title": "Home-brewed logging to a text file with Sveltekit",
        "description": "Home-brewed logging to a text file with Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Okay, logging to a text file in SvelteKit requires understanding where your code is running:\n\nServer-Side: Code in +page.server.js, +layout.server.js, API routes (+server.js), and hooks.server.js runs on the server (Node.js environment by default). This code can directly write to the filesystem.\nClient-Side (Browser): Code in +page.svelte, +layout.svelte, client-side hooks (hooks.client.js), and regular .js/.ts files imported by Svelte components runs in the user's browser. This code cannot directly write to the server's filesystem due to security restrictions.\n\nTherefore, the strategy depends on where you need to log from.\n\nMethod 1: Logging from Server-Side Code (Recommended for most backend logic)\n\nThis is the most straightforward approach for logging events happening on the server.\n\nCreate a Logging Utility (Optional but Recommended):\n    Create a helper file, for example, src/lib/server/log.js. The server directory ensures this module is only bundled for the server.\n\n    // src/lib/server/log.js\n    import fs from \"fs/promises\"; // Use promises for async operations\n    import path from \"path\";\n\n    // Determine log file path (adjust as needed)\n    // process.cwd() gives the root of your project when running npm run dev or node build\n    const logFilePath = path.join(process.cwd(), \"app.log\");\n\n    // Ensure the log directory exists (optional, but good practice)\n    // You might want to run this once at startup if possible, e.g., in hooks.server.js\n    async function ensureLogFileExists() {\n        try {\n            await fs.access(path.dirname(logFilePath));\n        } catch (error) {\n            if (error.code === \"ENOENT\") {\n                // Directory doesn't exist, create it\n                await fs.mkdir(path.dirname(logFilePath), { recursive: true });\n            } else {\n                // Other error accessing directory\n                console.error(\"Error ensuring log directory exists:\", error);\n                throw error; // Re-throw if critical\n            }\n        }\n        // Optionally touch the file to ensure it exists\n        try {\n            await fs.appendFile(logFilePath, \"\"); // Append empty string to create if not exists\n        } catch (error) {\n            console.error(\"Error ensuring log file exists:\", error);\n            throw error;\n        }\n    }\n    // Consider calling ensureLogFileExists() appropriately\n\n    /**\n     Appends a log message to the application's log file.\n     @param {string} level - Log level (e.g., 'INFO', 'WARN', 'ERROR').\n     @param {string} message - The main log message.\n     @param {any} [data] - Optional additional data (will be JSON.stringified).\n     */\n    export async function logToFile(level, message, data) {\n        // Await ensureLogFileExists(); // Call here if ensuring on every log is acceptable\n\n        const timestamp = new Date().toISOString();\n        let logEntry = ${timestamp} [${level.toUpperCase()}] ${message};\n\n        if (data) {\n            try {\n                // Handle potential circular references or errors during stringification\n                const dataString = JSON.stringify(data, null, 2); // Pretty print JSON\n                logEntry += \\nData: ${dataString};\n            } catch (error) {\n                logEntry += \\nData: [Error serializing data: ${error.message}];\n            }\n        }\n\n        logEntry += \"\\n\"; // Add a newline for separation\n\n        try {\n            // Use appendFile to add to the end of the file\n            await fs.appendFile(logFilePath, logEntry, \"utf8\");\n        } catch (err) {\n            // Fallback to console if file logging fails\n            console.error(Failed to write to log file (${logFilePath}):, err);\n            console.error(\"Original Log Message:\", logEntry.trim());\n        }\n    }\n\n    // Convenience functions\n    export const logInfo = (message, data) => logToFile(\"info\", message, data);\n    export const logWarn = (message, data) => logToFile(\"warn\", message, data);\n    export const logError = (message, error) => {\n        // Special handling for Error objects to include stack trace\n        if (error instanceof Error) {\n            logToFile(\"error\", message, {\n                errorMessage: error.message,\n                stack: error.stack,\n            });\n        } else {\n            logToFile(\"error\", message, error);\n        }\n    };\n    export const logDebug = (message, data) =>\n        logToFile(\"debug\", message, data);\n\n    // Call ensureLogFileExists once when the module loads (or handle startup logic elsewhere)\n    ensureLogFileExists().catch((err) => {\n        console.error(\"Initial log file setup failed:\", err);\n    });\n\nUse the Logger in Server Modules:\n    Import and use the functions in your server-side files.\n\n    // src/routes/some-route/+page.server.js\n    import { logInfo, logError } from \"$lib/server/log\";\n\n    export async function load({ params }) {\n        logInfo(\"Loading data for route\", { routeId: params.slug });\n        try {\n            // ... fetch data or perform actions ...\n            const data = { success: true, message: Data for ${params.slug} };\n            logInfo(\"Data loaded successfully\"); // Log success\n            return data;\n        } catch (error) {\n            logError(\"Failed to load data for route\", error); // Log the actual error object\n            // Handle the error appropriately for the user\n            // throw error(...) or return an error state\n            return { success: false, error: \"Failed to load data\" };\n        }\n    }\n\n    // --- OR ---\n\n    // src/routes/api/users/+server.js\n    import { json } from \"@sveltejs/kit\";\n    import { logWarn } from \"$lib/server/log\";\n\n    export async function POST({ request }) {\n        const userData = await request.json();\n        logWarn(\"Received POST request for user creation\", userData);\n        // ... process user creation ...\n        return json({ userId: \"123\" }, { status: 201 });\n    }\n\nAdd Log File to .gitignore:\n    Make sure your log file isn't committed to version control. Add this line to your .gitignore file:\n\n    app.log\nOr if you create a logs directory:\n    /logs/\n\nMethod 2: Logging from Client-Side Code (Requires an API Endpoint)\n\nSince the browser cannot write directly to the server's file system, you need to send the log message from the client to a server endpoint, which then uses the server-side logging utility (from Method 1).\n\nEnsure you have the Server-Side Logger (Method 1) set up.\n\nCreate an API Endpoint for Logging:\n    Create a file like src/routes/api/log/+server.js.\n\n    // src/routes/api/log/+server.js\n    import { json } from \"@sveltejs/kit\";\n    import { logToFile } from \"$lib/server/log\"; // Import your server-side logger\n\n    /** @type {import('./$types').RequestHandler} */\n    export async function POST({ request }) {\n        try {\n            const { level = \"info\", message, data } = await request.json();\n\n            if (!message) {\n                return json(\n                    { error: \"Log message is required\" },\n                    { status: 400 }\n                );\n            }\n\n            // Use the server-side logger to write the file\n            // Prefix client-side logs for clarity\n            await logToFile(level, [CLIENT] ${message}, data);\n\n            return json({ success: true }, { status: 200 });\n        } catch (error) {\n            // Log the error that occurred within the logging endpoint itself to the console\n            console.error(\"Error processing client log request:\", error);\n\n            // Don't try to log this error using logToFile, as it might cause an infinite loop if file writing is the problem\n            return json(\n                { error: \"Internal Server Error while logging\" },\n                { status: 500 }\n            );\n        }\n    }\n\nCreate a Client-Side Logging Utility:\n    Create a helper, e.g., src/lib/clientLog.js (or place it in src/lib/log.js and use environment checks if you want a unified logger).\n\n    // src/lib/clientLog.js\n\n    /**\n     Sends a log message to the server's logging endpoint.\n     @param {string} level\n     @param {string} message\n     @param {any} [data]\n     */\n    export async function logToServer(level, message, data) {\n        try {\n            const response = await fetch(\"/api/log\", {\n                // The path to your API endpoint\n                method: \"POST\",\n                headers: {\n                    \"Content-Type\": \"application/json\",\n                },\n                body: JSON.stringify({ level, message, data }),\n            });\n\n            if (!response.ok) {\n                // Log failure to console if sending fails\n                console.error(\n                    Failed to send log to server (${response.status}): ${message},\n                    data\n                );\n            }\n        } catch (error) {\n            // Log network or other errors to console\n            console.error(\"Error sending log to server:\", error);\n            console.error(\"Original Log Message:\", { level, message, data });\n        }\n    }\n\n    // Convenience functions\n    export const logClientInfo = (message, data) =>\n        logToServer(\"info\", message, data);\n    export const logClientWarn = (message, data) =>\n        logToServer(\"warn\", message, data);\n    export const logClientError = (message, data) =>\n        logToServer(\"error\", message, data);\n    export const logClientDebug = (message, data) =>\n        logToServer(\"debug\", message, data);\n\nUse the Client Logger in Components/Client Modules:\n\n    \n    \n        import { onMount } from 'svelte';\n        import { logClientInfo, logClientError } from '$lib/clientLog';\n\n        onMount(() => {\n            logClientInfo('Interactive page mounted');\n        });\n\n        function handleClick() {\n            try {\n                logClientInfo('Button clicked!');\n                // ... some operation that might fail ...\n                if (Math.random() \n\n    Click Me\n\nImportant Considerations:\n\nLog Rotation: For production applications, log files can grow very large. Implement log rotation (automatically archiving or deleting old logs). Libraries like rotating-file-stream can help, or use system tools like logrotate.\nPerformance: Excessive logging, especially synchronous file writing (fs.appendFileSync), can impact server performance. Use asynchronous methods (fs.promises.appendFile) where possible. For high-volume logging, consider more robust logging libraries.\nStructured Logging: Using JSON or another structured format for logs makes them easier to parse and analyze with log management tools.\nError Handling: Make your logging robust. What happens if the log file cannot be written to (permissions, disk full)? Have fallbacks (like logging to console.error).\nConfiguration: Make the log file path and log level configurable (e.g., via environment variables).\nLibraries: For more advanced features (multiple transports like console + file, different formats, levels, etc.), consider using dedicated logging libraries like winston or pino. You would integrate them similarly within the server-side utility.\nDeployment: Ensure the directory where you intend to write logs exists and has the correct write permissions in your deployment environment. This location might differ from your local development setup. Persistent storage might be needed if deploying to ephemeral containers.",
        "locale": "en",
        "slug": "technical-posts/home-brewed-logging-to-a-text-file-with-sveltekit",
        "folder": ""
    },
    {
        "title": "Use reflection with C# to create a map-driven Excel export process",
        "description": "Use reflection with C# to create a map-driven Excel export process",
        "date_created": "2025-06-05T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "excel"
        ],
        "content": "I have this C# code that fetches values from an Excel spreadsheet:\n\naccount.AccountNumber = GetCellValue(\"AccountNumber\", worksheet.Cell(row, \"A\"));\naccount.PostDate = GetCellValue(\"PostDate\", worksheet.Cell(row, \"B\"));\naccount.Check = GetCellValue(\"Check\", worksheet.Cell(row, \"C\"));\n\nI'd like to make the code more declarative and drive with a column map of some kind, perhaps Json, maybe like this:\n\n[ \t\t\t\t\t\t\n\t{\n\t   name: AccountNumber,\n\t   type: \"int\",\n\t   column: \"A\"\n\t},\n\t{\n\t   name: PostDate,\n\t   type: \"DateType\",\n\t   column: \"B\"\n\t},\n\t{\n\t   name: Check,\n\t   type: \"string\",\n\t   column: \"C\"\n\t}\n]\n\nHow can I translate the type from something like \"int\" to be able to make the call to the generic GetCellValue function?\n\nAnswer \n\nOf course! This is an excellent goal. Moving to a declarative, data-driven approach makes your code much more flexible and maintainable.\n\nThe core of your question is: \"How do I use a string like \"int\" to call a generic method like GetCellValue()?\"\n\nYou cannot do this directly at compile time, as the compiler needs to know the specific type T for the generic method. The solution is to use Reflection at runtime. Reflection allows you to inspect and invoke types and methods dynamically.\n\nHere's a complete, step-by-step guide to achieve your goal.\n\nStep 1: Define the Configuration and C# Models\n\nFirst, let's create the C# classes that will represent your JSON configuration and your target Account object.\n\nJSON Configuration (columnMappings.json)\n\nYour proposed JSON is great. Let's stick with standard JSON naming conventions (camelCase).\n\n[\n  {\n    \"propertyName\": \"AccountNumber\",\n    \"type\": \"int\",\n    \"column\": \"A\"\n  },\n  {\n    \"propertyName\": \"PostDate\",\n    \"type\": \"DateTime\",\n    \"column\": \"B\"\n  },\n  {\n    \"propertyName\": \"Check\",\n    \"type\": \"string\",\n    \"column\": \"C\"\n  }\n]\n\nC# Mapping Class\n\nThis class will hold the deserialized data from the JSON file.\n\npublic class ColumnMapping\n{\n    // The name of the property on your target C# object (e.g., Account)\n    public string PropertyName { get; set; } = string.Empty;\n\n    // The string representation of the C# type\n    public string Type { get; set; } = string.Empty;\n\n    // The Excel column letter\n    public string Column { get; set; } = string.Empty;\n}\n\nC# Target Object\n\nThis is the Account class you are trying to populate.\n\npublic class Account\n{\n    public int AccountNumber { get; set; }\n    public DateTime PostDate { get; set; }\n    public string? Check { get; set; }\n\n    public override string ToString()\n    {\n        return $\"Account: {AccountNumber}, Date: {PostDate:d}, Check: '{Check}'\";\n    }\n}\n\nStep 2: Translate the Type String to a System.Type\n\nThe Type.GetType() method can work, but it often requires fully qualified names (e.g., \"System.Int32\" instead of \"int\"). A more robust and user-friendly approach is to create a simple dictionary that maps your friendly JSON type names to the actual System.Type.\n\nprivate static readonly Dictionary TypeMap = new(StringComparer.OrdinalIgnoreCase)\n{\n    { \"int\", typeof(int) },\n    { \"string\", typeof(string) },\n    { \"datetime\", typeof(DateTime) },\n    { \"decimal\", typeof(decimal) },\n    { \"double\", typeof(double) },\n    { \"bool\", typeof(bool) }\n    // Add any other types you need\n};\n\npublic Type GetSystemType(string typeName)\n{\n    if (TypeMap.TryGetValue(typeName, out var type))\n    {\n        return type;\n    }\n    throw new ArgumentException($\"The type '{typeName}' is not a supported type.\");\n}\n\nThis makes your JSON configuration much cleaner (\"int\" is nicer than \"System.Int32\").\n\nStep 3: Use Reflection to Call the Generic Method\n\nThis is the key part. We will create a processor class that reads the mappings and populates the object row by row.\n\nGet the MethodInfo for your GetCellValue method.\nUse MethodInfo.MakeGenericMethod(type) to create a specific, closed generic method (e.g., the equivalent of GetCellValue).\nUse MethodBase.Invoke() to call that method and get the result.\nUse reflection again to find the corresponding property on the Account object and set its value.\n\nHere's a full ExcelMapper class that brings it all together.\n\nusing ClosedXML.Excel; // Assuming you use ClosedXML, as is common\nusing System.Reflection;\nusing System.Text.Json;\n\npublic class ExcelMapper\n{\n    // Assume you have your GetCellValue method in this class.\n    // This is a mock implementation for demonstration.\n    public T GetCellValue(string propertyName, IXLCell cell)\n    {\n        // Your actual implementation would have robust error handling\n        // and type conversion from the cell's value.\n        try\n        {\n            return cell.GetValue();\n        }\n        catch (Exception ex)\n        {\n            // A good place to add more context to the error\n            throw new InvalidCastException(\n                $\"Failed to get value for property '{propertyName}' from cell {cell.Address}.\", ex);\n        }\n    }\n\n    // The type mapping dictionary from Step 2\n    private static readonly Dictionary TypeMap = new(StringComparer.OrdinalIgnoreCase)\n    {\n        { \"int\", typeof(int) },\n        { \"string\", typeof(string) },\n        { \"datetime\", typeof(DateTime) },\n        { \"decimal\", typeof(decimal) },\n        { \"double\", typeof(double) },\n        { \"bool\", typeof(bool) }\n    };\n\n    public Type GetSystemType(string typeName)\n    {\n        if (TypeMap.TryGetValue(typeName, out var type))\n        {\n            return type;\n        }\n        // Fallback for fully qualified names if you want\n        var systemType = Type.GetType(typeName);\n        if (systemType != null) return systemType;\n        \n        throw new ArgumentException($\"The type '{typeName}' is not a supported or recognized type.\");\n    }\n\n    // The main method that drives the mapping process\n    public T MapRowToObject(IXLWorksheet worksheet, int row, IEnumerable mappings) where T : new()\n    {\n        var targetObject = new T();\n\n        // Get the MethodInfo for our generic GetCellValue method.\n        // We only need to do this once.\n        MethodInfo? getCellValueMethod = typeof(ExcelMapper).GetMethod(\"GetCellValue\");\n        if (getCellValueMethod == null)\n        {\n            throw new InvalidOperationException(\"Could not find the 'GetCellValue' method via reflection.\");\n        }\n\n        foreach (var mapping in mappings)\n        {\n            // 1. Get the target cell from the worksheet\n            var cell = worksheet.Cell(row, mapping.Column);\n\n            // 2. Translate the string type to a System.Type\n            Type propertyType = GetSystemType(mapping.Type);\n\n            // 3. Create the specific generic method (e.g., GetCellValue)\n            MethodInfo genericMethod = getCellValueMethod.MakeGenericMethod(propertyType);\n\n            // 4. Invoke the method dynamically\n            //    The first argument is the instance to call the method on ('this').\n            //    The second is an array of arguments for the method.\n            object? value = genericMethod.Invoke(this, new object[] { mapping.PropertyName, cell });\n\n            // 5. Find the property on the target object and set its value\n            PropertyInfo? propInfo = typeof(T).GetProperty(mapping.PropertyName);\n            propInfo?.SetValue(targetObject, value);\n        }\n\n        return targetObject;\n    }\n}\n\nStep 4: Putting it all to use\n\nNow, your main processing logic becomes much cleaner.\n\npublic class Program\n{\n    public static void Main()\n    {\n        // --- Setup (Create a dummy Excel file for testing) ---\n        using var workbook = new XLWorkbook();\n        var worksheet = workbook.Worksheets.Add(\"Accounts\");\n        worksheet.Cell(\"A1\").Value = \"AccountNumber\";\n        worksheet.Cell(\"B1\").Value = \"PostDate\";\n        worksheet.Cell(\"C1\").Value = \"Check\";\n\n        worksheet.Cell(\"A2\").Value = 12345;\n        worksheet.Cell(\"B2\").Value = new DateTime(2023, 10, 27);\n        worksheet.Cell(\"C2\").Value = \"CHK-987\";\n        // --- End Setup ---\n\n        // 1. Load the JSON mapping configuration\n        string jsonConfig = File.ReadAllText(\"columnMappings.json\");\n        var mappings = JsonSerializer.Deserialize>(jsonConfig)!;\n        \n        // 2. Create an instance of our mapper\n        var mapper = new ExcelMapper();\n        \n        // 3. Process a row\n        int rowToProcess = 2;\n        Account account = mapper.MapRowToObject(worksheet, rowToProcess, mappings);\n        \n        // 4. Use the populated object\n        Console.WriteLine(account); \n        // Expected Output: Account: 12345, Date: 10/27/2023, Check: 'CHK-987'\n    }\n}\n\nSummary of Benefits\n\nDeclarative: Your logic for mapping columns to properties is now declared in a simple JSON file, not hidden in C# code.\nMaintainable: To add a new field, you just add an entry to the JSON file and a property to the Account class. You don't need to change the mapping code at all.\nFlexible: You can easily have different mapping files for different Excel layouts without recompiling your application.\nReusable: The ExcelMapper class can be used to map data to any target object (`), not just Account`.",
        "locale": "en",
        "slug": "technical-posts/cs-type-mapping-for-excel-export",
        "folder": ""
    },
    {
        "title": "Sveltekit locals object",
        "description": "Sveltekit locals object",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Sveltelkit hooks are a way to intercept the incoming request pipeline. You can modify it or add things to it. In this case, the zevon variable is injected into the locals object.\n\nThe locals object is a per-request object into which you can assign values you need persisted throughout the life of the request. A canonical locals example is capturing authentication-related values to have available later in the request.\n\nfile: src/hooks.server.ts\n\nexport const handle = async({event, resolve}) => {\n    event.locals.zevon = \"Excitable Boy\"\n\n    const response = await resolve(event);\n\n return response;\n}\n\nIn the file below, the zevon variable is fetched in a page.server.ts\n\nfile: page.server.ts\n\nexport const load = async (event ) => {\n    const form = await superValidate(event, newUserSchema)\n\n    const w = event.locals.zevon\n\n    return {form}\n}\n\n\n\nHere is another example using hooks.server.ts:\n\n// src/app.d.ts\ndeclare global {\n    namespace App {\n        // interface Error {}\n        interface Locals {\n            // To store user data\n            user?: { \n                id: string;\n                email: string;\n            };\n            // Add any other properties you want to set \n            // on event.locals\n        }\n        // interface PageData {}\n        // interface Platform {}\n    }\n}\n\nexport {};\n\n// src/hooks.server.ts\nimport type { Handle } from '@sveltejs/kit';\n\nexport const handle: Handle = async ({ event, resolve }) => {\n    // Example: Try to get user from a cookie (pseudo-code)\n    const sessionId = event.cookies.get('sessionid');\n    if (sessionId) {\n        const user = await getUserBySessionId(sessionId); \n        // Your auth logic\n        if (user) {\n             event.locals.user = { id: user.id, \n                                   email: user.email };\n        }\n    }\n\n    const response = await resolve(event);\n    return response;\n};",
        "locale": "en",
        "slug": "technical-posts/hooks-the-locals-object",
        "folder": ""
    },
    {
        "title": "Docker",
        "description": "Docker",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "containers",
            "docker"
        ],
        "content": "[[What is a docker file]]\n\n[[Example docker file]]\n\n[[Example docker compose file for a sveltekit app]]\n\n[[Docker file tutorial]]\n\n![[Docker file tutorial]]\n\n[[Docker compose file tutorial]]\n\n[[Get podman running]]",
        "locale": "en",
        "slug": "technical-posts/docker",
        "folder": ""
    },
    {
        "title": "Close process associated with a specific port",
        "description": "Close process associated with a specific port",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "These two command lines are a way to kill a process associated with a port\n\n$processId = (Get-NetTCPConnection -State Listen | Where-Object { $_.LocalPort -eq 5173 }).OwningProcess\n\nStop-Process -Id $processId -Force\n\nTwo ways to do that on one line:\n\nStop-Process -Id (Get-NetTCPConnection -State Listen | Where-Object { $_.LocalPort -eq 5173 }).OwningProcess -Force\n\n$processId = (Get-NetTCPConnection -State Listen | Where-Object { $_.LocalPort -eq 5173 }).OwningProcess; Stop-Process -Id $processId -Force\n\nShow the local port and process id:\n\nGet-NetTCPConnection -State Listen | Where-Object { $_.LocalPort -eq 5173 } | Select-Object LocalPort, OwningProcess\n\nConfirm it with netstat:\n\nnetstat -ano | findstr :5173",
        "locale": "en",
        "slug": "technical-posts/close-process-associated-with-a-specific-port",
        "folder": ""
    },
    {
        "title": "Importing Json with Node static dynamic",
        "description": "Importing Json with Node static dynamic",
        "date_created": "2025-08-10T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "json"
        ],
        "content": "Static import\n\nimport markdownSearchData from '../data/search-index.json' with { type: 'json' };\n\nDynamic import\n\nimport { pathToFileURL } from 'node:url';\nimport { ROOT } from './constants.js';\n\n...\n\nconst src_location = path.join(ROOT, 'src/lib/data');\nconst jsonPath = path.join(src_location, 'search-index.json');\nconst markdownSearchData = await import(pathToFileURL(jsonPath).href, {\n\twith: { type: 'json' }\n});\n\n// Then reference markdownSearchData.default\n\nconstants.js\n\nexport const md_folders = ['kb', 'white-paper', 'case-study', 'newsletter', 'blog'];\nexport const locales = ['en', 'es'];\nexport const ROOT = 'C:\\\\Users\\\\thumb\\\\Documents\\\\projects\\\\asna\\\\_asna.com\\\\_project_ocho';",
        "locale": "en",
        "slug": "technical-posts/import-json-with-node",
        "folder": ""
    },
    {
        "title": "css-scope-tutorial",
        "description": "CSS @scope tutorial",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-05T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Here is a concise tutorial on CSS @scope.\n\nWhat is @scope?\n\nHistorically, CSS relies on Specificity (how \"heavy\" a selector is) and Source Order (what comes last in the file).\n\n@scope introduces a new logic: Proximity. It allows you to select elements based on which container they are physically closer to in the HTML, and allows you to create boundaries where styles stop applying.\n\nStep 1: Basic Scoping (The Root)\n\nThe most basic usage is defining a \"Root\". Styles defined inside the block only apply to elements inside that root.\n\nThe Scenario: You have a .card component. You want to style ` tags inside it without writing long selectors like .card > header > .content > img`.\n\n@scope (.card) {\n  /* This selects any  inside .card */\n  img {\n    border-radius: 10px;\n    box-shadow: 0 4px 10px rgba(0,0,0,0.1);\n  }\n\n  /* You can target the root element itself using :scope */\n  :scope {\n    background: white;\n    padding: 1rem;\n  }\n}\n\nWhy is this better than .card img?\nIt keeps specificity low. In standard CSS, .card img creates a heavier selector. With @scope, the specificity of img is just (0,0,1). This makes it much easier to override these styles later if necessary.\n\nStep 2: Donut Scoping (The Limit)\n\nThis is the most powerful feature. You can define a Root (start) and a Limit (end). The styles apply to everything between the two, but stop once the Limit is reached.\n\nThe Scenario: You are styling a .tab-component. However, inside the tabs, you have a generic .content-area where users can put whatever they want. You don't want your tab styles messing up the user content.\n\n/* Start at .tab-component, STOP at .content-area */\n@scope (.tab-component) to (.content-area) {\n  \n  /* Applies to text in the tabs, but NOT inside .content-area */\n  h2 {\n    color: blue;\n    font-size: 2rem;\n  }\n\n  /* Applies to buttons in the tabs, NOT in the content */\n  button {\n    background: blue; \n    color: white;\n  }\n  \n}\n\nStep 3: Proximity (The \"Theme\" Solver)\n\nThis is where @scope beats standard CSS.\n\nIn standard CSS, if you nest a Light Theme inside a Dark Theme, the browser applies whichever rule appears last in your CSS file. With @scope, the browser applies the rule from the nearest scope in the HTML.\n\nThe HTML:\n\n  I am Dark\n  \n  \n    I should be Light (because I am closer to light-theme)\n  \n\n\nThe CSS:\n@scope (.dark-theme) {\n  a { color: white; }\n}\n\n@scope (.light-theme) {\n  a { color: black; }\n}\n\nThe Result:\nThe link inside .light-theme will be black, regardless of which @scope block you wrote last in your CSS file. The browser sees that the link is physically closer to the .light-theme wrapper.\n\nSummary Checklist\n\nEncapsulation: Use it to bundle styles for a component without naming conflicts (similar to BEM, but native).\nLow Specificity: Styles inside @scope are easy to override because they don't add the class weight of the parent.\nDonut Holes: Use to (...) to protect nested content from inheriting parent styles.\nProximity: Use it for theming to ensure nested themes work correctly based on DOM distance.",
        "locale": "en",
        "slug": "technical-posts/css-scope-tutorial",
        "folder": ""
    },
    {
        "title": "CSS focus-within pseudo selector",
        "description": "CSS focus-within pseudo selector",
        "date_created": "2025-11-01T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "this has an error",
        "locale": "en",
        "slug": "technical-posts/has-error",
        "folder": ""
    },
    {
        "title": "javascxript-tagged-functions",
        "description": "javascxript-tagged-functions",
        "date_created": "2025-05-29T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "function customFormatter(strings, ...values) {\n    // console.log(\"Literal strings:\", strings);\n\n    strings.forEach((str, index) => {\n        console.log(String ${index}:, str);\n    });\n\n    console.log(\"strings\", strings);\n    console.log(\"raw\", strings.raw);\n    console.log(\"Interpolated values:\", values);\n}\n\nconst item = \"Book\";\nconst price = 25;\n\ncustomFormatterItem: ${item}, Price: $${price}.;\n\nThe code above produces this output: \n\nString 0: Item:\nString 1: , Price: $\nString 2: .\nstrings [ \"Item: \", \", Price: $\", \".\", raw: [ \"Item: \", \", Price: $\", \".\" ] ]\nraw [ \"Item: \", \", Price: $\", \".\" ]\nInterpolated values: [ \"Book\", 25 ]\n\nYou can do anything you want with the string components and raw values. You'd probably use the raw property of the strings argument which is a an array of the string components.",
        "locale": "en",
        "slug": "technical-posts/javascxript-tagged-functions",
        "folder": ""
    },
    {
        "title": "global rune",
        "description": "global rune",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Okay, let's create a global reactive string using Svelte 5 runes and react to its changes in the root layout (+layout.svelte).\n\nSvelte 5 runes ($state, $effect, etc.) need to be enabled in your svelte.config.js:\n\n// svelte.config.js\nimport adapter from \"@sveltejs/adapter-auto\"; // or your preferred adapter\nimport { vitePreprocess } from \"@sveltejs/vite-plugin-svelte\";\n\n/** @type {import('@sveltejs/kit').Config} */\nconst config = {\n    // Consult https://kit.svelte.dev/docs/integrations#preprocessors\n    // for more information about preprocessors\n    preprocess: vitePreprocess(),\n\n    kit: {\n        adapter: adapter(),\n        // ... other kit options\n    },\n\n    // --- ADD THIS ---\n    compilerOptions: {\n        runes: true, // Enable runes mode\n    },\n    // --- END ADD ---\n};\n\nexport default config;\n\nImportant: After changing svelte.config.js, stop and restart your development server (npm run dev).\n\nNow, let's create the global store:\n\n1. Create the Global Store File (src/lib/sharedStore.ts)\n\nIt's best practice to put shared, reusable state logic in the src/lib directory. We'll use TypeScript here for better type safety.\n\n// src/lib/sharedStore.ts\nimport { browser } from \"$app/environment\";\n\n// Define the structure of our store's state\ntype SharedStore = {\n    trackedString: string;\n};\n\n// Create the reactive state using $state\n// Initialize with a default value\nlet store = $state({\n    trackedString: \"Initial Value\",\n});\n\n// Optional: Log when the store module is loaded\nif (browser) {\n    console.log(\"Shared store module loaded (client)\");\n} else {\n    console.log(\"Shared store module loaded (server)\");\n}\n\n// --- Export the reactive store object ---\n// Components will import this 'sharedStore' object\n// and access/modify its properties directly (e.g., sharedStore.trackedString)\nexport const sharedStore = store;\n\n// --- Alternatively, you could export getter/setter functions ---\n// This provides more control but is often less direct with runes\n/*\nexport function getTrackedString(): string {\n    return store.trackedString;\n}\n\nexport function setTrackedString(newValue: string): void {\n    console.log(Setting trackedString from '${store.trackedString}' to '${newValue}');\n    store.trackedString = newValue; // Direct assignment triggers reactivity\n}\n*/\n\nExplanation:\n\n$state({...}): This is the core Svelte 5 rune for creating reactive state. We provide a type SharedStore and an initial object. Any property within this object (trackedString) becomes reactive.\nexport const sharedStore = store;: We export the entire reactive store object. This is the common pattern with runes  components directly interact with the properties of the exported state object.\n\n2. Modify the Rune in a Component or Page (Example: src/routes/some-page/+page.svelte)\n\n\n\n\timport { sharedStore } from '$lib/sharedStore';\n\n\t// No need for 'let' or '$:' for reactivity when reading/writing rune state properties\n\t// The component automatically subscribes when 'sharedStore.trackedString' is used in the template\n\n\tfunction updateString() {\n\t\t// Directly modify the property on the imported store object\n\t\tsharedStore.trackedString = 'Updated from page at ' + new Date().toLocaleTimeString();\n\t\tconsole.log('Updated sharedStore.trackedString from page');\n\t}\n\n\tfunction appendToString() {\n\t\tsharedStore.trackedString += ' Appended!';\n\t\tconsole.log('Appended to sharedStore.trackedString from page');\n\t}\n\n\nPage Example\n\n\n\tThe current shared string is: {sharedStore.trackedString}\n\n\nSet New String Value\nAppend to String\n\n\n\tbutton { margin: 5px; }\n\n\nExplanation:\n\nimport { sharedStore } from '$lib/sharedStore';: Import the reactive store object.\nsharedStore.trackedString = ...: Directly assign a new value to the trackedString property. Because sharedStore was created with $state, Svelte automatically detects this change and updates any parts of the UI that depend on it.\n{sharedStore.trackedString}: Reading the value in the template automatically makes this component reactive to changes in that specific property.\n\n3. React to Changes in the Root Layout (src/routes/+layout.svelte)\n\nHere, we use the $effect rune to run code whenever sharedStore.trackedString changes.\n\n\n\n\timport { sharedStore } from '$lib/sharedStore';\n\timport { browser } from '$app/environment';\n\n\t// Use $effect to run side-effects when dependencies change\n\t$effect(() => {\n\t\t// Access the reactive property inside the effect\n\t\t// This tells Svelte that this effect depends on sharedStore.trackedString\n\t\tconst currentStringValue = sharedStore.trackedString;\n\n\t\t// IMPORTANT: Avoid modifying the state you are tracking within the effect\n\t\t// unless you have specific logic (like debouncing) to prevent infinite loops.\n\t\t// Here, we are just reacting.\n\n\t\tif (browser) {\n            // Perform actions only on the client-side if needed\n\t\t\tconsole.log(%cLAYOUT EFFECT: trackedString changed to: \"${currentStringValue}\", 'color: blue; font-weight: bold;');\n\n\t\t\t// --- Firing a custom event (Example) ---\n\t\t\t// You might want to notify non-Svelte parts of your app,\n            // or structure event handling this way.\n\t\t\tconst event = new CustomEvent('sharedstringchanged', {\n\t\t\t\tdetail: { newValue: currentStringValue }\n\t\t\t});\n\t\t\twindow.dispatchEvent(event);\n\t\t\tconsole.log('LAYOUT EFFECT: Dispatched \"sharedstringchanged\" window event.');\n\t\t} else {\n            // You could log on the server too if needed during SSR,\n            // though UI side-effects usually happen client-side.\n             console.log(LAYOUT EFFECT (SSR): trackedString is: \"${currentStringValue}\");\n        }\n\n        // Optional: Cleanup function\n        // This runs just before the effect re-runs, or when the component is unmounted.\n        return () => {\n            if (browser) {\n                console.log(LAYOUT EFFECT: Cleanup before re-run or unmount (previous value was \"${currentStringValue}\"));\n                // Example: remove event listeners added above if they were attached to specific elements\n            }\n        };\n\t});\n\n    // You can still display the value directly in the layout too\n\n\n\n\tMy App Layout\n\tLayout sees shared string: {sharedStore.trackedString}\n    Check the browser console for effect logs and events when the string changes.\n    \n        Home |\n        Some Page (Changes String)\n        \n    \n\n\n\n\t\n\n\n\n\t\n\n\nExplanation:\n\nimport { sharedStore } from '$lib/sharedStore';: Import the store.\n$effect(() => { ... });: This rune registers a side effect.\nconst currentStringValue = sharedStore.trackedString;: Crucially, you must read the reactive state property (sharedStore.trackedString) inside the effect's callback function. This establishes the dependency. Svelte now knows to re-run this effect whenever sharedStore.trackedString changes.\nif (browser): Often, side effects like logging specific messages or dispatching DOM events are only relevant in the browser environment.\nconsole.log(...): Logs a message every time the effect runs (initially and on subsequent changes).\nnew CustomEvent(...) / window.dispatchEvent(...): This demonstrates how to fire a standard DOM event when the value changes. Other parts of your application (even non-Svelte JavaScript) could listen for this sharedstringchanged event on the window.\nreturn () => { ... }: The optional cleanup function returned by the effect runs before the effect re-runs due to a dependency change, or when the component instance is destroyed. It's useful for cleanup tasks like removing manually added event listeners or clearing timers.\n\nNow, when you navigate to /some-page and click the buttons, sharedStore.trackedString will be updated. Svelte's reactivity system will:\n\nUpdate the displayed value in /some-page/+page.svelte.\nUpdate the displayed value in /routes/+layout.svelte.\nTrigger the $effect in /routes/+layout.svelte, causing it to log the message and dispatch the custom event.",
        "locale": "en",
        "slug": "technical-posts/global-rune",
        "folder": ""
    },
    {
        "title": "davinci-resolve-notes",
        "description": "davinci-resolve-notes",
        "date_created": "2025-09-09T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "davinci",
            "resolve"
        ],
        "content": "This video is pretty good. It has some good suggestions for default values.\nhttps://www.youtube.com/watch?v=Mzd-BRZmWec\n\nFusion Tutorial\nhttps://www.youtube.com/watch?v=gaFT_Kj2yeQ\n\nNodes\ntriangles are inputs \nboxes are outputs \n\npath colors: \nyellow - background\ngreen - foreground\nblue - effect mask \n\n\nNode types\n\n\n\nGenerator - nodes with output\nMerge - \n\nThis is the Fusion composition that shows the intro text screen.\n\n{\n\tTools = ordered() {\n\t\tMediaOut1 = MediaOut {\n\t\t\tInputs = {\n\t\t\t\tIndex = Input { Value = \"0\", },\n\t\t\t\tInput = Input {\n\t\t\t\t\tSourceOp = \"Merge4\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t}\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 1412, 223.44 } },\n\t\t},\n\t\tMerge2 = Merge {\n\t\t\tInputs = {\n\t\t\t\tBackground = Input {\n\t\t\t\t\tSourceOp = \"Merge1\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t},\n\t\t\t\tForeground = Input {\n\t\t\t\t\tSourceOp = \"presentedTask\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t},\n\t\t\t\tCenter = Input { Value = { 0.497, 0.5 }, },\n\t\t\t\tPerformDepthMerge = Input { Value = 0, }\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 958.667, 215.848 } },\n\t\t},\n\t\tpresentedTask = TextPlus {\n\t\t\tNameSet = true,\n\t\t\tInputs = {\n\t\t\t\tGlobalIn = Input { Value = -30, },\n\t\t\t\tGlobalOut = Input { Value = 80, },\n\t\t\t\tWidth = Input { Value = 1920, },\n\t\t\t\tHeight = Input { Value = 1080, },\n\t\t\t\tUseFrameFormatSettings = Input { Value = 1, },\n\t\t\t\t[\"Gamut.SLogVersion\"] = Input { Value = FuID { \"SLog2\" }, },\n\t\t\t\tWrap = Input { Value = 1, },\n\t\t\t\tCenter = Input { Value = { 0.47, 0.278 }, },\n\t\t\t\tLayoutRotation = Input { Value = 1, },\n\t\t\t\tTransformRotation = Input { Value = 1, },\n\t\t\t\tRed1 = Input { Value = 0.992156862745098, },\n\t\t\t\tGreen1 = Input { Value = 0.992156862745098, },\n\t\t\t\tBlue1 = Input { Value = 0.992156862745098, },\n\t\t\t\tSoftness1 = Input { Value = 1, },\n\t\t\t\tStyledText = Input { Value = \"In the next video we'll look at the objects needed\", },\n\t\t\t\tFont = Input { Value = \"Open Sans\", },\n\t\t\t\tStyle = Input { Value = \"Semibold Italic\", },\n\t\t\t\tSize = Input { Value = 0.0551, },\n\t\t\t\tVerticalJustificationNew = Input { Value = 3, },\n\t\t\t\tHorizontalJustificationNew = Input { Value = 3, }\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 738.667, 215.242 } },\n\t\t},\n\t\tTransform2 = Transform {\n\t\t\tCtrlWZoom = false,\n\t\t\tInputs = {\n\t\t\t\tCenter = Input {\n\t\t\t\t\tSourceOp = \"Path1\",\n\t\t\t\t\tSource = \"Position\",\n\t\t\t\t},\n\t\t\t\tInput = Input {\n\t\t\t\t\tSourceOp = \"Merge2\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t}\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 1145.33, 187.364 } },\n\t\t},\n\t\tPath1 = PolyPath {\n\t\t\tDrawMode = \"InsertAndModify\",\n\t\t\tCtrlWZoom = false,\n\t\t\tInputs = {\n\t\t\t\tDisplacement = Input {\n\t\t\t\t\tSourceOp = \"Path1Displacement\",\n\t\t\t\t\tSource = \"Value\",\n\t\t\t\t},\n\t\t\t\tPolyLine = Input {\n\t\t\t\t\tValue = Polyline {\n\t\t\t\t\t\tPoints = {\n\t\t\t\t\t\t\t{ Linear = true, LockY = true, X = -2.213, Y = 0, RX = 0.737666666666667, RY = 0 },\n\t\t\t\t\t\t\t{ Linear = true, LockY = true, X = 0, Y = 0, LX = -0.737666666666667, LY = 0 }\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t\tPath1Displacement = BezierSpline {\n\t\t\tSplineColor = { Red = 255, Green = 0, Blue = 255 },\n\t\t\tCtrlWZoom = false,\n\t\t\tKeyFrames = {\n\t\t\t\t[1] = { 0, RH = { 14.6666666666667, 0 }, Flags = { Linear = true, LockedY = true } },\n\t\t\t\t[42] = { 1, LH = { 28.3333333333333, 1 }, Flags = { LockedY = true } }\n\t\t\t}\n\t\t},\n\t\tproductName = TextPlus {\n\t\t\tNameSet = true,\n\t\t\tInputs = {\n\t\t\t\tGlobalIn = Input { Value = -30, },\n\t\t\t\tGlobalOut = Input { Value = 80, },\n\t\t\t\tWidth = Input { Value = 1920, },\n\t\t\t\tHeight = Input { Value = 1080, },\n\t\t\t\tUseFrameFormatSettings = Input { Value = 1, },\n\t\t\t\t[\"Gamut.SLogVersion\"] = Input { Value = FuID { \"SLog2\" }, },\n\t\t\t\tWrap = Input { Value = 1, },\n\t\t\t\tCenter = Input { Value = { 0.325, 0.409 }, },\n\t\t\t\tLayoutRotation = Input { Value = 1, },\n\t\t\t\tTransformRotation = Input { Value = 1, },\n\t\t\t\tSoftness1 = Input { Value = 1, },\n\t\t\t\tStyledText = Input { Value = \"Monarch in Action\", },\n\t\t\t\tFont = Input { Value = \"Open Sans\", },\n\t\t\t\tStyle = Input { Value = \"Bold\", },\n\t\t\t\tVerticalJustificationNew = Input { Value = 3, },\n\t\t\t\tHorizontalJustificationNew = Input { Value = 3, }\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 736.667, 166.424 } },\n\t\t},\n\t\tMerge1 = Merge {\n\t\t\tInputs = {\n\t\t\t\tBackground = Input {\n\t\t\t\t\tSourceOp = \"Merge3\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t},\n\t\t\t\tForeground = Input {\n\t\t\t\t\tSourceOp = \"productName\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t},\n\t\t\t\tPerformDepthMerge = Input { Value = 0, }\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 958.667, 165.545 } },\n\t\t},\n\t\tMerge3 = Merge {\n\t\t\tInputs = {\n\t\t\t\tBackground = Input {\n\t\t\t\t\tSourceOp = \"Transform1\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t},\n\t\t\t\tCenter = Input { Value = { 0.418, 0.491 }, },\n\t\t\t\tPerformDepthMerge = Input { Value = 0, }\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 960, 110.394 } },\n\t\t},\n\t\tTransform1 = Transform {\n\t\t\tInputs = {\n\t\t\t\tCenter = Input { Value = { 0.23, 0.5 }, },\n\t\t\t\tSize = Input { Value = 1.95, },\n\t\t\t\tInput = Input {\n\t\t\t\t\tSourceOp = \"ASNAlogo\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t}\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 815.333, 111 } },\n\t\t},\n\t\tASNAlogo = MediaIn {\n\t\t\tNameSet = true,\n\t\t\tCustomData = { MEDIA_ID = \"87f7a501-177c-4edb-a1c5-969ead0763b7\", MediaProps = { MEDIA_NUM_LAYERS = 0, MEDIA_FORMAT_TYPE = \"PNG\", MEDIA_PAR = 1, MEDIA_LAYER_DESC = {\n\t\t\t\t\t}, MEDIA_NAME = \"asna-logo-white-600.png\", MEDIA_PATH = \"C:\\\\Users\\\\thumb\\\\Downloads\\\\asna-logo-white-600.png\", MEDIA_START_FRAME = 0, MEDIA_IS_SOURCE_RES = true, MEDIA_SRC_FRAME_RATE = 30, MEDIA_MARK_OUT = 0, MEDIA_WIDTH = 600, MEDIA_NUM_FRAMES = 1, MEDIA_MARK_IN = 0, MEDIA_HEIGHT = 40 }, },\n\t\t\tInputs = {\n\t\t\t\tGlobalIn = Input { Value = -30, },\n\t\t\t\tGlobalOut = Input { Value = 80, },\n\t\t\t\tMediaSource = Input { Value = FuID { \"MediaPool\" }, },\n\t\t\t\tMediaID = Input { Value = \"87f7a501-177c-4edb-a1c5-969ead0763b7\", },\n\t\t\t\tAudioTrack = Input { Value = FuID { \"Timeline Audio\" }, },\n\t\t\t\tLayer = Input { Value = \"\", },\n\t\t\t\tClipTimeEnd = Input { Value = 0, },\n\t\t\t\tLoop = Input { Value = 1, },\n\t\t\t\t[\"Gamut.SLogVersion\"] = Input { Value = FuID { \"SLog2\" }, },\n\t\t\t\tDeepOutputMode = Input {\n\t\t\t\t\tValue = 0,\n\t\t\t\t\tDisabled = true,\n\t\t\t\t},\n\t\t\t\tLeftAudio = Input {\n\t\t\t\t\tSourceOp = \"Left\",\n\t\t\t\t\tSource = \"Data\",\n\t\t\t\t},\n\t\t\t\tRightAudio = Input {\n\t\t\t\t\tSourceOp = \"Right\",\n\t\t\t\t\tSource = \"Data\",\n\t\t\t\t}\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 660, 110.394 } },\n\t\t\tVersion = 1\n\t\t},\n\t\tLeft = AudioDisplay {\n\t\t\tCtrlWZoom = false,\n\t\t},\n\t\tRight = AudioDisplay {\n\t\t\tCtrlWZoom = false,\n\t\t},\n\t\tMerge4 = Merge {\n\t\t\tInputs = {\n\t\t\t\tBackground = Input {\n\t\t\t\t\tSourceOp = \"Merge5\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t},\n\t\t\t\tForeground = Input {\n\t\t\t\t\tSourceOp = \"Transform2\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t},\n\t\t\t\tPerformDepthMerge = Input { Value = 0, }\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 1254, 103.121 } },\n\t\t},\n\t\tBackground1 = Background {\n\t\t\tInputs = {\n\t\t\t\tGlobalIn = Input { Value = -30, },\n\t\t\t\tGlobalOut = Input { Value = 80, },\n\t\t\t\tWidth = Input { Value = 1920, },\n\t\t\t\tHeight = Input { Value = 1080, },\n\t\t\t\tUseFrameFormatSettings = Input { Value = 1, },\n\t\t\t\t[\"Gamut.SLogVersion\"] = Input { Value = FuID { \"SLog2\" }, },\n\t\t\t\tTopLeftGreen = Input { Value = 0.333333333333333, },\n\t\t\t\tTopLeftBlue = Input { Value = 1, }\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 814.667, 74.6364 } },\n\t\t},\n\t\tMerge5 = Merge {\n\t\t\tInputs = {\n\t\t\t\tBackground = Input {\n\t\t\t\t\tSourceOp = \"Background1\",\n\t\t\t\t\tSource = \"Output\",\n\t\t\t\t},\n\t\t\t\tPerformDepthMerge = Input { Value = 0, }\n\t\t\t},\n\t\t\tViewInfo = OperatorInfo { Pos = { 1252.67, 60.0909 } },\n\t\t}\n\t}\n}",
        "locale": "en",
        "slug": "technical-posts/davinci-resolve-notes",
        "folder": ""
    },
    {
        "title": "DataGate license manager versions",
        "description": "DataGate license manager versions",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "datagate"
        ],
        "content": "DataGate license manager version to DG version:\n\nC.6 = Datagate 17.x\nB.6 = Datagate 16.x\n9.5 = DataGate 15.x\n8.5 = DataGate 14.x\n7.5 = DataGate 12.x\n6.5 = DataGate 11.x\n5.5 = DataGate 10.x\n4.5 = DataGate 9.x\n3.5 = DataGate 7.x or 8.x (same key/license schema)\n2.5 = DataGate 5.0\n\nFor example,\n\nThis v3.5 license manager indicates that DG 7 or 8 is installed.\n\n![[DataGate license manager versions.png|500]]",
        "locale": "en",
        "slug": "technical-posts/datagate-license-manager-versions",
        "folder": ""
    },
    {
        "title": "Sveltekit server-side redirect",
        "description": "Sveltekit server-side redirect",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "This JoyOfCode video shows how to change the incoming HTML with a hook, as well as how to fetch the URL of a the request. Both of these things are probably necessary for authentication.\nThis Huntabyte video shows how to project routes with hooks.\n\nA multi-language site needs to unconditionally redirect the request for the root to a specific page. In this case, it needs to redirect to the /en route.\n\nThe incoming path request is available in the [[Event object]] object (as shown below).\n\nimport { redirect } from '@sveltejs/kit';\n\n// State 1 - request received.\nexport const handle = async ({ event, resolve }) => {\n\t// Stage 2 - do something with incoming request.\n\n\tconsole.log(event);\n\n\t// Root requested?\n\tif (event.url.pathname == '/') {\n\t\tthrow redirect(302, '/en');\n\t}\n\n\t// Stage 3 - Send response\n\tconst response = await resolve(event);\n\n\treturn response;\n};",
        "locale": "en",
        "slug": "technical-posts/hooks-server-side-redirect",
        "folder": ""
    },
    {
        "title": "CSS subgrid",
        "description": "CSS subgrid",
        "date_created": "2025-08-05T00:00:00.000Z",
        "date_updated": "2025-12-17T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css/subgrid"
        ],
        "content": "CSS Subgrid is a powerful feature that solves a whole class of layout problems that were previously very difficult to handle with pure CSS.\n\nThe Analogy: Lined Paper\n\nImagine you have a sheet of lined paper. This is your parent grid. The horizontal lines are your grid-template-rows.\n\nBefore Subgrid (Nested Grids): You place several smaller, unlined sticky notes onto the paper. Each sticky note is a nested grid (.column). You can write on two lines within each sticky note, but the lines you draw yourself on one note will not line up with the lines on the note next to it. This is what happened with your first solutioneach column created its own independent rows.\n\nWith Subgrid: You take transparent overlays (your .column elements) and place them on the lined paper. The subgrid keyword essentially tells the overlay: \"Don't create your own lines. Use the lines from the paper underneath.\" Now, when you write on the first line of any overlay, it perfectly aligns with the first line on all other overlays, because they are all sharing the same master set of lines from the parent paper.\n\nThe Core Concept: Borrowing Tracks\n\nA normal nested grid creates its own, independent set of tracks (rows and columns) that have no relationship to the parent grid's tracks.\n\nA subgrid is a grid container that, instead of creating its own tracks, borrows the tracks from its direct parent grid.\n\nThis means that items within the subgrid can be aligned to other items in sibling subgrids, because they are all ultimately being aligned on the same master grid defined by their common parent.\n\nBreaking Down Your Solution Step-by-Step\n\nLet's look at the key pieces of CSS from the solution and analyze what each one does.\n\n1. The Parent Grid: .column-wrapper\n\n.column-wrapper {\n    display: grid;\n    grid-auto-flow: column;\n    grid-template-rows: auto auto; /* The \"Master Blueprint\" for rows */\n}\n\ndisplay: grid;: This establishes .column-wrapper as the master grid container.\ngrid-template-rows: auto auto;: This is the most critical declaration on the parent.\n    It defines two explicit horizontal tracks (rows) for the entire grid.\n    The auto keyword is crucial. It tells the browser: \"For this row, find the tallest piece of content among all the items in this row (across all columns), and make the row exactly that tall.\"\n    So, the first auto creates a row tall enough for your Win 11 and windows 10... content. The second auto creates a row tall enough for the tallest .requirements content.\n\n2. The Child Subgrid: .column\n\n.column {\n    display: grid;          /* Required to become a subgrid */\n    grid-template-rows: subgrid; /* The magic keyword */\n    grid-row: span 2;         /* Tells the column to occupy the parent's tracks */\n}\n\ndisplay: grid;: A container must be a grid itself before it can be a subgrid.\ngrid-template-rows: subgrid;: This is the command that makes it all work. It tells .column:\n    \"Forget about creating your own rows.\"\n    \"Look at my parent (.column-wrapper).\"\n    \"Inherit and use the row tracks that the parent has defined (auto auto).\"\ngrid-row: span 2;: This is an essential supporting piece. The .column element itself is an item placed on the parent grid. We need to tell it to occupy the space of both rows that the parent has defined. If we omitted this, the .column would only take up the first row by default, and its children (.platform, .requirements) wouldn't have a second row to flow into.\n\nNow, the two direct children of .column (.platform and .requirements) are placed into the two subgridded rows, which are perfectly aligned with the rows of all the other .column elements.\n\nKey Benefits Summarized\n\nTrue Alignment: It solves the core problem of aligning nested items across different parent containers.\nMaintained HTML Semantics: You don't have to break your logical HTML structure (keeping platform and requirements together in a column) just to achieve a certain layout.\nConsistent Gapping: If you define a gap on the parent grid, that gap is inherited by the subgrid, ensuring consistent spacing both between and within your columns.\nSource of Truth: The layout logic for the rows lives in one place (the parent grid), making the layout easier to understand and maintain.\n\nBrowser Support and Fallbacks\n\nSubgrid is supported in all major modern browsers (Chrome, Firefox, Safari, Edge). The main holdout was Chrome, which added support in late 2022. Its support is now widespread.\n\nIf you needed to support older browsers, you could use a feature query (@supports) to provide a simpler, non-aligned layout as a fallback:\n\n/* Fallback for older browsers */\n.column {\n    display: grid;\n    grid-auto-rows: auto;\n}\n\n/* Modern browsers that support subgrid will get the perfect layout */\n@supports (grid-template-rows: subgrid) {\n  .column-wrapper {\n    display: grid;\n    grid-auto-flow: column;\n    grid-template-rows: auto auto;\n  }\n  \n  .column {\n    grid-template-rows: subgrid;\n    grid-row: span 2;\n  }\n}\n\nFinal solution\n\nThis CSS governs the product matrices  at: https://www.asna.com/en/support/product-requirements\n\n![[image-40.png]]\n\n\n/* The Parent Wrapper - No changes here */\n.column-wrapper {\n    display: grid;\n    grid-auto-flow: column;\n    grid-auto-columns: 1fr;\n    grid-template-rows: auto auto; \n}\n\n/* The Subgrid Column - No changes here */\n.column {\n    display: grid;\n    grid-template-rows: subgrid;\n    grid-row: span 2;\n    border-right: 1px solid white;\n}\n\n/* \n  The Top-Row Item - THIS IS THE CORRECTED RULE \n*/\n.platform {\n  /* This makes the .platform element a flex container, allowing it\n     to control the alignment of its own children. */\n  display: flex;\n\n  /* This aligns the child items (the div with the text) to the\n     bottom of the flex container (.platform). */\n  align-items: flex-end;\n}  \nYou described it perfectly as an \"amalgam of solutions.\" It's a great showcase of how different CSS layout modules are designed to work together, each solving a specific part of a complex problem.\n\nA quick recap of the layers:\n\nThe Foundation (CSS Grid): We used display: grid on the main .column-wrapper to establish the overall multi-column structure and, most importantly, to define the two master rows whose height would be shared by everyone.\n\nThe Alignment Bridge (CSS Subgrid): This was the magic ingredient. grid-template-rows: subgrid on each .column allowed them to \"borrow\" the master row tracks from the parent. This solved the primary problem of keeping the top and bottom rows perfectly aligned across all the columns.\n\nThe Finisher (Flexbox): When we needed to align content within a grid cell without disturbing the cell's size, we turned .platform into a display: flex container. This allowed us to use align-items: flex-end to position the text at the bottom, while the .platform element itself continued to stretch and fill the full height of its grid row.\n\nIt's a perfect demonstration of the \"right tool for the right job\" principle:\nGrid for the overall page structure.\nSubgrid for aligning nested structures.\nFlexbox for fine-tuning the alignment of items within a container.\n",
        "locale": "en",
        "slug": "technical-posts/css-subgrid",
        "folder": ""
    },
    {
        "title": "Defining JSDoc types globally in Sveltekit",
        "description": "Defining JSDoc types globally in Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "jsdoc"
        ],
        "content": "[!question]\nIf I name global.d.ts with the .ts extension, intellisense can't find the type. It can if I rename the file to global.d.js. Is that ok?\n\nYes, renaming the file to global.d.js works because it aligns with how JSDoc and JavaScript work together for type hinting. However, using .js instead of .ts has a few implications:\n\n1. Why global.d.js Works\n\nBy renaming the file to global.d.js and using JSDoc syntax within it, youre effectively providing type information directly in JavaScript, which allows editors like VS Code to pick up the types for IntelliSense. This works because JSDoc is designed to provide type annotations in plain JavaScript files.\n\n2. When to Use .d.ts vs .d.js\n\n.d.ts files are TypeScript declaration files specifically used for TypeScript projects to define types globally. These files are designed for TypeScript to know about types at compile time.\n.d.js files are JavaScript files using JSDoc to declare types. This is often used when youre not fully using TypeScript but still want strong IntelliSense support in JavaScript.\n\nSince you're trying to make JSDoc work in your SvelteKit app, using .d.js with JSDoc annotations is perfectly fine, especially if you are not using full TypeScript.\n\n3. Considerations\n\nUsing global.d.js: It's totally fine if you're sticking with JavaScript and JSDoc for type annotations. IntelliSense will work as expected, and the global types will be available across your SvelteKit app.\nUsing global.d.ts: If you want to migrate fully to TypeScript in the future, global.d.ts is the preferred method. You might need to troubleshoot why IntelliSense isn't picking it up if you're using global.d.ts. Ensure that your project is configured for TypeScript correctly (i.e., you have a tsconfig.json with the right settings).\n\nConclusion:\n\nIt is absolutely fine to use global.d.js with JSDoc comments if you're primarily using JavaScript. If you plan to use TypeScript later, switching to global.d.ts and resolving IntelliSense issues might be a better long-term solution.",
        "locale": "en",
        "slug": "technical-posts/defining-jsdoc-types-globally-in-sveltekit",
        "folder": ""
    },
    {
        "title": "flex-search",
        "description": "FlexSearch in action",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-23T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "search"
        ],
        "content": "\nsearch-worker.ts\n\nThis Web worker creates and searches an index. \n\nimport { createIndex, searchIndex } from './search.ts';\nimport contents from '$lib/data/flex-search-input.json';\nimport type { Result } from './search.ts';\n\nimport { StopWatch } from '$lib/stopwatch.js';\n\n// Listen for messages\naddEventListener('message', async (e) => {\n    const { type, payload } = e.data;\n    console.log('type', type);\n\n    if (type === 'load') {\n        const sw = new StopWatch();\n        sw.start();\n\n        // Create search index.\n        createIndex(contents);\n\n        sw.elapsedMs('built index');\n\n        // Tell requester FlexSearch is ready.\n        postMessage({ type: 'ready' });\n    }\n\n    if (type === 'search_en') {\n        const searchTerm = payload.searchTerm;\n\n        // Search posts index.\n        const results = searchIndex('en', searchTerm);\n\n        // Send message with results and search term.\n        postMessage({ type: 'results', payload: { results, searchTerm } });\n    }\n\n    if (type === 'search_es') {\n        // Set search term.\n        const searchTerm = payload.searchTerm;\n\n        // Search posts index.\n        const results = searchIndex('es', searchTerm);\n\n        // const pageResults = results?.filter((result: Result) => result.isPage);\n        // const postResults = results?.filter((result: Result) => !result.isPage);\n        // const allResults = [...pageResults, ...postResults];\n\n        // Send message with results and search term.\n        postMessage({ type: 'results', payload: { results, searchTerm } });\n    }\n});\n\nSearchButton.svelte\n\nThis Svelte component presents a pseudo search button with the logic to invoke a search.\n\n\n    import { onMount } from 'svelte';\n    import { browser } from '$app/environment';\n    //import { createPostsIndex, searchPostsIndex, type Result } from '$lib/search'\n    import SearchWorker from '$lib/search-worker.ts?worker';\n\n    import { type Result } from '$lib/search';\n\n    import { StopWatch } from '$lib/stopwatch.js';\n\n    let searchButton: HTMLButtonElement | null = null;\n\n    // Svelete runes reactivity governs the search dialog behavior.\n    let search: 'idle' | 'load' | 'ready' = $state('idle');\n    let searchTerm = $state('');\n\n    // The search results are displayed when results has at least one element.\n    let results = $state([]);\n\n    let searchWorker: Worker;\n    let popoverElement: HTMLDivElement | null = null;\n\n    interface ToggleEvent extends Event {\n        newState: 'open' | 'closed';\n        oldState: 'open' | 'closed';\n    }\n\n    function sortSearchResults() {\n        if (results && results.length > 0) {\n            results.sort((a: Result, b: Result) => {\n                // Primary sort: isPage (true comes first)\n                if (a.isPage !== b.isPage) {\n                    return b.isPage ? 1 : -1; // Changed the logic here\n                }\n                // Secondary sort: title alphabetically\n                return a.title.localeCompare(b.title);\n            });\n        }\n    }\n\n    /*\n\t | Hardcoded for testing! Change for production.\n\t */\n    const locale: string = 'en';\n\n    onMount(() => {\n        // Create FlexSearch web worker.\n        searchWorker = new SearchWorker();\n        // Listen for messages from the worker.\n        searchWorker.addEventListener('message', (e) => {\n            const { type, payload } = e.data;\n            // When worker is ready, set the search state to ready.\n            if (type === 'ready') {\n                search = 'ready';\n            }\n            // After worker creates search results get them.\n            if (type === 'results') {\n                results = payload.results;\n                if (results) {\n                    console.log($state.snapshot(results[0]));\n                    sortSearchResults(results);\n                }\n            }\n        });\n\n        // Initialize the FlexSearch web worker when the component mounts.\n        searchWorker.postMessage({ type: 'load' });\n\n        popoverElement?.addEventListener('beforetoggle', (event: Event) => {\n            if ((event as ToggleEvent).newState === 'open') {\n                //document.body.setAttribute('inert', 'true')\n                searchTerm = '';\n            } else {\n                //document.body.removeAttribute('inert')\n            }\n        });\n\n        // The autofocus attribute causes issues with Sveltekit\n        // accessibility rules. This is a workaround.\n        popoverElement?.addEventListener('toggle', (event: Event) => {\n            if ((event as ToggleEvent).newState === 'open') {\n                popoverElement?.querySelector('input')?.focus();\n            }\n        });\n\n        //console.log(popoverElement)\n    });\n\n    $effect(() => {\n        /*\n\t\t | Search initiated here.\n\t\t */\n        // When search is ready and the search term changes send a\n        // search message to the worker.\n\n        if (search === 'ready') {\n            if (locale === 'en') {\n                searchWorker.postMessage({ type: 'search_en', payload: { searchTerm } });\n            } else {\n                searchWorker.postMessage({ type: 'search_es', payload: { searchTerm } });\n            }\n        }\n    });\n\n    if (browser) {\n        // Listen for the Ctrl + K key combination to open the search dialog.\n        window.addEventListener('keydown', (event) => {\n            if (event.ctrlKey && (event.key === 'k' || event.key === 'K')) {\n                event.preventDefault();\n                searchButton?.click();\n            }\n        });\n    }\n\n\n\n    \n        \n            \n            \n        Search...\n        \n            Ctrl K\n        \n    \n\n\n\n\n    \n        \n\n        \n            {#if results}\n                \n                    {#each results as result}\n                        \n                            path here -->\n                            \n                                {@html result?.title}\n                                {result?.isPage}\n                            \n                            {@html result.tags} -->\n                            \n                                \n                                    {@html result.description}\n                                \n                            \n                        \n                    {/each}\n                \n            {/if}\n        \n    \n\n\n\n\n\n    div.popover {\n        background-color: transparent;\n        border-radius: 15px;\n        padding: 0;\n        /* outline: 1px solid red; */\n    }\n\n    :global(body) {\n        font-family: 'Manrope', sans-serif;\n        font-size: 1.5rem;\n        color: hsl(220 10% 98%);\n        background-color: hsl(220 10% 10%);\n    }\n\n    :global(span.search-result-tag) {\n        color: darkslategray;\n        background-color: aqua;\n        font-size: 0.9rem;\n        padding: 4px 8px;\n        border-radius: 0.4rem;\n    }\n\n    :global(.search-results-tags-wrapper) {\n        margin-top: 0.6rem;\n        display: flex;\n        flex-wrap: wrap;\n        gap: 0.5rem;\n    }\n\n    :global(mark) {\n        /* background-color: hsl(128 71.1% 82.4%);\n\t\tfont */\n        color: yellow;\n        background-color: transparent;\n        font-style: italic;\n        /* color: hsl(220 10% 98%); */\n        border-radius: 5px;\n    }\n\n    .search {\n        width: 90vw;\n        top: 6rem;\n        position: fixed;\n        left: 50%;\n        transform: translateX(-50%);\n        max-width: 700px;\n        border-radius: 0.5rem;\n        box-shadow: 0px 0px 20px hsl(0 0% 0% / 40%);\n        overflow: hidden;\n\n        & input {\n            width: 100%;\n            padding: 1.5rem;\n            color: hsl(220 10% 98%);\n            background-color: hsl(220 10% 20%);\n            font: inherit;\n            border: none;\n            outline: none;\n        }\n    }\n\n    .results {\n        max-height: 48vh;\n        padding: 1.5rem;\n        background-color: hsl(220 10% 14%);\n        overflow-y: auto;\n        scrollbar-width: thin;\n\n        & ul {\n            display: grid;\n            gap: 1rem;\n            padding: 0px;\n            margin: 0px;\n            list-style: none;\n\n            & li p {\n                color: silver;\n                font-size: 1.2rem;\n            }\n\n            & li:not(:last-child) {\n                padding-block: 0.5rem;\n                border-bottom: 1px solid hsl(220 10% 20%);\n            }\n        }\n\n        & a {\n            display: block;\n            font-size: 1.5rem;\n            color: hsl(220 10% 80%);\n            text-decoration: none;\n            transition: color 0.3s ease;\n\n            &:hover {\n                color: aqua;\n            }\n\n            &:focus {\n                outline: none;\n                /* background-color: gray; */\n                text-decoration: underline;\n            }\n        }\n    }\n\n    div.search-button-container {\n        pointer-events: auto;\n\n        & button {\n            width: 15rem;\n            display: flex;\n            justify-content: flex-start;\n            align-items: center;\n            border-radius: 5px;\n            padding: 4px;\n            color: #36454f;\n            outline: none;\n            cursor: pointer;\n\n            & svg {\n                margin-right: 2px;\n            }\n\n            & span.keystroke-container {\n                margin-left: auto;\n            }\n\n            & span.keystroke {\n                font-size: 0.9rem;\n                /* border: 1px solid lightgray; */\n                padding-inline: 4px;\n                border-radius: 3px;\n                background-color: #fff;\n                outline: 1px solid gray;\n            }\n\n            & span.keystroke {\n                padding-inline: 6px;\n            }\n        }\n\n        & button:focus {\n            box-shadow:\n                rgba(204, 85, 0, 1) 0px 0px 0px 1px,\n                rgb(209, 213, 219) 0px 0px 0px 1px inset;\n        }\n    }\n\n\nsearch.ts\n\nThis TypeScript file displays search results.\n\nimport FlexSearch from 'flexsearch';\n\nexport type Content = {\n    locale: string;\n    content: string;\n    slug: string;\n    title: string;\n    description: string;\n    tags_list: string;\n    isPage: boolean;\n};\n\nexport type Result = {\n    content: string[];\n    slug: string;\n    title: string;\n    description?: string;\n    tags_list?: string;\n    isPage?: boolean;\n};\n\nlet contentsIndex: FlexSearch.Index;\nlet contents: Content[];\n\n/*\n Create a Flex search index for the content.\n */\nexport function createIndex(data: Content[]) {\n    contentsIndex = new FlexSearch.Index({ tokenize: 'forward' });\n\n    data.forEach((post, i) => {\n        // const item = ${post.title} ${post.content} ${post.description}\n        const item = ${post.content} ${post.description};\n        contentsIndex.add(i, item);\n    });\n\n    contents = data;\n}\n\nfunction convertTagsListToHTMLString(tags: string): string {\n    // Tags are indexed as a comma-separated string. This function converts that tags\n    // string into an HTML string. Each tag is wrapped in a span element:\n    // ${tag}\n\n    // The tags property of the indexing json files has already destructured the array\n    // in a list.\n    return ${tags};\n\n    // const tagEach = tags.split(\" \");\n\n    // const tagHTMLString = tagEach.reduce((acc, tag) => {\n    //     acc += ${tag};\n    //     return acc;\n    // }, \"\");\n\n    // return tagHTMLString;\n}\n\nfunction dedupeByPropertyMap(arr: any, prop: string) {\n    const uniqueMap = new Map();\n    arr.forEach((obj: any) => {\n        // We only set it if it's not already there, to keep the FIRST occurrence\n        if (!uniqueMap.has(obj[prop])) {\n            uniqueMap.set(obj[prop], obj);\n        }\n    });\n    return Array.from(uniqueMap.values());\n}\n\nexport function searchIndex(resultLocale: string, searchTerm: string) {\n    if (searchTerm.trim().length === 0) {\n        return;\n    }\n    const match = searchTerm.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&');\n    const results = contentsIndex.search(match, { limit: 50 });\n\n    if (searchTerm.length > 0 && results.length === 0) {\n        return [\n            {\n                title: 'No results found.',\n                content: ['Please try a different search term.'],\n                slug: ''\n            }\n        ];\n    }\n\n    const searchResults = results\n        .map((index) => contents[index as number])\n        .filter((content) => content.locale === resultLocale)\n        .map(({ slug, title, description, content, tags_list, isPage }) => {\n            const tagHTMLString = convertTagsListToHTMLString(tags_list);\n\n            const barPosition = title.indexOf('|');\n            if (barPosition !== -1) {\n                title = title.substring(0, barPosition);\n            }\n\n            return {\n                slug,\n                title: ${replaceTextWithMarker(title, match)},\n                description: ${replaceTextWithMarker(description, match)},\n                content: '',\n                tags: ${tagHTMLString},\n                isPage\n            };\n        });\n\n    //console.log(\"searchResults\", searchResults);\n\n    // Ensure only unique 'slug' entries are returned. \n    return dedupeByPropertyMap(searchResults, 'slug');\n}\n\n/*\n Find each search term match and surround it with a  tag.\n */\nfunction replaceTextWithMarker(text: string, match: string) {\n    // console.log(\"text\", text);\n    // console.log(\"match\", match);\n\n    const regex = new RegExp(match, 'gi');\n    return text.replaceAll(regex, (match) => ${match});\n}\n\n/*\n Substring each match.\n */\nfunction getMatches(text: string, searchTerm: string, limit = 1) {\n    const regex = new RegExp(searchTerm, 'gi');\n    const indexes = [];\n    let matches = 0;\n    let match;\n\n    while ((match = regex.exec(text)) !== null && matches  {\n        const start = index - 20;\n        const end = index + 120;\n        const excerpt = text.substring(start, end).trim();\n        return ...${replaceTextWithMarker(excerpt, searchTerm)}...;\n    });\n}",
        "locale": "en",
        "slug": "technical-posts/flex-search",
        "folder": ""
    },
    {
        "title": "C# data models for Excel using attributes and reflection",
        "description": "C# data models for Excel using attributes and reflection",
        "date_created": "2025-06-05T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "excel",
            "data-model"
        ],
        "content": "I spent a little time with Google Gemini exploring how to make better C# data models for Excel. This is the what I learned/did. \n\nThe working code is here:\n\nC:\\Users\\thumb\\Documents\\projects\\cs\\ExportExcelToSQLServer\n\nGiven a C# class like this, write a function that uses reflection to return a string list of the name of each property:\n\nnamespace Models\n{\n    public class LakeB2B\n    {\n        public int? Counter { get; set; }\n        public string? Tag { get; set; }\n        public string? Email { get; set; }\n        public string? CompanyName { get; set; }\n        public string? WebAddress { get; set; }\n        public string? Prefix { get; set; }\n        public string? ContactName { get; set; }\n        public string? FirstName { get; set; }\n        public string? MiddleName { get; set; }\n        public string? LastName { get; set; }\n        public string? Title { get; set; }\n        public string? Address { get; set; }\n        public string? Address1 { get; set; }\n        public string? City { get; set; }\n        public string? State { get; set; }\n        public string? ZipCode { get; set; }\n        public string? Country { get; set; }\n        public string? PhoneNumber { get; set; }\n        public string? EmployeesSize { get; set; }\n        public string? RevenueSize { get; set; }\n        public string? SicCodes { get; set; }\n        public string? Industry { get; set; }\n        public string? ApplicationType { get; set; }\n    }\n}\n\nAnswer\n\nOf course. Using reflection to inspect a class's members is a powerful feature in C#. Here is a function that accomplishes exactly what you've asked for, along with a complete, runnable example.\n\nWe'll create a generic method, which is the most common and flexible way to handle this.\n\nMethod 1: The Generic LINQ Approach (Recommended)\n\nThis approach is clean, concise, and type-safe. It uses a generic type parameter `` to represent the class you want to inspect.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Reflection; // Important: Add this using statement for reflection\n\nnamespace Models\n{\n    // The class provided in the request\n    public class LakeB2B\n    {\n        public int? Counter { get; set; }\n        public string? Tag { get; set; }\n        public string? Email { get; set; }\n        public string? CompanyName { get; set; }\n        public string? WebAddress { get; set; }\n        public string? Prefix { get; set; }\n        public string? ContactName { get; set; }\n        public string? FirstName { get; set; }\n        public string? MiddleName { get; set; }\n        public string? LastName { get; set; }\n        public string? Title { get; set; }\n        public string? Address { get; set; }\n        public string? Address1 { get; set; }\n        public string? City { get; set; }\n        public string? State { get; set; }\n        public string? ZipCode { get; set; }\n        public string? Country { get; set; }\n        public string? PhoneNumber { get; set; }\n        public string? EmployeesSize { get; set; }\n        public string? RevenueSize { get; set; }\n        public string? SicCodes { get; set; }\n        public string? Industry { get; set; }\n        public string? ApplicationType { get; set; }\n    }\n}\n\npublic class ReflectionExample\n{\n    /// \n    /// Uses reflection to get the names of all public properties for a given type.\n    /// \n    /// The type to inspect.\n    /// A List of strings containing the property names.\n    public static List GetPropertyNames()\n    {\n        // Get the type of the generic parameter\n        Type type = typeof(T);\n\n        // Get all public instance properties\n        PropertyInfo[] properties = type.GetProperties(BindingFlags.Public | BindingFlags.Instance);\n\n        // Use LINQ to select the name of each property and return it as a list\n        return properties.Select(p => p.Name).ToList();\n    }\n\n    public static void Main(string[] args)\n    {\n        // Call the function, passing LakeB2B as the type parameter\n        List propertyNames = GetPropertyNames();\n\n        Console.WriteLine(\"Properties of the LakeB2B class:\");\n        Console.WriteLine(\"---------------------------------\");\n\n        // Print each property name\n        foreach (string name in propertyNames)\n        {\n            Console.WriteLine(name);\n        }\n    }\n}\n\nHow It Works:\n\nusing System.Reflection;: This namespace is required to access reflection-related types like Type, PropertyInfo, and BindingFlags.\npublic static List GetPropertyNames(): We define a generic method. T will be the class we want to analyze (in this case, LakeB2B).\ntypeof(T): This gets the Type object that represents the class T. The Type object is the main entry point for reflection.\n.GetProperties(...): This method returns an array of PropertyInfo objects, each representing one property of the type.\nBindingFlags.Public | BindingFlags.Instance: These flags are important. They tell GetProperties to return only the properties that are public and are instance members (not static). This is the most common use case.\n.Select(p => p.Name): This is a LINQ extension method. It iterates through the PropertyInfo array and, for each property (p), it \"selects\" its Name property.\n.ToList(): This LINQ method converts the resulting collection of names into a List.\n\nMethod 2: Taking a Type Object as a Parameter\n\nSometimes, you might not know the type at compile time and will only have a Type object. This version of the function is useful in those scenarios.\n\n/// \n/// Uses reflection to get the names of all public properties for a given Type object.\n/// \n/// The Type object to inspect.\n/// A List of strings containing the property names.\npublic static List GetPropertyNames(Type type)\n{\n    if (type == null)\n    {\n        return new List();\n    }\n\n    return type.GetProperties(BindingFlags.Public | BindingFlags.Instance)\n               .Select(p => p.Name)\n               .ToList();\n}\n\n// You would call it like this in your Main method:\nList propertyNames = GetPropertyNames(typeof(Models.LakeB2B));\n\nThis version is functionally identical but offers a different way to pass in the type you want to inspect. The generic version is often preferred for its compile-time type safety.\n\nQuestion/Directive\n\nWrite a C# similar to that that reads the data model with reflection and returns a map of each property name and its data type\nAnswer\n\nExcellent request! Building on the previous example, this is a very common and useful reflection task.\n\nThe goal is to get a \"map\" of property names to their corresponding data types. In C#, the ideal data structure for this is a Dictionary.\n\nHere is a complete, runnable C# program that uses reflection to generate this map.\n\nRecommended: The Generic LINQ Approach\n\nThis is the most concise and idiomatic way to achieve this in modern C#. It uses the powerful ToDictionary() LINQ extension method.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Reflection; // You MUST have this for reflection\n\nnamespace Models\n{\n    // The same data model class\n    public class LakeB2B\n    {\n        public int? Counter { get; set; }\n        public string? Tag { get; set; }\n        public string? Email { get; set; }\n        public string? CompanyName { get; set; }\n        // ... (other properties from your example)\n        public string? Industry { get; set; }\n        public Guid RecordId { get; set; } // Added for type variety\n    }\n}\n\npublic class ReflectionDictionaryExample\n{\n    /// \n    /// Uses reflection to create a map of property names to their data types.\n    /// \n    /// The type to inspect.\n    /// A Dictionary where the key is the property name (string) \n    /// and the value is the property's data type (Type).\n    public static Dictionary GetPropertiesAndTypes()\n    {\n        return typeof(T)\n            .GetProperties(BindingFlags.Public | BindingFlags.Instance)\n            .ToDictionary(prop => prop.Name, prop => prop.PropertyType);\n    }\n\n    public static void Main(string[] args)\n    {\n        // Get the map for the LakeB2B class\n        Dictionary propertyMap = GetPropertiesAndTypes();\n\n        Console.WriteLine(\"Property Map for LakeB2B class:\");\n        Console.WriteLine(\"---------------------------------\");\n\n        // Iterate through the dictionary and print each key-value pair\n        foreach (KeyValuePair entry in propertyMap)\n        {\n            // entry.Value.Name gives a clean name like \"String\", \"Int32\", or \"Guid\"\n            Console.WriteLine($\"Property: {entry.Key,-15} | Type: {entry.Value.Name}\");\n        }\n    }\n}\n\nHow It Works:\n\nGetProperties(...): Just like before, this gets an array of PropertyInfo objects for our class.\n.ToDictionary(...): This is a LINQ extension method that converts a sequence into a dictionary. It requires two arguments (provided as lambda expressions):\n    prop => prop.Name: This is the key selector. For each PropertyInfo object (prop) in the sequence, it uses its Name as the dictionary key.\n    prop => prop.PropertyType: This is the value selector. For each PropertyInfo object, it uses its PropertyType (which is a Type object) as the dictionary value.\n\nSample Output:\n\nProperty Map for LakeB2B class:\nProperty: Counter         | Type: Nullable`1\nProperty: Tag             | Type: String\nProperty: Email           | Type: String\nProperty: CompanyName     | Type: String\nProperty: Industry        | Type: String\nProperty: RecordId        | Type: Guid\n\nAlternative: The foreach Loop Approach\n\nIf you prefer a more explicit, step-by-step approach without LINQ, a foreach loop works perfectly well and can be easier to debug for beginners.\n\npublic static Dictionary GetPropertiesAndTypesWithLoop()\n{\n    // Create an empty dictionary to store the results\n    var map = new Dictionary();\n\n    // Get the type and its properties\n    Type type = typeof(T);\n    PropertyInfo[] properties = type.GetProperties(BindingFlags.Public | BindingFlags.Instance);\n\n    // Loop through each property\n    foreach (PropertyInfo prop in properties)\n    {\n        // Add the property's name and type to the dictionary\n        map.Add(prop.Name, prop.PropertyType);\n    }\n\n    return map;\n}\nThis function produces the exact same result as the LINQ version.\n\nA Note on Nullable Types (like int?)\n\nYou may have noticed Nullable1 in the output for the Counter` property. This is how the .NET runtime sees nullable value types.\n\nint? is just C# syntactic sugar for System.Nullable.\nNullable'1 is the runtime's name for a generic type Nullable with one type parameter (T).\n\nWhen you get the Type object for a nullable property, you are getting the Nullable type itself. You can check for this and get the underlying type if needed:\n\nType propertyType = entry.Value; // e.g., the Type for Nullable\n\n// Check if it's a nullable type\nType underlyingType = Nullable.GetUnderlyingType(propertyType);\n\nif (underlyingType != null)\n{\n    // It was nullable!\n    Console.WriteLine($\"The type is Nullable of {underlyingType.Name}\");\n}\nelse\n{\n    // It was a regular, non-nullable type\n    Console.WriteLine($\"The type is {propertyType.Name}\");\n}\n\nQuestion/Directive\n\nI want to add an attribute to each property that provides the corresponding Excel column identifier and column name, like this: \n\n[ExcelInfo(\"C\", \"Email\")] \npublic string? Email { get; set; }\n\n[ExcelInfo(\"D\", \"Company Name\")]\n\nWrite the ExcelInfo attribute function and then modify the function that returns the map of property name and type to also return the column identifier and the column name\nAnswer\n\nOf course! This is a fantastic use case for custom attributes and reflection. It allows you to embed metadata directly into your data models.\n\nHere is the complete solution, broken down into three parts:\nThe ExcelInfoAttribute class: The custom attribute you will use to decorate your properties.\nThe PropertyMetadata record: A simple data structure to hold the combined information for each property.\nThe updated reflection function: The method that reads the properties and their attributes, returning the final map.\n\nStep 1: Define the ExcelInfoAttribute\n\nFirst, you need to create the attribute itself. An attribute is a special kind of class that inherits from System.Attribute.\n\nusing System;\n\n/// \n/// Specifies the Excel column details for a property.\n/// \n[AttributeUsage(AttributeTargets.Property, Inherited = false, AllowMultiple = false)]\npublic sealed class ExcelInfoAttribute : Attribute\n{\n    /// \n    /// The Excel column identifier (e.g., \"A\", \"B\", \"C\").\n    /// \n    public string ColumnId { get; }\n\n    /// \n    /// The friendly name for the Excel column header.\n    /// \n    public string ColumnName { get; }\n\n    /// \n    /// Initializes a new instance of the ExcelInfoAttribute class.\n    /// \n    /// The Excel column identifier (e.g., \"A\", \"B\").\n    /// The friendly name for the column header.\n    public ExcelInfoAttribute(string columnId, string columnName)\n    {\n        ColumnId = columnId;\n        ColumnName = columnName;\n    }\n}\n\nKey Points:\n\n[AttributeUsage(AttributeTargets.Property)]: This is crucial. It specifies that this attribute can only be applied to properties.\nInherited = false, AllowMultiple = false: These are good practices. They mean the attribute isn't inherited by subclasses and you can't apply it more than once to the same property.\nThe constructor takes the columnId and columnName and stores them in public, read-only properties, which our reflection code will access later.\n\nStep 2: Create a Data Structure for the Results\n\nTo return all the required data (property type, column ID, and column name), we need a container. A C# record is perfect for this as it's a lightweight, immutable data-transfer object.\n\n/// \n/// Holds metadata extracted from a class property via reflection.\n/// \n/// The .NET type of the property.\n/// The friendly Excel column name from the ExcelInfo attribute.\n/// The Excel column identifier from the ExcelInfo attribute.\npublic record PropertyMetadata(Type DataType, string? ColumnName, string? ColumnId);\nWe use nullable strings (string?) because a property might not have the ExcelInfo attribute, in which case this data would be null.\n\nStep 3: The Complete Reflection Function and Example\n\nNow we can write the main function. It will inspect a class, look for the ExcelInfoAttribute on each property, and build a dictionary mapping the property name to its PropertyMetadata.\n\nHere is the full, runnable code:\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Reflection;\n\nnamespace Models\n{\n    // The updated data model with attributes\n    public class LakeB2B\n    {\n        // This property does NOT have the attribute, to show how it's handled.\n        public int? Counter { get; set; }\n        \n        [ExcelInfo(\"B\", \"Unique Tag\")]\n        public string? Tag { get; set; }\n\n        [ExcelInfo(\"C\", \"Email Address\")]\n        public string? Email { get; set; }\n\n        [ExcelInfo(\"D\", \"Company Name\")]\n        public string? CompanyName { get; set; }\n        \n        [ExcelInfo(\"E\", \"Website\")]\n        public string? WebAddress { get; set; }\n    }\n}\n\npublic class AdvancedReflectionExample\n{\n    /// \n    /// Uses reflection to create a map of property metadata, including custom attributes.\n    /// \n    /// The type to inspect.\n    /// A dictionary mapping the C# property name to its collected metadata.\n    public static Dictionary GetPropertyMetadata()\n    {\n        return typeof(T)\n            .GetProperties(BindingFlags.Public | BindingFlags.Instance)\n            .ToDictionary(\n                // The Key for the dictionary is the property's C# name (e.g., \"CompanyName\")\n                prop => prop.Name,\n                // The Value is a new PropertyMetadata object\n                prop =>\n                {\n                    // Use reflection to find our custom attribute on the property\n                    var excelAttr = prop.GetCustomAttribute();\n\n                    // Create the metadata object.\n                    // If excelAttr is null, the null-conditional operator (?.) will result in null.\n                    return new PropertyMetadata(\n                        prop.PropertyType,\n                        excelAttr?.ColumnName, \n                        excelAttr?.ColumnId   \n                    );\n                }\n            );\n    }\n\n    public static void Main(string[] args)\n    {\n        var metadataMap = GetPropertyMetadata();\n\n        Console.WriteLine($\"{\"C# Property\",-20} | {\"Data Type\",-12} | {\"Excel Column\",-15} | {\"Excel Header\",-20}\");\n        Console.WriteLine(new string('-', 80));\n\n        foreach (var entry in metadataMap)\n        {\n            string propertyName = entry.Key;\n            PropertyMetadata metadata = entry.Value;\n\n            // Use the null-coalescing operator (??) to provide a default value for printing\n            Console.WriteLine(\n                $\"{propertyName,-20} | {metadata.DataType.Name,-12} | {metadata.ColumnId ?? \"N/A\",-15} | {metadata.ColumnName ?? \"N/A\",-20}\"\n            );\n        }\n    }\n}\n\nHow the Reflection Function Works:\n\nGetProperties(...): Gets all public instance properties as PropertyInfo objects.\n.ToDictionary(...): Converts this collection into a dictionary.\nprop => prop.Name: This lambda defines how to get the key for each dictionary entry (we use the property's C# name).\nprop => { ... }: This lambda defines how to create the value for each entry.\nprop.GetCustomAttribute(): This is the core of the new logic. It inspects the current property (prop) and tries to find an attribute of type ExcelInfoAttribute. It returns the attribute instance if found, or null if not.\nnew PropertyMetadata(...): We create our metadata record.\nexcelAttr?.ColumnName: The magic is the null-conditional operator (?.). If excelAttr is not null, it accesses the ColumnName property. If excelAttr is null (because the attribute wasn't on the property), the expression evaluates to null without causing a NullReferenceException.\n\nExpected Output:\n\nC# Property          | Data Type    | Excel Column    | Excel Header\nCounter              | Nullable`1   | N/A             | N/A\nTag                  | String       | B               | Unique Tag\nEmail                | String       | C               | Email Address\nCompanyName          | String       | D               | Company Name\nWebAddress           | String       | E               | Website\n\nQuestion/Directive\n\nModify the PropertyMetaData record to be:  \npublic record PropertyMetadata(Type DataType, Boolean NullCapable, string? ColumnName, string? ColumnId);\n\nwhere, for nullable properties, the DataType is the underlying data type, otherwise show the non-nullable data type and where NullCapable is true if the column is nullable\n\nAnswer\n\nExcellent. This is a great evolution of the previous task and requires a more sophisticated use of reflection to correctly handle C#'s nullability features.\n\nThe key challenge is that nullability works differently for value types (like int, Guid) and reference types (like string). We will need to handle both cases correctly.\n\nHere is the complete solution.\n\nStep 1: Update the PropertyMetadata Record\n\nFirst, we'll define the new record as you specified.\n\n/// \n/// Holds metadata extracted from a class property via reflection.\n/// \n/// The underlying .NET type of the property (e.g., Int32, String).\n/// True if the property can hold a null value.\n/// The friendly Excel column name from the ExcelInfo attribute.\n/// The Excel column identifier from the ExcelInfo attribute.\npublic record PropertyMetadata(Type DataType, bool NullCapable, string? ColumnName, string? ColumnId);\n\nStep 2: Update the Data Model for Demonstration\n\nTo properly test our function, let's update the LakeB2B class to include a variety of nullable and non-nullable types.\n\n// The attribute class remains the same as before\n[AttributeUsage(AttributeTargets.Property)]\npublic sealed class ExcelInfoAttribute : Attribute\n{\n    public string ColumnId { get; }\n    public string ColumnName { get; }\n    public ExcelInfoAttribute(string columnId, string columnName)\n    {\n        ColumnId = columnId;\n        ColumnName = columnName;\n    }\n}\n\nnamespace Models\n{\n    public class LakeB2B\n    {\n        // 1. Nullable value type\n        [ExcelInfo(\"A\", \"Counter\")]\n        public int? Counter { get; set; }\n\n        // 2. Nullable reference type\n        [ExcelInfo(\"C\", \"Email Address\")]\n        public string? Email { get; set; }\n\n        // 3. Non-nullable value type\n        [ExcelInfo(\"F\", \"Record ID\")]\n        public Guid RecordId { get; set; }\n\n        // 4. Non-nullable reference type (requires an initializer in a nullable context)\n        [ExcelInfo(\"D\", \"Company Name\")]\n        public string CompanyName { get; set; } = string.Empty;\n    }\n}\n\nStep 3: The Updated Reflection Function\n\nThis is where the new logic lives. We will use two key reflection tools:\nNullable.GetUnderlyingType(): For nullable value types (like int?).\nNullabilityInfoContext: For nullable reference types (like string?). This is the modern, correct way to check nullability information added by the C# compiler.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Reflection;\n\npublic class UltimateReflectionExample\n{\n    /// \n    /// Uses reflection to get detailed metadata for each property in a class,\n    /// correctly identifying underlying types and nullability.\n    /// \n    /// The type to inspect.\n    /// A dictionary mapping property names to their full metadata.\n    public static Dictionary GetPropertyMetadata()\n    {\n        // This context is used to correctly interpret nullable reference types (e.g., string?)\n        var nullabilityContext = new NullabilityInfoContext();\n\n        return typeof(T)\n            .GetProperties(BindingFlags.Public | BindingFlags.Instance)\n            .ToDictionary(\n                prop => prop.Name,\n                prop =>\n                {\n                    // --- Determine Data Type and Nullability ---\n\n                    // Check for nullable value types (e.g., int?, Guid?)\n                    Type underlyingType = Nullable.GetUnderlyingType(prop.PropertyType);\n                    \n                    // Use the NullabilityInfoContext for reference types (e.g., string)\n                    var nullabilityInfo = nullabilityContext.Create(prop);\n\n                    // Logic:\n                    // 1. The final data type is the underlying type if it exists, otherwise the property type.\n                    // 2. The property is null-capable if it's a Nullable OR if the compiler's\n                    //    nullability info marks it as nullable.\n                    Type finalType = underlyingType ?? prop.PropertyType;\n                    bool isNullable = (underlyingType != null) || (nullabilityInfo.ReadState == NullabilityState.Nullable);\n\n                    // --- Get Custom Attribute Data ---\n                    var excelAttr = prop.GetCustomAttribute();\n\n                    return new PropertyMetadata(\n                        finalType,\n                        isNullable,\n                        excelAttr?.ColumnName,\n                        excelAttr?.ColumnId\n                    );\n                }\n            );\n    }\n\n    public static void Main(string[] args)\n    {\n        var metadataMap = GetPropertyMetadata();\n\n        Console.WriteLine($\"{\"C# Property\",-15} | {\"Data Type\",-10} | {\"Nullable?\",-10} | {\"Excel ID\",-10} | {\"Excel Header\",-20}\");\n        Console.WriteLine(new string('-', 80));\n\n        foreach (var entry in metadataMap)\n        {\n            string propertyName = entry.Key;\n            PropertyMetadata metadata = entry.Value;\n\n            Console.WriteLine(\n                $\"{propertyName,-15} | {metadata.DataType.Name,-10} | {metadata.NullCapable,-10} | {metadata.ColumnId ?? \"N/A\",-10} | {metadata.ColumnName ?? \"N/A\",-20}\"\n            );\n        }\n    }\n}\n\nHow the New Logic Works\n\nInside the ToDictionary lambda, for each property (prop):\n\nNullable.GetUnderlyingType(prop.PropertyType):\n    If prop is int? (which is Nullable), this returns typeof(int).\n    If prop is string or int, this returns null.\n\nnew NullabilityInfoContext().Create(prop):\n    This creates a NullabilityInfo object that reads the special metadata the compiler emits for nullable reference types.\n    nullabilityInfo.ReadState will be NullabilityState.Nullable for string? and NullabilityState.NotNull for string.\n\nType finalType = underlyingType ?? prop.PropertyType;:\n    The null-coalescing operator ?? provides a clean way to get the base type.\n    For int?, underlyingType is int, so finalType becomes int.\n    For string or Guid, underlyingType is null, so finalType becomes the original property type (string or Guid).\n\nbool isNullable = (underlyingType != null) || (nullabilityInfo.ReadState == NullabilityState.Nullable);:\n    This boolean expression covers all cases:\n        For int?: underlyingType is not null, so the expression is true. Correct.\n        For Guid: underlyingType is null, and ReadState is NotNull. The expression is false. Correct.\n        For string?: underlyingType is null, but ReadState is Nullable. The expression is true. Correct.\n        For string: underlyingType is null, and ReadState is NotNull. The expression is false. Correct.\n\nExpected Output\n\nThe program will now produce this detailed and accurate output:\n\nC# Property     | Data Type  | Nullable?  | Excel ID   | Excel Header\nCounter         | Int32      | True       | A          | Counter\nEmail           | String     | True       | C          | Email Address\nRecordId        | Guid       | False      | F          | Record ID\nCompanyName     | String     | False      | D          | Company Name\n\nQuestion/Directive\n\nusing C# 12 and ClosedXML, write a function that reads the column names in an Excel spread and returns a dictionary of column names and Excel data types:\n\nAnswer\n\nAnalyzing the Core Task\n\nI'm focusing now on the essence of the task: extracting column names from an Excel file using C# and ClosedXML.  I'm breaking down the objective, keeping the language and chosen library in mind. This is the first step toward crafting a precise solution.\n\n\nMapping the Approach\n\nI've been mapping the approach. Focusing on the necessary ClosedXML methods, I will read the file and get the active worksheet.  I intend to iterate through the first row to collect the header names. I plan to use the XLDataType enum to identify the column data types based on the content of the cells below the headers.\n\n\nImplementing the Function\n\nI'm now implementing the function, focusing on handling file paths and sheet names. The goal is to make it robust, able to read from any valid Excel file using ClosedXML. I'm prioritizing error handling and ensuring the function can gracefully manage different file formats.\n\n\nStructuring the Logic\n\nI'm now structuring the logic of the GetExcelColumnHeadersAndTypes function. I'll open the workbook and get the appropriate worksheet, handling potential null values gracefully. I will iterate through the first row to capture the column names, and then, using ClosedXML, ascertain the Excel data type of the cells directly below the header. The code should be concise and well-commented.\n\n\nCrafting the Core Logic\n\nI'm now implementing the core logic of iterating through the header row and then examining the cell directly below each header to determine the column data type, using XLDataType. I'll handle potential exceptions and empty worksheets gracefully. I'm focusing on ensuring the code is readable and maintainable. I'll need to handle the case of an empty sheet, and gracefully return an empty dictionary.\n\n\nDefining the Core Structure\n\nI'm solidifying the logic, especially the crucial part of determining the data type for each column. The main function will take the file path and optional sheet name, open the workbook, and then iterate through the header row and the first data row below. I'm focusing on ClosedXML methods for sheet and cell access to make this function robust.\n\n\nCrafting the Implementation\n\nI'm now integrating the logic into the GetExcelColumnHeadersAndTypes function. I've considered handling the situation when the sheet is empty to gracefully return an empty dictionary. The function's aim is to parse the header row, determine the data type of the column below it, and populate the dictionary accordingly. I also aim to include a helper function to build a sample Excel file for the sake of demo code.\n\n\nRefining the Logic\n\nI'm now refining the implementation of the GetExcelColumnHeadersAndTypes function, incorporating comments for clarity. I've focused on graceful handling of different edge cases. I'll integrate the demo code, including file creation, in the Main method.  I'm also focusing on error handling with try-catch blocks and the appropriate NuGet package.\n\n\nRefining the Approach\n\nI'm now iterating through the steps, working to refine the logic for determining the data types. I'll need to account for empty cells and sheets, and ensure the code is clear. I'm focusing on ClosedXML's API to ensure the code is robust. The main method will also be adapted to demonstrate the code's functionality, including error handling.\n\nQuestion/Directive\n\nmodify GetExcelColumnHeadersAndTypes to also return the column address (ie, \"A\", \"B\", \"AA\", etc)\n\nAnswer\n\nExcellent. Modifying the function to include the column address is a very practical enhancement. We'll achieve this by changing the return value from a simple XLDataType to a more structured object that can hold both the data type and the column address. A C# record is perfect for this.\n\nHere is the updated, complete solution.\n\nStep 1: Define a Record to Hold the Column Information\n\nInstead of returning just a data type, we need a structure to hold the type and the address. A record is the ideal, modern C# way to create this lightweight data-transfer object.\n\n/// \n/// Holds metadata about a specific column in an Excel sheet.\n/// \n/// The inferred data type of the column's data.\n/// The column letter identifier (e.g., \"A\", \"B\", \"AA\").\npublic record ExcelColumnInfo(XLDataType DataType, string Address);\n\nStep 2: The Modified C# Code\n\nWe'll update the GetExcelColumnHeadersAndTypes function (renaming it to GetExcelColumnInfo for clarity) to return a Dictionary. The logic to get the column address from a ClosedXML cell is straightforward.\n\nusing ClosedXML.Excel;\nusing System.IO;\n\n// --- New Record Definition ---\n/// \n/// Holds metadata about a specific column in an Excel sheet.\n/// \n/// The inferred data type of the column's data.\n/// The column letter identifier (e.g., \"A\", \"B\", \"AA\").\npublic record ExcelColumnInfo(XLDataType DataType, string Address);\n\n\n// --- Main Program Logic (C# 12 Top-level statements) ---\nConsole.WriteLine(\"--- ClosedXML Column Info Reader ---\");\n\nstring filePath = \"SalesData.xlsx\";\n\n// 1. Create a sample file (same as before)\ntry\n{\n    CreateSampleExcelFile(filePath);\n    Console.WriteLine($\"Successfully created sample file: {filePath}\");\n}\ncatch (Exception ex)\n{\n    Console.WriteLine($\"Error creating sample file: {ex.Message}\");\n    return;\n}\n\n// 2. Read the full column info from the file\ntry\n{\n    // Call the updated function\n    Dictionary columnMap = GetExcelColumnInfo(filePath, \"SalesData\");\n\n    Console.WriteLine(\"\\nDiscovered Columns and Their Metadata:\");\n    Console.WriteLine(new string('-', 60));\n    Console.WriteLine($\"{\"Column Name\",-20} | {\"Address\",-10} | {\"Inferred Data Type\"}\");\n    Console.WriteLine(new string('-', 60));\n\n    foreach (var entry in columnMap)\n    {\n        Console.WriteLine($\"{entry.Key,-20} | {entry.Value.Address,-10} | {entry.Value.DataType}\");\n    }\n}\ncatch (Exception ex)\n{\n    Console.WriteLine($\"\\nAn error occurred while reading the Excel file: {ex.Message}\");\n}\nfinally\n{\n    // Clean up\n    if (File.Exists(filePath))\n    {\n        File.Delete(filePath);\n    }\n}\n\n\n/// \n/// Reads the headers of an Excel sheet and returns a map of column names to their metadata,\n/// including the column address and inferred data type.\n/// \n/// The full path to the Excel file.\n/// The name of the worksheet to read. If null, the first sheet is used.\n/// A dictionary mapping column names to an ExcelColumnInfo object.\npublic static Dictionary GetExcelColumnInfo(string filePath, string? sheetName = null)\n{\n    if (!File.Exists(filePath))\n    {\n        throw new FileNotFoundException(\"The specified Excel file was not found.\", filePath);\n    }\n\n    using var workbook = new XLWorkbook(filePath);\n\n    var worksheet = string.IsNullOrEmpty(sheetName)\n        ? workbook.Worksheets.FirstOrDefault()\n        : workbook.Worksheet(sheetName);\n\n    if (worksheet == null || worksheet.LastCellUsed() == null)\n    {\n        throw new InvalidOperationException(\"The worksheet is empty or could not be found.\");\n    }\n    \n    var headerRow = worksheet.FirstRowUsed();\n    if (headerRow == null)\n    {\n        return []; // Return an empty dictionary\n    }\n\n    // The dictionary now stores our new record type.\n    var columnMap = new Dictionary();\n\n    foreach (var headerCell in headerRow.CellsUsed())\n    {\n        string columnName = headerCell.GetString();\n        \n        // * GET THE COLUMN ADDRESS HERE *\n        // The Address property of a cell has a ColumnLetter property.\n        string columnAddress = headerCell.Address.ColumnLetter;\n        \n        // Infer the data type from the cell below the header.\n        var firstDataCell = headerCell.CellBelow();\n        XLDataType columnType = firstDataCell.DataType;\n\n        // Create the new info object and add it to the map.\n        var info = new ExcelColumnInfo(columnType, columnAddress);\n        columnMap.TryAdd(columnName, info);\n    }\n\n    return columnMap;\n}\n\n/// \n/// Helper function to create a sample Excel file (unchanged).\n/// \npublic static void CreateSampleExcelFile(string filePath)\n{\n    using var workbook = new XLWorkbook();\n    var worksheet = workbook.Worksheets.Add(\"SalesData\");\n    worksheet.Cell(\"A1\").Value = \"OrderID\";\n    worksheet.Cell(\"B1\").Value = \"OrderDate\";\n    worksheet.Cell(\"C1\").Value = \"CustomerName\";\n    worksheet.Cell(\"D1\").Value = \"Quantity\";\n    worksheet.Cell(\"E1\").Value = \"IsShipped\";\n    worksheet.Cell(\"F1\").Value = \"Notes\";\n    worksheet.Cell(\"A2\").Value = 1001;\n    worksheet.Cell(\"B2\").Value = new DateTime(2023, 10, 26);\n    worksheet.Cell(\"C2\").Value = \"John Smith\";\n    worksheet.Cell(\"D2\").Value = 5;\n    worksheet.Cell(\"E2\").Value = true;\n    worksheet.Columns().AdjustToContents();\n    workbook.SaveAs(filePath);\n}\n\nKey Changes and How They Work\n\npublic record ExcelColumnInfo(...): We introduced this record to act as a clean data container for our results. It's more descriptive and type-safe than using a Tuple.\n\nFunction Signature Change: The return type of the function was changed to Dictionary to reflect the new, richer data being returned. The function was also renamed to GetExcelColumnInfo to better describe its purpose.\n\nGetting the Column Address: The core of the new logic is this line:\n    string columnAddress = headerCell.Address.ColumnLetter;\n    Every IXLCell object (like our headerCell) has an Address property.\n    This Address property is an IXLAddress object, which contains detailed location information.\n    The ColumnLetter property of the IXLAddress gives us exactly what we need: the string representation of the column (\"A\", \"B\", ..., \"Z\", \"AA\", etc.).\n\nPopulating the Dictionary: Instead of adding the XLDataType directly, we now create an instance of our new record and add that to the dictionary:\n    var info = new ExcelColumnInfo(columnType, columnAddress);\n    columnMap.TryAdd(columnName, info);\n\nExpected Output\n\nThe program's output will now include the column address, formatted nicely:\n\n--- ClosedXML Column Info Reader ---\nSuccessfully created sample file: SalesData.xlsx\n\nDiscovered Columns and Their Metadata:\nColumn Name          | Address    | Inferred Data Type\nOrderID              | A          | Number\nOrderDate            | B          | DateTime\nCustomerName         | C          | Text\nQuantity             | D          | Number\nIsShipped            | E          | Boolean\nNotes                | F          | Blank\n\nQuestion/Directive\n\nTake the columnMap dictionary that GetExcelColumnInfo produced:  \n\nDictionary columnMap\nand create a c# data model class that looks like this:\n\npublic class LakeB2B\n{\n    [ExcelInfo(\"A\", \"Counter\")]\n    public int? Counter { get; set; }\n\n    [ExcelInfo(\"B\", \"Tag\")]\n    public string? Tag { get; set; }\n\n    [ExcelInfo(\"C\", \"Email\")] \n    public string? Email { get; set; }\n\n    [ExcelInfo(\"D\", \"Company Name\")]\n    public string? CompanyName { get; set; }\n\nAnswer \n\nThis is a fantastic and highly practical request that combines everything we've built. We will write a C# code generator!\n\nThe process will be:\nDefine a function that takes the Dictionary and generates the C# class as a string.\nCreate helper methods to format Excel column names into valid C# property names and to map Excel data types to C# data types.\nWrite the final string to a .cs file.\n\nFor maximum utility, the generated file will also include the ExcelInfoAttribute class definition, making it a self-contained, compilable file.\n\nThe Complete C# Code Generation Program\n\nHere is the full program. It uses the GetExcelColumnInfo function from our previous step and then calls a new GenerateCSharpClassFile function to complete the task.\n\nusing ClosedXML.Excel;\nusing System.IO;\nusing System.Text;\nusing System.Text.RegularExpressions;\n\n// --- Data Structures from Previous Steps ---\n\n/// \n/// Holds metadata about a specific column in an Excel sheet.\n/// \npublic record ExcelColumnInfo(XLDataType DataType, string Address);\n\n// --- Main Program Logic (C# 12 Top-level statements) ---\n\nConsole.WriteLine(\"--- C# Model Generator from Excel ---\");\n\nstring excelFilePath = \"SalesData.xlsx\";\nstring outputModelPath = \"LakeB2B.cs\";\n\n// 1. Create a sample Excel file to act as our source schema.\nCreateSampleExcelFile(excelFilePath);\nConsole.WriteLine($\"Step 1: Successfully created sample Excel file: {excelFilePath}\");\n\n// 2. Read the schema from the Excel file into our dictionary.\nDictionary columnMap = GetExcelColumnInfo(excelFilePath);\nConsole.WriteLine(\"Step 2: Successfully read column schema from Excel.\");\n\n// 3. Generate the C# class file from the schema.\ntry\n{\n    GenerateCSharpClassFile(columnMap, \"LakeB2B\", outputModelPath);\n    Console.WriteLine($\"Step 3: Success! Generated C# model file at: {Path.GetFullPath(outputModelPath)}\");\n    Console.WriteLine(\"\\n--- Generated File Content ---\");\n    Console.WriteLine(File.ReadAllText(outputModelPath));\n    Console.WriteLine(\"--------------------------\");\n}\ncatch (Exception ex)\n{\n    Console.WriteLine($\"An error occurred during code generation: {ex.Message}\");\n}\nfinally\n{\n    // Clean up the created files.\n    if (File.Exists(excelFilePath)) File.Delete(excelFilePath);\n    if (File.Exists(outputModelPath)) File.Delete(outputModelPath);\n}\n\n\n//================================================================//\n// THE NEW CODE GENERATION LOGIC\n//================================================================//\n\n/// \n/// Generates a C# class file from a map of Excel column information.\n/// \n/// The dictionary mapping column names to their metadata.\n/// The name for the generated C# class.\n/// The path where the .cs file will be saved.\npublic static void GenerateCSharpClassFile(Dictionary columnMap, string className, string outputFilePath)\n{\n    // StringBuilder is highly efficient for building strings piece by piece.\n    var sb = new StringBuilder();\n\n    // --- File Header ---\n    sb.AppendLine(\"using System;\");\n    sb.AppendLine();\n    sb.AppendLine(\"namespace Models\");\n    sb.AppendLine(\"{\");\n\n    // --- Attribute Definition (to make the file self-contained) ---\n    sb.AppendLine(\"    /// \");\n    sb.AppendLine(\"    /// Specifies the Excel column details for a property.\");\n    sb.AppendLine(\"    /// \");\n    sb.AppendLine(\"    [AttributeUsage(AttributeTargets.Property)]\");\n    sb.AppendLine(\"    public sealed class ExcelInfoAttribute : Attribute\");\n    sb.AppendLine(\"    {\");\n    sb.AppendLine(\"        public string ColumnId { get; }\");\n    sb.AppendLine(\"        public string ColumnName { get; }\");\n    sb.AppendLine(\"        public ExcelInfoAttribute(string columnId, string columnName) { ColumnId = columnId; ColumnName = columnName; }\");\n    sb.AppendLine(\"    }\");\n    sb.AppendLine();\n\n    // --- Class Definition ---\n    sb.AppendLine($\"    public class {className}\");\n    sb.AppendLine(\"    {\");\n\n    // --- Properties ---\n    foreach (var entry in columnMap)\n    {\n        string excelColumnName = entry.Key;\n        ExcelColumnInfo info = entry.Value;\n\n        string propertyName = FormatToPropertyName(excelColumnName);\n        string csharpType = MapExcelTypeToCSharpType(info.DataType);\n\n        // [ExcelInfo(\"D\", \"Company Name\")]\n        sb.AppendLine($\"        [ExcelInfo(\\\"{info.Address}\\\", \\\"{excelColumnName}\\\")]\");\n        // public string? CompanyName { get; set; }\n        sb.AppendLine($\"        public {csharpType} {propertyName} {{ get; set; }}\");\n        sb.AppendLine(); // Add a blank line for readability\n    }\n\n    // --- Closing Braces ---\n    sb.AppendLine(\"    }\");\n    sb.AppendLine(\"}\");\n\n    // Write the final string to the specified file.\n    File.WriteAllText(outputFilePath, sb.ToString());\n}\n\n/// \n/// Formats an Excel column name into a valid C# property name.\n/// (e.g., \"Company Name\" -> \"CompanyName\", \"Order-ID\" -> \"OrderID\").\n/// \npublic static string FormatToPropertyName(string excelColumnName)\n{\n    if (string.IsNullOrWhiteSpace(excelColumnName))\n        return \"UnnamedProperty\";\n\n    // Remove invalid characters using a regular expression.\n    // This keeps letters, numbers, and underscores.\n    string sanitized = Regex.Replace(excelColumnName, @\"\", \"\");\n    \n    // Ensure the property name doesn't start with a number.\n    if (char.IsDigit(sanitized[0]))\n    {\n        sanitized = \"_\" + sanitized;\n    }\n    \n    return sanitized;\n}\n\n/// \n/// Maps a ClosedXML XLDataType to a C# type string.\n/// Note: All types are made nullable, as any cell in Excel can be empty.\n/// \npublic static string MapExcelTypeToCSharpType(XLDataType excelType)\n{\n    // C# 8.0 switch expression provides a very clean syntax for this mapping.\n    return excelType switch\n    {\n        XLDataType.Number   => \"double?\",  // double is the safest type for Excel numbers.\n        XLDataType.DateTime => \"DateTime?\",\n        XLDataType.Boolean  => \"bool?\",\n        XLDataType.TimeSpan => \"TimeSpan?\",\n        XLDataType.Text     => \"string?\",\n        XLDataType.Blank    => \"string?\",  // Treat blank columns as string properties.\n        _                   => \"object?\"   // Fallback for unknown types.\n    };\n}\n\n\n//================================================================//\n// HELPER FUNCTIONS FROM PREVIOUS STEPS (slightly modified)\n//================================================================//\npublic static Dictionary GetExcelColumnInfo(string filePath, string? sheetName = null)\n{\n    using var workbook = new XLWorkbook(filePath);\n    var worksheet = string.IsNullOrEmpty(sheetName) ? workbook.Worksheets.First() : workbook.Worksheet(sheetName);\n    var headerRow = worksheet.FirstRowUsed();\n    if (headerRow == null) return [];\n\n    var columnMap = new Dictionary();\n    foreach (var headerCell in headerRow.CellsUsed())\n    {\n        string columnName = headerCell.GetString();\n        string columnAddress = headerCell.Address.ColumnLetter;\n        XLDataType columnType = headerCell.CellBelow().DataType;\n        var info = new ExcelColumnInfo(columnType, columnAddress);\n        columnMap.TryAdd(columnName, info);\n    }\n    return columnMap;\n}\n\npublic static void CreateSampleExcelFile(string filePath)\n{\n    using var workbook = new XLWorkbook();\n    var worksheet = workbook.Worksheets.Add(\"SalesData\");\n    worksheet.Cell(\"A1\").Value = \"Order ID\";\n    worksheet.Cell(\"B1\").Value = \"Order Date\";\n    worksheet.Cell(\"C1\").Value = \"Customer Name\";\n    worksheet.Cell(\"D1\").Value = \"Is Shipped\";\n    worksheet.Cell(\"A2\").Value = 1001;\n    worksheet.Cell(\"B2\").Value = new DateTime(2023, 10, 26);\n    worksheet.Cell(\"C2\").Value = \"John Smith\";\n    worksheet.Cell(\"D2\").Value = true;\n    workbook.SaveAs(filePath);\n}\n\nHow the Magic Happens\n\nGenerateCSharpClassFile: This is the orchestrator. It uses a StringBuilder which is the correct tool for building strings in loops, as it avoids creating many intermediate, temporary string objects.\nFormatToPropertyName: This helper is crucial for ensuring the output is valid C#. It takes a string like \"Order ID\" and uses a Regular Expression (Regex) to strip out any character that isn't a letter, number, or underscore, resulting in \"OrderID\". It also prefixes with an underscore if the name happens to start with a number.\nMapExcelTypeToCSharpType: This function is the \"translator.\" It uses a modern C# 8 switch expression for a clean, readable way to map the XLDataType enum from ClosedXML to the appropriate nullable C# type string (e.g., XLDataType.Number becomes \"double?\"). We make all types nullable (?) because any cell in Excel can be empty, which translates naturally to null in C#.\nString Building: Inside the main loop, the code constructs the two lines for each propertythe [ExcelInfo(...)] attribute and the public type Name { get; set; } declarationand appends them to the StringBuilder with proper indentation.\nFile Output: Finally, File.WriteAllText() takes the completed string from the StringBuilder and writes it to the LakeB2B.cs file.\n\nFinal Generated File (LakeB2B.cs)\n\nWhen you run the program, it will create a file named LakeB2B.cs with the following beautiful, clean, and ready-to-use content:\n\nusing System;\n\nnamespace Models\n{\n    /// \n    /// Specifies the Excel column details for a property.\n    /// \n    [AttributeUsage(AttributeTargets.Property)]\n    public sealed class ExcelInfoAttribute : Attribute\n    {\n        public string ColumnId { get; }\n        public string ColumnName { get; }\n        public ExcelInfoAttribute(string columnId, string columnName) { ColumnId = columnId; ColumnName = columnName; }\n    }\n\n    public class LakeB2B\n    {\n        [ExcelInfo(\"A\", \"Order ID\")]\n        public double? OrderID { get; set; }\n\n        [ExcelInfo(\"B\", \"Order Date\")]\n        public DateTime? OrderDate { get; set; }\n\n        [ExcelInfo(\"C\", \"Customer Name\")]\n        public string? CustomerName { get; set; }\n\n        [ExcelInfo(\"D\", \"Is Shipped\")]\n        public bool? IsShipped { get; set; }\n\n    }\n}\n\nQuestion/Directive\n\nCreate an SQL Insert Into statement from the columnMap value created with GetExcelColumnInfo().\n\nAnswer \n\nAbsolutely. This is a very common requirement: taking a schema (in this case, from Excel columns) and generating a parameterized SQL INSERT statement. This is crucial for safely inserting data into a database.\n\nWe will create a function GenerateSqlInsertStatement that takes the columnMap and a table name, and then uses the logic from our previous step to generate valid parameter names.\n\nThe Complete C# Program\n\nThis program includes the necessary helpers from our previous work to make it a fully runnable and testable example. The new core logic is in the GenerateSqlInsertStatement function.\n\nusing ClosedXML.Excel;\nusing System.IO;\nusing System.Text.RegularExpressions;\n\n// --- Data Structures & Helpers from Previous Steps ---\n\npublic record ExcelColumnInfo(XLDataType DataType, string Address);\n\n// --- Main Program Logic (C# 12 Top-level statements) ---\n\nConsole.WriteLine(\"--- SQL INSERT Statement Generator ---\");\n\nstring excelFilePath = \"SalesDataForSql.xlsx\";\nstring tableName = \"LakeB2B\";\n\n// 1. Create a sample Excel file.\nCreateSampleExcelFile(excelFilePath);\nConsole.WriteLine($\"Step 1: Successfully created sample Excel file: {excelFilePath}\");\n\n// 2. Read the schema from the Excel file into our dictionary.\nDictionary columnMap = GetExcelColumnInfo(excelFilePath);\nConsole.WriteLine(\"Step 2: Successfully read column schema from Excel.\");\n\n// 3. Generate the SQL INSERT statement.\nstring insertQuery = GenerateSqlInsertStatement(tableName, columnMap);\n\nConsole.WriteLine($\"\\nStep 3: Generated SQL INSERT statement for table '{tableName}':\");\nConsole.WriteLine(\"------------------------------------------------------------------\");\nConsole.WriteLine(insertQuery);\nConsole.WriteLine(\"------------------------------------------------------------------\");\n\n// Clean up the created file.\nif (File.Exists(excelFilePath))\n{\n    File.Delete(excelFilePath);\n}\n\n\n//================================================================//\n// THE NEW SQL GENERATION FUNCTION\n//================================================================//\n\n/// \n/// Generates a parameterized SQL INSERT statement from a map of column names.\n/// \n/// The name of the database table.\n/// A dictionary where keys are the column names.\n/// A formatted, parameterized SQL INSERT statement string.\npublic static string GenerateSqlInsertStatement(string tableName, Dictionary columnMap)\n{\n    // Handle edge cases where the map is empty or null.\n    if (columnMap == null || columnMap.Count == 0)\n    {\n        return $\"-- No columns provided to generate INSERT statement for table '{tableName}'.\";\n    }\n\n    // Use LINQ to create the two lists needed for the query.\n    \n    // 1. Create the list of column names, wrapped in square brackets\n    //    e.g., \"[Company Name]\"\n    var columnNames = columnMap.Keys.Select(name => $\"[{name}]\");\n    \n    // 2. Create the list of parameter names, formatted and prefixed with @\n    //    e.g., \"@CompanyName\"\n    var parameterNames = columnMap.Keys.Select(name => $\"@{FormatToPropertyName(name)}\");\n\n    // Define the indentation for clean formatting.\n    const string indent = \"                    \";\n\n    // Join the collections into comma-separated strings with proper newlines and indentation.\n    string columnList = string.Join($\",\\n{indent}\", columnNames);\n    string parameterList = string.Join($\",\\n{indent}\", parameterNames);\n\n    // Use a C# verbatim interpolated string ($@) to build the final query.\n    string insertQuery = $@\"\n                INSERT INTO [dbo].[{tableName}] (\n                    {columnList}\n                )\n                VALUES (\n                    {parameterList}\n                )\";\n\n    return insertQuery;\n}\n\n/// \n/// Formats a string into a valid C# property name, which is also a valid SQL parameter name (without the @).\n/// (e.g., \"Company Name\" -> \"CompanyName\", \"Order-ID\" -> \"OrderID\").\n/// \npublic static string FormatToPropertyName(string inputName)\n{\n    if (string.IsNullOrWhiteSpace(inputName))\n        return \"UnnamedParameter\";\n\n    // Removes any character that is not a letter, number, or underscore.\n    string sanitized = Regex.Replace(inputName, @\"\", \"\");\n\n    // Ensure it doesn't start with a number.\n    if (char.IsDigit(sanitized[0]))\n    {\n        sanitized = \"_\" + sanitized;\n    }\n\n    return sanitized;\n}\n\n\n//================================================================//\n// HELPER FUNCTIONS FROM PREVIOUS STEPS (Unchanged)\n//================================================================//\npublic static Dictionary GetExcelColumnInfo(string filePath, string? sheetName = null)\n{\n    using var workbook = new XLWorkbook(filePath);\n    var worksheet = string.IsNullOrEmpty(sheetName) ? workbook.Worksheets.First() : workbook.Worksheet(sheetName);\n    var headerRow = worksheet.FirstRowUsed();\n    if (headerRow == null) return [];\n\n    var columnMap = new Dictionary();\n    foreach (var headerCell in headerRow.CellsUsed())\n    {\n        string columnName = headerCell.GetString();\n        string columnAddress = headerCell.Address.ColumnLetter;\n        XLDataType columnType = headerCell.CellBelow().DataType;\n        var info = new ExcelColumnInfo(columnType, columnAddress);\n        columnMap.TryAdd(columnName, info);\n    }\n    return columnMap;\n}\n\npublic static void CreateSampleExcelFile(string filePath)\n{\n    using var workbook = new XLWorkbook();\n    var worksheet = workbook.Worksheets.Add(\"SalesData\");\n    worksheet.Cell(\"A1\").Value = \"Counter\";\n    worksheet.Cell(\"B1\").Value = \"Tag\";\n    worksheet.Cell(\"C1\").Value = \"Email\";\n    worksheet.Cell(\"D1\").Value = \"Company Name\";\n    worksheet.Cell(\"E1\").Value = \"Web Address\";\n    worksheet.Cell(\"A2\").Value = 1;\n    worksheet.Cell(\"B2\").Value = \"TagA\";\n    worksheet.Cell(\"C2\").Value = \"test@example.com\";\n    worksheet.Cell(\"D2\").Value = \"ACME Corp\";\n    worksheet.Cell(\"E2\").Value = \"acme.com\";\n    workbook.SaveAs(filePath);\n}\n\nHow It Works\n\nInput: The function takes the tableName and the columnMap dictionary. It only needs the Keys from the dictionary (the Excel column names).\n\nLINQ Select: The power of LINQ is used to transform the collection of column name strings into the two required lists:\n    columnMap.Keys.Select(name => $\"[{name}]\"): This iterates through each column name (e.g., \"Company Name\") and wraps it in square brackets [...] to create a list of valid SQL column identifiers.\n    columnMap.Keys.Select(name => $\"@{FormatToPropertyName(name)}\"): This iterates through each name, first sanitizes it using our existing FormatToPropertyName helper (e.g., \"Company Name\" becomes \"CompanyName\"), and then prepends an @ to create a valid SQL parameter name (e.g., @CompanyName).\n\nstring.Join: This method is used to assemble the lists into single strings. The separator \",\\n                    \" is cleverly designed to place a comma, a newline, and the correct indentation between each item, ensuring perfect formatting.\n\nVerbatim Interpolated String ($@\"...\"): This C# feature is the final piece. It allows us to create a multi-line string that preserves whitespace (the @ part) and to embed our generated columnList and parameterList strings directly into the template (the $ part).\n\nExpected Output\n\nWhen you run this program, it will produce the following perfectly formatted and secure SQL INSERT statement:\n\n--- SQL INSERT Statement Generator ---\nStep 1: Successfully created sample Excel file: SalesDataForSql.xlsx\nStep 2: Successfully read column schema from Excel.\n\nStep 3: Generated SQL INSERT statement for table 'LakeB2B':\n                INSERT INTO [dbo].[LakeB2B] (\n                    [Counter],\n                    [Tag],\n                    [Email],\n                    [Company Name],\n                    [Web Address]\n                )\n                VALUES (\n                    @Counter,\n                    @Tag,\n                    @Email,\n                    @CompanyName,\n                    @WebAddress\n                )",
        "locale": "en",
        "slug": "technical-posts/cs-model-for-excel",
        "folder": ""
    },
    {
        "title": "lakeb2b-october-2025-list-update",
        "description": "lakeb2b-october-2025-list-update",
        "date_created": "2025-10-20T00:00:00.000Z",
        "date_updated": "2025-11-19T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "lakeb2b"
        ],
        "content": "Rows without emails from original list\n\nThere are 3500 rows in the Prospects (AKA Original) table that haven't been sent any emails. \n\n-- This SQL counts the rows in the original LakeB2B list that haven't yet been sent emails. \nSELECT\n    count(*)\nFROM\n    comparo AS B\nLEFT JOIN\n    original AS A ON B.email = A.email\nWHERE\n    A.email IS NULL AND b.country = 'USA' and b.tag is not null;\nThere are two tables in the lakeb2b db:\n\noriginal - the complete, original lakeb2b table\ncomparo - the complete, updated lakeb2b table\n\n[!info]\nThe comparo table has a region column that the original table did not have. That column has been added to the original table -- but its values are all empty. I added it so I could use the same import facility. \n\n\nThis query selects rows in Original not present in Comparo\n\nSELECT\n    FROM\n    original AS B\nLEFT JOIN\n    comparo AS A ON B.email = A.email\nWHERE\n    A.email IS NULL AND b.country = 'USA';\n\nBut this is better query because it is easier to read and clearly signals \"I am filtering data,\" whereas a JOIN signals \"I am combining data.\" Also, the LEFT JOIN query returns all columns from both tables. The query below returns only rows in original that don't exist in comparo\n\nSELECT * \nFROM [dbo].[original] AS o\nWHERE NOT EXISTS (\n    SELECT 1 \n    FROM [dbo].[comparo] AS c \n    WHERE c.Email = o.Email\n) AND o.Country = 'USA'\n\n\n\nWITH RowsToUpdate AS (\n    SELECT TOP 1000 comparo.email \n    FROM comparo\n    LEFT JOIN original ON comparo.email = original.email\n    WHERE\n        original.email IS NULL\n        AND comparo.country = 'USA'\n        AND comparo.tag IS NULL\n    ORDER BY comparo.email -- Ensure deterministic selection of TOP 1000\n)\nUPDATE comparo\nSET comparo.tag = 'lakeb2b-2025-11-13' \nFROM comparo\nINNER JOIN RowsToUpdate AS RTU ON comparo.email = RTU.email\n\nThe query above selects 1000 rows in comparo where country = 'USA' and tag is NULL, that do not exist in orginal and sets their tag column with the new tag value.\n\nDefines a Common Table Expression (CTE) namedRowsToUpdate:\n    \n    This CTE selects theTOP 1000email addressesfrom thecomparotable.        \n    It filters these email addresses based on several conditions:        \n        LEFT JOIN original ON comparo.email = original.email: It joinscomparowithoriginalusing theemailcolumn.            \n        WHERE original.email IS NULL: This is the crucial condition indicating that the email address fromcomparodoes not existin theoriginaltable. In other words, these are emails found incomparobut not inoriginal. \n        AND comparo.country = 'USA': Further filters to only include records where thecountryincomparois 'USA'.\n        AND comparo.tag IS NULL: Further filters to only include records where thetagcolumn incomparois currentlyNULL.\n    ORDER BY comparo.email: Ensures that if there are more than 1000 matching rows, the specific 1000 chosen byTOPare consistently the ones with the lowest (alphabetically) email addresses.\n        \nUpdates thecomparotable:    \n    It takes thecomparotable and performs anINNER JOINwith theRowsToUpdateCTE.        \n    The join condition isON comparo.email = RTU.email, meaning it matches rows incomparowhere theemailaddress is one of the 1000 selected in the CTE.        \n    For all these matchedcomparorows, it sets thecomparo.tagcolumn to the specific string value'lakeb2b-2025-11-13'.\n        \nIn simpler terms, this query identifies the top 1000 records from thecomparotable (ordered by email) that meet these criteria:\nThey have an email address that does not exist in theoriginaltable.    \nTheircountryis 'USA'.    \nTheirtagcolumn is currentlyNULL.\n    \nAnd then, for those specific 1000 records, it updates theirtagcolumn to'lakeb2b-2025-11-13'.",
        "locale": "en",
        "slug": "technical-posts/lakeb2b-october-2025-list-update",
        "folder": ""
    },
    {
        "title": "Custom JavaScript console",
        "description": "Custom JavaScript console",
        "date_created": "2025-07-14T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "function customLog(...args) {\n    const err = new Error();\n    const stack = err.stack.split(\"\\n\");\n    // The first line is 'Error', the second is the current function, so we take the third.\n    let callerInfo = stack[2].trim();\n    console.log([${callerInfo}] + \"\\r\\n -->\", ...args);\n}",
        "locale": "en",
        "slug": "technical-posts/custom-js-console",
        "folder": ""
    },
    {
        "title": "JavaScript spread syntax; spread operator",
        "description": "JavaScript spread syntax; spread operator",
        "date_created": "2025-07-17T00:00:00.000Z",
        "date_updated": "2025-11-07T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "Of course! The three dots (...) in that JavaScript code are using the Object Spread Syntax.\n\nIn simple terms, it's a shortcut for copying all the properties from an existing object into a new one.\n\nLet's break down your specific example piece by piece.\n\nconfig = {\n    // 1. First spread ...config\n    ...config,\n\n    // 2. A new or updated property\n    [instanceId]: {\n        componentType,\n\n        // 3. Second spread ...props\n        ...props\n    }\n};\n\n1. ...config (The Outer Spread)\n\nThis takes the existing config object and \"spreads out\" all of its key-value pairs into the new object we are creating. It's essentially making a copy of the original config.\n\nWhy do this? This pattern is a cornerstone of immutability. Instead of changing (mutating) the original config object directly, you create a brand new object that contains all the old data plus your new changes. This is crucial in frameworks like React for predictable state updates.\n\n2. [instanceId]: { ... } (The New/Updated Property)\n\nThis part adds a new property to the object or updates an existing one.\n\n[instanceId] is a computed property name. It means the key for this property is not literally \"instanceId\", but the value stored in the instanceId variable. For example, if instanceId is 'my-modal-1', the key will be 'my-modal-1'.\nBecause this line comes after ...config, if a key with the same value as instanceId already existed in the original config, it will be overwritten by this new definition. This is how you update a specific entry.\n\n3. ...props (The Inner Spread)\n\nThis does the exact same thing as the first spread, but for the inner object. It takes all the key-value pairs from an object called props and copies them into the new object being created for the [instanceId] key.\n\nA Concrete Example\n\nLet's see it in action. Imagine this is your starting state:\n\n// The original object\nlet config = {\n    'header-main': { componentType: 'Header', title: 'My App' },\n    'footer-123': { componentType: 'Footer', text: ' 2023' }\n};\n\n// The new data you want to add/update\nconst instanceId = 'modal-confirm';\nconst componentType = 'Modal';\nconst props = {\n    title: 'Are you sure?',\n    confirmText: 'Yes, proceed'\n};\n\nNow, let's run your code:\n\nconfig = {\n    ...config, // 1. Copy existing properties\n    [instanceId]: { // 2. Add a new key: 'modal-confirm'\n        componentType, // 3. Add componentType: 'Modal'\n        ...props // 4. Copy properties from props\n    }\n};\n\nHere is the step-by-step creation of the new config object:\n\nStart with ...config: The new object becomes:\n    {\n      \"header-main\": { \"componentType\": \"Header\", \"title\": \"My App\" },\n      \"footer-123\": { \"componentType\": \"Footer\", \"text\": \" 2023\" }\n    }\nAdd [instanceId]: The new key is 'modal-confirm'. Its value is a new object.\nBuild the inner object:\n    It starts with componentType, which is 'Modal'.\n    Then, ...props copies title: 'Are you sure?' and confirmText: 'Yes, proceed'.\n    The final inner object is { componentType: 'Modal', title: 'Are you sure?', confirmText: 'Yes, proceed' }.\n\nThe final result assigned to config is:\n\n// The new config object\n{\n    // Copied from the original\n    'header-main': { componentType: 'Header', title: 'My App' },\n    'footer-123': { componentType: 'Footer', text: ' 2023' },\n\n    // The new entry\n    'modal-confirm': {\n        componentType: 'Modal',\n        title: 'Are you sure?',\n        confirmText: 'Yes, proceed'\n    }\n}\n\nSummary\n\nThe three dots (...) are the Object Spread Syntax, used here to:\n\nCreate an immutable copy of the config object.\nAdd or update a specific property within that copy.\nMerge properties from another object (props) into a nested object.\n\nIt's a modern, concise, and highly readable way to manipulate objects in JavaScript.",
        "locale": "en",
        "slug": "technical-posts/javascript-spread-syntax",
        "folder": ""
    },
    {
        "title": "Docker file tutorial",
        "description": "Docker file tutorial",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "containers"
        ],
        "content": "What is a Dockerfile?\n\nA Dockerfile is a text document that contains all the commands a user could call on the command line to assemble a Docker image. Images are like blueprints or templates for creating Docker containers. Dockerfiles automate the process of image creation, making it repeatable, versionable, and easy to share.\n\nThink of it this way:\n\nDockerfile: The recipe to bake a cake.\nDocker Image: The actual cake, ready to be sliced.\nDocker Container: A slice of the cake being served (a running instance of the image).\n\nPrerequisites:\n\nDocker Engine installed (includes the docker build command).\n\nThe Dockerfile (File Name)\n\nBy convention, this file is simply named Dockerfile (with a capital 'D' and no extension) and resides in the root directory of your project or the specific component it builds.\n\nStructure of a Dockerfile:\n\nA Dockerfile consists of a series of instructions followed by arguments. Each instruction creates a new layer in the Docker image.\n\nINSTRUCTION argument\nINSTRUCTION argument argument\n...\n\nPrimary Instructions in a Dockerfile:\n\nHere are some of the most fundamental and commonly used instructions:\n\nFROM : (Required):\n\n    This must be the first instruction in a Dockerfile (except for optional ARGs).\n    Specifies the base image to build upon. Your image inherits all the layers from this base image.\n    Examples: FROM ubuntu:22.04, FROM python:3.11-slim, FROM node:18-alpine. Choose a base image that has the core tools you need (like an operating system or a language runtime).\n\nWORKDIR /path/to/workdir:\n\n    Sets the working directory for subsequent instructions like RUN, CMD, ENTRYPOINT, COPY, ADD.\n    If the directory doesn't exist, it will be created.\n    It's good practice to set a WORKDIR early on.\n    Example: WORKDIR /app\n\nCOPY  :\n\n    Copies files or directories from your build context (usually the directory containing the Dockerfile) into the image's filesystem at the specified `` path.\n    `` path is relative to the build context.\n    Example: COPY . . (copies everything from the build context into the current WORKDIR inside the image), COPY requirements.txt .\n\nRUN :\n\n    Executes commands in a new layer during the image build process.\n    Typically used to install software packages, update the OS, compile code, or set up the environment.\n    Each RUN instruction creates a new image layer. It's often good practice to chain related commands using && to reduce the number of layers.\n    Example: RUN apt-get update && apt-get install -y curl\n    Example: RUN pip install -r requirements.txt\n\nEXPOSE /:\n\n    Informs Docker that the container will listen on the specified network ports at runtime.\n    This instruction does not actually publish the port to the host machine. It serves as documentation and allows for easier linking between containers. Publishing is done using the -p flag with docker run or the ports section in Docker Compose.\n    Example: EXPOSE 80, EXPOSE 5432/tcp\n\nCMD [\"executable\", \"param1\", \"param2\"] (Exec Form - Preferred) or CMD command param1 param2 (Shell Form):\\\\\n\n    Provides defaults for an executing container. There can only be one CMD instruction in a Dockerfile. If you list more than one CMD, only the last one will take effect.\n    The primary purpose of CMD is to provide the default command to run when a container starts from the image.\n    If the user specifies arguments to docker run, they will override the default specified in CMD.\n    Example (Exec form): CMD [\"python\", \"app.py\"]\n    Example (Shell form): CMD python app.py\n\nENTRYPOINT [\"executable\", \"param1\", \"param2\"] (Exec Form - Preferred) or ENTRYPOINT command param1 param2 (Shell Form):\\\\\n    Configures a container that will run as an executable.\n    It's similar to CMD, but commands passed to docker run are appended as arguments to the ENTRYPOINT executable, rather than overriding it entirely.\n    Often used in combination with CMD to specify default arguments that can be overridden.\n    Example: ENTRYPOINT [\"/usr/sbin/nginx\", \"-g\", \"daemon off;\"]\n\nSimple Example: A Basic Python Flask App\n\nLet's create an image for a simple web application.\n\nCreate a directory for your project, e.g., my_docker_app.\nInside my_docker_app, create a simple Python file named app.py:\n\n    from flask import Flask\n    import os\n\n    app = Flask(name)\n\n    @app.route('/')\n    def hello():\n        return f\"Hello from inside a Docker container!\"\n\n    if name == \"main\":\nListen on all available interfaces within the container\n        app.run(host='0.0.0.0', port=5000)\n\nCreate a requirements.txt file in the same directory:\n    Flask==2.3.3 # Use a specific version if desired\nCreate a file named Dockerfile (no extension) in the same directory:\n\nStart with the official Python 3.11 slim base image\n    FROM python:3.11-slim\n\nSet the working directory inside the container\n    WORKDIR /app\n\nCopy the requirements file into the working directory\n    COPY requirements.txt .\n\nInstall the Python dependencies specified in requirements.txt\n--no-cache-dir reduces image size slightly\n--upgrade pip ensures we have a recent pip version\n    RUN pip install --no-cache-dir --upgrade pip && \\\n        pip install --no-cache-dir -r requirements.txt\n\nCopy the rest of the application code (app.py in this case)\ninto the working directory\n    COPY . .\n\nInform Docker that the container listens on port 5000\n    EXPOSE 5000\n\nDefine the default command to run when the container starts\nUse the exec form for CMD\n    CMD [\"python\", \"app.py\"]\n\nBuilding the Docker Image:\n\nOpen your terminal and navigate to the directory containing your Dockerfile and source code (my_docker_app).\nRun the build command:\n-t gives the image a name (tag) in the format name:tag\n. tells Docker to use the current directory as the build context\n    docker build -t my-python-app:latest .\n    You'll see Docker executing each step from your Dockerfile.\n\nRunning a Container from the Image:\n\nOnce the build is complete, run a container:\n-d runs the container in detached mode (background)\n-p maps port 8080 on your host to port 5000 inside the container\n--name gives the running container a specific name (optional)\nmy-python-app:latest is the image we just built\n    docker run -d -p 8080:5000 --name my-running-app my-python-app:latest\nVerify it's running: Open your web browser and go to http://localhost:8080. You should see \"Hello from inside a Docker container!\".\nCheck logs: docker logs my-running-app\nStop and remove the container:\n    docker stop my-running-app\n    docker rm my-running-app\n\nThat's the essence of creating a Dockerfile! It allows you to define a consistent, portable environment for your application by packaging it and its dependencies into a reusable image.",
        "locale": "en",
        "slug": "technical-posts/docker-file-tutorial",
        "folder": ""
    },
    {
        "title": "How the multi-locale feature is implemented at ASNA.com",
        "description": "How he multi-locale feature is implemxented at ASNA.com",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "asna-com"
        ],
        "content": "\nimport { redirect } from '@sveltejs/kit';\n\nJust stashing these notes here for now:\n\nGet locale from route in layout.server.js\n\nexport async function load({ route }) {\n\tconsole.log(route.id);\n\treturn {\n\t\tlocales: 'en'\n\t};\n}\n\nget local from +page.svelte or +layout.svelte\n\nimport { page } from '$app/stores';\nconst locale = $page.url.pathname.slice(1, 3);\n\nAll routes should specify a locale. A server hook ensures that the route '/' is redirected to /en\n\nThe hard-coded en will later be changed to a app constant.\n\nif (event.url.pathname == '/') {\n\tconsole.log('root requested');\n\tthrow redirect(302, '/en');\n}\n\nhooks.server.js\n\nNote that the server hook also sets the language for the page.\n\n// State 1 - request received.\nexport const handle = async ({ event, resolve }) => {\n\t// Stage 2 - do something with incoming request.\n\n\tconsole.log(event.url.pathname);\n\n\tif (event.url.pathname == '/') {\n\t\tconsole.log('root requested');\n\t\tthrow redirect(302, '/en');\n\t}\n\n\t// Stage 2\n\tconst locale = event.url.pathname.slice(1, 3) || 'en';\n\n\tconst response = await resolve(event, {\n\t\t// Swap out the %locale% value (that is present in ~/app.html)\n\t\ttransformPageChunk: ({ html }) => html.replace('%locale%', locale)\n\t});\n\n\t// Stage 3\n\treturn response;\n};\n\napp.html\n\nThe app.html file includes the following so that the lang value is swapped out to the current locale at runtime.\n\n\n\nImplementing multiple locales with Sveltekit is a bit of a challenge. The routes folder shows an example with a site for English and Spanish locals.\n\nLocale-specific posts\n\nPages versus posts -- pages are the grouped (mostly) static parts of the site grouped. Page groups includes products, services, etc and are presented on the main nav bar. Posts are the more dynamic parts of the site. Post groups are knowledge base, case studies, and newsletter. Access to these parts are from the main nav bar.  \nTodo: each of the post groups needs a paginated home page for that group.\n\nLocale-specific roots are specified with the +layout.svelte and +page.svelte at the roots of en and es. These files are language-specific versions for their respective languages.\n\nDirectly under the locale folder (en or es in this case) is a kb folder with dynamic routes specified (via the [slug] folder). kb is the \"Knowledge Base folder.\" There is no direct content in the [slug] folder. Rather, logic in the +page.js file tries to load the requested route from a markdown file in the content-kb folder.\n\n    kb\n           [slug]\n               +page.js\n               +page.svelte\n\nThe logic for fetching a markdown file looks at the locale specified in the url. If the url starts with en, then a markdown file with the file named of the current slug + '.en.md' is displayed. If the url starts with any locale except en, then if the corresponding markdown file for that locale exists it is displayed, otherwise the English version of the markdown file is displayed. These markdown files are not server-\n\nIn addition to the content-kb folder, there would probably also be content-nl and a content-cs folders. These folder work just like the content-kb folder, providing access to newsletter and case study content respectively. Think of the kb, nl, and cs folders are providing access to posts for the site.\n\nOther folders immediately off the locale root provide access to pages such as products or services. These pages are locale-specific Svelte components. (all pages should be server-side rendered). The full list of page sections (folders) are:\n\nmigration\nproducts\nservices\nsupport\ncompany\nprivacy\n\nThe +page.js and +page.js files\n\n.\n routes\n     en\n        kb\n           [slug]\n               +page.js\n               +page.svelte\n        +layout.svelte\n        +page.svelte\n     en\n         kb\n            [slug]\n                +page.js\n                +page.svelte\n         +layout.svelte\n         +page.svelte\n\nroutes\n  en\n    kb\n      [slug]\n        +page.js\n        +page.svelte\n    +layout.svelte\n    +page.svelte\n  en\n    kb\n      [slug]\n        +page.js\n        +page.svelte\n    +layout.svelte\n    +page.svelte\n\nroutes\n  en\n    kb\n      [slug]\n        +page.js\n        +page.svelte\n    +layout.svelte\n    +page.svelte\n  en\n    kb\n      [slug]\n        +page.js\n        +page.svelte\n    +layout.svelte\n    +page.svelte\n\nnew notes 2023-11-30\n\nThis is the code for any dynamic [slug] folder for fetching the markdown for a given slug. It gets the folder (ie, 'kb' or 'case-study') and the locale from the route value. Then it first tries to get the markdown for the locale specified. If that attempt fails, in then tries again with the default locale (which is 'en' in this example).\n\nThis provides locale-specific content for any dynamic route. If the content exists in the specific locale, that content is displayed. Otherwise, the content falls back to the content for the default locale.\n\nThis +page.js works for any folder and any locale.\n\n+page.js\n\nimport { error } from '@sveltejs/kit';\n\nconst DEFAULT_LOCALE = 'en';\n\nexport const load = async ({ url, route, params }) => {\n\t// route = { id: '/en/kb/[slug]' }\n\tconst folder = route.id.split('/')[2];\n\tconst locale = route.id.slice(1, 3);\n\n\ttry {\n\t\tconst post = await import(../../../markdown/${folder}/${params.slug}.${locale}.md);\n\t\treturn {\n\t\t\tcontent: post.default,\n\t\t\tmeta: post.metadata\n\t\t};\n\t} catch {\n\t\ttry {\n\t\t\tconst post = await import(../../../markdown/${folder}/${params.slug}.${DEFAULT_LOCALE}.md);\n\t\t\treturn {\n\t\t\t\tcontent: post.default,\n\t\t\t\tmeta: post.metadata\n\t\t\t};\n\t\t} catch {\n\t\t\tthrow error(404, Could not find);\n\t\t}\n\t}\n};",
        "locale": "en",
        "slug": "technical-posts/implementing-multiple-locales-at-asna-com",
        "folder": ""
    },
    {
        "title": "LakeB2B Constant Contact Import",
        "description": "Helpful SQL",
        "date_created": "2025-08-24T00:00:00.000Z",
        "date_updated": "2025-11-19T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "lakeb2b"
        ],
        "content": "The prospects and original table are the same data except for the tag column. The original's tag column is populated. In prospects it is empty. \n\nTarget table\n  \nUSE lakeb2b]\nGO\n\n/* Object:  Table [dbo].[june-12]    Script Date: 8/25/2025 2:07:46 PM */\nSET ANSI_NULLS ON\nGO\n\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[cc-group](\n\t[Id IDENTITY(1,1) NOT NULL,\n\tTag(200) NULL,\n\tEmail(200) NULL,\n\tCompanyName(200) NULL,\n\tFirstName(200) NULL,\n\tLastName(200) NULL,\n\tTitle(200) NULL,\nPRIMARY KEY CLUSTERED \n(\n\t[Id] ASC\n)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]\n) ON [PRIMARY]\nGO\n\nCount rows for all tags\n\nShow counts of all tags\n\nselect tag, count(tag) from prospects\ngroup by tag\norder by tag\n\nResults shown with query above:\n\nlakeb2b-2025-05-22\t  50\nlakeb2b-2025-06-05\t 518\nlakeb2b-2025-06-12\t 601\nlakeb2b-2025-06-26\t2000\nlakeb2b-2025-08-07\t1092\n                    4261\n\n\n\n\n\n\nselect ApplicationType, count(ApplicationType) from prospects \ngroup by ApplicationType\norder by ApplicationType\n\nHandy SQL \n\nselect tag, count(tag) from prospects\nwhere country = 'USA'\ngroup by tag\norder by tag",
        "locale": "en",
        "slug": "technical-posts/lakeb2b-constant-contact-import",
        "folder": ""
    },
    {
        "title": "dataview",
        "description": "dataview",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "obsidian"
        ],
        "content": "dataview\n\nTABLE WITHOUT ID (tag + \"(\" + length(rows.file.link) + \")\") AS Tags,\nSORT (rows.file.link) AS Files\nFROM \"\"\nWHERE file.tags\nFLATTEN file.tags AS tag\nGROUP BY tag\nSORT length(rows.file.link) DESC",
        "locale": "en",
        "slug": "technical-posts/dataview",
        "folder": ""
    },
    {
        "title": "CSS Pseudo classes",
        "description": "CSS Pseudo classes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Pseudo Classes\n\nAlways start with a single colon\nAlways reflect state (ie, :hover or :focus)\nAlways selects an entire element\nI think CSS nesting does support pseudo classes\n\nEach cell in this list is populated with the pseudo-classes from the provided list, organized in four columns.\n\n| Selector             | Example               | Example Description                                                                                  |\n| -------------------- | --------------------- | ---------------------------------------------------------------------------------------------------- |\n| :active              | a:active              | Selects the active link                                                                              |\n| :checked             | input:checked         | Selects every checked input element                                                                |\n| :disabled            | input:disabled        | Selects every disabled input element                                                               |\n| :empty               | p:empty               | Selects every p element that has no children                                                       |\n| :enabled             | input:enabled         | Selects every enabled input element                                                                |\n| :first-child         | p:first-child         | Selects every p elements that are the first child of its parent                                    |\n| :first-of-type       | p:first-of-type       | Selects every p element that is the first p element of its parent                                |\n| :focus               | input:focus           | Selects the input element that has focus                                                           |\n| :hover               | a:hover               | Selects links on mouse over                                                                          |\n| :in-range            | input:in-range        | Selects input elements with a value within a specified range                                       |\n| :invalid             | input:invalid         | Selects all input elements with an invalid value                                                   |\n| :lang(language)      | p:lang(it)            | Selects every p element with a lang attribute value starting with \"it\"                             |\n| :last-child          | p:last-child          | Selects every p elements that are the last child of its parent                                     |\n| :last-of-type        | p:last-of-type        | Selects every p element that is the last p element of its parent                                 |\n| :link                | a:link                | Selects all unvisited links                                                                          |\n| :not(selector)       | :not(p)               | Selects every element that is not a p element                                                      |\n| :nth-child(n)        | p:nth-child(2)        | Selects every p element that is the second child of its parent                                     |\n| :nth-last-child(n)   | p:nth-last-child(2)   | Selects every p element that is the second child of its parent, counting from the last child       |\n| :nth-last-of-type(n) | p:nth-last-of-type(2) | Selects every p element that is the second p element of its parent, counting from the last child |\n| :nth-of-type(n)      | p:nth-of-type(2)      | Selects every p element that is the second p element of its parent                               |\n| :only-of-type        | p:only-of-type        | Selects every p element that is the only p element of its parent                                 |\n| :only-child          | p:only-child          | Selects every p element that is the only child of its parent                                       |\n| :optional            | input:optional        | Selects input elements with no \"required\" attribute                                                |\n| :out-of-range        | input:out-of-range    | Selects input elements with a value outside a specified range                                      |\n| :read-only           | input:read-only       | Selects input elements with a \"readonly\" attribute specified                                       |\n| :read-write          | input:read-write      | Selects input elements with no \"readonly\" attribute                                                |\n| :required            | input:required        | Selects input elements with a \"required\" attribute specified                                       |\n| :root                | root                  | Selects the document's root element                                                                  |\n| :target              | #news:target        | Selects the current active #news element (clicked on a URL containing that anchor name)            |\n| :valid               | input:valid           | Selects all input elements with a valid value                                                      |\n| :visited             | a:visited             | Selects all visited links                                                                            |\n\nPseudo Elements\n\nAlways start with a double colon\n    For backwards compatibility, browsers do allow these pseudo elements ::before, ::after, ::first-line, and ::first-letter to use a single colon--that isn't a good practice.\nAlways selects a part of an element (ie, ::before or ::first-letter)\nCSS nesting does not support pseudo elements\n\nPseudo elements are:\n\nHere's a Markdown table of CSS pseudo-elements and their meanings:\n\n| Pseudo-Element                 | Meaning                                                                                 |\n| ------------------------------ | --------------------------------------------------------------------------------------- |\n| :active                      | Represents an element being activated                                                   |\n| ::after/:after               | Represents content inserted after an element                                            |\n| ::backdrop (experimental)    | Represents a backdrop layer (experimental)                                              |\n| ::before/:before             | Represents content inserted before an element                                           |\n| :checked                     | Represents a checked input element                                                      |\n| :default                     | Represents a default input element                                                      |\n| :dir (experimental)          | Represents the direction of text (experimental)                                         |\n| :disabled                    | Represents a disabled input element                                                     |\n| :empty                       | Represents an empty element                                                             |\n| :enabled                     | Represents an enabled input element                                                     |\n| :first-child                 | Represents the first child of its parent                                                |\n| ::first-letter/:first-letter | Represents the first letter of an element                                               |\n| ::first-line/:first-line     | Represents the first line of an element                                                 |\n| :first-of-type               | Represents the first element of its type                                                |\n| :focus                       | Represents an element with focus                                                        |\n| :fullscreen (experimental)   | Represents a fullscreen element (experimental)                                          |\n| :hover                       | Represents an element being hovered over                                                |\n| :in-range                    | Represents an input element with a value within a specified range                       |\n| :indeterminate               | Represents an input element in an indeterminate state                                   |\n| :invalid                     | Represents an input element with an invalid value                                       |\n| :lang                        | Represents an element with a specific language attribute                                |\n| :last-child                  | Represents the last child of its parent                                                 |\n| :last-of-type                | Represents the last element of its type                                                 |\n| :link                        | Represents an unvisited link                                                            |\n| :not                         | Represents an element that does not match a selector                                    |\n| :nth-child                   | Represents an element that is the nth child of its parent                               |\n| :nth-last-child              | Represents an element that is the nth child of its parent, counting from the last child |\n| :nth-last-of-type            | Represents an element that is the nth element of its type, counting from the last child |\n| :nth-of-type                 | Represents an element that is the nth element of its type                               |\n| :only-child                  | Represents an element that is the only child of its parent                              |\n| :only-of-type                | Represents an element that is the only element of its type                              |\n| :optional                    | Represents an input element with no \"required\" attribute                                |\n| :out-of-range                | Represents an input element with a value outside a specified range                      |\n| ::placeholder (experimental) | Represents placeholder text in an input element (experimental)                          |\n| :read-only                   | Represents an input element with a \"readonly\" attribute specified                       |\n| :read-write                  | Represents an input element with no \"readonly\" attribute                                |\n| :required                    | Represents an input element with a \"required\" attribute specified                       |\n| :root                        | Represents the document's root element                                                  |\n| ::selection                  | Represents the portion of an element selected by the user                               |\n| :scope (experimental)        | Represents the scope of a style (experimental)                                          |\n| :target                      | Represents an element targeted by a URL fragment identifier                             |\n| :valid                       | Represents an input element with a valid value                                          |\n| :visited                     | Represents a visited link                                                               |\n\nYou can copy and paste this Markdown table into your document.\n\nExamples\n\na[href*='https']::after {\n   font-family: \"Font Awesome 5 Free\";\n   font-weight: 800;\n   content: \"\\f08e\";\n   margin-left: .25rem;\n}",
        "locale": "en",
        "slug": "technical-posts/css-pseudo-classes-and-pseudo-elements",
        "folder": ""
    },
    {
        "title": "Handling API errors with Sveltekit",
        "description": "Handling API errors with Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "ricettaparmigianadipatate"
        ],
        "content": "The canonical way to handle errors in SvelteKit HTTP APIs (i.e., in your +server.js or +server.ts files) is to use the error helper from @sveltejs/kit.\n\nThis helper allows you to throw an error that SvelteKit will automatically convert into an appropriate HTTP response with the correct status code and a JSON body.\n\nHere's a breakdown:\n\nUsing the error Helper:\n\n    Import it: import { error } from '@sveltejs/kit';\n    Throw it: throw error(statusCode, messageOrObject);\n        statusCode: An HTTP status code (e.g., 400, 401, 403, 404, 500).\n        messageOrObject:\n            If a string is provided, the response body will be {\"message\": \"your string\"}.\n            If an object is provided, that object will be stringified and used as the JSON response body. This is useful for providing more detailed error information.\n\nHow it Works:\n    When you throw error(...) inside a GET, POST, PUT, DELETE, etc., handler in a +server.js file:\n    SvelteKit catches this specific type of error.\n    It stops further execution of the handler.\n    It constructs an HTTP Response object with:\n        The statusCode you provided.\n        A Content-Type of application/json.\n        A body generated from your messageOrObject.\n    This Response is then sent back to the client that made the API request.\n\nExamples:\n\nsrc/routes/api/items/+server.js\n\nimport { error, json } from \"@sveltejs/kit\";\n\nlet items = [\n    { id: 1, name: \"Gadget\" },\n    { id: 2, name: \"Widget\" },\n];\n\n// GET /api/items\nexport async function GET() {\n    return json(items);\n}\n\n// GET /api/items/[id] - This would be in src/routes/api/items/[id]/+server.js\n// But for simplicity, let's imagine it's part of a more complex GET handler here\n// or a POST handler that needs to find an item.\n\n// POST /api/items\nexport async function POST({ request }) {\n    try {\n        const newItemData = await request.json();\n\n        if (!newItemData.name || typeof newItemData.name !== \"string\") {\n            // 400 Bad Request - Invalid input\n            throw error(400, {\n                message: \"Item name is required and must be a string.\",\n            });\n        }\n\n        // Simulate authentication check\n        const isAuthenticated =\n            request.headers.get(\"Authorization\") === \"Bearer valid-token\";\n        if (!isAuthenticated) {\n            // 401 Unauthorized\n            throw error(401, \"Authentication required.\");\n        }\n\n        // Simulate trying to create an item that already exists (conceptually a 409 Conflict)\n        if (\n            items.find(\n                (item) =>\n                    item.name.toLowerCase() === newItemData.name.toLowerCase()\n            )\n        ) {\n            throw error(409, {\n                message: Item with name '${newItemData.name}' already exists.,\n            });\n        }\n\n        const newItem = {\n            id: Math.max(0, ...items.map((i) => i.id)) + 1,\n            name: newItemData.name,\n        };\n        items.push(newItem);\n\n        return json(newItem, { status: 201 }); // 201 Created\n    } catch (e) {\n        // If it's already a SvelteKit error, rethrow it\n        if (e.status) {\n            // SvelteKit errors have a 'status' property\n            throw e;\n        }\n        // For unexpected errors, log them and return a generic 500\n        console.error(\"Unexpected error:\", e);\n        throw error(500, \"An internal server error occurred.\");\n    }\n}\n\n// Example for a specific item (imagine this is in /api/items/[id]/+server.js)\nexport async function GET_ITEM_EXAMPLE({ params }) {\n    // Not a real SvelteKit export name\n    const itemId = parseInt(params.id, 10);\n    const item = items.find((i) => i.id === itemId);\n\n    if (!item) {\n        // 404 Not Found\n        throw error(404, { message: Item with ID ${itemId} not found. });\n    }\n    return json(item);\n}\n\nHandling on the Client-Side (e.g., in a load function or component):\n\nWhen you fetch these API endpoints:\n\n// In a +page.svelte or +page.js load function\nexport async function load({ fetch }) {\n    // --- Fetching all items (likely successful) ---\n    const allItemsResponse = await fetch(\"/api/items\");\n    let allItems = [];\n    if (allItemsResponse.ok) {\n        allItems = await allItemsResponse.json();\n    } else {\n        console.error(Failed to fetch items: ${allItemsResponse.status});\n        // Potentially show an error to the user or return an error prop\n    }\n\n    // --- Attempting to create an item (demonstrating error handling) ---\n    let newItemResponse;\n    let createdItemError = null;\n    try {\n        newItemResponse = await fetch(\"/api/items\", {\n            method: \"POST\",\n            headers: {\n                \"Content-Type\": \"application/json\",\n                // 'Authorization': 'Bearer valid-token' // Uncomment for success\n            },\n            body: JSON.stringify({ name: \"New Gizmo\" }), // Try with { name: 123 } for bad request\n        });\n\n        if (!newItemResponse.ok) {\n            // The error helper on the server sent a JSON response\n            const errorData = await newItemResponse.json();\n            console.error(\n                API Error (${newItemResponse.status}):,\n                errorData.message || errorData\n            );\n            createdItemError =\n                errorData.message || Error ${newItemResponse.status};\n            // throw new Error(errorData.message); // If you want to propagate to SvelteKit's error page\n        } else {\n            const createdItem = await newItemResponse.json();\n            console.log(\"Successfully created:\", createdItem);\n        }\n    } catch (e) {\n        // Network errors or other unexpected client-side fetch issues\n        console.error(\"Fetch request itself failed:\", e);\n        createdItemError = \"Network request failed.\";\n    }\n\n    return {\n        allItems,\n        createdItemError,\n    };\n}\n\nKey Advantages of throw error(...):\n\nStandardization: Provides a consistent way to signal errors from your API.\nClarity: Makes the intent clear in your server code.\nAutomatic JSON Response: SvelteKit handles creating the correct Response object with application/json content type.\nIntegration with SvelteKit: If an error thrown this way is unhandled during server-side rendering (e.g., in a load function that calls this API internally), SvelteKit can render the nearest +error.svelte page. For direct API calls, the client simply gets the JSON error.\nCustomizable Payload: You can pass an object to provide more detailed error information (e.g., validation errors for specific fields).\n\nAlternative (Less Canonical for API errors):\n\nYou could manually create and return a Response object using the json helper:\n\nimport { json } from \"@sveltejs/kit\";\n\nexport async function GET({ params }) {\n    // ...\n    if (!item) {\n        return json(\n            { message: \"Item not found\", errorCode: \"ITEM_NOT_FOUND\" },\n            { status: 404 }\n        );\n    }\n    // ...\n}\n\nWhile this works, throw error(...) is generally preferred for signaling actual error conditions because it's more idiomatic and integrates more deeply with SvelteKit's error handling mechanisms, especially if these API routes are also consumed by your server-side load functions. For simple status responses that aren't strictly \"errors\" but rather expected outcomes (e.g. an empty list), returning json([], { status: 200 }) is fine.\n\nGlobal Error Handling (handleError hook):\n\nFor unhandled errors that occur on the server (including those not caught in your API routes or unexpected exceptions), you can use the handleError server hook in src/hooks.server.js. This is more for logging, reporting, or transforming unexpected errors before they are sent to the client or SvelteKit's default error page.\n\n// src/hooks.server.js\n/** @type {import('@sveltejs/kit').HandleServerError} */\nexport async function handleError({ error, event }) {\n    // Log the error\n    console.error(\"Unhandled Server Error:\", error);\n    console.error(\"Event details:\", event.url.pathname, event.request.method);\n\n    // You can customize the response for the user here\n    // This is especially useful if SvelteKit is rendering an HTML error page\n    return {\n        message: \"Whoops! Something went wrong on our end.\",\n        // You can add a custom error code or other details\n        // code: 'UNEXPECTED_ERROR'\n    };\n}\n\nIn summary, throw error(status, messageOrObject); is the canonical and most SvelteKit-idiomatic way to handle expected errors within your HTTP API route handlers.",
        "locale": "en",
        "slug": "technical-posts/handling-api-errors-with-sveltekit",
        "folder": ""
    },
    {
        "title": "What are github packages",
        "description": "What are github packages",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-14T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "github"
        ],
        "content": "GitHub Packages is a software package hosting service that allows you to host your software packages privately or publicly and use them as dependencies in your projects. It is essentially GitHubs version of package registries like npm (for JavaScript) or Maven Central (for Java), but integrated directly into your GitHub repository and workflow.\n\nHere is a detailed breakdown of what it is and how it works:\n\n1. What does it do?\nIt provides a place to store and manage the \"artifacts\" (compiled code, libraries, or containers) that your code produces. Instead of publishing your library to a third-party site like npmjs.com or NuGet.org, you can publish it directly to GitHub. This allows you to keep your code, issues, and packages all in one place.\n\n2. Supported Package Types\nGitHub Packages supports several popular package managers and formats:\nContainers: Docker and OCI images (via the Container Registry ghcr.io)\nJavaScript: npm\nJava: Apache Maven and Gradle\nRank: RubyGems\n.NET: NuGet\n\n3. Key Features & Benefits\nIntegrated with GitHub Actions: This is one of the biggest advantages. You can build a CI/CD pipeline that automatically tests your code and, if successful, publishes the package to GitHub Packages without needing external credentials for other services.\nUnified Permissions: It uses the same permissions as your repository. If a developer has \"Read\" access to your private repository, they can automatically download the private packages associated with it.\nPublic and Private Hosting:\n    Public: You can host open-source packages for free that anyone can use.\n    Private: You can host proprietary internal packages that only your team or organization can access.\n\n4. The Container Registry vs. Docker Registry\nYou may see references to two different container services.\nContainer Registry (ghcr.io): This is the newer, recommended service. It allows for granular permissions (packages can belong to an organization rather than just a specific repository) and supports anonymous pulls for public images.\nDocker Registry (docker.pkg.github.com): This is the legacy service where packages were strictly tied to a specific repository.\n\n5. Pricing\nPublic Packages: Free.\nPrivate Packages: Free up to a certain limit of storage and data transfer depending on your GitHub plan (e.g., Free, Pro, Team). After you hit the limit, it is pay-as-you-go.\n\nSummary\nIf you are developing a library or a Docker image, GitHub Packages lets you \"publish\" that product right next to the source code that built it, making it easier to manage versions and permissions for your team.",
        "locale": "en",
        "slug": "technical-posts/github-packages",
        "folder": ""
    },
    {
        "title": "delray-archives",
        "description": "delray-archives",
        "date_created": "2025-05-28T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "delray"
        ],
        "content": "There is lots of stuff here:\n\nE:\\luther-backup-2.0\\delray\\users-thumb-Documents\\Projects",
        "locale": "en",
        "slug": "technical-posts/delray-archives",
        "folder": ""
    },
    {
        "title": "Una Kravets/Adam Argyle CSS presentation",
        "description": "Una Kravets/Adam Argyle CSS presentation",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "![[css-logical-properies.png]]![[new-viewport-units.png]]\ncolor-mix venn diagram",
        "locale": "en",
        "slug": "technical-posts/css-presentation-una-kravets-and-adam-argyle",
        "folder": ""
    },
    {
        "title": "Remove a file from a Git repo",
        "description": "Remove a file from a Git repo",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "git"
        ],
        "content": "Remove a file from the Git repo. This is necessary when you later decide to ignore a given file.\n\ngit rm --cached file.txt\n\nShow all ignored files:\n\ngit status --ignored",
        "locale": "en",
        "slug": "technical-posts/git",
        "folder": ""
    },
    {
        "title": "Displaying toasts with Sveltekit",
        "description": "Displaying toasts with Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "asna-com"
        ],
        "content": "FlashMessage is a small Svelte component that displays a flash message.\n\nA JavaScript object defaults a flash message with four properties.\n\nlet options = {\n\tduration: 4000,\n\tmessage: 'Your message here',\n    state: \"success\",\n    manualClose: true\n};\n\nThe properties are (the default values are shown above):\n\nduration - time to show the message in milliseconds\nmessage- message text. This is the only required property.\nstate - one of: success | fail | info | warning. These states govern the icon and accent of a toast. See example below.\nmanualClose - if true, the user can close the toast by clicking on the 'x' in the upper right-hand corner. If false, the 'x' isn't shown and the user can't explicitly close the toast.\n\nsuccess:\n![[toast-ok.png]]\n\nfail:\n![[toast-error.png]]\n\ninfo:\n![[toast-information.png]]\n\nwarning:\n![[toast-exclamation.png]]\n\nA small progress bar at the bottom of the toast counts down the toast's display duration.\n\nDeclaring a FlashMessage\n\nAdd the FlashMessage.svelte file to lib/components folder.\nAdd the flashstore.js file to the lib./components folder.\nImport the FlashMessage component in a top-level +layout.svelte file.\nAdd the ` tag to the +layout.svelte` file.\nAdd FlashMessage.css to your project.\n\nFor example, the +layout.svelte file would look like this:\n\n\n    import FlashMessage from \"$lib/components/FlashMessage.svelte\";\n    ...\n\nUsing FlashMessage\n\nImport the {flashMessage} store in the +page.svelte file\nAssign a FlashMessage object to the $flashMessage store. You only need to declare the properties of the FlassMessage that you want changed from the default values.\n\n\n\n    import { flashMessage } from \"$lib/components/flashstore\";\n\n\tif (some_condition) {\n\t    $flashMessage = {\n            state: \"info\",\n            message: \"The data appears to be correct\",\n            duration: 6000,\n\t     };\n    }\n\n\n\nThe FlashMessage component subscribes to the flashMessage store and when it sees that one has been added, it displays it.\n\nThe above code would display an info message for 6 seconds. The toast could be closed by the user (because the manualClose property default is true).\n\nConditional toast display\n\nUsing query string values is a good way to get flash messages displayed.\n\n\nimport { page } from '$app/stores';\nimport { flashMessage } from \"$lib/components/flashstore\";\n\nif ($page.url.searchParams.has('emailflash')) {\n    const email = $page.url.searchParams.get('email')\n\n    $flashMessage = {\n        state: \"info\",\n        message: An email confirming your request was sent to ${email},\n        duration: 4000,\n    };\n}\n\n\nIn the example above, the emailflash key indicates a flash message needs to be sent and the email address to display in the toast text is provided with the email key.\n\nDiscuss here how flash conditions with special query string values could be used in the +layout.svelte file to display flash messages on the next request.\n\nLimitations\n\nYou can't display more than one toast at a time. (this may be a future enhancement)\nThe toast position is hardcoded to top center of the page.",
        "locale": "en",
        "slug": "technical-posts/flashmessage",
        "folder": ""
    },
    {
        "title": "pnpm-local-package",
        "description": "How to create and use a local PNPM package",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-14T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "pnpm"
        ],
        "content": "Typical workflow\n\nMake changes to utils. \nUpdateversionin package.json:\"1.0.1\"\"1.0.2\"\nCommit:git commit -m \"v1.0.2: Added new helper functions\"\nTag:git tag v1.0.2\nPush:git push && git push --tags\n\n\nIn consuming app \n\nDo this from the command line for the consuming project:\npnpm add github:rogerpence/rp-utils#v1.0.13\n\nis yourone-command solutionfor upgrading to any version! \nFetch v1.0.13 from GitHub\nUpdatepackage.jsonto reference v1.0.13\nUpdatepnpm-lock.yaml\nReplace the files innode_modules\n\nDeclare rp-utils like this and be sure to update version number before doing pnpm update\n\n\"dependencies\": {\n    \"@rogerpence/rp-utils\": \"github:rogerpence/rp-utils#v1.0.12\",\n\nThis code creates a local NPM package:\n\npackage.json\n\n{\n    \"name\": \"@rogerpence/rp-utils\",\n    \"version\": \"1.0.12\",\n    \"type\": \"module\",\n    \"main\": \"./src/index.js\",\n    \"exports\": {\n        \".\": \"./src/index.ts\",\n        \"./console\": \"./src/console.ts\",\n        \"./date\": \"./src/date.ts\",\n        \"./filesystem\": \"./src/filesystem.ts\",\n        \"./markdown\": \"./src/markdown.ts\",\n        \"./string\": \"./src/string.ts\"\n    },\n    \"files\": [\n        \"src\"\n    ],\n    \"dependencies\": {\n        \"@types/js-yaml\": \"^4.0.9\",\n        \"@types/node\": \"^22.19.1\",\n        \"ansis\": \"^4.2.0\",\n        \"js-yaml\": \"^4.1.1\"\n    }\n}\n\ntsconfig.json\n\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ES2022\",\n    \"moduleResolution\": \"bundler\",\n    \"lib\": [\"ES2022\"],\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"resolveJsonModule\": true,\n    \"declaration\": true,    \n    \"outDir\": \"./dist\"\n  },\n  \"include\": [\"src/*/\"]\n}\n\nExample TS file\n\nstring.ts\n\nexport const roger = \"pence\";\n\nBarrel export \n\nThis is optional, but it allows easy importing of any export the package offers (regardless of which file it's in)\n\nFor example, with the barrel export provided:\n\nimport { getAllDirEntries, parseMarkdownFile } from '@rogerpence/rp-utils';\n\ninstead of \n\nimport { getAllDirEntries } from '@rogerpence/rp-utils/filesystem';\nimport { parseMarkdownFile } from '@rogerpence/rp-utils/markdown';\n\nindex.ts\n\n// This is known as the barrel export pattern.\nexport * from \"./console\";\nexport * from \"./date\";\nexport * from \"./filesystem\";\nexport * from \"./markdown\";\nexport * from \"./string\";\n\n\n\ngit tag v1.0.12 git push && git push --tags\n\n\n\n\n\n\n\n\n\nHere's how to create a standalone utils package and reference it from your SvelteKit project:\n\nStep 1: Create standalone utils package\n\ncd c:\\Users\\thumb\\Documents\\projects\\svelte\nmkdir utils\ncd utils\n\npnpm init`\n\nStep 2: Set up the utils package structure\n\nutils/\n package.json\n tsconfig.json\n src/\n     index.ts\n     filesystem.ts\n     console.ts\n     date.ts`\n\nStep 3: Create package.json\n\n{\n  \"name\": \"@yourteam/utils\",\n  \"version\": \"1.0.0\",\n  \"type\": \"module\",\n  \"main\": \"./src/index.ts\",\n  \"exports\": {\n    \".\": \"./src/index.ts\",\n    \"./filesystem\": \"./src/filesystem.ts\",\n    \"./console\": \"./src/console.ts\",\n    \"./date\": \"./src/date.ts\"\n  },\n  \"files\": [\n    \"src\"\n  ],\n  \"dependencies\": {\n    \"@types/node\": \"^22\"\n  }\n}`\n\nStep 4: Create utils/tsconfig.json\n\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ES2022\",\n    \"moduleResolution\": \"bundler\",\n    \"lib\": [\"ES2022\"],\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"resolveJsonModule\": true,\n    \"declaration\": true,\n    \"outDir\": \"./dist\"\n  },\n  \"include\": [\"src/*/\"]\n}`\n\nStep 5: Move your utility files\n\nimport { fileURLToPath } from 'url';\nimport path from 'path';\n\nexport function getRelativePath() {\n\tconst fullFilePath = fileURLToPath(import.meta.url);\n\treturn path.dirname(fullFilePath);\n}\n\nexport function truncatePathAfterDirectory(fullPath: string, lastDirectory: string): string {\n\tconst parts = fullPath.split(path.sep);\n\tconst srcIndex = parts.indexOf(lastDirectory);\n\tif (srcIndex === -1) {\n\t\tconsole.error(Path doesn't contain the '${lastDirectory}' directory);\n\t\tprocess.exit(1);\n\t}\n\treturn parts.slice(0, srcIndex + 1).join(path.sep);\n}\n\nexport function getPathForCli(...segments: string[]): string {\n\tconst srcPath = truncatePathAfterDirectory(process.cwd(), 'src');\n\tif (segments.length == 1) {\n\t\tsegments = ['lib', 'data', ...segments];\n\t}\n\treturn path.join(srcPath, ...segments);\n}`\n\n\nindex.ts contents\n\nexport * from './filesystem.js';\nexport * from './console.js';\nexport * from './date.js';`\n\nStep 6: Initialize git in utils (optional but recommended)\n\ncd c:\\Users\\thumb\\Documents\\projects\\svelte\\utils\ngit init\ngit add .\ngit commit -m \"Initial utils package\"`\n\nStep 7: Link utils to your mdsvex project\n\nOption A: Using pnpm link (simplest for local development)\n\nIn the utils directory\ncd c:\\Users\\thumb\\Documents\\projects\\svelte\\utils\npnpm link --global\n\nIn your mdsvex project\ncd c:\\Users\\thumb\\Documents\\projects\\svelte\\mdsvex\npnpm link --global @yourteam/utils`\n\nOption B: Using file: protocol (better for team)\n\nUpdate your mdsvex package.json:\n\n{\n\t\"name\": \"mdsvex\",\n\t\"private\": true,\n\t\"version\": \"0.0.1\",\n\t\"type\": \"module\",\n\t\"scripts\": {\n\t\t\"dev\": \"vite dev\",\n\t\t\"build\": \"vite build\",\n\t\t\"preview\": \"vite preview\",\n\t\t\"prepare\": \"svelte-kit sync || echo ''\",\n\t\t\"check\": \"svelte-kit sync && svelte-check --tsconfig ./tsconfig.json\",\n\t\t\"check:watch\": \"svelte-kit sync && svelte-check --tsconfig ./tsconfig.json --watch\",\n\t\t\"format\": \"prettier --write .\",\n\t\t\"lint\": \"prettier --check . && eslint .\"\n\t},\n\t\"devDependencies\": {\n\t\t\"@eslint/compat\": \"^1.4.0\",\n\t\t\"@eslint/js\": \"^9.38.0\",\n\t\t\"@sveltejs/adapter-auto\": \"^7.0.0\",\n\t\t\"@sveltejs/kit\": \"^2.47.1\",\n\t\t\"@sveltejs/vite-plugin-svelte\": \"^6.2.1\",\n\t\t\"@types/js-yaml\": \"^4.0.9\",\n\t\t\"@types/node\": \"^22\",\n\t\t\"ansis\": \"^4.2.0\",\n\t\t\"eslint\": \"^9.38.0\",\n\t\t\"eslint-config-prettier\": \"^10.1.8\",\n\t\t\"eslint-plugin-svelte\": \"^3.12.4\",\n\t\t\"globals\": \"^16.4.0\",\n\t\t\"js-yaml\": \"^4.1.0\",\n\t\t\"prettier\": \"^3.6.2\",\n\t\t\"prettier-plugin-svelte\": \"^3.4.0\",\n\t\t\"svelte\": \"^5.41.0\",\n\t\t\"svelte-check\": \"^4.3.3\",\n\t\t\"typescript\": \"^5.9.3\",\n\t\t\"typescript-eslint\": \"^8.46.1\",\n\t\t\"vite\": \"^7.1.10\"\n\t},\n\t\"pnpm\": {\n\t\t\"onlyBuiltDependencies\": [\n\t\t\t\"esbuild\"\n\t\t]\n\t},\n\t\"dependencies\": {\n\t\t\"@yourteam/utils\": \"file:../utils\",\n\t\t\"zod\": \"^4.1.12\"\n\t}\n}`\n\nThen run:\ncd c:\\Users\\thumb\\Documents\\projects\\svelte\\mdsvex\npnpm install`\n\nOption C: Using git repository (best for team sharing)\n\nPush utils to a git repository first\ncd c:\\Users\\thumb\\Documents\\projects\\svelte\\utils\ngit remote add origin https://github.com/yourteam/utils.git\ngit push -u origin main`\n\nThen in your mdsvex package.json:\n\n{\n  \"dependencies\": {\n    \"@yourteam/utils\": \"github:yourteam/utils#main\",\n    \"zod\": \"^4.1.12\"\n  }\n}`\n\nStep 8: Update TypeScript config in mdsvex\n\n{\n  \"extends\": \"./.svelte-kit/tsconfig.json\",\n  \"compilerOptions\": {\n    \"allowJs\": true,\n    \"checkJs\": true,\n    \"esModuleInterop\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"resolveJsonModule\": true,\n    \"skipLibCheck\": true,\n    \"sourceMap\": true,\n    \"strict\": true,\n    \"moduleResolution\": \"bundler\",\n    \"paths\": {\n      \"$lib\": [\"./src/lib\"],\n      \"$lib/\": [\"./src/lib/\"],\n      \"@yourteam/utils\": [\"../utils/src/index.ts\"],\n      \"@yourteam/utils/\": [\"../utils/src/\"]\n    }\n  }\n}`\n\nStep 9: Use in your mdsvex project\n\nimport { getPathForCli, truncatePathAfterDirectory } from '@yourteam/utils/filesystem';\n\n// ...existing code...\n\nconst dataPath = getPathForCli('index.json');\nconsole.log(dataPath);`\n\nRecommendation: Use Option B (file: protocol)\n\nAdvantages:\n Works for whole team (relative path)\n Changes in utils immediately available\n No global state like pnpm link\n Works in CI/CD\n Simple to set up\n\nYour file structure:\nsvelte/\n utils/              # Standalone package\n    package.json\n    tsconfig.json\n    src/\n mdsvex/            # Your SvelteKit app\n     package.json   # References: \"file:../utils\"\n     src/`\n\nThis approach keeps utils completely separate while making it easy to share across multiple projects!\n\nSimilar code found with 3 license types\n\nInstalling packages from GitHub that need a build step\n\nSetting up package.json\n\nTo install your package directly from GitHub using pnpm add github:rogerpence/sv-pkg, you need to ensure it builds automatically when installed. Here are the changes needed: \n\nYour package will automatically build when installed from GitHub. The key change is the prepare script, which runs automatically after pnpm install.\n\nHow to use it:\n\nPush your code to GitHub at github.com/rogerpence/sv-pkg\n\nIn another project, install it:\n   pnpm add github:rogerpence/sv-pkg\n\n   Or with a specific branch/tag:\n   pnpm add github:rogerpence/sv-pkg#main\n   pnpm add github:rogerpence/sv-pkg#v1.0.0\n\nUse it in your project:\n   \n     import { HelloWorld } from 'sv-pkg';\n   \n\n   \n\nWhat changed:\nThe prepare script now runs svelte-package, so the package builds automatically when installed from GitHub\nAdded a separate package script for manual builds with publint\nThe prepack script runs before publishing to npm (if you do that later)\n\nThe package will build the dist folder automatically when someone runs pnpm add github:rogerpence/sv-pkg.\n\nWhat is this type of packaging called\n\nThis type of packing use  [[github-packages]] as an alternative to npmjs.com (the NPM repository).\n\nIt may also be \"Git dependencies\" or \"installing packages from Git\". It's a standard npm/pnpm/yarn feature that lets you use Git repositories (GitHub, GitLab, Bitbucket, etc.) as package sources instead of the npm registry.\n\nCommon terms for this:\nGit dependencies\nGit-based packages\nDirect Git installation\nGitHub packages (informal)\n\nSyntax variations:\nGitHub shorthand\npnpm add github:rogerpence/sv-pkg\n\nFull Git URL\npnpm add git+https://github.com/rogerpence/sv-pkg.git\n\nSpecific branch/tag/commit\npnpm add github:rogerpence/sv-pkg#main\npnpm add github:rogerpence/sv-pkg#v1.0.0\npnpm add github:rogerpence/sv-pkg#abc1234\n\nBenefits:\n No need to publish to npm registry\n Works with private repos (if you have access)\n Can install specific branches/commits\n Keeps packages internal to your organization\n\nImportant: The prepare script is what makes this work - it builds the package after installation since Git repos don't include the built dist folder (which is typically gitignored).\n\nYour setup is perfect for distributing reusable Svelte component libraries without publishing to npm!\n\nThe prepare script is key\n\nThe prepare script is an npm lifecycle script that's automatically called by npm/pnpm/yarn in these situations:\n\n1. When someone installs your package from Git:\npnpm add github:rogerpence/sv-pkg\nProcess flow:\npnpm clones the repo\nRuns pnpm install (installs your devDependencies)\nAutomatically runs prepare script  builds your dist folder\nPackage is ready to use\n\n2. During local development:\npnpm install  # in your package directory\nThe prepare script runs after installing dependencies.\n\n3. Before publishing to npm:\nnpm publish\nRuns prepare before publishing (though you're not using this).\n\nOther lifecycle scripts for reference:\nprepublishOnly - Only runs before npm publish\nprepack - Runs before creating a tarball\npostinstall - Runs after package is installed (in the consuming project)\n\nSo for Git dependencies, pnpm (or npm/yarn) automatically calls the prepare script after cloning and installing dependencies. This is why your package builds automatically when installed from GitHub!\n\nDo GitHub packages run in serverless environments like Vercel\n\nYes, they work on Vercel and other serverless platforms, but with some important considerations:\n\n What works:\nVercel, Netlify, Cloudflare Pages all support Git dependencies\nThe prepare script runs during the build phase\nYour package gets built automatically before deploying\n\n Important considerations:\n\n1. Build time increases:\nYour package builds during every deployment\nThe consuming project must install your devDependencies (@sveltejs/package, etc.)\n\n2. Installation mode:\nMost platforms install dependencies differently:\nDevelopment (runs prepare) - used by Vercel\npnpm install\n\nProduction only (might skip prepare on some platforms)\npnpm install --prod\n\n3. Best practices for Vercel:\nYour current setup should work, but consider:\n\nKeep your prepare script fast\nEnsure devDependencies are reasonable in size\nConsider caching strategies\n\nAlternative for production:\nIf builds become problematic, you could:\nCreate GitHub releases with pre-built dist folder\nInstall from release tags: pnpm add github:rogerpence/sv-pkg#v1.0.0\nInclude dist in that specific release (using .npmignore instead of .gitignore)\n\nYour current setup will work fine on Vercel - the platform handles Git dependencies well during builds.\n\nsimplified workflow\n\nImplicitly including '/dist' (in .gitignore)\n\nSimplified workflow:\nMake changes to your components\nBuild\npnpm run package\n\nCommit everything (including dist)\ngit add .\ngit commit -m \"feat: update HelloWorld component\"\ngit push\n\nCreate release when ready\ngit tag v0.0.5\ngit push origin v0.0.5\n\nThen in your consuming projects:\npnpm add github:rogerpence/sv-pkg#v0.0.5\n\nSince it's your own project and not a public library, you don't need to follow public npm package conventions. This approach is cleaner and more reliable for your needs!>)\n\n\n\nYou're missing one critical step! Here's the complete workflow:\n\n1. Create SvelteKit library & develop\nAdd components to components\nExport them in index.ts\nTest in routes (not included in package) \n\n2. Build the package\npnpm run package\nThis creates the dist folder with compiled components  Don't skip this!\n\n3. Commit and push with tag\ngit add .\ngit commit -m \"feat: add new components\"\ngit push\n\ngit tag v0.0.6\ngit push origin v0.0.6\n\n4. Install in consuming project\npnpm add github:rogerpence/sv-pkg#v0.0.6\n\nThe key: Always run pnpm run package before committing changes that you want in a release. The dist folder is what actually gets imported by consuming projects.\n\nThat's it! You've got it now.\n\nWith this simplified workflow, (except for the 'package' script) there isn't anything specific in package.json 'scripts' that is doing anything to make this work,\n\nFor the GitHub package workflow, only package is essential:\n\nRequired:\npackage - Builds the dist folder\n\nOptional (for local development only):\ndev - Run development server to test routes\nbuild - Not needed for the package workflow\npreview - Preview the dev server\ncheck, format, lint - Code quality tools\nrelease - Redundant, you can just run package directly\n\nThe important parts for making this work are actually in the package.json metadata, not the scripts:\n\n\"files\": [\"dist\", ...],          // What gets included\n\"svelte\": \"./dist/index.js\",     // Entry point\n\"types\": \"./dist/index.d.ts\",    // TypeScript types\n\"exports\": { \".\" : { ... } }     // Module exports\n\nThose fields tell consuming projects where to find your compiled components!\n\nFurther reading\n\nGreat question! Here are the best resources:\n\nOfficial Documentation:\nnpm docs on Git dependencies: https://docs.npmjs.com/cli/v10/configuring-npm/package-json#git-urls-as-dependencies\n  The canonical reference for Git-based installs\n  \npnpm docs: https://pnpm.io/cli/add#install-from-git-repository\n  pnpm-specific Git install options\n\nSvelteKit packaging guide: https://kit.svelte.dev/docs/packaging\n  How svelte-package works\n\nKey Topics to Search:\n\"npm git dependencies\"\n\"installing packages from github\"\n\"git+https dependencies\"\n\"npm package.json files field\"\n\"npm lifecycle scripts\" (prepare, prepack, etc.)\n\nUseful Tools:\npublint (you're already using it!) - Validates package exports\narethetypeswrong - Checks TypeScript exports: https://arethetypeswrong.github.io/\n\nAdvanced:\nGit submodules vs npm git dependencies - Different approaches to sharing code\nMonorepos with pnpm workspaces - If you have multiple related packages\n\nThe npm docs on Git URLs are really the definitive guide - everything else builds on those concepts. Your setup is solid now, so you've got a great foundation!\n\nDetail on publint\n\npublint validates your package.json configuration to catch common packaging mistakes.\n\nIt checks:\n exports field is correct\n files field includes necessary files\n Entry points (main, svelte, types) exist\n Dual package hazards (ESM/CJS issues)\n Missing files that should be published\n TypeScript types are accessible\n\nExample issues it catches:\n\"You're exporting index.js but it doesn't exist\"\n\"Your types field points to a file not in the files array\"\n\"You have CJS and ESM conflicts\"\n\nIn your workflow:\n\"package\": \"svelte-kit sync && svelte-package && publint\"\n\nAfter svelte-package builds the dist folder, publint checks that everything is configured correctly. If there's an issue, it warns you before you commit and push.\n\nTry it:\npnpm run package\n\nIf publint finds issues, it'll output warnings/errors. If it's silent, your package is properly configured!\n\nYou can also check it online: https://publint.dev/\n\nIt's basically a pre-flight check to ensure your package will work correctly when installed.",
        "locale": "en",
        "slug": "technical-posts/pnpm-local-package",
        "folder": ""
    },
    {
        "title": "C# command line processor; cocona; commandlineparser; cli",
        "description": "C# command line processor; cocona; commandlineparser; cli",
        "date_created": "2025-07-13T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "csharp",
            "cli"
        ],
        "content": "Command Line Parser Nuget package\n\n![[image-37.png]]\n\nusing CommandLine;\n\nnamespace cli_test\n{\n    internal class Program\n    {\n        public class Options\n        {\n            [Option('v', \"verbose\", Required = false, HelpText = \"Set output to verbose messages.\")]\n            public bool Verbose { get; set; }\n\n            [Option('p', \"FilePath\", Required = false, HelpText = \"Set file path.\")]\n            public string FilePath { get; set; } = \"Roger\";\n\n        }\n        static void Main(string[] args)\n        {\n            Parser.Default.ParseArguments(args)\n                .WithParsed(o =>\n                {\n                    if (o.Verbose)\n                    {\n                        Console.WriteLine($\"Current Arguments (verbose): -v {o.Verbose}\");\n                        Console.WriteLine(\"Quick Start Example! App is in Verbose mode!\");\n                    }\n                    else\n                    {\n                        Console.WriteLine($\"Current Arguments: -v {o.Verbose}\");\n                        Console.WriteLine(\"Quick Start Example!\");\n                    }\n\n                    Console.WriteLine(o.FilePath);\n                });\n        }\n    }\n}",
        "locale": "en",
        "slug": "technical-posts/cs-command-line-parser",
        "folder": ""
    },
    {
        "title": "Monarch content notes",
        "description": "Monarch content notes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-com",
            "monarch"
        ],
        "content": "ASNA Monarch content ideas\n\nMonarch is more than simple source conversion\n    This aims to make prospects ponder the broad and deep things that must be considered in an application migration. It discusses how a Monarch migration is much more than simply translating source code.\nWhich database is right your migration?\n    This aims to make migration customers understand that in many cases it is rational to leave the application DB on the IBM i. It discusses the merits of both Monarch DB targets (IBM i and SQL Server)\nCalculating the cost of technical debt\n    This aims to quantify the cost of technical debt of staying on the IBM i with green-screen RPG.\nDataGate Linear\n    This aims to provide an answer when a prospect balks at the notion of a database runtime. It discusses the technical and licensing aspects of DataGate Linear.\nHow the Monarch Cocoon helps manage application migrations\n    This article drills into details, with several pictures, of what and how Monarch Cocoon fits into the Monarch migration process.\nFive ways Monarch is the best choice (done)\n    This aims to provide a short, easily-read list of the competitive advantages that Monarch offers.\n        99% of your code is migrated\n        RPG and C#\n        Staged Migration\n        Database two ways\n        Mature and proven methodology\nWhy .NET is a superb enterprise target\n    This aims to answer how C# and .NET are superb targets for an RPG migration. This is for the prospect undecided on the target platform (ie, those giving equal or similar weight to Java, Node, and .NET)\n    It provides reasons why .NET and C# are better for an IBM i shop than Java or Node.\nWhy RPG may be a strategic migration target (done)\n    The notion of using RPG as a language target is anathema to migrating away from RPG! But it's often be a very good idea for some prospects.\n    There is already some content on asna.com about this. This content would lean harder into why RPG could be a very good target for some prospects.\n    This aims to provide explain how ASNA Monarch and Encore make migrating to RPG (as a first phase) a strategic possibility.\nCodify the Monarch toolset\n    This may be internal, but it provides a bird's eye-view of the Monarch toolset.\n    A little article is slated for the Q4 newsletter that discusses Monarch's new docs.\n    This will probably be in the KB section of ASNA.com. Are the docs worthy of a bigger mention?\n    If you haven't noticed, there is a link to the new Monarch docs in the Support menu at ASNA.com.",
        "locale": "en",
        "slug": "technical-posts/monarch-content-notes",
        "folder": ""
    },
    {
        "title": "Mapping podman volume to local windows folder",
        "description": "Mapping podman volume to local windows folder",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "podman",
            "containers"
        ],
        "content": "To map a Podman volume to a local Windows folder, you need to use the podman machine set command to configure the mapping and then use the -v flag with the podman run command to mount the volume. [1, 2]\n\nSteps:\n\nSet the Podman machine to use rootful mode:\n    Open a terminal or command prompt.\n    Run the following command:\n\n|                                             |\n| ------------------------------------------- |\n|      podman machine set --rootful |\n\nThis ensures that Podman can access the host's file system. [1]\n\nCreate a Podman volume:\n    Use the following command to create a named volume:\n\n|                                               |\n| --------------------------------------------- |\n|      podman volume create my_volume |\n\nReplace my_volume with your desired volume name. [1, 3]\n\nMount the volume:\n    Use the podman volume mount command to map the volume to a directory on the host machine:\n\n|                                                                            |\n| -------------------------------------------------------------------------- |\n|      podman volume mount my_volume /path/to/local/windows/folder |\n\nReplace /path/to/local/windows/folder with the actual path to your local Windows folder. [1, 4]\n\nRun a container with the mounted volume:\n    Use the -v flag in the podman run command to mount the volume to a directory inside the container:\n\n|                                                                                             |\n| ------------------------------------------------------------------------------------------- |\n|      podman run -it --rm -v my_volume:/container/path -t docker.io/bash /bin/bash |\n\nmy_volume is the name of the volume you created. [1]\n/container/path is the directory inside the container where the volume will be mounted. [1]\ndocker.io/bash is the image you're running. [1]\n/bin/bash is the command to run inside the container. [1]\n\nExample: [1, 2]\n\nLet's say you want to map a volume named my_data to the Windows folder C:\\Users\\yourname\\Documents\\Podman and mount it inside the container at /app/data: [1, 2]\n\npodman machine set --rootful\npodman volume create my_data\npodman volume mount my_data C:\\Users\\yourname\\Documents\\Podman\npodman run -it --rm -v my_data:/app/data -t docker.io/ubuntu /bin/bash\n\nNow, any changes made in the container's /app/data directory will be reflected in your local Windows folder C:\\Users\\yourname\\Documents\\Podman, and vice-versa. [1, 2]\n\nImportant Notes: [2, 5]\n\nEnsure you have Podman installed and running on your Windows machine. [2, 5]\nThe podman machine set --rootful command may require administrator privileges, according to a post on Stack Overflow. [1]\nYou might need to adjust the volume name, paths, and container image according to your specific needs. [1, 2]\nWhen using a volume created with the podman volume create command, it is typically stored in the ~/.local/share/containers/storage/volumes directory on the host machine, says a Stack Overflow post. [1, 6]\n\nGenerative AI is experimental.\n\n1(https://stackoverflow.com/questions/69298356/how-to-mount-a-volume-from-a-local-machine-on-podman)\n\n2(https://www.redhat.com/en/blog/run-podman-windows)\n\n3(https://docs.podman.io/en/latest/markdown/podman-volume-create.1.html)\n\n4(https://docs.podman.io/en/latest/markdown/podman-volume-mount.1.html)\n\n5(https://stackoverflow.com/questions/77330102/change-podman-storage-folder)\n\n6(https://docs.podman.io/en/stable/markdown/podman.1.html)",
        "locale": "en",
        "slug": "technical-posts/podman-and-local-windows-volumes",
        "folder": ""
    },
    {
        "title": "CSS nuggets",
        "description": "CSS nuggets",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Center a wrapper\n\nCenter a wrapper section. The first value is margin top and bottom. The second value provides auto to margin left and right.\n\nmargin: 50px auto;\n\n[[Container query example]]",
        "locale": "en",
        "slug": "technical-posts/css-nuggets",
        "folder": ""
    },
    {
        "title": "JavaScript's spread operator",
        "description": "JavaScript's spread operator",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "In the code snippet, ...rest is using object destructuring with the rest parameter syntax. Here's what's happening:\n\nconst { ...rest } = formData;\n\nThis line is actually creating a shallow copy of the entire formData object. In this specific case, since there's no property being explicitly destructured before the ...rest, it's equivalent to:\n\nconst rest = { ...formData };\n\nThe ... syntax has different uses:\n\nWhen used in destructuring (like in your code): it collects all remaining properties into a new object\nWhen used in spreading: it expands an object's properties into another object\n\nHere's a more typical example to illustrate the difference:\n\nconst formData = {\n    name: \"John\",\n    email: \"john@example.com\",\n    age: 30,\n};\n\n// Destructuring with rest\nconst { name, ...remainingFields } = formData;\nconsole.log(name); // 'John'\nconsole.log(remainingFields); // { email: 'john@example.com', age: 30 }\n\nIn your code, since no properties are being extracted before the ...rest, you're essentially just creating a copy of formData. This might be redundant unless there's a specific reason for creating this copy.\n\nA more idiomatic way to write this could be to simply use formData directly if you don't need a separate copy:\n\ntry {\n    const result = formSchema.parse(formData);\n    console.log(\"SUCCESS\");\n    console.log(\"result\", result);\n} catch (err: any) {\n    const { fieldErrors: errors } = err.flatten();\n\n    return {\n        data: formData, // Just use formData directly\n        errors,\n        formData,\n    };\n}",
        "locale": "en",
        "slug": "technical-posts/javascript-spread-operator",
        "folder": ""
    },
    {
        "title": "podman named volumes",
        "description": "podman named volumes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "containers"
        ],
        "content": "That's a great question, and it highlights a key difference between named volumes and bind mounts.\n\nThe data for a Podman named volume (like postgres_data you created) is not stored directly in an easily accessible folder like C:\\Users\\thumb\\... on your Windows host filesystem.\n\nInstead, it's stored:\n\nInside the Podman Machine's Filesystem: Podman on Windows typically runs using a lightweight Linux Virtual Machine (VM). This VM has its own virtual disk and filesystem.\nManaged by Podman: Within that VM's filesystem, Podman manages a specific directory structure for its storage, including volumes. The typical path inside the VM where volume data is stored is:\n    /var/lib/containers/storage/volumes//_data\n\nSo, for your postgres_data volume, the actual PostgreSQL files would reside at this path inside the Podman VM:\n\n/var/lib/containers/storage/volumes/postgres_data/_data\n\nHow to Access or Interact with the Volume Data (If Needed):\n\nVia podman machine ssh (Recommended for exploration): You can access the shell of the Podman VM and navigate to the path:\n\nStart an SSH session into the default podman machine\n    podman machine ssh\n\nOnce inside the VM's shell:\n    sudo ls -l /var/lib/containers/storage/volumes/postgres_data/_data\nOr:\n    cd /var/lib/containers/storage/volumes/postgres_data/_data\n    ls -la\n    exit\n\n    (You might need sudo depending on the VM's configuration). This lets you see the files as they exist within the Linux environment where Postgres is actually running.\n\nMounting into another container: You can temporarily mount the volume into another utility container (like alpine or ubuntu) to inspect or copy files using podman cp.\n\nRun an interactive alpine container with the volume mounted at /inspect\n    podman run -it --rm -v postgres_data:/inspect alpine sh\n\nInside the temporary container's shell:\n    ls -l /inspect\nYou could tar files here, etc.\n    exit\n\nBackups (Correct Way): The best practice for backing up PostgreSQL data is not to copy files directly from the volume location (whether it's a named volume or bind mount), especially while the database is running, as this can lead to inconsistent or corrupt backups. Use standard PostgreSQL tools like pg_dump instead:\nExample: Dump all databases to a file on your host\n    podman exec -t postgresdb pg_dumpall -U postgres > C:\\Users\\thumb\\Documents\\db_backup.sql\n(You might need to adjust the user '-U' if you configured a different one)\n\nWhere is the VM itself stored?\n\nThe VM's virtual disk file (which contains the volume data) is stored on your Windows host, but its exact location depends on the virtualization provider Podman is using (usually WSL or Hyper-V by default):\n\nWSL: Managed within the WSL distribution's storage (often complex paths under %LOCALAPPDATA%\\Packages).\nHyper-V: Typically in the path configured in Hyper-V Manager, often defaulting to somewhere like C:\\ProgramData\\Microsoft\\Windows\\Hyper-V or C:\\Users\\Public\\Documents\\Hyper-V\\Virtual Hard Disks.\n\nIn Summary:\n\nNamed volumes offer better compatibility and performance by keeping the data within the Linux environment Podman controls. The trade-off is that the data isn't directly browsable via Windows File Explorer. Use podman machine ssh for inspection and standard database tools (pg_dump) for backups.",
        "locale": "en",
        "slug": "technical-posts/podman-named-volumes",
        "folder": ""
    },
    {
        "title": "directory watcher in go",
        "description": "directory watcher in go",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "go"
        ],
        "content": "created using\nOkay, here's a Go program that utilizes fsnotify to monitor a directory for file changes and calls a handler function when changes occur:\n\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/fsnotify/fsnotify\"\n)\n\n// handleFileChange is the function that gets called when a file change is detected.\nfunc handleFileChange(filePath string) {\n\tfmt.Println(\"File changed:\", filePath)\n\t// Add your custom processing logic here\n\t// For example:\n\n    file, err := os.ReadFile(filePath)\n    if err != nil {\n        fmt.Println(\"Error reading file:\", err)\n        return\n    }\n\n    fmt.Println(\"Contents of the file:\\n\", string(file))\n\n\n}\n\nfunc watchDirectory(dirPath string) error {\n\twatcher, err := fsnotify.NewWatcher()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error creating watcher: %w\", err)\n\t}\n\tdefer watcher.Close()\n\n\terr = watcher.Add(dirPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error adding directory to watcher: %w\", err)\n\t}\n\n\tfmt.Println(\"Watching directory:\", dirPath)\n\n    done := make(chan bool)\n\tgo func() {\n        defer close(done)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase event, ok := \")\n\t\treturn\n\t}\n\n\tdirPath := os.Args[1]\n\tabsDirPath, err := filepath.Abs(dirPath)\n    if err != nil {\n        log.Fatalf(\"Error getting absolute path: %v\", err)\n    }\n\n    _, err = os.Stat(absDirPath)\n    if os.IsNotExist(err) {\n        log.Fatalf(\"Directory '%s' does not exist\", absDirPath)\n    }\n\n    // Create a dummy file if the watched directory is empty, for demonstration purposes\n    files, err := os.ReadDir(absDirPath)\n    if err != nil {\n        log.Fatalf(\"Error reading watched directory: %v\", err)\n    }\n\n    if len(files) == 0 {\n        dummyFilePath := filepath.Join(absDirPath, \"dummy.txt\")\n        err := os.WriteFile(dummyFilePath, []byte(\"Initial Content\"), 0644)\n        if err != nil {\n             log.Fatalf(\"Error creating dummy file in watched directory: %v\", err)\n        }\n    }\n\n\n\terr = watchDirectory(absDirPath)\n\tif err != nil {\n\t\tlog.Fatal(\"Error watching directory:\", err)\n\t}\n}\n\nExplanation:\n\nImports:\n\n    fmt: For printing output.\n    log: For error logging.\n    os: For interacting with the operating system (command-line arguments, creating files, etc.)\n    path/filepath: For handling file paths\n    time: For time related functions\n    github.com/fsnotify/fsnotify: The core library for file system notifications.\n\nhandleFileChange(filePath string) Function:\n\n    This is the function you'll customize to process changed files.\n    It currently prints the file path to the console and reads the file content.\n    Modify this function to perform your desired actions (e.g., send data to a server, trigger builds, update configuration).\n\nwatchDirectory(dirPath string) Function:\n\n    fsnotify.NewWatcher(): Creates a new watcher instance.\n    defer watcher.Close(): Ensures the watcher is closed when the function exits.\n    watcher.Add(dirPath): Adds the directory to be watched.\n    The for loop and select statement are the core of the watcher logic:\n        <-watcher.Events: Receives file change events from the watcher's event channel.\n        event.Op & fsnotify.Write == fsnotify.Write: Checks if the event was a write operation. Add/Remove/Rename events are also checked\n        handleFileChange(event.Name): Calls your custom handler function if a change is detected.\n        <-watcher.Errors: Receives any errors from the watcher's error channel.\n\nmain() Function:\n    Argument Parsing: Checks if the user provided the directory path as a command-line argument.\n    Getting absolute path: Ensure that the provided path is absolute\n    Check if directory exists: Exit program if the provided directory does not exist\n    Create dummy file: Create a dummy file in the watched directory to demostrate the program if the directory is empty\n    Error handling: Log fatal errors using log.Fatalf\n    watchDirectory(absDirPath): Starts the directory watching process.\n\nHow to Run:\n\nSave: Save the code as main.go.\nInstall fsnotify:\n    go get github.com/fsnotify/fsnotify\nRun from the terminal:\n    go run main.go /path/to/your/directory\n    (Replace /path/to/your/directory with the actual path you want to monitor)\n\nKey Points:\n\nEvent Types: The fsnotify.Op enum represents different types of events: Write, Create, Remove, Rename, and more. You can choose which events to react to in the select statement.\nError Handling: Check errors for robustness. The program includes some error handling but you may want to implement more elaborate error checking and recovery.\nCross-Platform: fsnotify is designed to work on multiple operating systems.\nPerformance: File system events are generally efficient. If you're dealing with a directory with massive numbers of files, there are strategies you could implement to reduce memory usage.\nCustomization: The handleFileChange function is where you implement your specific requirements.\nNo Blocking: The watchDirectory function launches a goroutine for watching the file system events. This means that you can continue to interact with your terminal after running the program.\n\nLet me know if you have any more questions or want to explore more advanced aspects!",
        "locale": "en",
        "slug": "technical-posts/directory-watcher-in-go",
        "folder": ""
    },
    {
        "title": "Human friendly sorting in TypeScript",
        "description": "Human friendly sorting in Typescript",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "Also, called \"natural sort order,\" this code performs  a smart string comparison using JavaScript's localeCompare method. Here's the breakdown:\nWhat it does:\n\n// String comparison (case-insensitive)\ncomparison = String(aVal).localeCompare(String(bVal), undefined, {\n    sensitivity: 'base',\n    numeric: true\n});`\n\nParameters explained:\n\n1. String(aVal) and String(bVal)\nConverts the values to strings before comparing\nHandles cases where values might not already be strings\n\n2. undefined (locale parameter)\nUses the browser/system's default locale\nYou could specify like 'en-US' for specific locale rules\n\n3. sensitivity: 'base'\nMakes comparison case-insensitive and accent-insensitive\nExamples:\n  'Apple' equals 'apple' \n  'caf' equals 'cafe' \n  'a' comes before 'b' \n\n4. numeric: true (the important one!)\nTreats numeric substrings as numbers, not strings\nWithout `numeric: true:\n  `\"10\"  \n  a.localeCompare(b, undefined, { numeric: true })\n)\n// Result: ['item1', 'item2', 'item10']  Correct!\n\n// Case insensitive with sensitivity: 'base'\n'Apple'.localeCompare('apple', undefined, { sensitivity: 'base' })\n// Result: 0 (equal)\n\nWhy this is useful in your sort function:\n\nHandles tag names that might have numbers: \"version1\", \"version10\", \"version2\"\nCase-insensitive: \"JavaScript\" and \"javascript\" sort together\nNatural sorting: Matches how humans expect things to be ordered\n\nThis is called \"natural sort order\" or \"human-friendly sorting\"! \n\nThis code applies the sort order direction (ascending or descending) to the comparison result.\n\nHow it works:\n\nconst multiplier = order === 'desc' ? -1 : 1;\n\nif (comparison !== 0) {\n    return comparison * multiplier;\n}\nBreaking it down:\n\n1. The comparison value:\n-1 means a  b (a comes after b)\n\n2. The multiplier:\n1 for ascending ('asc') - keeps the comparison as-is\n-1 for descending ('desc') - flips the comparison\n\n3. Multiplying:\ncomparison * 1 = no change (ascending order)\ncomparison * -1 = reverses the order (descending)\n\nExamples:\n\nAscending (multiplier = 1):\n// Comparing numbers: 5 vs 10\ncomparison = 5 - 10 = -5 (negative, so 5 comes first)\nreturn -5 * 1 = -5  //  5 before 10 (ascending)\n\nDescending (multiplier = -1):\n// Comparing numbers: 5 vs 10\ncomparison = 5 - 10 = -5 (negative, so normally 5 comes first)\nreturn -5 * -1 = 5   //  10 before 5 (descending, reversed!)\n\nWhy check !== 0 first?\n\nIf comparison === 0, the values are equal for this property, so we need to check the next property in the props array. Only when we find a difference (!== 0) do we return and stop comparing.\n\nExample with multiple properties:\nsortObjArray(tags, ['count', 'tag'], ['desc', 'asc']);\n\n// If two items have the same count (comparison = 0):\n// - Don't return yet\n// - Continue to next iteration and compare 'tag'\n// - If tags are different, return that comparison\n\nThis is how multi-level sorting works - it only uses the next property as a \"tie-breaker\"! ",
        "locale": "en",
        "slug": "technical-posts/human-friendly-sorting-in-typescript",
        "folder": ""
    },
    {
        "title": "A pretty CSS checkbox",
        "description": "A pretty CSS checkbox",
        "date_created": "2025-08-07T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "See this CodePen\nhttps://codepen.io/rogerpence/pen/QwjMGEK\n\nOf course! This is a classic and very useful CSS technique that doesn't require any JavaScript.\n\nThe key is to use the :checked pseudo-class on the checkbox input, combined with a sibling selector (+ or ~) to target the div.\n\nHere is the breakdown, from the simplest method to a more advanced, styled example.\n\nThe Core Concept\n\n:checked Pseudo-class: This CSS selector applies styles to a checkbox (or radio button) only when it is in the checked state.\nAdjacent Sibling Selector (+): This selector targets an element that is the very next sibling in the HTML document.\nGeneral Sibling Selector (~): This selector is more flexible. It targets any sibling that comes after the first element in the HTML.\n\nFor this to work, the div you want to style must come after the `` tag in your HTML.\n\nMethod 1: The Basic Implementation (Using +)\n\nThis is the simplest way. The div must be placed immediately after the checkbox.\n\nHTML\n\n\nToggle Background\n\n\n\n  The background of this box will change.\n\nCSS\n/* 1. Default style for the div */\n.my-box {\n  background-color: #e0e0e0; /* Default light gray background */\n  padding: 20px;\n  border: 1px solid #ccc;\n  border-radius: 5px;\n  margin-top: 10px;\n  /* Add a smooth transition for the color change */\n  transition: background-color 0.4s ease;\n}\n\n/* 2. The magic rule: when the checkbox is checked... */\n#bg-toggle:checked + .my-box {\n  /* ...target the adjacent .my-box div and change its background */\n  background-color: #a7d7c5; /* A nice mint green */\n}\nHow it works: When the checkbox with id=\"bg-toggle\" is checked, the rule #bg-toggle:checked + .my-box becomes active and overrides the default background color. The transition property makes the color change smooth.\n\nMethod 2: More Flexible (Using ~)\n\nWhat if your label is between the checkbox and the div? The + selector won't work anymore. This is where the general sibling selector (~) is perfect.\n\nHTML\n\n\n\nToggle Background (Flexible)\n\n\n  This works even with other elements in between.\n\nCSS\n.my-box-2 {\n  background-color: #f7f5dd; /* Default cream background */\n  padding: 20px;\n  border: 1px solid #ccc;\n  transition: all 0.4s ease;\n}\n\n/* Use the general sibling selector (~) instead of (+) */\n#bg-toggle-2:checked ~ .my-box-2 {\n  background-color: #fca3b0; /* A nice salmon pink */\n  color: white;\n  border-color: #e48391;\n}\nHow it works: The ~ selector finds any .my-box-2 that is a sibling of #bg-toggle-2 and comes after it, regardless of what's in between.\n\nFull \"Toggle Switch\" Example\n\nLet's combine these concepts to create a modern-looking toggle switch that controls the background, hiding the ugly default checkbox.\n\nHTML\n\n  \n  \n\n  \n  \n\n  Turn on the lights!\n\n\n\n\n  The room is dark...\n\nCSS\n/* Hide the actual checkbox */\n.toggle-checkbox {\n  display: none;\n}\n\n/* Style the label to look like a switch */\n.toggle-label {\n  display: inline-block;\n  width: 50px;\n  height: 28px;\n  background-color: #ccc;\n  border-radius: 14px;\n  cursor: pointer;\n  position: relative;\n  transition: background-color 0.3s ease;\n}\n\n/* Style the \"nub\" of the switch */\n.toggle-label::after {\n  content: '';\n  position: absolute;\n  width: 24px;\n  height: 24px;\n  border-radius: 50%;\n  background-color: white;\n  top: 2px;\n  left: 2px;\n  transition: transform 0.3s ease;\n}\n\n/* Container for the toggle switch and text */\n.toggle-container {\n  display: flex;\n  align-items: center;\n  gap: 10px;\n  margin-bottom: 20px;\n}\n\n/* --- The Toggling Logic --- */\n\n/* When the checkbox is checked, move the nub on the label */\n.toggle-checkbox:checked + .toggle-label {\n  background-color: #4CAF50; /* Green */\n}\n\n.toggle-checkbox:checked + .toggle-label::after {\n  transform: translateX(22px);\n}\n\n/* When the checkbox is checked, change the background of the div */\n#room {\n  background-color: #2c3e50; /* Dark blue \"off\" state */\n  color: #7f8c8d;\n  padding: 30px;\n  text-align: center;\n  font-size: 1.5em;\n  border-radius: 8px;\n  transition: all 0.5s ease-in-out;\n}\n\n/* Here's the magic for the div's background */\n.toggle-checkbox:checked ~ #room {\n  background-color: #f1c40f; /* Bright yellow \"on\" state */\n  color: #333;\n}\n\n/* Also change the text inside the div */\n.toggle-checkbox:checked ~ #room p::after {\n  content: ' Now it\\'s bright!';\n}\n\nKey Takeaway\n\nTo toggle a div's background with a checkbox using only CSS:\nPlace the div after the input[type=\"checkbox\"] in your HTML.\nUse the CSS selector input:checked + div (for immediate siblings) or input:checked ~ div (for any following sibling).\nAdd a transition property to the div for a smooth visual effect.\n\nThat first take didn't work! this one does.\n\nYou are absolutely right to point that out, and my apologies for the error in the third example. You've caught a fantastic and subtle but critical detail about how CSS selectors work. Thank you for the sharp eye!\n\nLet me explain why method #2 might have failed for you and why method #3 was definitely wrong as I wrote it. Then I will provide the corrected code.\n\nThe Crucial Rule: The Sibling Relationship\n\nThe + (adjacent sibling) and ~ (general sibling) selectors only work on siblings.\n\nSiblings are elements that share the same immediate parent.\n\n\n \n  \n  Toggle\n  This will work.\n\n\n\n\n  \n     \n  \n  This will NOT work with a selector like #toggle:checked ~ div\n\n\nWith that in mind, let's re-examine the methods.\n\nMethod 1: Correct (As you confirmed)\n\nThis works because the .my-box div is the direct next sibling of the checkbox.\n\n#bg-toggle:checked + .my-box { ... }\n\nMethod 2: Why It Might Have Failed For You\n\nThis method uses the general sibling selector (~), which is also correct. It should work perfectly as long as the sibling rule is followed.\n\nHTML (Correct Structure)\n\nToggle Background (Flexible)\n\n  This works even with other elements in between.\n\nCSS\n/* This selector targets ANY .my-box-2 that comes after the checked input */\n#bg-toggle-2:checked ~ .my-box-2 {\n  background-color: #fca3b0;\n}\n\nThe most common reason this would fail is if you accidentally broke the sibling relationship, often by wrapping the input inside its label:\n\nIncorrect HTML Structure (A Common Mistake)\n\n\n   \n  Toggle Background (Flexible)\n\n\n\n\n  My background won't change!\n\nIn the \"incorrect\" example, the CSS selector #bg-toggle-2:checked ~ .my-box-2 fails because the div is no longer a sibling of the input.\n\nMethod 3: The Mistake in My Original Example (and the Fix)\n\nThis is where my original example was flawed. I wrapped the input and label in a div, breaking the sibling relationship with the target div.\n\nMy Incorrect HTML\n\n\n  \n  \n  Turn on the lights!\n\n\n\n\n  The room is dark...\n\nMy original CSS (.toggle-checkbox:checked ~ #room) could not work because the selector can't \"escape\" the .toggle-container to find its sibling.\n\nThe Corrected, Modern Solution (using :has())\n\nThe modern and most powerful way to fix this is with the :has() pseudo-class. It allows you to style a parent or a sibling based on a condition inside the parent.\n\nBrowser support for :has() is now excellent across all modern browsers.\n\nHTML (The structure can stay the same and semantically clean)\n\n  \n  \n  Turn on the lights!\n\n\n\n  The room is dark...\n\nCorrected CSS using :has()\n/* Style the room by default */\n#room {\n  background-color: #2c3e50; /* Dark */\n  color: #7f8c8d;\n  padding: 30px;\n  border-radius: 8px;\n  transition: all 0.5s ease-in-out;\n}\n\n/*\n  THE NEW RULE:\n  Find the .toggle-container that :has() a .toggle-checkbox:checked inside it.\n  Then, use the sibling selector (~) to target the #room div that follows.\n*/\n.toggle-container:has(.toggle-checkbox:checked) ~ #room {\n  background-color: #f1c40f; /* Bright yellow */\n  color: #333;\n}\n\n.toggle-container:has(.toggle-checkbox:checked) ~ #room p::after {\n  content: ' Now it\\'s bright!';\n}\n\n\n/* --- The styles for the toggle switch itself (these are unchanged and correct) --- */\n.toggle-checkbox { display: none; }\n.toggle-container { display: flex; align-items: center; gap: 10px; margin-bottom: 20px; }\n.toggle-label { display: inline-block; width: 50px; height: 28px; background-color: #ccc; border-radius: 14px; cursor: pointer; position: relative; transition: background-color 0.3s ease; }\n.toggle-label::after { content: ''; position: absolute; width: 24px; height: 24px; border-radius: 50%; background-color: white; top: 2px; left: 2px; transition: transform 0.3s ease; }\n.toggle-checkbox:checked + .toggle-label { background-color: #4CAF50; }\n.toggle-checkbox:checked + .toggle-label::after { transform: translateX(22px); }\n\nSummary of the Fix\n\nThe Rule: The element you are styling (div) must be a sibling of and come after the element you are checking (input).\nThe Problem: In complex layouts, you often want to group the input/label in a container, which breaks the sibling relationship.\nThe Solution: Use the :has() pseudo-class to select the container when the checkbox inside it is checked, and then use the ~ selector from there to target the div. This is the most robust and modern CSS-only approach.",
        "locale": "en",
        "slug": "technical-posts/pretty-css-checkbox-toggle",
        "folder": ""
    },
    {
        "title": "GoTo Webinar Register Attendee",
        "description": "GoTo Webinar Register Attendee",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "gotowebinar"
        ],
        "content": "[[GoTo Webinar refresh API access]]\n\nThese are rp@asna.com keys.\n\nOrganizer key\n\n200000000000313229\n\nWebinar key (get from GoTo Webinar dashboard)\n\n5372803598377463127\n\nhttps://api.getgo.com/G2W/rest/v2/organizers/{organizerKey}/webinars/{webinarKey}/registrants\n\nhttps://api.getgo.com/G2W/rest/v2/organizers/200000000000313229/webinars/5372803598377463127/registrants\n\n![[image-10.png]]\n\nThe Authorization/Bearer above is the renewed access code.\n\n![[image-13.png]]\n\nThe data needs to be sent as Json!\n\n![[image-11.png]]\n\n![[image-12.png]]\n\n{\n  \"registrantKey\": 3426482200061586524,\n  \"joinUrl\": \"https://global.gotowebinar.com/join/5372803598377463127/729896629\",\n  \"status\": \"APPROVED\",\n  \"asset\": true\n}",
        "locale": "en",
        "slug": "technical-posts/goto-webinar-register-attendee",
        "folder": ""
    },
    {
        "title": "Find large files with the Windows Explorer",
        "description": "Find large files with the Windows Explorer",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows"
        ],
        "content": "![[find large files in Windows.png|700]]\n\nPut one of these in the Explorer search input:\n\nsize:large\nsize:huge\nsize:gigantic",
        "locale": "en",
        "slug": "technical-posts/find-large-files-in-windows",
        "folder": ""
    },
    {
        "title": "Kinisis keyboard configuration",
        "description": "Kinisis keyboard configuration",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "https://gaming.kinesis-ergo.com/wp-content/uploads/2019/04/Freestyle-Edge-RGB-Quick-Start-Guide-v2.12.19.pdf\n\n![image-30.png|800]]\n\nTo map the Kinesis drive, press SmartSet key and F8\n\nThis toggles the Kinesis drive.\n\n![[Kinesis keyboard notes.png|350]]\n\n| Description          | Keystroke                         | Notes                          |\n| -------------------- | --------------------------------- | ---------------------------------- |\n| Toggle keyboard lock | SmartSet + Shift + L              |                                    |\n| Toggle v-drive       | SmartSet + F8                     |                                    |\n| Hard Reset           | SmartSet + F12 + plug keyboard in | Be sure to copy layout1.txt first! |\n| Load Profile 1 - 3   | Profile key                       | Profile key rotates 1 - 3          |\nIf the Program Lock is enabled, the indicator LEDs flash 4 times.When you disable Program Lock, the indicator LEDs flashes twice.\n\nThe \"Layout\" key is also called the \"Profile key\"\n\nPressing \"SmartKey\" key and the \"Layout/Profile\" key to reload the keyboard configuration.\n\nThe Kinesis Keyboard PDF is at:\nhttps://drive.google.com/file/d/1sfS09DCbGbQAg5EmxzOrjvCxX2njaw9d/view?usp=drive_link\n\nMy current keyboard configuration is available on the \"seagate-4tb-desktop\" drive at:\n\nE:\\backup\\kinesis-keyboard\n\nMy layout is in\n\nx:\\layouts\\layout1.txt\n\nwhere x: is the Kinesis virtual drive (toggled with Kinesis SmartSet Key and F8). Press the {Profile} key until the left light is on to load the #1 layout.\n\nlayout1.txt contents as of 2024-10-08:\n\n|               |            |\n| ------------- | ---------- |\n| Function keys |            |\n| h             | ``         |\n| j             | []         |\n| k             | {}         |\n| l             | ()         |\n| y             | $          |\n| :             | -          |\n| '             | \\_         |\n| 7             | {home}     |\n| 8             | {end}      |\n|               |            |\n| Kinesis keys  |            |\n| 1             | copy       |\n| 2             | paste      |\n| 3             | ctrl/E     |\n| 4             | upaste     |\n| 5             | code fence |\n\nPowderfinger = 2025-05-10\n\n[caps]>[fnshf]\nfn [caps]>[fnshf]\n \nPairs  \nj = {}\nk = []\nl = ()\nh = ``\nSingles \nu = +\ny = $\ni = * \n: = dash\n' = underscore\np = equal\n\nfn {7}>{x1}{s3}{home}\nfn {8}>{x1}{s3}{end}\n\nfn {h}>{s5}{x1}{tilde}{tilde}{lft}\nfn {j}>{x1}{s3}{-lshft}{obrk}{+lshft}{-lshft}{cbrk}{+lshft}{lft}\nfn {k}>{x1}{s3}{obrk}{cbrk}{lft}\nfn {l}>{x1}{s3}{-lshft}{9}{+lshft}{-lshft}{0}{+lshft}{lft}\nfn [colon]>-\nfn {apos}>{x1}{s6}{-lshft}{hyph}{+lshft}\n\nfn {y}>{x1}{s3}{-lshft}{4}{+lshft}\nfn {u}>{x1}{s3}{-lshft}{=}{+lshft}\nfn {i}>{x1}{s3}{-lshft}{8}{+lshft}\nfn {o}>{x1}{s3}{-lctrl}{s}{+lctrl}\nfn {p}>{x1}{s3}{=}\n\nfn {bspc}>{s9}{x1}{-lshft}f{+lshft}u{-lshft}Z{+lshft}{-lshft}Z{+lshft}y{-lshft}N{+lshft}u{d020}u{-lshft}S{+lshft}o{d020}o{-lshft}1{+lshft}{-lshft}4{+lshft}1{d020}7{d020}3{d020}1{d020}{ent}\n\n\nfn {m}>{x1}{s3}{-lctrl}{a}{+lctrl}\nfn [n]>[lwin]\n\nKinesis keys\n\nEscape \n[hk0]>[esc]\n\nCopy \n{hk1}>{x1}{-lctrl}{c}{+lctrl}\n\nPaste\n{hk2}>{x1}{-lctrl}{v}{+lctrl}\n\nCtrl/E (toggle entry mode in Obsidian\n{hk3}>{x1}{-lctrl}{e}{+lctrl}\n\nUnformated paste\n{hk4}>{x1}{s6}{x1}{-lctrl}{-lshft}{v}{+lshft}{+lctrl}\n\nCode fences - ready for input\nh5 = Obsidian\nh6 = other\n{hk5}>{s3}{x1}{tilde}{tilde}{tilde}{ent}\n{hk6}>{s6}{x1}{tilde}{tilde}{tilde}{ent}{d125}{tilde}{tilde}{tilde}{home}{ent}{up}\n\nKeystrokes mapped\n\n![[Kinesis keyboard notes-2.png|1200](https://gaming.kinesis-ergo.com/wp-content/uploads/2023/05/Freestyle-Edge-Direct-Programming-Guide-Expansion-Pack-2-May-24-2023.pdf)\n\n\n| Command                                  | Description                       |\n| ---------------------------------------- | --------------------------------- |\n| [caps]>[fnshf]                           | Caps key = Kinesis Fn key         |\n| fn [caps]>[fnshf]                        | Thi probably shouldn't be here!  |\n| fn [i]>[up]                              | Fn + i = up                       |\n| fn [h]>[home]                            | Fn + h = home                     |\n| fn [j]>[lft]                             | Fn + j = left                     |\n| fn [k]>[dwn]                             | Fn + k = down                     |\n| fn [l]>[rght]                            | Fn + l = right                    |\n| fn [n]>[lwin]                            | Fn + n = left Windows key         |\n| fn [colon]>-                             | Fn + ; = - (dash)                 |\n| fn {apos}>{x1}{s6}{-lshft}{hyph}{+lshft} | Fn + ' = \\_ (underscore)          |\n| [hk0]>[esc]                              | H0 (Top-left key) = Escape        |\n| {hk1}>{x1}{-lctrl}{c}{+lctrl}            | H1 = copy                         |\n| {hk2}>{x1}{-lctrl}{v}{+lctrl}            | H2 = paste                        |\n| {hk3}>{x1}{-lctrl}{e}{+lctrl}            | H3 = Ctrl/E (mostly for Obsidian) |\n| {hk4}>{x1}                               | H4 = unformatted paste            |\n| fn {bspc}*(see below)                    | Fn + backspace = password         |\n\nEnter password and press enter.\n\nfn {bspc}>{s9}{x1}{-lshft}f{+lshft}u{-lshft}Z{+lshft}{-lshft}Z{+lshft}y{-lshft}N{+lshft}u{d020}u{-lshft}S{+lshft}o{d020}o{-lshft}1{+lshft}{-lshft}4{+lshft}1{d020}7{d020}3{d020}1{d020}{ent}\n\nThis reassignment is especially interesting\n\nfn {apos}>{x1}{s6}{-lshft}{hyph}{+lshft}\n\nIt translates to:\n\n| Token       | Description                          |\n| ----------- | ------------------------------------ |\n| fn {apos} > | Assign the {apos} key                |\n| {x1}        | {x1} limit playback to one time      |\n| {s6}        | {s6} speed                           |\n| {-lshft}    | Left shift key down                  |\n| {hyph}      | Type shifted {hyph} key (underscore) |\n| {+lshft}    | Left shift key up                    |\n\nfn {apos}>{x1}{s6}{-lshft}{hyph}{+lshft}\n{hk1}>{x1}{-lctrl}{c}{+lctrl}\n{hk2}>{x1}{-lctrl}{v}{+lctrl}\n{hk3}>{x1}{s6}{x1}{-lctrl}{-lshft}{v}{+lctrl}{+lshft}\n\nThe {s6} speed value can vary, but for what I do six is a good value for the speed. The programming doc referenced above goes into some detail on the tokens used here.\n\nClearing an accidentally remapped key.\n\nThe other day I had this issue with my Kinesis keyboard:\n\nMy Kinesis keyboard has worked like a champ since I bought it. Today, all of sudden, the F10 key started doing weird things. Pressing it causes whatever is on the clipboard to spray across the screen. I first noticed this pressing F10 while debugging code in Visual Studio. After ending VS, the keyboard was permanently in Caps Lock mode and several keys (arrow keys and other nav keys) wouldn't respond. To clear things up I had to reboot the computer. I can repeat this issue. I tested pressing F10 with Teams, Notepad, and VS Code and F10 goes nuts there, too. I did confirm that the PC works fine with other keyboards--no issue with F10 there. I can make a video of this if that would help. thank you.\n\nTech support replied:\n\nIt seems like you programmed a macro or two on accident. Lets reset the key to make sure you didnt program anything to it by accident.\n\n\nRemap it back to itself (Unmap a key):\n\nTap {Remap}\nTap {F10} twice.\nDone! Test.\nErase any macro:\n\nTap {Macro}\nTap {F10}\nTap {Macro}\nDone! Test.\n\nNote: I tried the soft reset below. It worked, but it zapped my config file! I had to restore it from my backup (see above). I probably should have done the steps above.\n\nOr, try a soft reset. A soft reset will erase the active profile to its default state and remove all programming. [SmartSet+Shift+F12] will soft reset the active profile.\n\nHold {SmartSet}\nHold {Shift}\nTap {12}\nRelease all keys.\nIndicator LEDs, and the LED above the SmartSet key will flash four times to indicate the profile has been reset.\n",
        "locale": "en",
        "slug": "technical-posts/kinesis-keyboard-notes",
        "folder": ""
    },
    {
        "title": "Protected routes in Sveltekit",
        "description": "Protected routes in Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "From a thread on Discord]\nhooks.server.[js will run before any page is loaded (any request, but not relevant now)\nthen you can get the session id from cookies and get the current session via that, then once you have the session you can check the url and if the person with a certain session is accessing a certain url and you don't want that to happen you can redirect that user\n\nThis is a pattern I often follow:\n\nconst PROTECTED_ROUTES = [..., ..., ...];\n\nexport const handle: Handle = async ({ event, resolve }) => {\n    const url = event.url.pathname;\n    const session = getSession(event.request);\n    PROTECTED_ROUTES.forEach(route => {\n        if (url.startsWith(route) && !session) {\n          // Non logged in user trying to access the protected route\n      }\n    });\n    return await resolve(event);\n}",
        "locale": "en",
        "slug": "technical-posts/protected-routes",
        "folder": ""
    },
    {
        "title": "Obsidian dataviewjs",
        "description": "Obsidian dataviewjs",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css",
            "dataviewjs"
        ],
        "content": "TABLE\n\tdurationformat((date(now) - file.mtime), \"d\") as \"Days since edited\",\n    dateformat(file.mtime, \"MM-dd\") AS \"Edited on\"\n\nFROM \"css\"\nWHERE file.name != \"CSS Main\"\nSORT date(now) - file.mtime",
        "locale": "en",
        "slug": "technical-posts/css-main",
        "folder": ""
    },
    {
        "title": "HTML snippets",
        "description": "HTML snippets",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "front-end"
        ],
        "content": "details/summary tags\n\nCause opened details element to close when another opens by giving all details elements the same name attribute value.\n\n",
        "locale": "en",
        "slug": "technical-posts/html-snippets",
        "folder": ""
    },
    {
        "title": "CSS :is pseudo selector",
        "description": "CSS :is pseudo selector",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Example\n\nheader :is(h1, a) {\n\t...\n}\n\nheader: This selects any HTML element with the tag name ``.\n\n  (Space): This is the descendant combinator. It means the selector that follows must match an element that is inside (a descendant of) the element matched by the selector before it. It doesn't have to be a direct child; it can be nested multiple levels deep.\n\n:is(h1, a): This is the :is() pseudo-class. It takes a list of selectors as arguments and matches any element that can be selected by any of the selectors in that list. It essentially acts like an \"OR\" condition for the element it's applied to.\n    h1: Selects any `` element.\n    a: Selects any `` (anchor/link) element.\n\nPutting it all together:\n\nThe selector header :is(h1, a) selects:\n\nAny ` element that is a descendant (anywhere inside) of a ` element.\nOR\nAny ` element that is a descendant (anywhere inside) of a ` element.\n\nIn simpler terms: It finds all ` tags and all  tags that are located somewhere within a ` tag.\n\nExample:\n\n\n    Main Title\n    \n    \n        \n            Home\n            \n            About\n            \n        \n    \n    Some text inline link\n     is Selected -->\n\n\n\n    Another Title\n    ) -->\n    Another Link\n    ) -->\n\n\nIn this example, the ` inside the  and all three  tags inside the  would be selected by header :is(h1, a). The  and  inside the ` element would not be selected.\n\nWhy use :is()?\n\nWithout :is(), you would have to write this as:\n\nheader h1,\nheader a {\n    /* styles */\n}\n\n:is() makes the selector shorter, more readable, and easier to maintain, especially when dealing with more complex descendant paths or longer lists of potential elements.\n\nWhat is the difference between :is and :where?\n\nThe difference between :is() and :where() is subtle but crucial, and it boils down entirely to specificity. They function almost identically in terms ofwhich elements they match.\n\nFunctionally, they do the same thing:\n\nBoth take a list of selectors as arguments.\nBoth match any element that can be selected by any of the selectors in that list (like an \"OR\" condition).\n\nThe key difference lies in how they contribute to the overall specificity of the selector:\n\n:is():\n\n    The specificity of the :is() pseudo-class itself is replaced by the specificity of its most specific argument in the selector list.\n    Think: It adopts the highest specificity from within its list.\n    Consequence: Using :is() can potentially increase the overall specificity of your rule, making it harder to override later if one of its arguments is highly specific (like an ID).\n\n:where():\n    The specificity of the :where() pseudo-class (and its arguments) is always zero (0).\n    Think: It contributes nothing to the specificity calculation, regardless of how specific its arguments are.\n    Consequence: Using :where() allows you to group selectors without adding any specificity. This makes the rule very easy to override and is ideal for setting base styles or defaults.\n\nAnalogy:\n\nImagine you have two groups of people (:is() and :where()) applying for a priority pass.\n\nThe :is() group gets a priority level equal to the highest priority member within their group. If they have a VIP (#id) with them, the whole group gets VIP priority.\nThe :where() group always gets the lowest priority (zero), no matter who is in their group, even if they have VIPs with them.\n\nExample:\n\nLet's look at how specificity is calculated:\n\n/* --- Using :is() --- */\n\n/* Selects any h1, h2, or element with class .title inside a  */\n/* Specificity calculation: */\n/* section (0,0,1) + :is(h1(0,0,1), h2(0,0,1), .title(0,1,0)) */\n/* :is() takes the specificity of .title (0,1,0) because it's the highest */\n/* Total Specificity: (0,1,1) - same as 'section .title' */\nsection :is(h1, h2, .title) {\n    color: blue;\n}\n\n/* Selects any h1, h2, or element with ID #main-title inside a  */\n/* Specificity calculation: */\n/* section (0,0,1) + :is(h1(0,0,1), h2(0,0,1), #main-title(1,0,0)) */\n/* :is() takes the specificity of #main-title (1,0,0) */\n/* Total Specificity: (1,0,1) - same as 'section #main-title' */\nsection :is(h1, h2, #main-title) {\n    font-style: italic;\n}\n\n/* --- Using :where() --- */\n\n/* Selects any h1, h2, or element with class .title inside a  */\n/* Specificity calculation: */\n/* section (0,0,1) + :where(h1, h2, .title) */\n/* :where() and its arguments contribute ZERO specificity (0,0,0) */\n/* Total Specificity: (0,0,1) - same as just 'section' */\nsection :where(h1, h2, .title) {\n    font-weight: normal; /* Easy to override */\n}\n\n/* Selects any h1, h2, or element with ID #main-title inside a  */\n/* Specificity calculation: */\n/* section (0,0,1) + :where(h1, h2, #main-title) */\n/* :where() and its arguments contribute ZERO specificity (0,0,0) */\n/* Total Specificity: (0,0,1) - same as just 'section' */\nsection :where(h1, h2, #main-title) {\n    text-decoration: underline; /* Easy to override, even the ID rule */\n}\n\nWhen to Use Which:\n\nUse :is() when:\n\n    You are simply grouping selectors for brevity and want the natural specificity of those selectors to apply.\n    You intend for the rule to have a certain level of precedence based on its contents.\n\nUse :where() when:\n    You want to create low-specificity rules that are easily overridden.\n    You are writing baseline styles, resets (like setting default margins/paddings on headings), or theme defaults in a library/framework.\n    You want to avoid accidentally increasing specificity when grouping selectors.\n\nIn summary: is(A, B) selects A or B and has the specificity of the more specific one between A and B. where(A, B) selects A or B but always has zero specificity itself.",
        "locale": "en",
        "slug": "technical-posts/css-is-pseudo-selector",
        "folder": ""
    },
    {
        "title": "Git - using stash",
        "description": "Git - using stash",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "git"
        ],
        "content": "A brief guide to using git stash:\n\nWhat is git stash?\n\ngit stash temporarily shelves (or stashes) your uncommitted changes (both staged and unstaged) in your working directory. This lets you quickly switch context (e.g., to another branch, pull updates) without making a premature or messy commit.\n\nWhy Use It?\n\nQuickly switch branches: You're working on Feature A, but an urgent bug fix is needed on main. Stash your changes, switch to main, fix, then switch back and unstash.\nPull remote changes: You have local changes, but you need to git pull. Stashing avoids potential merge conflicts with your uncommitted work.\nClean working directory: Need to run tests or a build on a clean state without committing.\n\nCore Workflow:\n\nStashing Your Changes:\n\n    git stash: Stashes tracked files that are modified or staged. Untracked files are not stashed by default.\n    git stash save \"Your descriptive message\": Same as above, but adds a message to your stash, making it easier to identify later. (Recommended)\n\nViewing Your Stashes:\n\n    git stash list: Shows all your stashes, with the most recent at the top (e.g., stash@{0}, stash@{1}, etc.).\n\nApplying Your Stash:\n    git stash pop: Applies the most recent stash (stash@{0}) and then removes it from the stash list. Use this if you're confident you want the changes back and don't need the stash entry anymore.\n    git stash apply: Applies the most recent stash (stash@{0}) but keeps it in the stash list. Useful if you want to apply the same changes to multiple branches or just want to test them.\n    git stash pop stash@{2} or git stash apply stash@{2}: Applies a specific stash from the list (replace 2 with the desired index).\n\nUseful Options & Commands:\n\nStashing Untracked Files:\n    git stash -u or git stash --include-untracked: Stashes modified, staged, AND untracked files.\nStashing All Files (including ignored):\n    git stash -a or git stash --all: Stashes everything: modified, staged, untracked, AND ignored files. Use with caution.\nStashing Specific Files:\n    git stash --  : Stashes only the specified files/paths.\nViewing Stash Contents:\n    git stash show stash@{0}: Shows a summary of changes in the specified stash.\n    git stash show -p stash@{0}: Shows the full diff (patch) of the specified stash.\nCreating a Branch from a Stash:\n    git stash branch  [stash@{n}]: Creates a new branch based on the commit your stash was created from, applies the stash, and then drops the stash if successful. Great for turning stashed work into a proper feature branch.\nDeleting Stashes:\n    git stash drop [stash@{n}]: Deletes a specific stash (e.g., stash@{1}). If no stash is specified, it deletes the latest (stash@{0}). Use this if you've apply'd a stash and are sure you don't need it.\n    git stash clear: Deletes ALL stashes. Use with extreme caution!\n\nQuick Tips:\n\nAlways try to git stash save \"message\": Descriptive messages save you headaches later.\nDon't let stashes accumulate: They are temporary. If work is significant, commit it to a feature branch.\nStashes are local: They are not pushed to the remote repository with git push.\nConflicts can happen: If the branch has diverged significantly since you stashed, you might encounter conflicts when applying the stash. Resolve them like regular merge conflicts.\n\ngit stash is a powerful tool for managing work-in-progress. Use it wisely to keep your workflow clean and efficient!",
        "locale": "en",
        "slug": "technical-posts/git-using-stash",
        "folder": ""
    },
    {
        "title": "Pino Logger docs",
        "description": "Pino Logger docs",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-13T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "pino"
        ],
        "content": "A custom Pino-based logging utility for SvelteKit applications with PostgreSQL database integration.\n\nTable of Contents\nPino Logger Documentation\n  Table of Contents\n  Overview\n  Features\n  Installation\n  Configuration\n  Custom Log Levels\n  API Reference\n    Standard Logging Methods\n    Database Logging Methods\n  Usage Examples\n    Basic Logging\n    SvelteKit Integration\n    Download Tracking\n  Type Definitions\n  Error Handling\n  Best Practices\n  Performance Considerations\n  Troubleshooting\n\nOverview\n\nThis logger provides a structured logging solution that uses Pino for high-performance logging, supports custom log levels including a \"notice\" level, provides pretty-printing in development mode, integrates with PostgreSQL for persistent log storage, and includes specialized logging for download tracking.\n\nFeatures\n\nEnvironment-aware configuration: Pretty logging in development, JSON in production\nCustom log levels: Standard levels plus a custom \"notice\" level (severity 35)\nDatabase integration: Optional PostgreSQL logging with automatic error handling\nType-safe: Full TypeScript support with custom level types\nStructured logging: Consistent message and data format across all methods\n\nInstallation\n\nEnsure you have the required dependencies:\n\nnpm install pino pino-pretty pg\npnpm add pino pino-pretty pg\n\nConfiguration\n\nThe logger is configured automatically based on the environment:\n\nDevelopment Mode:\nLog level: debug\nOutput: Pretty-printed with colors via pino-pretty\nTimestamp: ISO 8601 format\n\nProduction Mode:\nLog level: info\nOutput: JSON format\nTimestamp: ISO 8601 format\n\nCustom Log Levels\n\nThe logger includes a custom notice level with severity 35, positioned between info (30) and warn (40):\n\ntype CustomLevels = Level | 'notice';\n\nAPI Reference\n\nStandard Logging Methods\n\nAll standard methods accept a message and optional data object.\n\nlogger.info(message: string, data?: any)\n\nLogs informational messages.\n\nlogger.info('User logged in', { userId: 123, username: 'john' });\n\nlogger.notice(message: string, data?: any)\n\nLogs important notices that are not warnings.\n\nlogger.notice('Rate limit approaching', { currentRate: 95, threshold: 100 });\n\nlogger.error(message: string, data?: any)\n\nLogs error messages.\n\nlogger.error('Database connection failed', { error: err.message });\n\nlogger.warn(message: string, data?: any)\n\nLogs warning messages.\n\nlogger.warn('Deprecated API usage', { endpoint: '/old-api', userId: 456 });\n\nlogger.debug(message: string, data?: any)\n\nLogs debug information (only in development by default).\n\nlogger.debug('Processing request', { requestId: 'abc123', params });\n\nDatabase Logging Methods\n\nlogger.db(pool: Pool, message: string, data?: any): Promise\n\nLogs to both console and database simultaneously.\n\nimport { pool } from '$lib/db';\n\nawait logger.db(pool, 'Payment processed', { \n  amount: 99.99, \n  currency: 'USD',\n  orderId: 'order_123' \n});\n\nDatabase Schema Requirements:\nCREATE TABLE app_logs (\n  id SERIAL PRIMARY KEY,\n  level VARCHAR(20) NOT NULL,\n  message TEXT NOT NULL,\n  data JSONB,\n  timestamp TIMESTAMP NOT NULL\n);\n\nlogger.download(pool: Pool, name: string, company: string, country: string, email_address: string, product: string, family: string): Promise\n\nSpecialized method for logging product downloads.\n\nawait logger.download(\n  pool,\n  'John Doe',\n  'Acme Corp',\n  'USA',\n  'john@acme.com',\n  'Product-X',\n  'Electronics'\n);\n\nDatabase Schema Requirements:\nCREATE TABLE downloads_log (\n  id SERIAL PRIMARY KEY,\n  name VARCHAR(255),\n  company VARCHAR(255),\n  country VARCHAR(100),\n  email_address VARCHAR(255),\n  product VARCHAR(255),\n  family VARCHAR(255),\n  timestamp TIMESTAMP DEFAULT NOW()\n);\n\nUsage Examples\n\nBasic Logging\n\nimport logger from '$lib/pino-logger';\n\n// Simple info log\nlogger.info('Application started');\n\n// Log with context data\nlogger.error('Failed to fetch user', { userId: 123, error: 'Not found' });\n\n// Notice level\nlogger.notice('System maintenance scheduled', { date: '2025-12-20' });\n\nSvelteKit Integration\n\nIn hooks.server.ts:\n\nimport type { Handle } from '@sveltejs/kit';\nimport logger from '$lib/pino-logger';\nimport { pool } from '$lib/db';\n\nexport const handle: Handle = async ({ event, resolve }) => {\n  event.locals.logger = logger;\n  event.locals.pool = pool;\n\n  logger.info('Request received', {\n    path: event.url.pathname,\n    method: event.request.method\n  });\n\n  const response = await resolve(event);\n  \n  return response;\n};\n\nIn +page.server.ts:\n\nimport type { PageServerLoad } from './$types';\n\nexport const load: PageServerLoad = async ({ locals }) => {\n  locals.logger.info('Page loaded', { page: 'home' });\n  \n  // With database logging\n  await locals.logger.db(\n    locals.pool,\n    'User viewed homepage',\n    { sessionId: locals.session?.id }\n  );\n  \n  return {};\n};\n\nIn +server.ts (API Routes):\n\nimport type { RequestHandler } from './$types';\nimport { json } from '@sveltejs/kit';\n\nexport const POST: RequestHandler = async ({ request, locals }) => {\n  try {\n    const data = await request.json();\n    \n    await locals.logger.db(\n      locals.pool,\n      'API request processed',\n      { endpoint: '/api/users', data }\n    );\n    \n    return json({ success: true });\n  } catch (error) {\n    locals.logger.error('API error', { error });\n    return json({ error: 'Internal error' }, { status: 500 });\n  }\n};\n\nDownload Tracking\n\nimport type { Actions } from './$types';\n\nexport const actions = {\n  download: async ({ request, locals }) => {\n    const formData = await request.formData();\n    \n    await locals.logger.download(\n      locals.pool,\n      formData.get('name'),\n      formData.get('company'),\n      formData.get('country'),\n      formData.get('email'),\n      formData.get('product'),\n      formData.get('family')\n    );\n    \n    return { success: true };\n  }\n} satisfies Actions;\n\nType Definitions\n\nimport type { CustomLevels } from '$lib/pino-logger';\n\ninterface Logger {\n  info: (message: string, data?: any) => void;\n  notice: (message: string, data?: any) => void;\n  error: (message: string, data?: any) => void;\n  warn: (message: string, data?: any) => void;\n  debug: (message: string, data?: any) => void;\n  db: (pool: Pool, message: string, data?: any) => Promise;\n  download: (\n    pool: Pool,\n    name: string,\n    company: string,\n    country: string,\n    email_address: string,\n    product: string,\n    family: string\n  ) => Promise;\n}\n\nError Handling\n\nDatabase logging failures are caught and logged to the console but do not throw errors. If DB logging fails, the error is logged but the application continues normally:\n\nawait logger.db(pool, 'Important event', { data });\n// Application continues even if DB insert fails\n\nBest Practices\n\nUse appropriate log levels: Use debug for detailed debugging info, info for general informational messages, notice for important notices, warn for warning conditions, and error for error conditions.\n\nInclude context data: Always provide relevant context with your logs.\n   // Good\n   logger.error('Login failed', { username, ip, attempt });\n   \n   // Less useful\n   logger.error('Login failed');\n\nAvoid logging sensitive data: Never log passwords, API keys, or other sensitive information.\n   // Bad\n   logger.info('User created', { password: '...' });\n   \n   // Good\n   logger.info('User created', { userId, email });\n\nUse database logging for critical events: Persist important events that you may need to query or analyze later.\n   await logger.db(pool, 'Payment received', { orderId, amount });\n\nPerformance Considerations\n\nPino is one of the fastest Node.js loggers available\nDatabase logging is asynchronous and non-blocking\nPretty printing is automatically disabled in production for better performance\nConsider implementing log rotation strategies for database tables to manage storage\n\nTroubleshooting\n\nLogs not appearing in development:\nVerify dev environment variable is true\nCheck that log level is debug or lower\n\nDatabase logging fails silently:\nVerify database connection pool is valid\nCheck table schema matches required structure\nReview console for error messages from the logger\n\nTypeScript errors with custom levels:\nImport the CustomLevels type: import type { CustomLevels } from '$lib/pino-logger'\n\n\nLicense: MIT",
        "locale": "en",
        "slug": "technical-posts/pinno-logger-docs",
        "folder": ""
    },
    {
        "title": "Missing or incorrect keys when parsing markdown front matter",
        "description": "Missing or incorrect keys when parsing markdown front matter",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "zod",
            "markdown"
        ],
        "content": "This is a Zod schema for markdown blog post. Using this, you can ensure that a markdown document is type safe.\nNote how the schema ends with .strict(); this ensures that the schema doesn't have extra or missing keys (it can have optional keys marked with the .optional() method).\n\nimport { z } from 'zod';\n\nconst dateFormatString = z\n  .string()\n  .regex(/^\\d{4}-\\d{2}-\\d{2}$/, {message: \"Date must be in the format YYYY-MM-DD\"})\n\nexport const RPBlogSchema = z.object({\n    title: z.string().min(1, 'Title cannot be empty'),\n    description: z.string().min(1, 'Description cannot be empty'),\n    date_created: 2025-01-05 12:00\ndate_updated: 2025-01-05 12:00\ndate_published:\ntags: z.array(z.string()).min(1, 'At least one tag is required'),\n    date_added: z.union([dateFormatString,  z.date(), z.null()]),\n    date_updated: z.union([dateFormatString, z.date(), z.null()]),\n    date_published: z.union([dateFormatString, z.date(), z.null()]),\n    pinned: z.boolean()\n}).strict();\n\nexport type RPBlogPost = z.infer;\n\nAt first, I missed the .strict() method and added my own logic to look for extra or missing keys. The code to do this is included in an early version of the getMarkdownObject() method. I'm saving the code here mostly just for future reference.\n\nexport const getMarkdownObject = (frontMatter: string[], content: string[], schema: z.ZodType): MarkdownResult => {\n  try {\n    const parsed: any = yaml.load(frontMatter.join('\\n'));\n\n    // If the schema is strict, you don't need to check for missing or extra fields.\n\n\t// Code not needed: start\n\t// const objectFields = Object.keys(parsed);\n    // const schemaShape = schema.shape as { [k: string]: z.ZodTypeAny };\n    // const requiredFields = Object.entries(schemaShape)\n    //     .filter(([_, type]) => !type.isOptional())\n    //      .map(([field]) => field);\n\n    // // checking array for presence of required fields. (uses includes method)\n    // const missingFields = requiredFields.filter(field => !objectFields.includes(field));\n\n    // // checking schema object for presence of extra fields. (uses in operator)\n    // const extraFields = objectFields.filter(field => !(field in schemaShape));\n\n    // if (missingFields.length > 0 || extraFields.length > 0) {\n    //   const errors = [];\n    //   if (missingFields.length) errors.push(Missing required field(s): ${missingFields.join(', ')});\n    //   if (extraFields.length) errors.push(Extra field(s): ${extraFields.join(', ')});\n\n    //   return {\n    //     success: false,\n    //     errorType: 'Missing/extra field(s)',\n    //     status: errors.join(';'),\n    //     message: errors.join(';')\n    //    };\n    // }\n  \t// Code not needed: end\n\n    const fm = schema.parse(parsed);\n    return {\n      success: true,\n      data: {\n        frontMatter: fm,\n        content: content.join('\\n'),\n      },\n      errorType: 'No error',\n      status: 'success'\n    };\n\n  } catch (error: any ) {\n    // let message = \"Unknown error\"\n    let status = 'Unknown error'\n    if (error.issues) {\n        status = getZodErrorMessage(error)\n    }\n\n    return {\n      success: false,\n      errorType: \"Parsing error\",\n      status,\n      issues: error.issues ?? ''\n    };\n  }\n};",
        "locale": "en",
        "slug": "technical-posts/missing-or-incorrect-keys-when-parsing-markdown-front-matter",
        "folder": ""
    },
    {
        "title": "CRUD app via Gemini",
        "description": "CRUD app via Gemini",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Okay, let's regenerate the SvelteKit 2.0 CRUD application using TypeScript.\n\nPrerequisites:\n\nEnsure you created your SvelteKit project with TypeScript enabled:bash\n    npm create svelte@latest my-family-crud-ts\nChoose: Skeleton project, YES to TypeScript, Add ESLint, Add Prettier\n    cd my-family-crud-ts\n    npm install\n    npm install postgres dotenv @types/pg # Install postgres driver and its types\nIf using pnpm or yarn, use appropriate install commands\n    Set up your .env file as described in the JavaScript version.\nMake sure your tsconfig.json is set up correctly (usually handled by create-svelte).\n\nSteps:\n\nDatabase Client Utility (src/lib/server/db.ts):typescript\n    // src/lib/server/db.ts\n    import postgres from 'postgres';\n    import { DATABASE_URL } from '$env/static/private'; // SvelteKit's env handling\n\n    if (!DATABASE_URL) {\n        throw new Error(\"DATABASE_URL environment variable is not set.\");\n    }\n\n    // Define the interface for the Family record\n    export interface Family {\n        id: string; // bigint is returned as string by 'postgres' driver\n        created_at: Date;\n        name: string;\n        description: string;\n        visual_studio_version: string;\n        availability_id: string | null; // bigint FK -> string | null\n        group_id: string | null;        // bigint FK -> string | null\n        sort_order: number;\n        download_page_section_heading: string;\n        download_page_order: number;\n        release_date: string | null;\n    }\n\n    // Define a type for the data used in forms (omitting generated fields)\n    export type FamilyFormData = Omit;\n    // Type for errors, mapping field names to error messages\n    export type FamilyFormErrors = Partial>;\n\n\n    // Configure connection options (optional, often inferred from DATABASE_URL)\n    const options: postgres.Options = { // Add type if needed, e.g. \n        // ssl: 'require', // Uncomment/configure if your DB requires SSL\n        max: 10,          // Max number of connections\n        idle_timeout: 30, // Idle timeout in seconds\n        // transform: {\n        //  undefined: null // Optional: Treat undefined query parameters as NULL\n        // }\n        // host, port, database, user, password can be added if not in DATABASE_URL\n    };\n\n    const sql = postgres(DATABASE_URL, options);\n\n    console.log('PostgreSQL client initialized (TypeScript).');\n\n    export default sql;\n    Form Data Helper (src/lib/server/formUtils.ts):typescript\n    // src/lib/server/formUtils.ts\n    import type { FamilyFormData, FamilyFormErrors } from './db'; // Import types\n\n    interface FormParseResult {\n        values: FamilyFormData;\n        errors: FamilyFormErrors | null;\n    }\n\n    /**\n     Extracts and sanitizes family data from FormData for Create/Update.\n     Handles type conversions and null values. Returns typed values and errors.\n     @param formData The FormData object from the request.\n     @returns Object containing sanitized values and potential validation errors.\n     */\n    export function getFamilyDataFromForm(formData: FormData): FormParseResult {\n        const values: Partial = {}; // Use Partial initially\n        const errors: FamilyFormErrors = {};\n\n        // Helper to safely get string values\n        const getString = (key: keyof FamilyFormData): string | undefined => formData.get(key as string)?.toString();\n\n        values.name = getString('name') ?? '';\n        values.description = getString('description') ?? '';\n        values.visual_studio_version = getString('visual_studio_version') ?? '';\n        values.download_page_section_heading = getString('download_page_section_heading') ?? '';\n\n        const availability_id_str = getString('availability_id');\n        const group_id_str = getString('group_id');\n        const sort_order_str = getString('sort_order');\n        const download_page_order_str = getString('download_page_order');\n        values.release_date = getString('release_date') || null; // Handle empty string as null\n\n        // --- Basic Validation ---\n        if (!values.name) errors.name = 'Name is required';\n        if (!values.description) errors.description = 'Description is required';\n        if (!values.visual_studio_version) errors.visual_studio_version = 'Visual Studio Version is required';\n        if (!values.download_page_section_heading) errors.download_page_section_heading = 'Download Page Section Heading is required';\n\n        // --- Type Conversion & Validation ---\n\n        // Optional BigInt FKs (keep as string)\n        values.availability_id = availability_id_str && availability_id_str !== '' ? availability_id_str : null;\n        values.group_id = group_id_str && group_id_str !== '' ? group_id_str : null;\n\n        // Sort Order (required number, default 6000)\n        if (sort_order_str === undefined || sort_order_str === '') {\n             errors.sort_order = 'Sort order is required';\n             values.sort_order = 6000; // Use default even on error for form refill\n        } else {\n            const parsed = parseInt(sort_order_str, 10);\n            if (isNaN(parsed)) {\n                errors.sort_order = 'Sort order must be a valid number';\n                 values.sort_order = 6000; // Default\n            } else {\n                values.sort_order = parsed;\n            }\n        }\n\n        // Download Page Order (required number, default 3200)\n         if (download_page_order_str === undefined || download_page_order_str === '') {\n             errors.download_page_order = 'Download page order is required';\n              values.download_page_order = 3200; // Default\n        } else {\n             const parsed = parseInt(download_page_order_str, 10);\n            if (isNaN(parsed)) {\n                errors.download_page_order = 'Download page order must be a valid number';\n                 values.download_page_order = 3200; // Default\n            } else {\n                values.download_page_order = parsed;\n            }\n        }\n\n        // Clean up release date (allow empty string -> null)\n        if (values.release_date && values.release_date.trim() === '') {\n            values.release_date = null;\n        }\n\n        // --- Final Check ---\n        const hasErrors = Object.keys(errors).length > 0;\n\n        // Cast to final type, ensuring all required fields are present (even if derived from defaults)\n        // This relies on defaults being set correctly above.\n        const finalValues = values as FamilyFormData;\n\n        return {\n            values: finalValues,\n            errors: hasErrors ? errors : null\n        };\n    }\n    Read (List) Page:\n    src/routes/read/+page.server.ts:typescript\n        // src/routes/read/+page.server.ts\n        import sql from '$lib/server/db';\n        import type { Family } from '$lib/server/db'; // Import the interface\n        import { fail } from '@sveltejs/kit';\n        import type { PageServerLoad, Actions } from './$types'; // Import generated types\n\n        // Type the load function using PageServerLoad\n        export const load: PageServerLoad = async () => {\n            try {\n                // Use the generic parameter with sql`` for type safety\n                const families = await sql[]>`\n                    SELECT id, name, description, visual_studio_version, sort_order, download_page_section_heading, download_page_order, release_date\n                    FROM public.family\n                    ORDER BY sort_order, name\n                `;\n                // No need to map IDs if postgres driver returns bigint as string\n\n                return {\n                    // families array conforms to the partial Family type selected\n                    families: families\n                };\n            } catch (error: any) {\n                console.error('Error loading families:', error);\n                // Use fail for recoverable errors expected during load\n                return fail(500, { message: Could not load families. ${error.message || ''} });\n            }\n        };\n\n        // Type the Actions object\n        export const actions: Actions = {\n            // Type the delete action\n            delete: async ({ request }) => {\n                const data = await request.formData();\n                const id = data.get('id')?.toString();\n\n                if (!id) {\n                    return fail(400, { message: 'Missing family ID for deletion.' });\n                }\n\n                // Basic check if ID looks like a number (since it's string bigint)\n                if (!/^\\d+$/.test(id)) {\n                     return fail(400, { message: 'Invalid family ID format for deletion.' });\n                }\n\n                try {\n                    const result = await sql`\n                        DELETE FROM public.family WHERE id = ${id}\n                    `;\n\n                    if (result.count === 0) {\n                        // Use status code 404 if not found, still use fail\n                        return fail(404, { message: Family with ID ${id} not found. });\n                    }\n\n                    // Return success status for the UI\n                    // Load function re-runs automatically, no need for redirect here\n                    return { success: true, deletedId: id };\n\n                } catch (error: any) {\n                    console.error(Error deleting family ${id}:, error);\n                    // Use fail for errors during the action\n                    return fail(500, { message: Failed to delete family ${id}. ${error.message || ''} });\n                }\n            }\n        };\n        src/routes/read/+page.svelte:html\n         // Add lang=\"ts\"\n            import { enhance } from '$app/forms';\n            import type { PageData, ActionData } from './$types'; // Use generated types\n\n            export let data: PageData; // Typed PageData\n            export let form: ActionData; // Typed ActionData for form results\n\n            // Type assertion for families if needed, though PageData should handle it\n            // $: families = data.families as App.Family[]; // Or use the imported interface if not global\n\n            // Helper for user confirmation\n            function confirmDelete(name: string, id: string): boolean {\n                 return confirm(Are you sure you want to delete family \"${name}\" (ID: ${id})?);\n            }\n\n        \n\n        \n            Read Families\n            \n            \n                /* Add or link your styles here */\n                 body { font-family: sans-serif; }\n                 table { width: 100%; border-collapse: collapse; margin-bottom: 1em; }\n                 th, td { border: 1px solid #ccc; padding: 0.5em; text-align: left; vertical-align: top;}\n                 th { background-color: #eee; }\n                 nav { margin-bottom: 1em; }\n                 nav a { margin-right: 1em; }\n                 .actions form { display: inline-block; margin-left: 10px; }\n                 .actions button { color: red; background: none; border: none; padding: 0; font: inherit; cursor: pointer; text-decoration: underline;}\n                 .message { margin: 1em 0; padding: 0.8em; border-radius: 4px; }\n                 .error { background-color: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; }\n                 .success { background-color: #d4edda; color: #155724; border: 1px solid #c3e6cb; }\n            \n        \n\n        Family List\n\n        \n            Read Families\n            Create New Family\n        \n\n        \n        {#if form?.message && !form?.success}\n            Error: {form.message}\n        {/if}\n        {#if form?.success}\n             Family ID {form.deletedId} deleted successfully.\n        {/if}\n\n        \n        {#if data?.message}\n             Error loading data: {data.message}\n        {:else if data.families && data.families.length > 0}\n            \n                \n                    \n                        ID\n                        Name\n                        Description\n                        VS Version\n                        Sort Order\n                        Release Date\n                        Actions\n                    \n                \n                \n                    {#each data.families as family (family.id)}\n                        \n                            {family.id}\n                            {family.name}\n                            {family.description}\n                            {family.visual_studio_version}\n                            {family.sort_order}\n                            {family.release_date ?? 'N/A'}\n                            \n                                Edit\n                                \n                                 {\n                                    return async ({ cancel }) => {\n                                        if (!confirmDelete(family.name, family.id)) {\n                                            cancel();\n                                        }\n                                    };\n                                }}>\n                                    \n                                    Delete\n                                \n                            \n                        \n                    {/each}\n                \n            \n        {:else if data.families}\n            No families found.\n        {/if}\n\n         Create New Family\n        Create Page:\n    src/routes/create/+page.server.ts:typescript\n        // src/routes/create/+page.server.ts\n        import sql from '$lib/server/db';\n        import { fail, redirect } from '@sveltejs/kit';\n        import { getFamilyDataFromForm } from '$lib/server/formUtils';\n        import type { Actions } from './$types';\n        import type { FamilyFormData } from '$lib/server/db'; // Import the form data type\n\n        // No load function needed for a simple create page\n\n        export const actions: Actions = {\n            default: async ({ request }) => {\n                const formData = await request.formData();\n                const { values, errors } = getFamilyDataFromForm(formData);\n\n                if (errors) {\n                    // Return validation errors and the submitted values for form repopulation\n                    // Use fail with status 400 for validation errors\n                    return fail(400, { values, errors });\n                }\n\n                try {\n                    // Destructure values for clarity in the query\n                    const {\n                        name, description, visual_studio_version, availability_id,\n                        group_id, sort_order, download_page_section_heading,\n                        download_page_order, release_date\n                    } = values;\n\n                    // SQL insertion using tagged template literal for safety\n                    await sql`\n                        INSERT INTO public.family (\n                            name, description, visual_studio_version, availability_id,\n                            group_id, sort_order, download_page_section_heading,\n                            download_page_order, release_date\n                        ) VALUES (\n                            ${name}, ${description}, ${visual_studio_version}, ${availability_id},\n                            ${group_id}, ${sort_order}, ${download_page_section_heading},\n                            ${download_page_order}, ${release_date}\n                        )\n                    `;\n                } catch (error: any) {\n                    console.error('Error creating family:', error);\n                    // Check for specific DB errors if needed (e.g., unique constraints) using error.code\n                    // Return a generic server error message using fail with status 500\n                    return fail(500, {\n                        values, // Return values for form refill\n                        errors: { form: Database error: Could not create family. ${error.message || ''} }\n                    });\n                }\n\n                // Redirect to the list page on success using 303 (See Other)\n                throw redirect(303, '/read');\n            }\n        };\n        src/routes/create/+page.svelte:html\n         // Add lang=\"ts\"\n            import { enhance } from '$app/forms';\n            import type { ActionData } from './$types'; // Use generated type\n\n            export let form: ActionData; // Typed ActionData\n\n            // Use form?.values for repopulation, provide defaults matching DB schema\n            $: nameValue = form?.values?.name ?? 'Otis';\n            $: descriptionValue = form?.values?.description ?? 'Visual Studio 2022 v17.1/11.1 .NET FW';\n            $: vsVersionValue = form?.values?.visual_studio_version ?? 'Visual Studio 2022';\n            $: availabilityIdValue = form?.values?.availability_id ?? '6';\n            $: groupIdValue = form?.values?.group_id ?? '4';\n            $: sortOrderValue = form?.values?.sort_order ?? 6000;\n            $: headingValue = form?.values?.download_page_section_heading ?? '17.1/11.1 - Visual Studio 2022 .NET Framework Otis Family';\n            $: pageOrderValue = form?.values?.download_page_order ?? 3200;\n            $: releaseDateValue = form?.values?.release_date ?? '';\n\n        \n\n        \n            Create Family\n             \n            \n                 body { font-family: sans-serif; }\n                 form { max-width: 600px; margin: 20px auto; padding: 20px; border: 1px solid #ccc; border-radius: 5px; }\n                 label { display: block; margin-bottom: 5px; font-weight: bold; }\n                 input[type=\"text\"], input[type=\"number\"], textarea {\n                     width: 100%; padding: 8px; margin-bottom: 10px; border: 1px solid #ccc; border-radius: 4px; box-sizing: border-box;\n                 }\n                 textarea { min-height: 80px; }\n                 button { background-color: #4CAF50; color: white; padding: 10px 15px; border: none; border-radius: 4px; cursor: pointer; font-size: 1em; }\n                 button:hover { background-color: #45a049; }\n                 .error-message { color: red; font-size: 0.9em; margin-top: -5px; margin-bottom: 10px; }\n                 .form-error { color: red; font-weight: bold; margin-bottom: 15px; border: 1px solid red; padding: 10px; background-color: #ffebeb; border-radius: 4px; }\n                 nav { margin-bottom: 1em; }\n                 nav a { margin-right: 1em; }\n            \n        \n\n        Create New Family\n\n         \n            Read Families\n            Create New Family\n        \n\n        \n        {#if form?.errors?.form}\n            { form.errors.form }\n        {/if}\n\n        \n            \n                Name:\n                \n                {#if form?.errors?.name}{form.errors.name}{/if}\n            \n\n            \n                Description:\n                \n                 {#if form?.errors?.description}{form.errors.description}{/if}\n            \n\n             \n                Visual Studio Version:\n                \n                 {#if form?.errors?.visual_studio_version}{form.errors.visual_studio_version}{/if}\n            \n\n             \n                Availability ID (Optional, BigInt as number input):\n                \n                \n                {#if form?.errors?.availability_id}{form.errors.availability_id}{/if}\n            \n\n             \n                Group ID (Optional, BigInt as number input):\n                \n                {#if form?.errors?.group_id}{form.errors.group_id}{/if}\n            \n\n             \n                Sort Order:\n                \n                 {#if form?.errors?.sort_order}{form.errors.sort_order}{/if}\n            \n\n            \n                Download Page Section Heading:\n                \n                 {#if form?.errors?.download_page_section_heading}{form.errors.download_page_section_heading}{/if}\n            \n\n             \n                Download Page Order:\n                \n                 {#if form?.errors?.download_page_order}{form.errors.download_page_order}{/if}\n            \n\n            \n                Release Date (YYYY-MM-DD or Text, Optional):\n                \n                 {#if form?.errors?.release_date}{form.errors.release_date}{/if}\n            \n\n            Create Family\n        \n        Update Page:\n    src/routes/update/[id]/+page.server.ts:typescript\n        // src/routes/update/[id]/+page.server.ts\n        import sql from '$lib/server/db';\n        import type { Family } from '$lib/server/db';\n        import { error, fail, redirect } from '@sveltejs/kit';\n        import { getFamilyDataFromForm } from '$lib/server/formUtils';\n        import type { PageServerLoad, Actions } from './$types';\n\n        export const load: PageServerLoad = async ({ params }) => {\n            const { id } = params;\n\n            // Validate ID format (basic check)\n            if (!id || !/^\\d+$/.test(id)) {\n                throw error(400, 'Invalid Family ID provided.'); // Use SvelteKit's error helper\n            }\n\n            try {\n                // Fetch the specific family, expect exactly one result\n                const families = await sql`\n                    SELECT * FROM public.family WHERE id = ${id}\n                `;\n\n                if (families.length === 0) {\n                    throw error(404, Family with ID ${id} not found.); // Throw 404 if not found\n                }\n\n                const family = families[0];\n                // Ensure related IDs are strings or null for the form\n                // The driver usually returns bigint as string, but explicit check is safe\n                 family.id = String(family.id);\n                 family.availability_id = family.availability_id ? String(family.availability_id) : null;\n                 family.group_id = family.group_id ? String(family.group_id) : null;\n\n                return { family }; // Return the found family object\n            } catch (err: any) {\n                // Re-throw SvelteKit HTTP errors, handle others\n                if (err.status >= 400 && err.status  {\n                const { id } = params; // Get ID from the route parameter\n                const formData = await request.formData();\n                const { values, errors } = getFamilyDataFromForm(formData);\n\n                // ID check (should always be present here)\n                 if (!id || !/^\\d+$/.test(id)) {\n                     return fail(400, { values, errors: { ...errors, form: 'Family ID is missing or invalid.' }});\n                 }\n\n                if (errors) {\n                    // Return validation errors + submitted values\n                    return fail(400, { id, values, errors }); // Include ID for context if needed\n                }\n\n                try {\n                    const {\n                        name, description, visual_studio_version, availability_id,\n                        group_id, sort_order, download_page_section_heading,\n                        download_page_order, release_date\n                    } = values;\n\n                    const result = await sql`\n                        UPDATE public.family\n                        SET\n                            name = ${name},\n                            description = ${description},\n                            visual_studio_version = ${visual_studio_version},\n                            availability_id = ${availability_id},\n                            group_id = ${group_id},\n                            sort_order = ${sort_order},\n                            download_page_section_heading = ${download_page_section_heading},\n                            download_page_order = ${download_page_order},\n                            release_date = ${release_date}\n                            -- created_at is typically not updated\n                        WHERE id = ${id}\n                        RETURNING id -- Optionally return ID to confirm update\n                    `;\n\n                     if (result.count === 0) {\n                        // Should not happen if load worked, but handle defensively\n                        return fail(404, {\n                            id, values, errors: { form: Family with ID ${id} not found for update. }\n                        });\n                    }\n\n                } catch (err: any) {\n                    console.error(Error updating family ${id}:, err);\n                     return fail(500, {\n                        id, values, errors: { form: Database error: Could not update family ${id}. ${err.message || ''} }\n                    });\n                }\n\n                // Redirect back to the list page after successful update\n                throw redirect(303, '/read');\n            }\n        };\n        src/routes/update/[id]/+page.svelte:html\n         // Add lang=\"ts\"\n            import { enhance } from '$app/forms';\n            import type { PageData, ActionData } from './$types'; // Use generated types\n            import type { Family } from '$lib/server/db'; // Import Family type if needed for casting\n\n            export let data: PageData; // Typed PageData from load()\n            export let form: ActionData; // Typed ActionData from actions\n\n            // Prioritize form data (if validation failed) over loaded data\n            // Use type assertion or careful checks as ActionData might only have values subset\n            $: familyData = (form?.values ?? data.family) as Partial; // Use Partial as form might not have all fields on error\n            $: id = data.family.id; // Get ID from originally loaded data\n\n        \n\n        \n            Update Family {id}\n            \n            \n                 body { font-family: sans-serif; }\n                 form { max-width: 600px; margin: 20px auto; padding: 20px; border: 1px solid #ccc; border-radius: 5px; }\n                 label { display: block; margin-bottom: 5px; font-weight: bold; }\n                 input[type=\"text\"], input[type=\"number\"], textarea {\n                     width: 100%; padding: 8px; margin-bottom: 10px; border: 1px solid #ccc; border-radius: 4px; box-sizing: border-box;\n                 }\n                 textarea { min-height: 80px; }\n                 button { background-color: #007bff; color: white; padding: 10px 15px; border: none; border-radius: 4px; cursor: pointer; font-size: 1em; }\n                 button:hover { background-color: #0056b3; }\n                 .error-message { color: red; font-size: 0.9em; margin-top: -5px; margin-bottom: 10px; }\n                 .form-error { color: red; font-weight: bold; margin-bottom: 15px; border: 1px solid red; padding: 10px; background-color: #ffebeb; border-radius: 4px; }\n                 nav { margin-bottom: 1em; }\n                 nav a { margin-right: 1em; }\n                 .cancel-link { margin-top: 1em; display: inline-block;}\n            \n        \n\n        Update Family (ID: {id})\n\n        \n            Read Families\n            Create New Family\n        \n\n        \n        {#if form?.errors?.form}\n            { form.errors.form }\n        {/if}\n\n        \n            \n            \n                Name:\n                \n                {#if form?.errors?.name}{form.errors.name}{/if}\n            \n\n            \n                Description:\n                {familyData.description ?? ''}\n                 {#if form?.errors?.description}{form.errors.description}{/if}\n            \n\n             \n                Visual Studio Version:\n                \n                 {#if form?.errors?.visual_studio_version}{form.errors.visual_studio_version}{/if}\n            \n\n             \n                Availability ID (Optional, BigInt as number input):\n                \n                {#if form?.errors?.availability_id}{form.errors.availability_id}{/if}\n            \n\n             \n                Group ID (Optional, BigInt as number input):\n                \n                {#if form?.errors?.group_id}{form.errors.group_id}{/if}\n            \n\n             \n                Sort Order:\n                \n                 {#if form?.errors?.sort_order}{form.errors.sort_order}{/if}\n            \n\n            \n                Download Page Section Heading:\n                \n                 {#if form?.errors?.download_page_section_heading}{form.errors.download_page_section_heading}{/if}\n            \n\n             \n                Download Page Order:\n                \n                 {#if form?.errors?.download_page_order}{form.errors.download_page_order}{/if}\n            \n\n            \n                Release Date (YYYY-MM-DD or Text, Optional):\n                \n                 {#if form?.errors?.release_date}{form.errors.release_date}{/if}\n            \n\n            Update Family\n        \n\n         Cancel and return to list\n        Root Route Redirect (src/routes/+page.server.ts):typescript\n    // src/routes/+page.server.ts\n    import { redirect } from '@sveltejs/kit';\n    import type { PageServerLoad } from './$types'; // Use generated type\n\n    // Use PageServerLoad for server-only load functions\n    export const load: PageServerLoad = async () => {\n        throw redirect(307, '/read'); // 307 Temporary Redirect\n    };\n    Layout (src/routes/+layout.svelte): (Ensure script tag has lang=\"ts\")html\n    \n     // Add lang=\"ts\"\n        import '../app.css'; // Example: import global CSS\n        // Layout-level TypeScript logic can go here if needed\n    \n\n    \n        /* Basic global styles - Adapt as needed */\n        :global(body) {\n            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;\n            margin: 0;\n            padding: 0;\n            background-color: #f4f4f4; /* Light background */\n            color: #333;\n            line-height: 1.6;\n        }\n\n        .layout-container {\n            max-width: 1100px;\n            margin: 2rem auto;\n            padding: 1rem 2rem 2rem 2rem;\n            background-color: #fff;\n            border-radius: 8px;\n            box-shadow: 0 0 15px rgba(0,0,0,0.1);\n        }\n\n        nav.main-nav {\n            background-color: #e9ecef; /* Light grey nav */\n            padding: 0.8rem 1.5rem;\n            border-bottom: 1px solid #dee2e6;\n            margin-bottom: 1.5rem;\n            border-radius: 8px 8px 0 0; /* Match container */\n        }\n\n        nav.main-nav a {\n            margin-right: 1rem;\n            text-decoration: none;\n            font-weight: 500;\n            color: #0056b3; /* Slightly darker blue */\n            padding: 0.3rem 0;\n            border-bottom: 2px solid transparent;\n            transition: color 0.2s ease, border-bottom-color 0.2s ease;\n        }\n\n        nav.main-nav a:hover,\n        nav.main-nav a[aria-current='page'] { /* Style for active link */\n            color: #003d80;\n            border-bottom-color: #0056b3;\n        }\n\n        main {\n            padding: 0 1rem; /* Add padding inside main content area */\n        }\n\n        /* Common button style */\n        button, a.button-link {\n             display: inline-block;\n             padding: 0.6em 1.2em;\n             border: none;\n             border-radius: 4px;\n             cursor: pointer;\n             font-size: 1em;\n             text-decoration: none;\n             text-align: center;\n             transition: background-color 0.2s ease;\n             margin-right: 0.5em; /* Spacing between buttons */\n        }\n        button[type=\"submit\"] { background-color: #28a745; color: white; } /* Green for Create/Update */\n        button[type=\"submit\"]:hover { background-color: #218838; }\n        a.button-link { background-color: #007bff; color: white; } /* Blue for links like Edit */\n        a.button-link:hover { background-color: #0056b3; }\n        button.delete-btn { background-color: #dc3545; color: white; } /* Red for Delete */\n        button.delete-btn:hover { background-color: #c82333; }\n\n    \n\n    \n        \n             \n            Read Families\n            Create New Family\n        \n\n        \n             \n        \n    \n    Note: Added $page store usage for active link styling in the layout. Make sure to import it: import { page } from '$app/stores'; in the layout's script tag.\n\nRun:bash\n    npm run dev -- --open\n    This provides the same functionality as the JavaScript version but leverages TypeScript for better type safety, code completion, and maintainability. Remember to run npm run check periodically to catch any type errors.`",
        "locale": "en",
        "slug": "technical-posts/crud-app-via-gemini",
        "folder": ""
    },
    {
        "title": "JavaScript notes",
        "description": "JavaScript notes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "This use of map creates a copy of the existing array, but overrides the selected property value. Overrides should come after the spread operator (ie, ...item).\n\ndata.familyListItems = data.familyListItems.map((item) => ({\n    ...item,\n    selected: item.value == value,\n}));\n\nThese two functions are functionally equivalent. The second is probably the better, more idiomatic way to write modern JavaScript.\n\nfunction setSelectedItem(listItems: ListItem[],  value: any) {\n    for (const listItem of listItems) {\n        listItem.selected = listItem.value == value;\n    }\n    return listItems\n}\n\nfunction setSelectedItem(listItems: ListItem[],  value: any) {\n    return listItems.map((item) => ({\n        ...item,\n        selected: item.value == value\n    }));\n}\n\nAlso, note the potential peril of using JavaScript's strict equality operator.\n\nfunction setSelectedItem(listItems: ListItem[],  value: any) {\n    return listItems.map((item) => ({\n        ...item,\n        selected: item.value === value\n    }));\n}\n\ntype AnyNumber = number | bigint;\n\nFind entries in arrays and objects\n\nUse the Array object's Includes method to find elements in an array.\n\nThe code below expects csny to be an array.\n\nconst extra = bs.filter(field => !csny.includes(field));\n\nUse JavaScript's in operator to find keys in an object.\n\nThe code below expects csny to be an object. Theinoperator is designed to check for property names in objects, not array elements. When used with arrays, it checks the index numbers, not the values.\n\nconst extra = bs.filter(field => !(field in csny));",
        "locale": "en",
        "slug": "technical-posts/javascript-notes",
        "folder": ""
    },
    {
        "title": "Programmatically create a DataGate from a Json document with CS",
        "description": "Programmatically create a DataGate from a Json document with CS",
        "date_created": "2025-08-25T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "csharp"
        ],
        "content": "[\n    {\n        \"sqlColumn\": \"Email\",\n        \"excelColumn\": \"Email\",\n        \"dataType\": \"string\"\n    },\n    {\n        \"sqlColumn\": \"CompanyName\",\n        \"excelColumn\": \"CompanyName\",\n        \"dataType\": \"string\"\n    },\n    {\n        \"sqlColumn\": \"FirstName\",\n        \"excelColumn\": \"First name\",\n        \"dataType\": \"string\"\n    },\n    {\n        \"sqlColumn\": \"LastName\",\n        \"excelColumn\": \"Last name\",\n        \"dataType\": \"string\"\n    },\n    {\n        \"sqlColumn\": \"Title\",\n        \"excelColumn\": \"Title\",\n        \"dataType\": \"string\"\n    }\n]\n\nClass to define the Json document\n\npublic class ColumnMapping\n{\n    public string SqlColumn { get; set; }\n    public string ExcelColumn { get; set; }\n    public string DataType { get; set; }\n}\n\nCode\n\nusing System;\nusing System.Data;\nusing System.Text.Json;\nusing System.Collections.Generic;\n\npublic class JsonToDataTable\n{\n    public static DataTable CreateDataTableFromJson(string jsonArray)\n    {\n        // Deserialize the JSON array into a list of ColumnMapping objects\n        List columnMappings = JsonSerializer.Deserialize>(jsonArray);\n\n        DataTable dataTable = new DataTable();\n\n        // Dynamically add columns based on the JSON data\n        foreach (var mapping in columnMappings)\n        {\n            Type columnType;\n            switch (mapping.DataType.ToLower())\n            {\n                case \"string\":\n                    columnType = typeof(string);\n                    break;\n                case \"int\":\n                case \"integer\":\n                    columnType = typeof(int);\n                    break;\n                case \"decimal\":\n                case \"double\":\n                case \"float\":\n                    columnType = typeof(decimal); // Using decimal for financial accuracy\n                    break;\n                case \"datetime\":\n                    columnType = typeof(DateTime);\n                    break;\n                case \"boolean\":\n                    columnType = typeof(bool);\n                    break;\n                // Add more cases as needed for other data types\n                default:\n                    columnType = typeof(string); // Default to string if type is unknown\n                    break;\n            }\n            dataTable.Columns.Add(mapping.SqlColumn, columnType);\n        }\n\n        return dataTable;\n    }\n\n    public static void Main(string[] args)\n    {\n\t    // Read Json file\n    \n        DataTable myTable = CreateDataTableFromJson(json);\n\n        Console.WriteLine(\"DataTable created with columns:\");\n        foreach (DataColumn column in myTable.Columns)\n        {\n            Console.WriteLine($\"- {column.ColumnName} ({column.DataType})\");\n        }\n    }\n}",
        "locale": "en",
        "slug": "technical-posts/programmatically-create-datatable-from-json-in-cs",
        "folder": ""
    },
    {
        "title": "PowerShell tXips and techniques",
        "description": "PowerShell tXips and techniques",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "Copy current path to the clipboard\n\n(pwd).Path | set-clipboard\n\nedit the PowerShell profile with VS Code\n\ncode $profile\n\nReload PowerShell after changing the PS profile\n\n. $profile\n\nEdit the PowerShell profile\n\nnotepad $profile\n\nMy functions\n\n| Name | Description              |\n| ---- | ------------------------ |\n| rund | Run pnpm run dev       |\n| runc | Start PostCSS            |\n| dlr  | Start downloads registry |\n\npowershell.exe -noexit -command Set-Location -literalPath '%V'\n\nC:\\Program Files\\PowerShell\\7\\pwsh.exe\"\n\nComputer\\HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\Powershell\\command\n\nGet a directory listing of files with specific extensions\n\nget-childitem . -r | where {$_.extension -in (\".png\", \".afdesign\", \".webp\")} | select-object -property fullname\n\nLonger form:\n\n$folder = Get-ChildItem C:\\test\\test1\\\n$wildcards = @(\".txt\",\".doc\",\".xls\")\n$files = Get-ChildItem -Path $folderPath | where {$_.extension -in $wildcards}\n$files\n\nGet all image files\n\nget-childitem d:\\luther-backup\\. -r\n  | where {$_.extension -in (\".png\", \".afdesign\", \".svg\", \".webp\")}\n  | select-object -property fullname\nluther-backup-image-assets.txt\n\nGet a list of image-related files separated by the pipe character.\n\n!info]\nThe line continuation character for PowerShell is the tilde\n\nget-childitem d:\\luther-backup\\. -r `\n     | where {$_.extension -in (\".png\", \".afdesign\", \".svg\", \".webp\")} `\n     | select-object name, fullname `\n     | export-csv -path test.txt -delimiter '|'\n\nGet all \\.vrproj and \\.vrm files.\n\nget-childitem d:\\luther-backup\\. -r\n  | where {$_.extension -in (\".vrproj\", \".vrm\")}\n  | select-object name, lastwritetime, fullname\n  | export-csv -path avr-projects.txt\n\nget-childitem c:\\users\\thumb\\documents\\. -r\n\t| Where-Object { $_.FullName -notlike '\\node_modules\\' }\n\t| where {$_.extension -in (\".png\", \".afdesign\", \".svg\", \".webp\")}\n\t| select-object name, fullname\n\t| export-csv -path c:\\users\\thumb\\documents\\image-vault.txt -delimiter '|'\n\n$f = get-childitem c:\\users\\thumb\\documents\\ -r |  Where-Object { $.FullName -notlike '\\node_modules\\' } | where {$.extension -in (\".png\", \".afdesign\", \".svg\", \".webp\")} |select-object name, fullname | export-csv -path test.txt -delimiter '|'\n\nThe force flag is important to avoid security restriction issues.\n\nget-childitem \"H:\\jerrybutler-disk\" -r -force  where {$_.extension -in (\".vr\", \".ashx.vr\", \".ashx\")}  select-object -property fullname export-csv -path test.txt -delimiter '|'\n\nTo get selected properties\n\nblah blah | select-object pname1,pname2\n\nwhere the default property is -property\n\nPause for keypress after doing something\n\nparam (\n\t[Parameter(Mandatory) $date\n)\n\nwrite-host Hello today is $date\n\ncmd /c pause\n\nTo get a list of file extensions output to a CSV file:\n\nget-childitem *.bak -r `\n   | select-object fullname, lastwritetime `\n   | export-csv -path .\\delray.bak.csv\n\nNote: CSV output avoid concatenating property names.\n\nMount and Unmount VHDX\n\nMount-DiskImage -ImagePath \"C:\\path\\to\\your\\file.vhdx\"\n\nDismount-DiskImage -ImagePath \"C:\\path\\to\\your\\file.vhdx\"\n\nDisable \"Running Scripts is Disabled on this System\"\n\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n\nBackup all SQL Server databases\n\nThis script backs up all SQL databases and copies the .bak files.\n\nget-sqldatabase -serverinstance . |\n    where { $_.Name -ne 'tempdb' } |\n    backup-sqldatabase\n\n$source_path = \"C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\Backup\\*.bak\"\n\n$target_path = \"\"C:\\Users\\thumb\\Documents\\sql-server-backup\"\"\n\ncopy-item $source_path -destination $target_path\n\nShow object properties\n\nget-childitem test2.ps1 | Get-Member -MemberType Properties\n\nUntested routines to write to a given point on the terminal\n\nfunction Write-To-Pos {\n    param(\n        string]$str,\n        [int]$x = 0,\n        [int]$y = 0,\n        [consolecolor]$backgroundColor = $Host.UI.RawUI.BackgroundColor,\n        [consolecolor]$foregroundColor = $Host.UI.RawUI.ForegroundColor\n    )\n\n    $oldPos = $Host.UI.RawUI.CursorPosition\n    $newPos = $oldPos\n    $newPos.X = $x\n    $newPos.Y = $y\n    $Host.UI.RawUI.CursorPosition = $newPos\n    $Host.UI.RawUI.BackgroundColor = $backgroundColor\n    $Host.UI.RawUI.ForegroundColor = $foregroundColor\n    Write-Host $str -NoNewline\n    $Host.UI.RawUI.CursorPosition = $oldPos\n}\n\nfunction Write-To-Pos {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory=$true) $Text,\n        Parameter(Mandatory=$true) $X,\n        Parameter(Mandatory=$true) $Y,\n        [ConsoleColor] $ForegroundColor = \"White\",\n        [ConsoleColor] $BackgroundColor = \"Black\"\n    )\n\n    $oldForegroundColor = $Host.UI.RawUI.ForegroundColor\n    $oldBackgroundColor = $Host.UI.RawUI.BackgroundColor\n\n    $Host.UI.RawUI.ForegroundColor = $ForegroundColor\n    $Host.UI.RawUI.BackgroundColor = $BackgroundColor\n\n    [Console]::SetCursorPosition($X, $Y)\n    [Console]::Write($Text)\n\n    $Host.UI.RawUI.ForegroundColor = $oldForegroundColor\n    $Host.UI.RawUI.BackgroundColor = $oldBackgroundColor\n}",
        "locale": "en",
        "slug": "technical-posts/powershell-tips-and-techniques",
        "folder": ""
    },
    {
        "title": "rp blog sveltekit blog engine",
        "description": "rp blog sveltekit blog engine",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Create mark-map.ts file\n\ncreate-markdown-map.ts produces the markdown-map.ts file, which is an array of the object below. create-markdown-map.ts imports the getMarkdownObjects function from get-markdown-objects.ts\n\n{\n\t\"data\": {\n\t\t\"frontMatter\": {\n\t\t\t\"title\": \"Test post 13\",\n\t\t\t\"description\": \"This is test post 13\",\n\t\t\t\"tags\": [\n\t\t\t\t\"test13\"\n\t\t\t],\n\t\t\t\"date_added\": \"2025-01-25T04:15:16.000Z\",\n\t\t\t\"date_updated\": \"2025-01-25T04:15:16.000Z\",\n\t\t\t\"date_published\": null,\n\t\t\t\"pinned\": false\n\t\t},\n\t\t\"content\": \"This is test post 13\"\n\t},\n\t\"mapInfo\": {\n\t\t\"success\": true,\n\t\t\"errorType\": \"No error\",\n\t\t\"status\": \"success\",\n\t\t\"fullPath\": \"C:\\\\Users\\\\thumb\\\\Documents\\\\projects\\\\svelte\\\\rp-blog\\\\src\\\\markdown\\\\posts\\\\test-post-13.md\",\n\t\t\"slug\": \"/posts/test-post-13\",\n\t\t\"folder\": \"posts\"\n\t}\n},\n\nA TypeScript type definition for the JSON object above. It is broken down into a few interfaces for better organization and readability.\n\ninterface FrontMatter {\n  title: string;\n  description: string;\n  date_published:\ntags: string[];\n  date_added: string; // ISO 8601 date string\n  date_updated: string; // ISO 8601 date string\n  date_published: string | null; // ISO 8601 date string or null\n  pinned: boolean;\n}\n\ninterface DataObject {\n  frontMatter: FrontMatter;\n  content: string;\n}\n\ninterface MapInfo {\n  success: boolean;\n  errorType: string; // Could be a literal type like \"No error\" | \"File not found\" if known\n  status: string;    // Could be a literal type like \"success\" | \"failure\" if known\n  fullPath: string;\n  slug: string;\n  folder: string;\n}\n\ninterface BlogPostResponse { // You can name this root type as you see fit\n  data: DataObject;\n  mapInfo: MapInfo;\n}\n\n// Example usage (optional, for demonstration):\nconst exampleJson: BlogPostResponse = {\n    \"data\": {\n        \"frontMatter\": {\n            \"title\": \"Test post 13\",\n            \"description\": \"This is test post 13\",\n            \"tags\": [\n                \"test13\"\n            ],\n            \"date_added\": \"2025-01-25T04:15:16.000Z\",\n            \"date_updated\": \"2025-01-25T04:15:16.000Z\",\n            \"date_published\": null,\n            \"pinned\": false\n        },\n        \"content\": \"This is test post 13\"\n    },\n    \"mapInfo\": {\n        \"success\": true,\n        \"errorType\": \"No error\",\n        \"status\": \"success\",\n        \"fullPath\": \"C:\\\\Users\\\\thumb\\\\Documents\\\\projects\\\\svelte\\\\rp-blog\\\\src\\\\markdown\\\\posts\\\\test-post-13.md\",\n        \"slug\": \"/posts/test-post-13\",\n        \"folder\": \"posts\"\n    }\n};\n\nconsole.log(exampleJson.data.frontMatter.title);\nconsole.log(exampleJson.mapInfo.slug);\n\nKey points:\n\nFrontMatter: Describes the metadata of your content.\n    date_added, date_updated, date_published: Typed as string because they are ISO 8601 date strings in the JSON. If you parse them into Date objects in your application, you might adjust the type there, but for representing the JSON itself, string is accurate. date_published can also be null.\nDataObject: Contains the frontMatter and the main content.\nMapInfo: Contains metadata about the file/mapping process.\n    errorType and status: Typed as string. If you have a fixed set of possible values for these (e.g., status: \"success\" | \"error\"), you can make them more specific using literal types.\nBlogPostResponse: The root type that combines data and mapInfo. You can choose a more descriptive name for this depending on your application's context (e.g., MarkdownFileProcessed, ApiPostResponse, etc.).\n\nThis structure should accurately type your JSON object.",
        "locale": "en",
        "slug": "technical-posts/rp-blog-sveltekit-blog-engine",
        "folder": ""
    },
    {
        "title": "Find all occurrences of a set of strings within a string",
        "description": "Find all occurrences of a set of strings within a string",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "...sourceStr.matchAll(new RegExp(searchStr, 'gi'))].map(a => a.index)\n\nExample:\n\nconst sourceStr = 'I learned to play the Ukulele in Lebanon.';\nconst searchStr = 'le';\nconst indexes = [...sourceStr.matchAll(new RegExp(searchStr, 'gi'))].map(a => a.index);\nconsole.log(indexes); // [2, 25, 27, 33(https://stackoverflow.com/questions/3410464/how-to-find-indices-of-all-occurrences-of-one-string-in-another-in-javascript)",
        "locale": "en",
        "slug": "technical-posts/find-all-occurrences-of-a-set-of-strings",
        "folder": ""
    },
    {
        "title": "CSS center a response image with a caption",
        "description": "CSS center a response image with a caption",
        "date_created": "2025-07-20T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Center a response image  with a caption \n\n\nfigure {\n  inline-size: fit-content;\n  margin-inline: auto;\n}\nfigcaption {\n  /* no size contribution for figcaption */\n  contain: inline-size;\n}\nimg {\n  /* you can have max-width */\n  max-width: 100%; \n}",
        "locale": "en",
        "slug": "technical-posts/css-image-with-caption",
        "folder": ""
    },
    {
        "title": "Rumin8 Fall 2025",
        "description": "Rumin8 Fall 2025",
        "date_created": "2025-10-09T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rumin8"
        ],
        "content": "Catch up from Rumin8 3 - February 12, 2025\n\tWe discussed acquiring an email list. \n\tWe discussed a new campaign to our existing email list. We identified those who have DG for IBM (included those with and without maintenance) and on March 25th sent out a campaign. We found 576 unique email addresses, but after applying the \"unsubscribe\" rule we had 356 recipients. This eblast featured Monarch migration and ASNA Services. There were five registrants for a Monarch Webcast. Four attended, but were very quiet and didn't ask any meaningful questions. There was one registrant (mreyes@bncollege.com - Barnes and Nobel) whose registration looked promising but she didn't attend. \n\tWe discussed AI and how it might help us with finding prospects--but no concrete ideas were present; nor have any been identified since. \n\tWe discussed face-to-face visits with prospects (and TikTok). \n\tWe discussed online cost estimator calculator is suggested as a way to generate leads by offering a quick, albeit potentially inaccurate, estimate for migration costs.  We also discussed at another meeting making our Monarch offerings more ala-carte, mostly in the effort to keep the initial costs down (ie, testing as a separate component, not included with the base migration price) \n\nCoordination meeting notes\n\tWindows 10 retirement\t\n\t\tWindows 10 support ended on Tuesday, October 14\n\t\tThe ASNA Newsletter also went out on Tuesday, October 14th and its lead story was Windows 10 retirement (by both MS and ASNA)\n\t\t\thttps://www.asna.com/en/newsletter/q3-2025\n\tLakeB2B mailing list efforts\n\t\tOn Thursday, Oct 9th, we sent out an eblast to the ~2500 to whom we've previously sent an eblast plus 5000 to whom we've never sent anything. \n\t\t\tFirst mailing - Oct 9th\n\t\t\t\topens: 47% clicks: 23%\n\t\t\tSecond mailing - Oct 14th\n\t\t\t\topens: 38% clicks: 24%\n\t\tOur open rate averages usually vary between 30%-40%, so we are on track there. However, our click rate is through the roof on this campaign. Average click rates usually vary between 2% and 3% ours is ten times that!  \n\t\tOther campaigns have been in the 25-30% open rate and 3%-5% \n\t\tFor comparison, the EN ASNA newsletter (sent to our list last week) had 40% opens and 30% clicks. ES had 48%/31%.\n\tWe'll have a follow-up email Thursday, Oct 16th, but our expectations aren't very high on that. \n\tWe have ~16,000 email addresses left to send. We'll have a new campaign next week for 5000 new recipients. We'll repeat that over the next three weeks until all 16,000 have been sent. \n\tMore on this topic later in the Rumin8 meeting. \nToday's meeting\n\tMailing lists \n\t\tThere are essentially two types of mailing list providers: Commodity list resellers and List researchers (my terms, not theirs) \n\t\t\t\"Commodity list resellers\" (like LakeB2B, the company from whom we purchased our list) sells mailing lists (with email addresses and contact info) and generally claim to be able to do fine-grained queries to find specific customers. For example, LakeB2B claimed it could provide a list that is exclusively shops with IBM i boxes.\n\t\t\t\"List researchers\" don't focus on selling lists, they focus on selling research to find you a list. These business are very clear that it's not possible to buy a list today of IBM i users. To get to that level of detail they do deep research (websites, job listing, and LinkedIn, primarily) to find the target audience. \n\t\t\t\tThe only two metrics that are generally known to worthwhile metrics are industry type and business size. Deeper digging may be available through a custom query.\n\t\t\tThere are many commodity list resellers within the mailing list community, it's apparently pretty well known that there are very few list wholesalers (30 or 40 years ago there were many) and that all commodity list resellers buy from the same few wholesalers. One list researching told me that market is well-known to be hyper-commoditized.\n\t\t\tAn engagement with \"list researchers\" starts at 10K per year, but quickly grows to two or three times that if it includes custom queries and cold calling services\n\t\tWhen we purchased our list from Lake B2B, we discussed Constant Contact with they didn't raise any flags. However, we have since learned that Constant Contact isn't a good place from which to send cold emails. LakeB2B assured us that their email addresses are approved for sending emails, but apparently Constant Contact may not agree with that. \n\t\tInstantly.ai -- cold email service. $1k per year for up to 7500 emails monthly or $2K per year for up to 200k per month. While Instantly has a leads database, it's apparently not rated very highly--but as a cold email service it is highly rated. \n\tSales team \n\nBudget\nWe budgeted $30K for marketing efforts this year. \n\tWe spent $9k with LakeB2B\nWith the year \n\t\n\nThings to do: \nCreate what list creators call an customer profile \nCold calls\n\tIt's generally recommended that a cold email campaign have follow-up callers contact those who responded to the email (the clickers_\n\t\tWe sent a cold email to 7946 unique email addresses. 1827 recipients (23%) clicked through to our landing page. \n\t\t\tit takes 200 cold calls to get an appointment.\n\t\t\ta caller averages 33-50 calls per day.\n\t\t\tit takes 55 person days to call those 1827 recipients.\n\tindustry averages\n\t\t72% of cold calls do not get through\n\t\t90% of CEOs say they never open cold emails or accept cold calls\nIdeas\nRockwell's list has 43000 US IBM i shops on it. Do we hire cold callers to start working that list? (at 33 calls per day, that's 1300 days of calling!)\nMore compelling Monarch marketing content\n\tWhich includes videos\n\tWhite papers and \"white sheet\" collateral to send prospects \n\tCase studies\n\nLeads this year:\nYear to date we've received 9 leads from ASNA.com\n   ![[image-53.png|397x180]]\n   I think every recent migration prospect has come to us from the site.\nFor 2024 we received 11 leads, 2023 we received 8 leads\n\nQuestions:\nDo our migration prospects immediately associate RPG to .NET as doing what we all know it does? \nOf these prospects/customers, has the primary driver for migrating been anything other than persisting their application into the future without RPG programmers?\n\tCVS\n\tSWBC\n\tMTA\n\tBird Precision\n\tCentral Texas Iron\n\tFarm Credit of Texas\n\tPremier Paper\n\tOSCO\n\tHow about the Japanese prospects we've had \nWhat are biggest roadblocks to getting a prospect to move forward\n\tfrom a phone call to a meeting\n\tfrom a meeting to an assessment/POC\n\tfrom a POC to a migration\nWhat do migration customers most need help with? \nWhat are they most confused about? \nWhat are they most surprised about?\n\nWhat should the primary heading of our home page say?\n\tWhat verb should we be using for what we do?\n\t\tModernize\n\t\tMigrate",
        "locale": "en",
        "slug": "technical-posts/rumin8-4",
        "folder": ""
    },
    {
        "title": "Docker compose file tutorial",
        "description": "Docker compose file tutorial",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-09-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "containers"
        ],
        "content": "Okay, let's create a small tutorial on Docker Compose files.\n\nWhat is Docker Compose?\n\nDocker Compose is a tool for defining and running multi-container Docker applications. Instead of starting each container manually with docker run and setting up networks and volumes via the command line, you define your entire application stack (services, networks, volumes) in a single YAML file (usually docker-compose.yml or compose.yml). This makes managing complex applications much easier.\n\nPrerequisites:\n\nDocker Engine installed.\nDocker Compose installed (often included with Docker Desktop).\n\nThe docker-compose.yml / compose.yml File\n\nThis file uses YAML syntax, which relies heavily on indentation (usually 2 spaces) to define structure. The default filename is docker-compose.yml, but the newer standard compose.yml is also recognized.\n\nPrimary Sections of a Compose File:\n\nHere are the most common top-level sections you'll encounter:\n\nservices (Required):\n\n    This is the heart of the Compose file.\n    It defines the individual containers that make up your application.\n    Each key under services is a custom name you give to a service (e.g., web, db, api).\n    Each service definition contains configuration for how to run its container. Common sub-keys include:\n        image: :: Specifies the Docker image to use (e.g., nginx:latest, postgres:15).\n        build: : Specifies a path to a directory containing a Dockerfile. Compose will build the image before starting the service. You can use build: . for the current directory.\n        ports: [\":\"]: Maps ports from your host machine to the container (e.g., \"8080:80\" maps host port 8080 to container port 80).\n        environment: [\"VAR_NAME=value\"] or environment: {VAR_NAME: value}: Sets environment variables inside the container.\n        volumes: [\":\"]: Mounts directories or named volumes into the container for persistent data or code sharing (e.g., \"/path/on/host:/app/data\" or \"db_data:/var/lib/postgresql/data\").\n        networks: []: Connects the service to specified networks.\n        depends_on: []: Defines startup dependencies. The service will start only after the listed services have started. Note: This only waits for the container to start, not necessarily for the application inside it to be ready.\n\nnetworks (Optional):\n\n    Allows you to define custom networks for your services to communicate over.\n    If omitted, Compose creates a default bridge network for your application.\n    Defining custom networks gives you more control over isolation and connectivity.\n    Example: networks: { app_net: { driver: bridge } } defines a network named app_net.\n\nvolumes (Optional):\n    Allows you to define named volumes. Named volumes are managed by Docker and are the preferred way to persist data generated by containers.\n    You define the volume here (e.g., volumes: { db_data: {} }) and then reference it within a service's volumes section (e.g., volumes: [\"db_data:/var/lib/postgresql/data\"]).\n\n(Note: The version key was required in older Compose file formats but is generally optional now with the modern Compose Specification.)\n\nSimple Example: A Web Server and a Redis Cache\n\nLet's create a simple stack with an Nginx web server and a Redis database.\n\nCreate a directory for your project, e.g., my_compose_app.\nInside that directory, create a file named docker-compose.yml (or compose.yml).\nPaste the following content into the file:\n\nservices:\nService 1: The web server\n    web:\n        image: nginx:alpine # Use the official lightweight Nginx image\n        ports:\n            \"8080:80\" # Map host port 8080 to container port 80\n        networks:\n            app-network # Connect to our custom network\n        depends_on:\n            cache # Wait for the cache service to start first\n\nService 2: The Redis cache\n    cache:\n        image: redis:alpine # Use the official lightweight Redis image\n        networks:\n            app-network # Connect to our custom network\n        volumes:\n            cache-data:/data # Mount the named volume 'cache-data' to /data inside the container\n\nDefine custom network(s)\nnetworks:\n    app-network:\n        driver: bridge # Use the standard bridge network driver\n\nDefine named volume(s) for persistent data\nvolumes:\n    cache-data: {} # Define the named volume 'cache-data'\n\nExplanation:\n\nWe define two services: web and cache.\nweb uses the nginx:alpine image and exposes port 8080 on the host, mapping it to port 80 inside the container.\ncache uses the redis:alpine image.\nBoth services are connected to a custom network app-network, allowing them to communicate using their service names (e.g., the web service could connect to Redis using the hostname cache).\nThe cache service uses a named volume cache-data to persist its data in /data inside the container. This volume is defined at the bottom level.\nweb depends_on cache, so Compose will attempt to start cache before starting web.\n\nRunning Your Compose Application:\n\nOpen your terminal and navigate to the directory containing your docker-compose.yml file (my_compose_app).\nRun the command:\n    docker compose up\n    This command builds images if necessary (using build:), creates networks/volumes if they don't exist, and starts all the services defined in the file. You'll see logs from all containers in your terminal.\n    Add the -d flag (docker compose up -d) to run the containers in detached mode (in the background).\nTo see the running services:\n    docker compose ps\nTo view logs (especially if running in detached mode):\n    docker compose logs\nor follow logs in real-time\n    docker compose logs -f\nor view logs for a specific service\n    docker compose logs web\nTo stop and remove the containers, networks, and (optionally) volumes defined in the compose file:\n    docker compose down\nAdd -v to remove named volumes as well\n    docker compose down -v\n\nThat's a basic introduction! Docker Compose offers many more options for configuration, scaling, health checks, and more, but understanding services, networks, and volumes is the foundation.",
        "locale": "en",
        "slug": "technical-posts/docker-compose-file-tutorial",
        "folder": ""
    },
    {
        "title": "Pending article notes",
        "description": "Pending article notes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "article-ideas",
            "newsletter",
            "article"
        ],
        "content": "Video notes",
        "locale": "en",
        "slug": "technical-posts/pendng-article-notes",
        "folder": ""
    },
    {
        "title": "What does the Rust ? operator do",
        "description": "What does the Rust ? operator do",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "The question mark (?) operator in Rust is used to propagate errors in functions that return a Result or Option. It simplifies error handling by avoiding explicit match statements.\n\nExplanation:\n\nWhen you use ? at the end of an expression like:\n\nlet file = File::open(file_path)?;\n\nIt is shorthand for:\n\nCheck if the operation succeeds: If the result is Ok, the value inside Ok is returned.\nPropagate the error: If the result is Err, the function immediately returns the Err without further execution.\n\nBreakdown:\n\nFile::open(file_path) returns a Result.\n    If the file is successfully opened, the Result contains Ok(file), and file is assigned to the variable.\n    If the file cannot be opened (e.g., the file doesn't exist), it returns an Err(io::Error).\n\nThe ? operator handles the Result:\n\nIf it's Ok(file), the value is returned, and the program continues.\nIf it's Err(e), the function that contains this line will return Err(e) immediately, propagating the error upwards.\n\nExample without ?:\n\nIf you were to write this explicitly without ?, it would look like:\n\nlet file = match File::open(file_path) {\n    Ok(file) => file,\n    Err(e) => return Err(e),\n};\n\nUsing ? simplifies this to a single line, improving readability. The function using ? must return a Result (or Option if using ? with Option).",
        "locale": "en",
        "slug": "technical-posts/rust-question-mark-operator",
        "folder": ""
    },
    {
        "title": "Find and kill a Windows process with taskkill",
        "description": "Find and kill a Windows process with taskkill",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows"
        ],
        "content": "Find the process using port 5173\n\nnetstat -ano|findstr \"PID :5173\"\n\nIf found it shows the process number--in this case 18264. It may show more than one entry. Take the PID from the PID with where State = LISTENING.\n\n![[Pasted image 20231108212021.png]]\n\nTo kill the process using port 5173, use task kill with that process's PID:\n\ntaskill /pid 18264 /f\n\n![[Pasted image 20231108212216.png]]",
        "locale": "en",
        "slug": "technical-posts/kill-a-windows-process-with-taskill",
        "folder": ""
    },
    {
        "title": "html-novalidate-attribute",
        "description": "html-novalidate-attribute",
        "date_created": "2025-06-02T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "html"
        ],
        "content": "The novalidate attribute, when applied to an HTML `` element, tells the browser not to perform its built-in client-side validation on the form's input fields before submission.\n\nHere's a breakdown:\n\nDefault Browser Validation:\n    Modern browsers have built-in mechanisms to validate form inputs based on HTML5 attributes like required, type=\"email\", type=\"url\", pattern, min, max, minlength, maxlength, etc.\n    If an input fails this validation (e.g., a required field is empty, or an email field doesn't look like an email), the browser will typically prevent the form from submitting and display an error message (often as a tooltip or bubble) near the invalid field.\n\nWhat novalidate Does:\n    When you add novalidate to your `` tag, you are essentially saying, \"Browser, don't worry about your default validation rules for this form. I'll handle it, or I don't want client-side validation right now.\"\n    The form will submit even if, according to the browser's default rules, some fields are invalid.\n\nSyntax:\n\nIt's a boolean attribute. Its presence means it's active.\n\n\n  Email:\n  \n\n  Name:\n  \n\n  Submit\n\n\nIn the example above, even though the email and name fields are required, if they are left empty and novalidate is present, the browser will not stop the form submission due to these fields being empty.\n\nWhy Use novalidate?\n\nCustom JavaScript Validation:\n    You might want to implement your own, more sophisticated, or stylistically different validation logic using JavaScript. The browser's default validation messages and behavior might interfere with your custom implementation.\nServer-Side Validation Only:\n    While client-side validation is great for user experience (providing immediate feedback), server-side validation is absolutely crucial for security and data integrity. You might decide to rely solely on server-side validation, though this generally leads to a poorer user experience if errors aren't caught until after a page reload.\nTesting/Debugging:\n    During development, you might want to temporarily disable client-side validation to test server-side error handling or other backend logic without being blocked by the browser.\nSpecific User Experience Flows:\n    For certain flows, like saving a draft of a form that isn't yet complete, you might want to allow submission even if some fields don't meet final validation criteria.\n\nImportant Considerations:\n\nServer-Side Validation is a Must: novalidate only affects client-side validation. Malicious users (or even just users with JavaScript disabled) can bypass client-side validation. Always validate data on the server.\nUser Experience: If you use novalidate to implement custom validation, ensure your custom solution provides clear and accessible error messages to the user. If you use it to bypass client-side validation entirely, users might submit data, get an error from the server, and lose their input, which is frustrating.\n\nIn summary, novalidate gives developers control over when and how client-side form validation occurs, typically to allow for custom JavaScript validation solutions or to bypass it for specific reasons.",
        "locale": "en",
        "slug": "technical-posts/html-novalidate-attribute",
        "folder": ""
    },
    {
        "title": "Git nuggets",
        "description": "Git nuggets",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "git"
        ],
        "content": "To wipe uncommitted changes from a specific file\n\ngit restore \n\nCommit pending changes to a new branch--leave old branch as it was\n\nget switch -c newbranch\n\nCommit changes to the new branck\ngit cm 'commit changes to the new branch'\n\nSwitch back to main\nThis will be clean with no changes pending\ngit switch main\n\nTest for merge conflicts before committing a branch\n\ngit fetch origin\n\n2. Create a temporary branch from your target branch (e.g., main)\ngit checkout main                 # Switch to the branch you want to merge INTO\ngit pull origin main            # Ensure it's up-to-date\ngit checkout -b temp-merge-check  # Create and switch to a temporary branch\n\n3. Try merging your feature branch into this temporary branch\ngit merge your-feature-branch\n\n4. Observe the output.\n- If it merges cleanly, you're good.\n- If there are conflicts, Git will tell you. You can inspect them.\n\n5. Clean up: Switch back and delete the temporary branch\ngit checkout main\ngit branch -D temp-merge-check\n\nWhat does 'git fetch origin' do\n\ngit fetch origin is a command that downloads new data from the remote repository named origin but does not integrate any of this new data into your local working branches.\n\nLet's break it down:\n\ngit: The command-line tool for Git.\nfetch: The specific Git operation. This tells Git to go and get the latest information from a remote repository.\norigin: This is the default conventional name for the remote repository from which your local repository was originally cloned. It's essentially an alias for the URL of your remote repository (e.g., on GitHub, GitLab, Bitbucket). You can have multiple remotes, and origin is just the most common one.\n\nWhat git fetch origin does specifically:\n\nConnects to the Remote: It establishes a connection to the remote repository specified by origin.\nDownloads New Data:\n    It downloads any commits from all branches on origin that your local repository doesn't have yet.\n    It downloads any new tags from origin.\n    It updates your local remote-tracking branches. These are special local references that act like read-only bookmarks to the state of the branches on the remote repository.\n        For example, if there's a main branch on origin, git fetch origin will update your local origin/main branch to point to the same commit that main points to on the remote.\n        Similarly, for a feature-x branch on origin, it will update your local origin/feature-x.\n\nWhat git fetch origin does NOT do:\n\nIt does NOT change your current local working branch (e.g., your local main, develop, or feature-x branch).\nIt does NOT modify your working directory or staging area. Your files remain untouched.\nIt does NOT automatically merge or rebase the fetched changes into your local branches.\n\nWhy is this useful?\n\nSafety: You can see what changes have been made on the remote repository before deciding how or if you want to integrate them into your local work.\nInspection: After fetching, you can:\n    Compare your local branch with its remote counterpart:\n        git log main..origin/main  # See commits on origin/main that aren't on your local main\n        git diff main origin/main  # See the differences in content\n    Check out a remote branch to inspect it (this creates a new local branch tracking the remote one):\n        git checkout feature-x # If origin/feature-x exists, this might create a local feature-x tracking it\nor more explicitly:\n        git checkout -b my-local-copy-of-feature-x origin/feature-x\nPreparation for Merge/Rebase: Once you've fetched and are happy with the remote changes, you can then integrate them into your local branch using:\n    git merge origin/main (while on your local main branch)\n    git rebase origin/main (while on your local main branch)\n\ngit fetch vs. git pull:\n\nA common point of confusion is the difference between git fetch and git pull.\ngit fetch origin (as described above) only downloads new data.\ngit pull origin main is essentially a shortcut for:\n    git fetch origin\n    git merge origin/main (if your local main is tracking origin/main and you're on main)\n        (or git rebase origin/main if configured to do so)\n\nSo, git pull does the fetch and then immediately tries to integrate the changes. Many developers prefer to use git fetch first to see what's coming, and then manually git merge or git rebase for more control.\n\nIn summary, git fetch origin updates your local \"view\" of the remote origin repository without touching your own local work, allowing you to inspect changes before merging them.\n\nIs there a way to a dry run with 'git fetch orgin'\n\nYes, git fetch has a --dry-run option.\n\nYou can use:\n\ngit fetch --dry-run origin\n\nor the shorthand:\n\ngit fetch -n origin\n\nWhat it does:\n\nThis command will connect to the remote repository (origin in this case) and report what would be fetched (i.e., which remote branches/tags would be updated and to which commit SHAs) without actually downloading any objects or updating your local remote-tracking branches (like origin/main, origin/feature-x, etc.).\n\nExample Output:\n\nIf there are updates on the remote, the output might look something like this:\n\nFrom \n   1234567..abcdef0  main       -> origin/main  (would update)\n [new branch]      feature-y  -> origin/feature-y\n [new tag]         v1.1       -> v1.1\n\nThis tells you:\nThe main branch on origin would be updated from commit 1234567 to abcdef0.\nA new branch feature-y would be created locally as origin/feature-y.\nA new tag v1.1 would be fetched.\n\nIf there are no changes, it will typically say something like:\n\nFrom \nor remain silent if there's truly nothing to report.\n\nAnother related command (for seeing remote refs without fetching):\n\nWhile not strictly a \"dry run\" of the fetch operation itself, git ls-remote can show you the references (branches, tags) on the remote repository without downloading anything beyond that list of references.\n\ngit ls-remote origin\n\nThis will list all the heads (branches), tags, and other refs on the origin remote, along with the commit SHAs they currently point to. You can then manually compare this with your local origin/* branches (e.g., git show-ref origin/main) to see what a git fetch would likely do.\n\nWhen to use which:\n\ngit fetch --dry-run origin: Use this when you want to specifically simulate the fetch command and see its intended actions on your remote-tracking branches.\ngit ls-remote origin: Use this when you just want a raw listing of what's on the remote, perhaps for scripting or a quick check of a specific branch's SHA on the remote.\n\nFor your purpose of a \"dry run on 'git fetch origin'\", git fetch --dry-run origin is the direct answer.",
        "locale": "en",
        "slug": "technical-posts/git-nuggets",
        "folder": ""
    },
    {
        "title": "minimal-html-outlook-email-skeleton",
        "description": "minimal-html-outlook-email-skeleton",
        "date_created": "2025-06-26T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "html",
            "outlook"
        ],
        "content": "Do not use margins... ever\nDo all \"margin\" work with cell padding\nBec\n\n\nOf course. Crafting an email template for Outlook's Word-based rendering engine is an exercise in defensive coding. The \"phantom horizontal lines\" are a notorious issue, often caused by a combination of background colors, odd-numbered element heights, and Outlook's interpretation of font-size and line-height.\n\nThis template is designed to be as minimal and robust as possible, specifically targeting and mitigating those issues.\n\nKey Strategies to Avoid Outlook Phantom Lines\n\nborder-collapse: collapse;: Applied to all tables to prevent weird spacing between cells.\nmso-table-lspace: 0pt; mso-table-rspace: 0pt;: Prevents Outlook from adding extra, unwanted space around tables.\nmso-line-height-rule: exactly;: Forces Outlook to respect your specified line-height instead of guessing. This is one of the most critical fixes.\nMatching Background Colors: The `` has a background color that matches the main content wrapper. If a 1px gap appears, it will be filled with the same color, making it invisible.\nTable-Based Bullets: Using ` and  tags is a recipe for disaster in Outlook. We will create \"fake\" bullets using a nested table where one column holds the bullet (&bull;`) and the other holds the text. This gives us pixel-perfect control.\nVML Namespace: Including the VML (Vector Markup Language) namespaces in a conditional comment helps Outlook render things more predictably.\nExplicit line-height: Every element containing text has an explicit line-height set in pixels or points, which is more reliable than relative units like em or % in Outlook.\n\nMinimal HTML Email Template for Outlook\n\nHere is the code. Copy and paste this into an .html file. The content goes inside the `` section.\n\n\n\n\n  \n  \n  \n  \n  \n  Your Email Title\n\n  \n  \n    \n      \n      96\n    \n  \n  \n\n  \n    body {\n      margin: 0 !important;\n      padding: 0 !important;\n      background-color: #f2f2f2; /* Match this with the outer table bgcolor */\n    }\n    table {\n      border-spacing: 0;\n      border-collapse: collapse;\n      mso-table-lspace: 0pt;\n      mso-table-rspace: 0pt;\n    }\n    td {\n      padding: 0;\n      mso-line-height-rule: exactly;\n    }\n    /* A good-to-have reset for other clients */\n    img {\n      border: 0;\n      -ms-interpolation-mode: bicubic;\n    }\n  \n\n\n\n\n  \n  \n  \n    \n      \n        \n        \n        \n        \n        \n        \n\n        \n        \n        \n          \n          \n            \n            \n\n              \n\n              Here is a list of items for your review:\n\n              \n              \n                \n                \n                \n                  \n                    &bull;&nbsp;\n                  \n                  \n                    This is the first bullet point. It is rendered inside a table cell for maximum compatibility and control over spacing.\n                  \n                \n\n                \n                &nbsp;\n\n                \n                \n                  \n                    &bull;&nbsp;\n                  \n                  \n                    The second bullet point follows the same robust pattern.\n                  \n                \n\n                \n                &nbsp;\n\n                \n                \n                  \n                    &bull;&nbsp;\n                  \n                  \n                    By controlling every element with tables, we prevent Outlook's rendering engine from making poor decisions.\n                  \n                \n              \n              \n\n              \n\n            \n          \n\t    \n        \n\n        \n        \n        \n        \n        \n        \n\n      \n    \n  \n  \n\n\n\n\nAdding an image row\n\nOf course. Here is a robust, Outlook-safe table row for a heading image, built with the same defensive principles as the skeleton.\n\nIt includes a clickable link, fluid/responsive behavior for modern clients, and specific fixes to prevent spacing and line issues in Outlook.\n\nThe HTML for the Header Image Row\n\nSimply place this ` inside your main content , likely as the very first row before your content `.\n\nKey Features and Why They Matter\n\n` Reset (font-size: 0px; line-height: 0px;`): This is the most important defense against phantom spacing issues in Outlook. It collapses the cell's \"line box,\" ensuring that only the image itself contributes to the cell's height.\n\n` Styling (display: block;`): This is a universal fix that removes the small gap that most email clients and browsers add below an image by default (treating it as an inline text element).\n\nFluid-Hybrid Image:\n    width=\"600\": The width attribute is a hard instruction for Outlook. It ensures your image renders at the correct size and doesn't get distorted.\n    style=\"width: 100%; max-width: 600px;\": The CSS width property allows the image to shrink proportionally on smaller screens (like phones), making it responsive. The max-width prevents it from ever growing larger than its original size.\n\nClickable Wrapper (`): The link wraps the image, and its display: block; style ensures the entire area of the image is clickable, not just some part of it. text-decoration: none; and border: 0;` prevent any unwanted styling (like a blue underline or border) from appearing.\n\nBy using this structure, you get an image that is both pixel-perfect and stable in Outlook while remaining flexible and modern for all other clients.\n\nUsing background colors\n\nExcellent question. This is one of the most fundamental and frequently misunderstood aspects of HTML email development.\n\nThe short answer is: YES, you apply the background color to each ` individually, NOT the `.\n\nApplying styles to a ` is highly unreliable and will fail in many email clients, most notably Outlook. Think of the  as a logical grouping element, while the ` is the actual visual container that gets rendered.\n\nThe Best Practice: The \"Belt and Suspenders\" `` Method\n\nFor maximum compatibility, you should use both the HTML bgcolor attribute and the CSS background-color property on the `` itself.\n\nbgcolor=\"#eeeeee\": This is the \"old school\" attribute that is read by legacy clients like Outlook.\nstyle=\"background-color: #eeeeee;\": This is the modern CSS property that is respected by clients like Apple Mail, Gmail, and most web clients.\n\nBy using both, you ensure your color renders everywhere.\n\nExample: A Single-Column Row with a Background Color\n\nHere's how you would create a content row with a light gray background. Notice the color is applied directly to the cell.\n\n\n\n  \n  \n    This is my content within a cell that has a background color. This method is the most reliable across all email clients.\nExample: A Multi-Column Row\n\nIf your row has multiple columns and you want them all to have the same background, you must apply the bgcolor and style to every single `` in that row.\n\n\n\n  \n  \n    Left Column Content\n  \n\n  \n  \n    Right Column Content\n  \n\n\nIf you forget one, you'll see a white gap in that cell.\n\nAdvanced Technique: The Full-Width \"Wrapper Table\"\n\nA common requirement is to have a background color that spans the entire width of the email client's window, while the content inside remains constrained to a 600px width.\n\nThe `` method is still the key, but you achieve this with a nested table structure.\n\nOuter Table (Fluid): A 100% width table. The `` of this table gets the full-width background color.\nInner Table (Fixed): A 600px width table nested inside the first one. This holds your actual content.\n\nHere's the bulletproof code for that:\n\n\n\n  \n\n    \n    \n    \n    \n    \n\n    \n    \n      \n        \n          My content is on a white background, but this entire section is inside a full-width gray stripe.\nSummary:\n\nAlways apply color to the ``.\nUse both the bgcolor attribute and the background-color CSS property.\nIf a row has multiple ``s, apply the color to all of them.\nFor full-width color stripes, use the nested wrapper table pattern.\n\nCentering content\n\nExcellent question. Centering content, especially images, is a fundamental task in email design, and doing it robustly for Outlook requires a specific approach.\n\nThe best and most reliable method is to use the parent `` to control the alignment. You apply alignment attributes and styles to the table cell, which then dictates how the content inside it should be positioned.\n\nThe Golden Rule for Centering in Email\n\nControl alignment from the container, not the content. The most bulletproof container in HTML email is the ``.\n\nHere are the three primary scenarios and how to handle them.\n\nMethod 1: Centering an Image (Most Common)\n\nTo center an image, you set its parent `` to be centered. This works because an image is treated like a single, large character (an \"inline\" element).\n\nUse align=\"center\" on the `. For good measure, also add the CSS text-align: center;`.\n\n\n\n  \n  \n\n    . -->\n    \n      \n    \n\n  \n\n\n\nWhy this works: The ` creates a \"line box\" that spans its full width. align=\"center\" tells Outlook to place any content inside that box in the middle. text-align: center;` does the same for modern clients like Gmail and Apple Mail.\n\nMethod 2: Centering Text\n\nThis follows the exact same principle as centering an image. You apply the centering rules to the parent ``.\n\n\n\n  \n    This line of text will be perfectly centered. \n    And so will this second line.\nMethod 3: Centering a \"Block\" of Content (e.g., A Button or a Card)\n\nThis is the most advanced case. What if you want to center something that has its own width, like a button (which is often a small table)? You can't just align=\"center\" the button table itself.\n\nThe solution is to wrap it. You put your content block inside a ` that is set to align=\"center\"`.\n\nHeres how to center a bulletproof button:\n\n\n\n  \n  \n\n    \n    \n      \n        \n        \n          \n            Click Here\n          \n        \n      \n    \n\n  \n\n\nWhy this works: The outer ` is told to center its contents. The inner  (our button) is the content. The outer ` dutifully centers the entire button table. This pattern is essential for creating centered, multi-column layouts within a full-width row.\n\nSummary: Why This Is the Best Way\n\nMaximum Compatibility: The align attribute on a `` is one of the oldest and most universally supported pieces of HTML, especially in email clients like Outlook.\nReliability: It avoids modern CSS like Flexbox or margin: auto;, which have spotty or nonexistent support in many email clients.\nSimplicity: The logic is consistent. To center something, you don't touch the thing itself; you tell its parent `` to center it. This makes debugging much easier.",
        "locale": "en",
        "slug": "technical-posts/minimal-html-outlook-email-skeleton",
        "folder": ""
    },
    {
        "title": "Find all given files",
        "description": "Find all given files",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "Get-ChildItem -Path \"E:\\luther-backup\\win10rp-1809\\users\\roger\\documents\\Programming\\.\" -recurse | Where-Object {$_.Extension -in \".vr\", \".cs\"} | select fullname | Out-File \"C:\\Output.txt\" -Encoding UTF8\n\nGet-ChildItem -Path \"E:\\.\" -recurse | Where-Object {$_.Extension -in \".vr\", \".cs\"} | select fullname | Out-File \"e:\\all-folders.txt\" -Encoding UTF8\n\nGet-ChildItem -Path \"E:\\.\" -recurse | `\n          Where-Object {$_.Extension -in \".vr\", \".cs\", \".sln\"} | `\n          ForEach-Object { [PSCustomObject]@{ FileName = $.Name; FullName = $.FullName } } | `\n          Export-Csv -Path \"e:\\file_list.csv\" -NoTypeInformation -delimiter \"|\"\n\nGet-ChildItem -Path \".\\.\" -recurse | `\n        Where-Object {$_.Extension -in \".vr\", \".dll\", \".sln\", \".aspx\", \".asmx\", \".ashx\", \"*.master\"} | `\n        ForEach-Object { [PSCustomObject]@{ FileName = $.Name; FullName = $.FullName; LastWriteLtime = $_.LastWriteTime } } | `\n        Export-Csv -Path \".\\file_list.csv\" -NoTypeInformation -delimiter \",\"\n\nGet-ChildItem -Path \"L:\\.\" -recurse | `\n           Where-Object {$_.Extension -in \".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".svg\" | `\n           ForEach-Object { [PSCustomObject]@{ FileName = $.Name; FullName = $.FullName } } | `\n           Export-Csv -Path \"c:\\users\\thumb\\documents\\do-images.csv\" -NoTypeInformation -delimiter \",\"\n\nHere is another way to get a list of files with PowerShell\n\nget-childitem *.md -recurse | select-object lastwritetime, fullname | sort-object -property lastwritetime -descending\n\nI moved these projects from AVR to CS\n\nDirectory: C:\\Users\\thumb\\Documents\\projects\\avr\\export-sql-query-to-csv\nDirectory: C:\\Users\\thumb\\Documents\\projects\\avr\\ImportCSV\nDirectory: C:\\Users\\thumb\\Documents\\projects\\avr\\read-dynamics-account-excel\nDirectory: C:\\Users\\thumb\\Documents\\projects\\avr\\export-sql-query-to-csv",
        "locale": "en",
        "slug": "technical-posts/find-all-given-files",
        "folder": ""
    },
    {
        "title": "PowerShell nuggets",
        "description": "PowerShell nuggets",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell",
            "nuggets"
        ],
        "content": "To deploy a PS1\n\nPut the script in this folder (which is in my path):\n\nC:\\Users\\thumb\\Documents\\rp-utilities\n\nShow installed versions of .NET Framework\n\nGet-ChildItem 'HKLM:\\SOFTWARE\\Microsoft\\NET Framework Setup\\NDP' -Recurse | Get-ItemProperty -Name Version,Release -EA 0 | Where { $_.PSChildName -match '^(?!S)\\p{L}'} | Select PSChildName, Version, Release\n\nCounting files in a folder\n\nThe @(..) is the array subexpression operator. This ensures that the output of Get-ChildItem is treated as an array, even if it finds zero files or only one file. This is important because single objects don't have a .Count property, but arrays do (even an empty array has a .Count of 0).\n\n$is_dg400_present = @(Get-ChildItem -Path $datagate_family_changes_path -Filter \"dg400*\" -File).Count\n\nCount\n\nBackup SQL Server\n\n[[sql-server-backup-with-powershell]]\n\nWork with processes\n\nGet processes that start with something (msedge in this case)\n\nGet-Process -Name msedge*\n\nStop all processes that start with something:\n\nStop-Process -Name msedge* -force",
        "locale": "en",
        "slug": "technical-posts/powershell-nuggets",
        "folder": ""
    },
    {
        "title": "Creating Json indexing files for ASNA.com site search",
        "description": "Creating Json indexing files for ASNA.com site search",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna"
        ],
        "content": "There are two existing Json files that provide data to the search engine:\n\nsrc\\lib\\data\\search-index.json\n\nwhich provides indexing info on markdown pages and is created by create-search-data.js. The input data for this comes from the data\\flatObjects.js file.\n\nsrc\\lib\\data\\search-index-pages.json\n\nwhich provides indexing info on HTML pages is created by generate-page-search-data.js.\n\nsrc\\lib\\cmd-line\\refresh-algolia-index.ps1\n\nThis PowerShell script calls create-search-data.js and generate-page-search-data.js to refresh the index data and then refresh-algolia-index.js to push the fresh search data to Algolia. refresh-algolia-index.ps1 is called by full-deploy.ps1",
        "locale": "en",
        "slug": "technical-posts/creating-json-indexing-files-for-asna",
        "folder": ""
    },
    {
        "title": "Javascript map function",
        "description": "Javascript map function",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "Okay, let's dive deeper into the JavaScript Map object. It's a powerful data structure for storing key-value pairs, offering advantages over plain JavaScript objects in certain scenarios.\n\nKey Characteristics of Map\n\nKey Flexibility:\n\n    Unlike regular JavaScript objects, which can only use strings or Symbols as keys, a Map can use any data type as a key. This includes:\n        Strings\n        Numbers\n        Booleans\n        null\n        undefined\n        Symbols\n        Objects (including other Maps)\n        Functions\n    This makes Map suitable for situations where you need to associate data with complex or non-string identifiers.\n\nOrder Preservation:\n\n    Map maintains the order in which key-value pairs are inserted. When you iterate over a Map, you get the entries in the order they were added. This behavior contrasts with plain JavaScript objects, where the order of properties is not guaranteed.\n\nSize:\n\n    Map has a built-in .size property that efficiently provides the number of key-value pairs it holds. This is different from a regular object, where you have to use Object.keys(obj).length to get the number of properties, which can be less performant.\n\nBuilt-in Methods:\n    Map provides several methods for adding, retrieving, updating, and removing entries:\n        set(key, value): Adds a new key-value pair or updates the value of an existing key.\n        get(key): Retrieves the value associated with the given key. Returns undefined if the key doesn't exist.\n        has(key): Checks if the given key exists in the map and returns a boolean\n        delete(key): Removes the key-value pair associated with the given key. Returns true if the key existed and was removed, false otherwise.\n        clear(): Removes all key-value pairs from the map.\n        keys(): Returns an iterator for the keys in the map.\n        values(): Returns an iterator for the values in the map.\n        entries(): Returns an iterator for the key-value pairs (as [key, value] arrays) in the map.\n        forEach(callbackFn): Executes a provided function once for each key-value pair in the map in insertion order.\n\nHow to use a Map\n\nCreating a Map:\n\n    const myMap = new Map(); // Create an empty map\n    const anotherMap = new Map([\n        // Create a map with initial key-value pairs\n        [\"key1\", \"value1\"],\n        [123, \"number key\"],\n        [{ id: 1 }, \"object key\"],\n    ]);\n\nAdding and Updating:\n\n    myMap.set(\"name\", \"Alice\");\n    myMap.set(\"age\", 30);\n    myMap.set(\"age\", 31); // Updates the age\n\nRetrieving:\n\n    console.log(myMap.get(\"name\")); // Output: Alice\n    console.log(myMap.get(\"age\")); // Output: 31\n    console.log(myMap.get(\"city\")); // Output: undefined\n\nChecking Existence:\n\n    console.log(myMap.has(\"name\")); // Output: true\n    console.log(myMap.has(\"city\")); // Output: false\n\nRemoving and Clearing:\n\n    myMap.delete(\"age\"); // Remove age\n    console.log(myMap.has(\"age\")); // Output: false\n    myMap.clear(); // Remove all entries\n    console.log(myMap.size); // Output: 0\n\nIterating:\n\n    const iterateMap = new Map([\n        [\"one\", 1],\n        [\"two\", 2],\n        [\"three\", 3],\n    ]);\n\n    for (const key of iterateMap.keys()) {\n        console.log(key);\n    }\n    // Output: one, two, three\n\n    for (const value of iterateMap.values()) {\n        console.log(value);\n    }\n    // Output: 1, 2, 3\n\n    for (const [key, value] of iterateMap.entries()) {\n        console.log(${key}: ${value});\n    }\n    // Output: one: 1, two: 2, three: 3\n\n    iterateMap.forEach((value, key) => {\n        console.log(${key}: ${value});\n    });\n    // Output: one: 1, two: 2, three: 3\n\nWhy Use Map Over Plain Objects?\n\nComplex Keys: When you need keys that are not strings or Symbols (e.g., objects or numbers).\nOrder Matters: When you need to preserve the order in which key-value pairs are added.\nSize Property: For efficiently getting the number of entries.\nAvoiding Prototype Pollution: Using a plain object for mapping can sometimes lead to unexpected conflicts with properties inherited from the object's prototype. Maps avoid this issue.\nBuilt-in Methods: Maps provide an API that provides all common usecases for this datatype\n\nWhen to Use Plain Objects:\n\nSimple key-value lookups with string keys.\nJSON object structure\nCreating an object literal.\n\nIn summary: The Map object is a versatile tool in JavaScript for managing key-value pairs, especially when you need keys of arbitrary types, order preservation, or the built-in methods it provides. It's an essential part of modern JavaScript development.",
        "locale": "en",
        "slug": "technical-posts/javascript-map-function",
        "folder": ""
    },
    {
        "title": "PC Setup",
        "description": "PC Setup",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "Terminal\n\nInstall Nerdfont (currently using Anonymous Pro )\nPowerShell 7 Terminal settings\n\n![[windows-terminal-powershell-config.png|500]]\n\nPowerShell\n\nGit---\n\ndescription:\ndate_created: 2025-01-05 12:00\ndate_updated: 2025-01-05 12:00\ndate_published:\ntags:\n\n\n\nGeneral setup https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup\nSSH https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\ngit config --global user.name \"John Doe\"\ngit config --global user.email johndoe@example.com\n\nGet-Service -Name ssh-agent | Set-Service -StartupType Manual\nStart-Service ssh-agent\n\nResilio\n\n[[Resilio folder syncing]]\n\nPowerShell\n\nInstall PowerShell 7\nMake it the default profile in Terminal\n\nInvoke-Expression (&starship init powershell)\n\nfunction get-pwd {\n    (pwd).path | Set-Clipboard\n}\n\nfunction Invoke-PowerShell {\n    powershell -nologo\n    Invoke-PowerShell\n}\n\nfunction start-live-server {\n--open=/public\n    live-server --port=5386\n}\n\nfunction runc {\n    pnpm run postcss:watch\n}\n\nfunction rund {\n    pnpm run dev\n}\n\nfunction runb {\n    pnpm run build\n}\n\nOpen a new MS Terminal tab at the current location.\n\nStarship\n\nHave a Nerdfont and PowerShell already configured\nInstall Starship\nAdd .config folder to c:\\users\\[user]\nAdd starship.toml to .config folder\n\nTweaks\n\nChrist Titus WinUtil\n\nirm \"https://christitus.com/win\" | iex\n\nIt's \"Advanced Tweaks\" removes the Gallery from the Explorer and restores the Win 20 context menu.",
        "locale": "en",
        "slug": "technical-posts/pc-setup",
        "folder": ""
    },
    {
        "title": "CSS Holy Grail - Una",
        "description": "CSS Holy Grail - Una",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "https://codepen.io/rogerpence/pen/vEBONzN\n\nSure, here's an explanation of what each line in the provided CSS file is doing:\n\ndiv.main-wrapper {\n    display: grid;\n    height: 100vh;\n    grid-template: auto 1fr auto/auto 1fr auto;\n}\n\ndiv.main-wrapper: Targets a div element with the class main-wrapper.\ndisplay: grid;: Sets the display of the div to a CSS Grid layout.\nheight: 100vh;: Sets the height of the div to 100% of the viewport height.\ngrid-template: auto 1fr auto/auto 1fr auto;: Defines a grid with three rows and three columns. The rows are auto (height determined by content), 1fr (remaining space), and auto. The columns are auto, 1fr, and auto.\n\nheader {\n    background: lightpink;\n    padding: 2rem;\n    grid-column: 1/4;\n}\n\nheader: Targets the header element.\nbackground: lightpink;: Sets the background color to light pink.\npadding: 2rem;: Adds padding of 2 rem units around the content.\ngrid-column: 1/4;: Spans the header across all three columns (from column 1 to 4).\n\n.left-sidebar {\n    background: lightblue;\n    grid-column: 1/2;\n}\n\nleft-sidebar: Targets an element with the class left-sidebar.\nbackground: lightblue;: Sets the background color to light blue.\ngrid-column: 1/2;: Places the element in the first column.\n\nmain {\n    background: coral;\n    grid-column: 2/3;\n}\n\nmain: Targets the main element.\nbackground: coral;: Sets the background color to coral.\ngrid-column: 2/3;: Places the element in the second column.\n\n.right-sidebar {\n    background: yellow;\n    grid-column: 3/4;\n}\n\nright-sidebar: Targets an element with the class right-sidebar.\nbackground: yellow;: Sets the background color to yellow.\ngrid-column: 3/4;: Places the element in the third column.\n\nfooter {\n    background: wheat;\n    padding: 2rem;\n    text-align: center;\n    grid-column: 1/4;\n}\n\nfooter: Targets the footer element.\nbackground: wheat;: Sets the background color to wheat.\npadding: 2rem;: Adds padding of 2 rem units around the content.\ntext-align: center;: Centers the text inside the footer.\ngrid-column: 1/4;: Spans the footer across all three columns (from column 1 to 4).\n\nbody {\n    font-family: system-ui, sans-serif;\n}\n\nbody: Targets the body element.\nfont-family: system-ui, sans-serif;: Sets the font family to system-ui with a fallback to sans-serif.\n\n.left-sidebar,\n.right-sidebar {\n    padding: 1rem;\n}\n\nleft-sidebar, .right-sidebar: Targets both .left-sidebar and .right-sidebar elements.\npadding: 1rem;: Adds padding of 1 rem unit around the content of both sidebars.",
        "locale": "en",
        "slug": "technical-posts/css-holy-grail-una",
        "folder": ""
    },
    {
        "title": "Rust's OK result variant",
        "description": "Rust's OK result variant",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "In Rust, the reason Ok(()) requires two sets of parentheses is due to the type system and the distinction between unit type (()) and the result type.\n\nKey Points:\n\nUnit Type ():\n\n    The unit type () in Rust represents \"nothing\" or \"void\", similar to void in other languages. Its often used to indicate that a function doesnt return a value or that theres no meaningful result.\n    When a function returns () (e.g., fn my_function() -> () {}), it signifies that the function doesn't return anything.\n\nResult Type:\n    In Rust, Result is used to represent a result that can be either Ok(T) for success or Err(E) for failure.\n    The Ok variant contains a value of type T. The Err variant contains a value of type E.\n\nOk(()) Explained:\n\nOk(()) means the function succeeded and it returns an Ok variant containing the unit type (), signifying no actual value is being returned.\nOk() would not compile because Ok requires an argument  in this case, its returning () to explicitly indicate a successful result without any associated value.\n\nExample Breakdown:\n\nIn your main function, you return io::Result:\n\nfn main() -> io::Result {\n    // your code\n    Ok(())\n}\n\nHeres what's happening:\n\nThe return type is io::Result, meaning it expects either:\n    Ok(()) on success, where () is the unit type (indicating the function succeeded without returning a meaningful value).\n    Or Err(e) on failure, where e is some error type (e.g., io::Error).\n\nSince your main function doesnt need to return any actual value but you want to indicate it completed successfully, you return Ok(()).\n\nSummary:\n\nOk(()) means the function succeeded and there is no meaningful value to return.\nThe () inside Ok is the unit type, which signifies \"nothing\" or \"void\". The outer Ok indicates success in the Result type.\n\nLet me know if you'd like further clarification on Rust's type system!\n\n\n[[Read a text file with Rust#What is Ok(lines) doing? | Read a text file with Rust]]\n\nNo, Ok is not a method, but rather a variant of the Result enum in Rust.\n\n[!info]\nWhen you write Ok(value), you are creating an instance of the Result enum where the operation was successful, and the value of T is contained inside Ok.\n\nExplanation of Result Enum:\n\nIn Rust, Result is an enum defined in the standard library with two variants:\n\nOk(T): Represents a successful result, containing a value of type T.\nErr(E): Represents a failure or error, containing a value of type E.\n\nHeres a simplified version of how Result is defined in Rust:\n\nenum Result {\n    Ok(T),\n    Err(E),\n}\n\nOk as a Variant:\n\nOk is a variant of the Result enum, not a method. Its used to indicate that the result of an operation is successful.\nWhen you write Ok(value), you are creating an instance of the Result enum where the operation was successful, and the value of T is contained inside Ok.\n\nExample:\n\nfn divide(a: f64, b: f64) -> Result {\n    if b == 0.0 {\n        Err(String::from(\"Cannot divide by zero\"))\n    } else {\n        Ok(a / b) // Ok variant contains the result of the division\n    }\n}\n\nIn this example:\n\nIf the division is successful, Ok(a / b) is returned.\nIf an error occurs (like dividing by zero), Err(\"...\") is returned.\n\nSummary:\n\nOk is a variant of the Result enum, not a method.\nIts used to signify the successful result of an operation in a Result type, containing the value of type T.\n\nIs OK(()) like returning void in C?\n\nYes, Ok(()) in Rust is conceptually similar to returning void in C, but with some key differences due to Rust's type system and the use of the Result enum.\n\nComparison to Cs void:\n\nIn C: A function that returns void does not return any value, indicating that no meaningful result is expected. For example:\n\n    void my_function() {\n        // Do something, but no return value\n    }\n\nIn Rust: The unit type () represents \"nothing\" or \"no meaningful value,\" which is the equivalent of void in C. A function can return () when it has no value to return. However, in Rust, Ok(()) is often used when a function that returns a Result needs to indicate success, but with no specific value.\n\n    Example:\n\n    fn my_function() -> () {\n        // Do something, but no return value\n    }\n\nOk(()) in Rust:\n\nOk(()) is used in functions that return Result, where () signifies that the function succeeded, but there is no value to return.\nIt combines the idea of a successful operation (Ok) with the concept of returning \"nothing\" (()).\n\nFor example:\n\nfn perform_operation() -> Result {\n    // Perform some operation\n    if success {\n        Ok(()) // Indicates success without returning any meaningful value\n    } else {\n        Err(String::from(\"Operation failed\"))\n    }\n}\n\nKey Differences Between C's void and Rust's Ok(()):\n\nC's void: Used in functions that dont return a value. It doesnt provide any context about success or failure.\nRust's Ok(()): Indicates that an operation succeeded (due to the Ok variant of Result) but has no value to return (represented by ()).\n\n    Ok indicates success, similar to how 0 might be returned in C for success.\n    Err(E) indicates failure and holds an error value of type E.\n\nSummary:\n\nOk(()) is similar to returning void in C but with the added advantage of Rust's Result type, which allows functions to return an indication of success or failure.\nIt provides better type safety and error handling than a void function in C.",
        "locale": "en",
        "slug": "technical-posts/rust-ok-result-variant",
        "folder": ""
    },
    {
        "title": "Pretty printing Trree output with line drawing ASCII characters.",
        "description": "Pretty printing Trree output with line drawing ASCII characters.",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "Question:\n\nPretty print this text replacing the plus signs, bar character, and dash character with higher order ascii line drawing characters:\n+---.vs\n   +---WorkWithAjax\n       +---config\n       +---CopilotIndices\n          +---17.13.441.19478\n       +---FileContentIndex\n       +---v17\n+---App_Code\n+---assets\n   +---css\n      +---smoothness\n          +---images\n   +---js\n+---Bin\n+---services\n+---views\n+---main\n\nAnswer:\n\n.vs\n   WorkWithAjax\n      config\n      CopilotIndices\n         17.13.441.19478  <-- Adjusted last item marker for clarity\n      FileContentIndex\n      v17                <-- Adjusted last item marker for clarity\nApp_Code\nassets\n   css\n      smoothness         <-- Adjusted last item marker for clarity\n          images         <-- Adjusted last item marker for clarity\n   js                   <-- Adjusted last item marker for clarity\nBin\nservices\nviews                    <-- Adjusted last item marker for clarity\n    main                 <-- Adjusted last item marker for clarity\n\nExplanation of Characters Used:\n\n (U+2502 Box Drawings Light Vertical): Replaced \n (U+2500 Box Drawings Light Horizontal): Replaced -\n (U+251C Box Drawings Light Vertical and Right): Replaced + when it's an intermediate item in a list.\n (U+2514 Box Drawings Light Up and Right): Replaced + when it should be the last item in a list",
        "locale": "en",
        "slug": "technical-posts/pretty-printing-tree-output",
        "folder": ""
    },
    {
        "title": "RowSpacer email component",
        "description": "RowSpacer email component",
        "date_created": "2025-07-23T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "email-builder-v8"
        ],
        "content": "Overview\n\nThe RowSpacer component creates vertical spacing in HTML emails, specifically designed for cross-client compatibility with Microsoft Outlook. It renders a table row with precise height control that works consistently across different email clients.\n\nFile Location\nsrc/lib/components/experimental/RowSpacer.svelte\n\nPurpose\n\nPrimary: Add precise vertical spacing between email content sections\nCross-client compatibility: Handles Outlook's unique rendering quirks\nFlexible: Supports custom heights and background colors\n\nProps Interface\n\ninterface Props {\n    eData: DynamicObject;          // Email data object (required)\n    height?: number | string;       // Spacer height in pixels (default: 20)\n    backgroundColor?: string;       // Background color (optional)\n}\n\nProp Details\n\n| Prop | Type | Default | Description |\n|------|------|---------|-------------|\n| eData | DynamicObject | required | Global email data object containing theme settings |\n| height | number \\| string | 20 | Height of the spacer in pixels |\n| backgroundColor | string | eData.global.emailBackgroundColor | Background color for the spacer |\n\nUsage Examples\n\nBasic Usage\n\nWith Custom Background\n\nNumeric Height\n\nMinimal Spacing\n\nTechnical Implementation\n\nOutlook Compatibility Strategy\n\nThe component uses a dual-rendering approach to handle Outlook's unique CSS processing:\n\nMSO Conditional: `` - Processed by Outlook with full CSS trust\nNon-MSO Conditional: `` - Processed by other email clients\nFallback: Plain HTML for maximum compatibility\n\nRendered HTML Structure\n\n\n    \n        \n            \n            ...\n            \n            \n            \n            ...\n            \n            \n            ...            \nNutty stuff going on! \n\nThe code above would seem to render two rows;  the mso or the non-mso row, and then the final plain HTML row. However, that isn't what happens. Only one of the three rows is rendered. \n\nThe old Outlook ( mso) and pure HTML in a browser (non-mso) render the respective markup and ignore the plain HTML row. \n\nFor Web clients, Gmail and Web Outlook ignore the plain HTML version and renders a slightly mangled version of the non-mso content. However, new Outlook renders ignores the mso and non-mso rows and correctly renders the third plain HTML row. \n\nThis means that you cannot get a high-fidelity rendering on all clients. You need target those clients that you think the majority of your readers are most using. It's also important to provide a \"read online\" option where the HTML does render well as an HTML document in browsers (and since browsers are seen as non-mso that's easy to. Alas, Web browser clients are not seen as either and therein lives the rub!\n\nKey CSS Properties\n\nheight: Sets exact pixel height\nline-height: Matches height for consistent spacing\nfont-size: 1px: Minimizes text impact on spacing\nmso-line-height-rule: exactly: Forces Outlook to respect exact measurements\nmso-line-height-alt: Outlook-specific line height override\n\nBest Practices\n\nRecommended Heights\nSmall gaps: 5-15px\nSection spacing: 20-40px  \nMajor breaks: 50-80px\n\nUsage Patterns\n\nBrowser/Client Support\n\n| Client | Support | Notes |\n|--------|---------|-------|\n| Outlook 2016+ |  | Primary target - uses MSO conditionals |\n| Gmail |  | Uses standard HTML/CSS |\n| Apple Mail |  | Uses standard HTML/CSS |\n| Thunderbird |  | Uses standard HTML/CSS |\n| Outlook.com |  | Modern web-based rendering |\n\nDependencies\n\nMsoTable: Parent table wrapper component\nhelpers: Utility functions (not directly used but imported)\n\nTroubleshooting\n\nCommon Issues\n\nInconsistent spacing in Outlook\n   Ensure both MSO and non-MSO conditionals are present\n   Don't remove the \"duplicate\" TD elements\n\nMinimum height enforced\n   Outlook may enforce 8-12px minimum regardless of settings\n   Use font-size: 0px for smaller spacing if needed\n\nBackground color not showing\n   Check eData.global.emailBackgroundColor is properly set\n   Verify background color format (hex, rgb, named colors)\n\nDebug Tips\n\nPerformance Notes\n\nLightweight component with minimal DOM impact\nTemplate string generation happens once per component instance\nNo reactive dependencies beyond initial prop evaluation",
        "locale": "en",
        "slug": "technical-posts/row-spacer-component",
        "folder": ""
    },
    {
        "title": "JSDoc example type definitions",
        "description": "JSDoc example type definitions",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "jsdoc"
        ],
        "content": "This type\n\n/**\n @typedef {Object} Product\n @property {string} name - The name of the product.\n @property {number} availability_status - Availability status of the product (0: unavailable, 1-6: available).\n @property {number} sort_order - The sort order for the product.\n @property {number} download_page_order - The order of appearance on the download page.\n @property {number} product_sort_order - The order for product sorting.\n @property {string} family_key - Key identifying the product family.\n @property {string} product_key - Key identifying the product.\n @property {string} unique_product_key - Unique key identifying the product.\n @property {string} vs_version - Version of Visual Studio compatible with the product.\n @property {boolean} is_com - Indicates if the product is COM-based.\n @property {string} release_date - The product's release date in YYYY-MM-DD format.\n @property {string} download_section_heading - Heading for the download section.\n @property {string} family_name - The name of the product family.\n @property {string} product_name - The name of the product.\n @property {string} product_trademark_name - The product's trademarked name.\n @property {string} product_version - The version of the product.\n @property {string} s3key - S3 key for the product's download executable.\n @property {string} s3_readme_key - S3 key for the product's README file.\n */\n\nsrc/routes/+page.server.js\n\nThis page gets the products array and returns it and an 'age' property to the owning page. In this contrived example, the 'age' property is added to show how you can define a return object with multiple properties.\n\n// src/routes/+page.server.js\n\n/**\n @returns Promise\n */\nexport async function load({ fetch }) {\n    const response = await fetch(\"/api/products\"); // Fetches data server-side\n\n    /**\n     @type {{ products: Product[] }}\n     */\n    const products = await response.json();\n\n    return {\n        products,\n        age: 71,\n    };\n}\n\nsrc/routes/+page.js\n\nIn the owning page, after defining the data variable's type you can see that Intellisense shows the type info.\n\nIf you omit this type defintion, Intellisense seems to know a little about the data value's type, but not enough to provide intellisense in the page's output area.\n\nThere is indeed a redundancy with this. The object type needs to be defined in both the +page.server.js page and in the +page.js. This could be avoided with a typedef, and that may be warranted in some cases. But in many cases, the value passed from the server to the client is a one-off special case. In those cases where you know the data definition is used in several places use a typedef.\n\n![[jsdoc-example-type-definitions.png|500]]\n\n[!info]\nThe blank lines between the type definition and the variable is to be able to fit the Intellisense window between the two. You would usually put the type definition immediately above the variable like this:\n\n/**\n  @type {{ products: Product[], age: number }}\n  */\n export let data\n\n[!info]\nNote the double braces {{}} in the type definition for the data variable. This defines that data is object.\n\nUsing a typedef\n\nJSDoc's typedef defines a reusable type. The typedef below defines a type definition for the products/age object.\n\n/**\n Represents the result of the load function\n @typedef {Object} LoadResult\n @property {Product[]} products - An array of Product objects\n @property {number} age - The age value\n */\n\nOr more concisely:\n\n[!tldr]\nI'm not sure about defining the LoadResult value twice. I need to check this.\n\n/**\n Represents the result of the load function\n @typedef {Object} LoadResult\n @property {{products: Product[], age: number}} LoadResult\n */",
        "locale": "en",
        "slug": "technical-posts/jsdoc-example-type-definitions",
        "folder": ""
    },
    {
        "title": "SQL Server nuggets",
        "description": "SQL Server nuggets. Get columns as a list",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sql",
            "lakeb2b"
        ],
        "content": "The list-history is a table containing all email addresses that have been sent an email. \n\nThere is a tag that denotes the campaign date (ie, 2025-06-22). On 9 October we sent out a campaign that went to 5000 new email addresses and 2500 email addresses that had already received an email. \n\nlist_history  is a list of email addresses. There may be duplicate email rows.\n\nThis query shows the countries represented and the number of email addresses for that country\n\nselect country, count(country) as [emails] from list_history\ngroup by country\norder by country\n\nThis query shows the country and the number of email dupes for that country\n\nwith recipients as (\n    select email, count(email) as [emails] from list_history\n    group by email\n) \n\nselect * from recipients \nwhere emails > 1\n\nFind and count all email addresses in list_history for each email address that occurs more than once.\n\nWITH EmailCounts AS (\n    SELECT \n        *,\n        COUNT(*) OVER (PARTITION BY email) as EmailCount\n    FROM list_history\n)\nSELECT *\nFROM EmailCounts\nWHERE EmailCount > 1\nORDER BY email;\n\nGet column names as a list\n\nSELECT STRING_AGG(name, ', ') WITHIN GROUP (ORDER BY column_id)\nFROM sys.columns\nWHERE object_id = OBJECT_ID('original');\n\nCopying a table\n\nThe \"best\" way depends entirely on whether you need a quick backup (just data) or a production-ready clone (indexes, keys, and constraints included).\n\nMethod 1: The Fastest Way (Data & Structure Only)\nUse SELECT INTO if you just need a backup copy for testing or safety. This creates the new table automatically on the fly. The target table cannot exist! \n\nSELECT *\nINTO MyTable_Backup\nFROM MyTable;\n\nPros: Extremely fast.\nCons: It DOES NOT copy Primary Keys, Indexes, Foreign Keys, or Defaults. It only copies the column definitions and the data.\n\nMethod 2: The \"Exact Clone\" Way (Includes Keys & Indexes)\nIf you need the new table to behave exactly like the old one (e.g., enforcing unique IDs, fast lookups), you cannot do this with a single query. You must use SQL Server Management Studio (SSMS):\n\nRight-click your table in Object Explorer.\nSelect Script Table as > CREATE To > New Query Editor Window.\nPress Ctrl+H (Find & Replace):\n    Find: [OriginalTable]\n    Replace: [NewTable]\nExecute the script (this creates the empty table with all rules/indexes).\nRun this query to move the data:\n\n-- Required if you have an Identity (auto-increment) column\nSET IDENTITY_INSERT NewTable ON;\n\nINSERT INTO NewTable (Column1, Column2, Column3)\nSELECT Column1, Column2, Column3\nFROM OriginalTable;\n\nSET IDENTITY_INSERT NewTable OFF;\n\nWhich one should you use?\nJust backing up before a risky update? Use Method 1.\nCreating a staging table for an application? Use Method 2.\n\n\n--UPDATE list_history\n--SET list_history.country = prospects.Country\n--FROM list_history\n--INNER JOIN prospects ON list_history.email = prospects.email;\n\nselect * from list_history where country is null\n\n\nselect country, count(country) as [emails] from list_history\ngroup by country\norder by country\n\nwith recipients as (\nselect email, count(email) as [emails] from list_history\ngroup by email\n) \n\nselect * from recipients \nwhere emails > 1\n\n\nselect count(distinct email) from list_history\n\n\nwith recipients as (\nselect email, count(email) as [emails] from list_history\ngroup by email\n) \n\nselect * from recipients \nwhere emails > 1\n\n\nselect count(distinct email) from list_history\n\nWITH EmailCounts AS (\n    SELECT \n        *,\n        COUNT(*) OVER (PARTITION BY email) as EmailCount\n    FROM list_history\n)\nSELECT *\nFROM EmailCounts\nWHERE EmailCount > 1\nORDER BY email;\n\nselect * from list_history where tag is null",
        "locale": "en",
        "slug": "technical-posts/sql-nuggets",
        "folder": ""
    },
    {
        "title": "How to refresh the GoTo API authorization code",
        "description": "How to refresh the GoTo API authorization code",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-06T00:00:00.000Z",
        "date_published": null,
        "pinned": true,
        "tags": [
            "asna-com",
            "goto-webinar"
        ],
        "content": "How to refresh the GoTo API authorization code\n\nYou need five values to get a GoTo access key:\n\nClient ID\nSecret\nThe base 64 value of [client id]:[secret]\nThe redirect URI\nThe API authorization code\n\nStep 1. Create an OAuth client\n\nhttps://developer.logmeininc.com/clients\n\nThis provides the Client ID and the secret. Use the site provided to calculate #3, the base 64 value.\n\nCheck these two checkboxes to create a client for GotoWebinar\n![[image-41.png|412x293]]\n\nThe URI is https://asna.com\n\nThese instructions come from this link: https://developer.goto.com/guides/Authentication/03_HOW_accessToken\n\nAfter creating the client, save in the Client ID and Secrets below.\n\n1. Client ID\n\n338a0da6-9332-41ac-baf0-8b826eb5f3b4\n\n2. Secret \n\nkYIVNFVVIoeiFbdTP2VQvhUp\n\n3. URL\n\nhttps://asna.com\n2. Obtain the one-time authorization code\n\nProvide\nClient ID\nSecret\non this form at http://localhost:5173/first-time-token:\n\n[!info]\nLeave the Base64 and Authorization code values empty. The Base64 is calculated for you (as you paste the two values in) and you get the Authorization code with step.\n\n![[image-45.png|678x463]]\n\n[!info]\nThe \"auth code request URL is on the clipboard--now use Bruno\" is misleading. Keep reading.\n\nClick the Create GotoUrl to get auth code button. It creates a URL and puts it on the clipboard.  Paste that URL in your browser and press enter. That takes you to ASNA and shows a 'code' search parameter.\n\n![[image-46.png|746x246]]\n\nCarefully copy that code value into the authorization code.  All four values of the form are now filled in. You can also paste into the value below just for safe keeping.\n\nAuthorization code:\n\neyJraWQiOiI2MjAiLCJhbGciOiJSUzUxMiJ9.eyJzYyI6ImNvbGxhYjogaWRlbnRpdHk6c2NpbS5tZSIsInN1YiI6IjIwMDAwMDAwMDAwMDMxMzIyOSIsImF1ZCI6ImVlNDJkZWVhLWM4YjQtNDZhOC1iNTIzLTE3ZDI1NDNiZjQ2ZCIsIm9nbiI6InB3ZCIsImxzIjoiZDNjMjU5YWYtNjMxYi00Nzc3LWE0MWQtMDg5OWI0MjFjYjkzIiwidHlwIjoiYyIsImxldmVsT2ZBc3N1cmFuY2UiOjIsImV4cCI6MTc2MDAzNjkxMSwiaWF0IjoxNzYwMDM2MzExLCJ1cmkiOiJodHRwczovL2FzbmEuY29tIiwianRpIjoiYzlhOWE1N2EtNjYwZC00YTExLTk4ODktZTEyZTM5NjIzMjFjIn0.UTy0qK4DghqfRdnycSYM9sqZODHCk_kKOhwjH2sGcdYbwsdvFjEuzuenC8ABv9mbfBF7m0vLWZpjXeH2Cy8Xpwe6xrGzJljDiupPeYsDIZpwkVEignbvpN2nEOKQvPMPzhJwf3BioFK3Dz9nfwCmZbQx1K13KYlZfe2yexQWb1_sjVW-P0dzyLdlQWwDleoVUCkoiUjALuS3oUgonbkUmDdZomywzhD-pLkEufwKF-3_iT2UXCemhSBelLm-SJDk3LFku7iWh4FtnF_RoaSPpW2t5eK7n_NZSzYHh40mDWi8LwGVQtFDTeqd95cWI-SWGgMTCA1nxMpNgbWXWD7CCQ\n\nWith the four values shown, now open Bruno and use this request:\n\n![[image-50.png|700]]\n\nreplacing the authorizationCode and base64 with the corresponding values from this screen:\n\n![[image-45.png|678x463]]\n\nRun the Bruno request. It provides a new access token and a new refresh token.\n\nRefresh the .env file with new:\n\nclient id\nGoto secret\naccess token\nrefresh token\nvalues\n\n\nThen open this form. it fetches the new values from the .env file.  Click the Refresh tokens button to be sure things are working. \n\n\n![[image-52.png|700]]\n\nRefresh the .env file with these values\nclient id\nGoto secret\naccess token\nrefresh token\n\nAlso replace these four .env values at Vercel and then redeploy the app.",
        "locale": "en",
        "slug": "technical-posts/how-to-refresh-the-goto-api-authorization-code",
        "folder": ""
    },
    {
        "title": "CSS Holy Grail - Chris Coyier",
        "description": "CSS Holy Grail - Chris Coyier",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "https://css-tricks.com/the-holy-grail-layout-with-css-grid/\n\nGlobal Styles\n\n{\n    box-sizing: border-box;\n}\n\nbox-sizing: border-box;: This sets the box-sizing property to border-box for all elements, ensuring that padding and border are included in the element's total width and height.\n\nbody\n\nbody {\n    background: #e4e4e4;\n    padding: 5px;\n    height: 100vh;\n    margin: 0;\n    font: 500 100% system-ui, sans-serif;\n    text-transform: uppercase;\n}\n\nbackground: #e4e4e4;: Sets the background color of the body to a light gray.\npadding: 5px;: Adds 5 pixels of padding inside the body.\nheight: 100vh;: Sets the height of the body to 100% of the viewport height.\nmargin: 0;: Removes the default margin around the body.\nfont: 500 100% system-ui, sans-serif;: Sets the font weight to 500, font size to 100%, and font family to system-ui and sans-serif.\ntext-transform: uppercase;: Transforms all text to uppercase.\n\n.page-wrap\n\n.page-wrap {\n    background: white;\n    height: calc(100vh - 10px);\n    box-shadow: 0 0 3px rgba(0, 0, 0, 0.33);\n    display: grid;\n    grid-template-columns: minmax(10px, 1fr) minmax(10px, 3fr);\n    grid-template-rows: min-content min-content 1fr min-content;\n    gap: 1px;\n}\n\nbackground: white;: Sets the background color to white.\nheight: calc(100vh - 10px);: Sets the height to the full viewport height minus 10 pixels.\nbox-shadow: 0 0 3px rgba(0, 0, 0, 0.33);: Adds a subtle shadow around the element.\ndisplay: grid;: Defines the element as a grid container.\ngrid-template-columns: minmax(10px, 1fr) minmax(10px, 3fr);: Defines two columns with minimum widths of 10 pixels and maximum widths of 1 fraction unit and 3 fraction units respectively.\ngrid-template-rows: min-content min-content 1fr min-content;: Defines four rows with varying heights.\ngap: 1px;: Sets a 1-pixel gap between grid items.\n\n.page-wrap > *\n\n.page-wrap > * {\n    padding: 1rem;\n    text-align: center;\n}\n\npadding: 1rem;: Adds 1 rem of padding inside each direct child of .page-wrap.\ntext-align: center;: Centers the text inside each direct child of .page-wrap.\n\nMedia Query for Small Screens\n\n@media (max-width: 600px) {\n    .page-wrap {\n        grid-template-columns: 100%;\n        grid-template-rows: auto;\n    }\n    .page-wrap > * {\n        grid-column: 1/-1 !important;\n        grid-row: auto !important;\n    }\n}\n\n@media (max-width: 600px) { ... }: Applies the enclosed styles only when the viewport width is 600 pixels or less.\n.page-wrap { grid-template-columns: 100%; grid-template-rows: auto; }: Changes the grid to a single column layout with automatic row heights.\n.page-wrap > * { grid-column: 1/-1 !important; grid-row: auto !important; }: Forces each direct child of .page-wrap to span the entire width of the grid and have automatic row placement.\n\n.page-header\n\n.page-header {\n    grid-column: 1/-1;\n    background: #ffcdd2;\n}\n\ngrid-column: 1/-1;: Makes the .page-header span from the first to the last column of the grid.\nbackground: #ffcdd2;: Sets the background color to a light red.\n\n.page-sidebar\n\n.page-sidebar {\n    grid-column: 1/2;\n    grid-row: 2/4;\n    background: #e1bee7;\n}\n\ngrid-column: 1/2;: Places the .page-sidebar in the first column of the grid.\ngrid-row: 2/4;: Makes the .page-sidebar span from the second to the fourth row of the grid.\nbackground: #e1bee7;: Sets the background color to a light purple.\n\n.page-nav\n\n.page-nav {\n    grid-column: 2/3;\n    background: #bbdefb;\n}\n\ngrid-column: 2/3;: Places the .page-nav in the second column of the grid.\nbackground: #bbdefb;: Sets the background color to a light blue.\n\n.page-main\n\n.page-main {\n    grid-column: 2/3;\n    background: #dcedc8;\n}\n\ngrid-column: 2/3;: Places the .page-main in the second column of the grid.\nbackground: #dcedc8;: Sets the background color to a light green.\n\n.page-footer\n\n.page-footer {\n    grid-column: 1/-1;\n    background: #ffecb3;\n}\n\ngrid-column: 1/-1;: Makes the .page-footer span from the first to the last column of the grid.\nbackground: #ffecb3;: Sets the background color to a light yellow.\n\ndetails p\n\ndetails p {\n    text-transform: none;\n    text-align: left;\n}\n\ntext-transform: none;: Ensures that the text inside p elements within details elements is not transformed.\ntext-align: left;: Aligns the text inside p elements within details elements to the left.\n\nSimilar code found with 1 license type",
        "locale": "en",
        "slug": "technical-posts/css-holy-grail-chris-coyier",
        "folder": ""
    },
    {
        "title": "microsoft-volume-pricing esu windows 11",
        "description": "microsoft-volume-pricing esu windows 11",
        "date_created": "2025-09-14T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "microsoft",
            "windows11"
        ],
        "content": "Microsoft June 25 blog entry\nMicrosoft October 2024 ESU announcement\n\"and organizations of all sizes\"\n\nOffice 2016 and 2019\nFurthermore, on October 14, the company will end support for Office 2016 and Office 2019 for all operating systems. In addition, Office 2021 and 2024 (as well as the LTSC versions of Office) will continue to run, but they will no longer receive support on Windows 10.\n\nThe ESU page says:\n\nThe Windows 10 Extended Security Updates (ESU) program gives customers the option to receive security updates for PCs enrolled in the program. ESU is a paid program that provides individuals and organizations of all sizes with the option to extend the use of Windows 10 devices past the end of support date in a more secure manner. For more information about the Windows 10 lifecycle, see theWindows Lifecycle FAQ.\n\n!info]\n\"ESU is a paid program that provides individuals and organizations of all sizes\" \n\nIndividuals or organizations who elect to continue using Windows 10 after support ends on October 14, 2025, will have the option of enrolling their PCs into a paid ESU subscription. The ESU program enables PCs to continue to receive critical and important security updates through an annual subscription service after support ends. The[Microsoft Security Response Centerdefines theseverity rating for security updates.\n\n![image-44.png|617x168]]\n\n[!info]\nThis implies that anyone can get the ESU updates. \n\nBut also says...  \n\nExtended Security Updates for organizations and businesses on Windows 10 can be purchased today through the Microsoft Volume Licensing Program, at $61 USD per device for Year One. For more information, see[When to use Windows 10 Extended Security Updates. The price doubles every consecutive year, for a maximum of three years. ESU is available at no additional cost for Windows 10 virtual machines in the following services:\n\nWindows 365\n\nPaying for ESU update\n\nUse Windows Backup to backup to OneDrive (5gb max!)\n1000 MS points\nPay $30 \n\nThis appears to be a per-Windows installation fee\n\nDoes your PC qualify\nCheck Windows 11 compatibility\n\tWindows Health Check\n\t\thttps://support.microsoft.com/en-us/windows/check-if-a-device-meets-windows-11-system-requirements-after-changing-device-hardware-f3bc0aeb-6884-41a1-ab57-88258df6812b\n\tMy Dell Precision 13-year-old () M-4700 Windows 10 laptop failed the qualify for Windows 11\n\t\tNo secure boot enabled\n\t\tTrusted Platform Module (TPM) 2.0 not enabled\n\t\tThe processor (Intel Core i7-3720QM 2.6Ghx )\n\nLeo \nhttps://www.youtube.com/watch?v=Yt14JqlR5wY&t=392s\n\n\nExtended Security Updates (ESU) program for Windows 10 | Microsoft Learn\n\nExtended Security Updates for organizations and businesses on Windows 10 can be purchased today through the Microsoft Volume Licensing Program, at $61 USD per device for Year One.\n\nTranslation: ESU is only available through the Microsoft Value Licensing Program, which is made available and managed by MS partners. \n\n\n![[Generated Image September 15, 2025 - 1_00PM.png|410x410]]\n\n\nhttps://www.microsoft.com/licensing/docs/view/licensing-programs\n\nEA (Enterprise Agreement)\n\thttps://www.microsoft.com/licensing/docs/view/Enterprise-Agreement-EA-EAS-SCE\n\t500 users and/or devices\n\tthree agreement\nCSP (Cloud Solution Provider)\n\thttps://partner.microsoft.com/en-be/partnership/cloud-solution-provider\n\t1m annually, advanced or premier support plan starting at $15k per year)\nMPSA\n\t250 or more users/devices\nOpen Value and Open Value Subscription\n\t5 or more desktop PCs\n\thttps://www.microsoft.com/en-us/licensing/licensing-programs/open-license\n\tfile:///C:/Users/thumb/Downloads/Open_Programs_Overview.pdf\n\n100 hours of support! \n\nNotes from Google Gemini\n\nThe Microsoft Volume Licensing program provides a way for organizations to acquire licenses for Microsoft software and cloud services in bulk. It's designed to make it easier and more cost-effective for businesses, government entities, educational institutions, and non-profits to license Microsoft products.\n\nHere's a breakdown of key aspects:\n\nHow it Works:\n\nCentralized Management: Instead of buying individual boxed copies of software for each computer, organizations can manage their licenses through a centralized agreement. This simplifies deployment, tracking, and compliance.\nCost Savings: Volume licensing generally offers discounts compared to retail pricing, especially as the number of licenses increases.\nFlexible Options: There are various programs designed to meet different organizational sizes, needs, and purchasing preferences.\nSoftware Assurance: Many volume licensing agreements include \"Software Assurance\" as an optional or integrated benefit. This provides a range of benefits like:\n    New Version Rights: Access to new software versions released during the coverage period.\n    Deployment Planning Services: Expert help with planning software deployments.\n    24x7 Phone and Web Support: Technical support.\n    Training Vouchers: Discounts or free training for IT staff.\n    Home Use Program: Allows employees to use licensed software on their home computers.\n    Extended Hotfix Support: Access to non-security related hotfixes.\n\nCommon Volume Licensing Programs:\n\nOpen Value:\n    Open Value (Company-wide): A three-year agreement for small to midsize organizations (typically 5 to 250 PCs) that want to standardize their IT infrastructure. Payments are spread annually.\n    Open Value Subscription: Similar to Open Value, but it's a subscription model. Organizations subscribe to the right to use the software and can increase or decrease licenses annually. This can be more flexible for organizations with fluctuating employee counts.\n\nSelect Plus (Being Phased Out/Replaced by MPSA):\n    Designed for larger organizations (250+ PCs) that want to acquire licenses and services on an as-needed basis across different departments or affiliates. It offers perpetual licenses with optional Software Assurance.\n\nEnterprise Agreement (EA):\n    For large organizations (250+ users or devices) that want to license software for their entire organization for a three-year term. It offers significant discounts, predictable budgeting through annual payments, and comprehensive Software Assurance benefits. It's a \"true-up\" model, meaning you report your usage annually and adjust payments accordingly.\n\nMicrosoft Products and Services Agreement (MPSA):\n    A transactional licensing agreement for commercial, government, and academic organizations with 250 or more users or devices. It simplifies purchasing by consolidating various agreements into a single agreement. It's designed to make it easier to purchase both on-premises software and cloud services.\n\nCSP (Cloud Solution Provider) Program:\n    This isn't strictly a \"volume licensing program\" in the traditional sense, but it's crucial for how many organizations acquire Microsoft cloud services (like Microsoft 365, Azure). Partners directly manage the customer relationship, billing, and support for Microsoft cloud services, offering more integrated solutions and support.\n\nBenefits of Volume Licensing:\n\nSimplified Management: Easier to track and manage licenses, reducing administrative overhead.\nCost Efficiency: Discounts for bulk purchases.\nBudget Predictability: Annual payments in some programs help with financial planning.\nFlexibility: Options to scale up or down as organizational needs change.\nSoftware Assurance Benefits: Access to a suite of benefits that enhance software value and support.\n\nChoosing the right volume licensing program depends on factors like the size of your organization, the number of licenses needed, your IT infrastructure, and whether you prefer perpetual licenses or subscriptions to cloud services. Many organizations work with a Microsoft licensing specialist or partner to determine the best program for their specific requirements.\n\nHere's a visual representation of how different licensing models might cater to various organizational needs: \n\n![[Generated Image September 15, 2025 - 12_59PM.png|510x510]]",
        "locale": "en",
        "slug": "technical-posts/microsoft-volume-pricing",
        "folder": ""
    },
    {
        "title": "Using Mdsvex to transform markdown",
        "description": "Using Mdsvex to transform markdown",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "The mdsvex component is a markdown processor for Svelte components. Beyond processing the markdown, mdsvex also supports embedding Svelte components in the markdown.\n\npnpm i -D mdsvex\n\nConfigure mdsvex in the svelte.config.js file in the project root:\n\nimport adapter from '@sveltejs/adapter-auto';\nimport { vitePreprocess } from '@sveltejs/kit/vite';\n\nimport { mdsvex } from 'mdsvex';\n\n/** @type {import('mdsvex').MdsvexOptions} */\nconst mdsvexOptions = {\n\textensions: ['.md']\n};\n\n/** @type {import('@sveltejs/kit').Config} */\nconst config = {\n\textensions: ['.svelte', '.md'],\n\tpreprocess: [vitePreprocess(), mdsvex(mdsvexOptions)],\n\n\tkit: {\n\t\tadapter: adapter()\n\t}\n};\n\nexport default config;\n\nDirectory structure\n\n.\n src\n     routes\n         posts\n             [slug]\n                 +page.js\n                 +page.svelte\n\n+page.js\n\n+page.js receives the params object and that provides the selected post's slug. Vite's import method uses the relative path of the markdown file (which, thanks to mdsvex, is also a Svelte component)\n\nexport const prerender = true;\n\nexport const load = async ({ params }) => {\n\tconst post = await\n\t\timport(../../../../src/posts/${params.slug}.md);\n\n\treturn {\n\t\tcontent: post.default,\n\t\tmeta: post.metadata\n\t};\n};\n\n+page.svelte\n\nThe svelte.component renders the dynamic component. {data.content} resolves to the markdown and the frontmatter properties are available through the {data.meta} object.\n\n\n\timport { page } from '$app/stores';\n\n\texport let data;\n\t// console.log(data);\n\n\n\n\t\n\t\t{data.meta.title}\n\t\n\t\n\t\n\t\t\n\t\n",
        "locale": "en",
        "slug": "technical-posts/fetch-a-markdown-post-with-mdsvex",
        "folder": ""
    },
    {
        "title": "How to intercept console output to keep test output clean",
        "description": "This shows how to intercept console output to keep test output clean.",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "vitest"
        ],
        "content": "it(\"should handle invalid dates gracefully by leaving them as strings\", () => {\n        const input = {\n            badDate: \"2025-99-99\", // Matches regex pattern but invalid date\n        };\n\n        //        Mock console.warn to keep test output clean\n        const consoleSpy = vi\n            .spyOn(console, \"warn\")\n            .mockImplementation(() => {});\n\n        const result = convertFrontmatterDateStrings(input);\n\n        expect(typeof result.badDate).toBe(\"string\");\n        expect(result.badDate).toBe(\"2025-99-99\");\n        expect(consoleSpy).toHaveBeenCalled();\n\n        consoleSpy.mockRestore();\n    })\n\n  The consoleSpy in this test is doing three things:\n\n1. Spying on console.warn:\nvi.spyOn(console, \"warn\")\nThis creates a spy that watches/intercepts all calls to console.warn. It tracks when and how the function is called.\n\n2. Mocking the implementation:\n.mockImplementation(() => {})\nThis replaces the actual console.warn with an empty function. This prevents the warning message from appearing in your test output, keeping it clean and readable.\n\n3. Verifying the warning was triggered:\nexpect(consoleSpy).toHaveBeenCalled();\nThis assertion checks that console.warn was actually called during the test. It confirms that your convertFrontmatterDateStrings function detected the invalid date (\"2025-99-99\") and attempted to log a warning about it.\n\n4. Cleaning up:\nconsoleSpy.mockRestore();\nThis restores console.warn to its original implementation so it doesn't affect other tests.\n\nIn summary: The spy lets you verify that error handling code (the warning) runs as expected, while keeping your test output clean by suppressing the actual warning message.",
        "locale": "en",
        "slug": "technical-posts/spying-on-console-with-vitest",
        "folder": ""
    },
    {
        "title": "PowerShell - making get-childitem faster and other tips",
        "description": "PowerShell - making get-childitem faster and other tips",
        "date_created": "2025-07-12T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "A primer on get-childitem performance\nExcellent question. This gets to the heart of how PowerShell processes command-line arguments and is a crucial distinction for both performance and correctness.\n\nWhile the two commands might seem to produce the same result in a simple directory, they work in fundamentally different ways.\n\nTL;DR Summary\n\nGet-ChildItem -Path . -Filter '.svelte' is the *correct and efficient** method. It asks the file system provider to do the filtering, which is extremely fast.\nGet-ChildItem '.svelte' is a *shortcut that relies on wildcard expansion* by the PowerShell engine *before the command runs. It's less efficient and only works for the current directory (not recursively).\n\n1. Get-ChildItem -Path . -Filter '*.svelte' (The Explicit, Efficient Way)\n\nThis is the provider-side filtering method we discussed.\n\nHow it works:\n    PowerShell sees the command Get-ChildItem.\n    It binds the argument . to the -Path parameter.\n    It binds the argument '*.svelte' to the -Filter parameter.\n    Get-ChildItem then makes a single, optimized call to the underlying file system API, saying: \"In the directory . (the current directory), please give me a list of all items that match the pattern *.svelte.\"\n    The operating system's file system driver, which is highly optimized for this task, performs the search and returns only the matching files.\n\nAdvantages:\n    Performance: This is the fastest way. The filtering is done at the source by a low-level, optimized component.\n    Clarity: The command is explicit and self-documenting. It's clear that you are specifying a path and a filter.\n    Consistency: It works the same way whether you add -Recurse or not.\n\n2. Get-ChildItem '*.svelte' (The Implicit, Less Efficient Way)\n\nThis method uses PowerShell's argument resolver and positional parameters.\n\nHow it works:\n    Before Get-ChildItem is even executed, the PowerShell engine sees the unquoted string .svelte. It recognizes `` as a wildcard character.\n    PowerShell expands this wildcard itself. It scans the current directory for any file names that match the pattern *.svelte.\n    Let's say your directory contains App.svelte and Card.svelte. The PowerShell engine effectively rewrites the command in memory to:\n        Get-ChildItem -Path 'App.svelte', 'Card.svelte'\n    Only then is Get-ChildItem executed with this pre-compiled list of file names passed to its first positional parameter, which is -Path.\n\nDisadvantages:\n    Performance: This is a two-step process (PowerShell expands, then gci runs) which is slower, especially with many matching files. PowerShell is not as fast at file name globbing as the file system itself.\n    Scope Limitation: This wildcard expansion only works in the current directory. It cannot be used with -Recurse to find all .svelte files in subdirectories. The command Get-ChildItem .svelte -Recurse would fail to find files in subfolders because the .svelte part is resolved before the recursion even starts.\n    Potential for Errors: If the wildcard expands to a huge number of files, you can potentially exceed the command-line argument length limit, leading to an \"Argument list too long\" error (this is more common in Linux/macOS shells but can happen in PowerShell).\n\nComparison Table\n\n| Feature                    | Get-ChildItem -Filter '.svelte'                       | Get-ChildItem '.svelte'                                                                      |\n| :------------------------- | :------------------------------------------------------- | :---------------------------------------------------------------------------------------------- |\n| Who Filters?           | File System Provider (OS)                            | PowerShell Engine (before command runs)                                                     |\n| Mechanism              | A single call to the OS API with a filter.               | PowerShell expands the wildcard into a list of names, then passes that list to Get-ChildItem. |\n| Performance            | Very High. The most efficient method.                | Lower. Slower due to the two-step process.                                                  |\n| Works with -Recurse? | Yes. Finds all matching files recursively.           | No. Only finds files in the starting directory.                                             |\n| Clarity & Robustness   | High. Explicit and less prone to side effects.       | Low. Relies on implicit behavior that can be confusing.                                     |\n| Recommended Use        | Always. The best practice for scripts and daily use. | Rarely. Only for quick, interactive commands in a single directory.                         |\n\nConclusion\n\nYou should always prefer Get-ChildItem -Filter '*.svelte'.\n\nIt is more performant, more reliable, more readable, and works intuitively with other parameters like -Recurse. The positional wildcard shortcut (gci *.svelte) should be avoided in any script and is generally not considered a best practice even for interactive use.\n\nQuick and dirty to exclude inner folders\n\nOf course. Excluding folders is a very common requirement, especially to avoid searching in directories like node_modules or build output folders like .svelte-kit.\n\nThe best and most reliable way to do this is by piping the results of Get-ChildItem to the Where-Object cmdlet to filter based on the full path.\n\nRecommended Method: Using Where-Object\n\nThis method is the most flexible and reliable. It lets Get-ChildItem find all files and then filters out the ones whose paths contain the folder names you want to exclude.\n\nHere is the updated command to exclude node_modules and .svelte-kit folders:\n\nGet-ChildItem -Path . -Filter '*.svelte' -Recurse | \n    Where-Object { $.FullName -notlike '\\node_modules\\' -and $.FullName -notlike '\\.svelte-kit\\' } | \n    Select-Object Name, FullName | \n    Sort-Object Name | \n    Out-GridView\n\nBreakdown of the Change:\n\n| Where-Object { ... }: We've inserted this clause into the pipeline after Get-ChildItem. Where-Object (alias: where) filters the objects that pass through it.\n$.FullName: For each file object ($) coming from Get-ChildItem, we look at its FullName property (e.g., C:\\project\\src\\routes\\Component.svelte).\n-notlike '*\\node_modules\\*': This is the core logic. It checks if the FullName is not like the specified pattern.\n    The asterisks * are wildcards, meaning \"match any characters\".\n    So \\node_modules\\ matches any path that contains a \\node_modules\\ folder anywhere in it.\n    We use backslashes \\ as the path separator for Windows. If you are on Linux/macOS, you might use / instead, but \\ often works cross-platform within PowerShell.\n-and: This operator ensures that a file is only included if both conditions are true (its path doesn't contain node_modules AND it doesn't contain .svelte-kit).\n\nAlternative Method: Using Regular Expressions with -notmatch\n\nIf you have many folders to exclude, using the -notmatch operator with a regular expression can be cleaner.\n\nGet-ChildItem -Path . -Filter '*.svelte' -Recurse | \n    Where-Object { $_.FullName -notmatch '\\\\(node_modules|\\\\.svelte-kit)\\\\' } |\n    Select-Object Name, FullName |\n    Sort-Object Name |\n    Out-GridView\n\nBreakdown of the Regex:\n\n-notmatch: This operator filters out items that match a regular expression.\n'\\\\(node_modules|\\\\.svelte-kit)\\\\': This is the regular expression pattern.\n    \\\\: A literal backslash. The first backslash escapes the second one.\n    (...): A capturing group.\n    node_modules | \\\\.svelte-kit: The | acts as an \"OR\". It will match node_modules OR \\.svelte-kit.\n    \\\\.: Since . is a special character in regex (meaning \"any character\"), we must escape it with a \\ to match a literal dot. Again, the first \\ escapes the second for the PowerShell string.\n    The full pattern looks for \\node_modules\\ or \\.svelte-kit\\ anywhere in the path.\n\nWhy Not Use Get-ChildItem -Exclude?\n\nYou might be tempted to use the -Exclude parameter of Get-ChildItem, like this:\n\nGet-ChildItem -Path . -Filter '*.svelte' -Recurse -Exclude 'node_modules', '.svelte-kit'\n\nThis approach fails for this use case. The -Exclude parameter applies to the name of the item being returned (the file or folder itself), not to the path it resides in. Since Get-ChildItem with -Filter '*.svelte' is only returning .svelte files, their names will never be node_modules or .svelte-kit, so -Exclude will never match and nothing will be excluded.\n\nThe Where-Object method correctly inspects the entire path of each file, making it the right tool for the job.\n\nFinal Recommendation\n\nStick with the first method using Where-Object and -notlike. It's readable, powerful, and the standard way to solve this problem in PowerShell.\n\nGet-ChildItem -Path . -Filter '.svelte' -Recurse | Where-Object { $.FullName -notlike '\\node_modules\\' -and $.FullName -notlike '\\.svelte-kit\\*' } | Select-Object Name, FullName | Sort-Object Name | Out-GridView\n\nUsing regex to exclude inner folders with get-childitem\n\nOf course. Using a list or an array to define the folders to exclude is the best practice for making your command clean, maintainable, and easy to modify.\n\nThe most powerful and efficient way to do this is to dynamically build a regular expression from your list of folder names.\n\nRecommended Method: Using a List and a Regular Expression\n\nThis approach is clean, scalable, and correctly handles special characters in folder names.\n\nDefine your list of folders to exclude.\nConvert that list into a single regex pattern.\nUse that pattern with the -notmatch operator.\n\nHere is the complete, ready-to-use command:\n\n1. Define the list of folder names to exclude\n$excludeFolders = @(\n    'node_modules',\n    '.svelte-kit',\n    'build',\n    'dist'\n)\n\n2. Build a regex pattern from the list. This makes the command robust.\nIt joins the names with '|' (OR) and escapes any special regex characters.\n$regexPattern = ($excludeFolders | ForEach-Object { [regex]::Escape($_) }) -join '|'\n\n3. Execute the command using the dynamically created regex\nGet-ChildItem -Path . -Filter '*.svelte' -Recurse | \n    Where-Object { $_.FullName -notmatch \"\\\\($regexPattern)\\\\\" } |\n    Select-Object Name, FullName |\n    Sort-Object Name |\n    Out-GridView\n\nBreakdown of This Method\n\nStep 1: The Array\n$excludeFolders = @(\n    'node_modules',\n    '.svelte-kit',\nAdd any other folder names here\n    'dist' \n)\nThis creates a standard PowerShell array. It's incredibly easy to add or remove folders from this list without touching the main command logic.\n\nStep 2: The Regex Builder\n$regexPattern = ($excludeFolders | ForEach-Object { [regex]::Escape($_) }) -join '|'\nThis is the clever part that makes the solution so robust:\n$excludeFolders | ForEach-Object { ... }: This iterates through each folder name in your list.\n[regex]::Escape($_): This is very important. It takes a folder name (like .svelte-kit) and escapes any characters that have a special meaning in regular expressions. For instance, it turns the . into \\., ensuring it matches a literal dot and not \"any character\".\n-join '|': This takes the list of escaped folder names and joins them together into a single string, separated by the | (OR) character.\n\nIf $excludeFolders is @('node_modules', '.svelte-kit'), the resulting $regexPattern will be node_modules|\\.svelte-kit.\n\nStep 3: The Filter\n... | Where-Object { $_.FullName -notmatch \"\\\\($regexPattern)\\\\\" } | ...\n$_.FullName -notmatch ...: We use -notmatch to filter against our regex pattern.\n\"\\\\($regexPattern)\\\\\": We embed our pattern inside the full check. The surrounding \\\\ characters ensure we are matching a full directory name (e.g., \\node_modules\\) and not just a partial file or folder name.\n\nAlternative: A Simpler (But More Verbose) Loop\n\nIf you are not comfortable with regular expressions, you can use a more explicit loop inside the Where-Object clause. This is slightly less performant but can be easier to read for some.\n\n$excludeFolders = @(\n    'node_modules',\n    '.svelte-kit',\n    'build',\n    'dist'\n)\n\nGet-ChildItem -Path . -Filter '*.svelte' -Recurse | \n    Where-Object {\n        $path = $_.FullName\nCheck if the path contains ANY of the excluded folder names\n        $isExcluded = $false\n        foreach ($folder in $excludeFolders) {\n            if ($path -like \"\\$folder\\\") {\n                $isExcluded = $true\n                break # Found a match, no need to check further, exit the loop\n            }\n        }\n        -not $isExcluded # Keep the item only if it was NOT found in the exclude list\n    } |\n    Select-Object Name, FullName |\n    Sort-Object Name |\n    Out-GridView\n\nComparison\n\n| Method | Pros | Cons |\n| :--- | :--- | :--- |\n| Regex (Recommended) | Concise and fast. The standard \"PowerShell\" way to solve this. Highly scalable. | Requires understanding a bit of regex. |\n| Verbose Loop | Very explicit and easy to read if you are unfamiliar with regex. | More code, slightly slower due to the nested loop for every file. |\n\nFor any real-world script, the regex method is superior. It's a common and powerful pattern in PowerShell scripting.\n\nExclude top-level folders\n\nExcellent question. This is a very common and important scenario, and solving it efficiently requires a different approach than simply filtering the final results.\n\nYou want to prevent Get-ChildItem from ever entering the bob and tom folders in the first place. This is much more performant than letting it search everywhere and then filtering the results afterward, especially if bob or tom were something huge like node_modules.\n\nThe strategy is to first get a list of the top-level directories you want to search, and then run your recursive search inside each of them.\n\nHere is the solution presented as a clear, step-by-step script.\n\nStep-by-Step Script to Exclude Top-Level Folders\n\nThis script is designed for clarity and is easy to modify.\n\nStep 1: Configure the folders you want to exclude.\nThis makes it easy to add or remove folders later without changing the main logic.\n$foldersToExclude = @(\n    'bob',\n    'tom'\nYou can add more folder names here, e.g., 'node_modules', '.git'\n)\n\nWrite-Host \"Starting search. Excluding top-level folders: $($foldersToExclude -join ', ')\" -ForegroundColor Yellow\n\nStep 2: Get all top-level directories in the current location, EXCLUDING the ones in our list.\nThe -Directory switch ensures we only get folders.\nThe -Exclude parameter efficiently filters them by name at the source.\n$foldersToSearch = Get-ChildItem -Path . -Directory -Exclude $foldersToExclude\n\nWrite-Host \"Found $($foldersToSearch.Count) folders to search in.\"\n\nStep 3: Loop through each of the approved folders and find all '*.svelte' files recursively inside them.\nThe results from each loop are automatically collected into the $allSvelteFiles variable.\n$allSvelteFiles = foreach ($folder in $foldersToSearch) {\n    Write-Host \" -> Searching in $($folder.FullName)...\"\n    Get-ChildItem -Path $folder.FullName -Filter '*.svelte' -Recurse\n}\n\nWrite-Host \"Found a total of $($allSvelteFiles.Count) svelte files.\" -ForegroundColor Green\n\nStep 4: Process the collected files.\nIf any files were found, select the desired properties, sort them, and show them in the grid.\nif ($allSvelteFiles) {\n    $allSvelteFiles | \n        Select-Object Name, FullName | \n        Sort-Object Name | \n        Out-GridView -Title \"Svelte Files (Excluding Specified Folders)\"\n} else {\n    Write-Host \"No '.svelte' files found in the searched directories.\"\n}\n\nHow to Run This Script\n\nSave the code above as a .ps1 file (e.g., Find-SvelteFiles.ps1).\nOpen a PowerShell terminal and navigate to the root directory where you want to start the search.\nRun the script by typing its path: .\\Find-SvelteFiles.ps1.\n\nWhy This Method is Better\n\nPerformance: The script completely avoids traversing the directory trees of bob and tom. If those folders contained thousands of files, you would save a significant amount of time and I/O operations.\nClarity: The logic is broken down into clear, understandable steps. You can see exactly which folders are being included and excluded.\nMaintainability: To change which folders are excluded, you only need to edit the $foldersToExclude array at the top. The rest of the script remains unchanged.\n\nOne-Liner Alternative\n\nIf you prefer a more compact command-line approach, you can chain these concepts together into a single pipeline. This does the exact same thing as the script but in a more condensed format.\n\nGet-ChildItem -Path . -Directory -Exclude 'bob','tom' | ForEach-Object { Get-ChildItem -Path $_.FullName -Filter '*.svelte' -Recurse } | Select-Object Name, FullName | Sort-Object Name | Out-GridView\n\nBreakdown of the one-liner:\n\nGet-ChildItem -Path . -Directory -Exclude 'bob','tom': Gets the list of top-level directories to search in.\n| ForEach-Object { ... }: For each of those directories...\nGet-ChildItem -Path $_.FullName -Filter '*.svelte' -Recurse: ...run the recursive search for .svelte files. The results are passed down the pipeline.\n| Select-Object ... | Sort-Object ... | Out-GridView: The final processing and display steps are applied to the combined results.",
        "locale": "en",
        "slug": "technical-posts/powershell-get-childitem-performance-and-tips",
        "folder": ""
    },
    {
        "title": "Check that a file, directory, or path exists with Node",
        "description": "Check that a file, directory, or path exists with Node",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript",
            "node"
        ],
        "content": "import {\n    truncatePathAfterDirectory,\n    getPathForCli,\n} from \"../../src/filesystem\";\nimport path from \"path\";\nimport { fileURLToPath } from \"url\";\nimport fs, { promises as fsa } from \"fs\";\n\nexport function getFileCurrentDirectory() {\n    //const __filename = fileURLToPath(import.meta.url);\n    //const __dirname = path.dirname(__filename);\n    const currentFileDir = path.resolve(\".\");\n    return currentFileDir;\n}\n\nexport function getTestDataPath(...segments: string[]) {\n    const currentFileDir = truncatePathAfterDirectory(\n        getFileCurrentDirectory(),\n        \"utils\"\n    );\n    const resultPath = path.join(currentFileDir, \"test-data\", ...segments);\n\n    return resultPath \n}\n\n// 1. Synchronous - Check if path exists\nfunction pathExists(pathToCheck: string): boolean {\n    return fs.existsSync(pathToCheck);\n}\n\n// 2. Async - Check if path exists (recommended)\nasync function pathExistsAsync(pathToCheck: string): Promise {\n    try {\n        await fsa.access(pathToCheck);\n        return true;\n    } catch {\n        return false;\n    }\n}\n\n// 3. Get detailed information about the path\nasync function getPathInfo(pathToCheck: string) {\n    try {\n        const stats = await fsa.stat(pathToCheck);\n        return {\n            exists: true,\n            isFile: stats.isFile(),\n            isDirectory: stats.isDirectory(),\n            size: stats.size,\n        };\n    } catch {\n        return { exists: false };\n    }\n}\n\n// Usage examples:\nconst testPath = getTestDataPath();\nconsole.log('Path:', testPath);\nconsole.log('Exists (sync):', pathExists(testPath));\n\n// Async usage:\n(async () => {\n    console.log('Exists (async):', await pathExistsAsync(testPath));\n    console.log('Info:', await getPathInfo(testPath));\n})();",
        "locale": "en",
        "slug": "technical-posts/path-exists-with-node",
        "folder": ""
    },
    {
        "title": "CSS Frameworks",
        "description": "CSS Frameworks",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Sakura--A good classless CSS framework",
        "locale": "en",
        "slug": "technical-posts/css-frameworks",
        "folder": ""
    },
    {
        "title": "JavaScript map function",
        "description": "JavaScript map function",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "Using Array.reduce() to transform a JavaScript array.\n\n// const sorted = content.reduce((acc: FolderAccumulator, item: MarkdownResult) => {\n    const folder: string = item.folder;\n    if (!acc[folder]) {\n        acc[folder] = [];\n    }\n    acc[folder].push(\n        {\n            title: item.data.frontMatter.title,\n            description: item.data.frontMatter.description,\n            date_created: 2025-01-05 12:00\ndate_updated: 2025-01-05 12:00\ndate_published:\ntags: item.data.frontMatter.tags,\n            date_added: item.data.frontMatter.date_added,\n            date_updated: item.data.frontMatter.date_updated,\n            date_published: item.data.frontMatter.date_published,\n            pinned: item.data.frontMatter.pinned,\n            content: item.data.content,\n            fullPath: item.fullPath,\n            slug: item.slug,\n            folder: item.folder\n        }\n    );\n    return acc;  // Add missing return statement\n}, {} as FolderAccumulator);",
        "locale": "en",
        "slug": "technical-posts/javascript-array-reduce-function",
        "folder": ""
    },
    {
        "title": "Creating entity models with Entity Framework",
        "description": "Creating entity models with Entity Framework",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "csharp"
        ],
        "content": "Instructionsthese are important\n\nhttps://learn.microsoft.com/en-us/ef/core/cli/dotnet\n\nCreate DB context\n\nC:\\Users\\thumb\\Documents\\Projects\\cs\\cs-misc\\create-model-with-ef-core>dotnet ef dbcontext scaffold\n\"Server=.\\;Database=downloads;Trusted_Connection=True;Integrated Security=True;TrustServerCertificate=True\"\nMicrosoft.EntityFrameworkCore.SqlServer\n-o Model -n MyNamespace\n\nAlso:\n-f                   force target files overwrite\n--no-pluralize       don't use pluralizer\n--use-databasenames  use table and column names exactly as they appear in DB\n\nBatch file\n\nThis deletes the files in the\n\n@echo off\nREM %1 = database name\nREM %2 = target base folder (under 'baseoutputdir' specified below)\nREM %3 = namespace\n\nset baseoutputdir=C:\\Users\\thumb\\Documents\\Projects\\rputilities\\librettox\\template_work\\output\\\nset connstring=Server=.\\;Database=%1;Trusted_Connection=True;Integrated Security=True;TrustServerCertificate=True\n\ndel %baseoutputdir%%2\\. /q\n\ndotnet ef dbcontext scaffold \"%connstring%\" Microsoft.EntityFrameworkCore.SqlServer -o %baseoutputdir%%2 -n %3\ndel model\\*context.cs",
        "locale": "en",
        "slug": "technical-posts/creating-entity-models-with-entity-framework",
        "folder": ""
    },
    {
        "title": "git-install",
        "description": "git-install",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "git"
        ],
        "content": "gitconfig--globaluser.name\"Roger Pence\"\ngitconfig--globaluser.email\"roger.pence@gmail.com\"\n\nssh-keygen -t rsa -b 4096 -C \"roger.pence@gmail.com\"\n\neval \"$(ssh-agent -s)\"\n\nssh-add /c/Users/thumb/.ssh/id_rsa\n\n.gitconfig\n\n[user]\n\tname = Roger Pence\n\temail = roger.pence@gmail.com\n[include]\n\tpath = ~/.gitconfig.aliases\n[push]\n\tautoSetupRemote = true\n\n[diff]\n\ttool = bc\n[difftool \"bc\"]\n\tpath = C:\\\\Program Files\\\\Beyond Compare 4\\\\BComp.exe\n\n.gitconfig.aliases\n\n\n[include] directive with the path to this file\n\n[include]\npath = ~/.gitconfig.aliases\n\nIf you don't have any existing includes, you can add this via the following command\n\ngit config --global include.path ~/.gitconfig.aliases\n\n[alias]\n    abort = rebase --abort\n    aliases = \"!git config -l | grep ^alias\\\\. | cut -c 7-\"\n    amend = commit -a --amend\nDeletes all branches merged into the specified branch (or the default branch if no branch is specified)\n    bclean = \"!f() { DEFAULT=$(git default); git branch --merged ${1-$DEFAULT} | grep -v \" ${1-$DEFAULT}$\" | xargs git branch -d; }; f\"\nSwitches to specified branch (or the dafult branch if no branch is specified), runs git up, then runs bclean.\n    bdone = \"!f() { DEFAULT=$(git default); git checkout ${1-$DEFAULT} && git up && git bclean ${1-$DEFAULT}; }; f\"\nLists all branches including remote branches\n    branches = branch -a\n    browse = !git open\nLists the files with the most churn\n    churn = !git --no-pager log --name-only --oneline | grep -v ' ' | sort | uniq -c | sort -nr | head\n    cleanup = clean -xdf -e .DotSettings -e s3_keys.ps1\nStages every file then creates a commit with specified message\n    cm = !git add -A && git commit -m\n    co = checkout\n    cob = checkout -b\nShow list of files in a conflict state.\n    conflicts = !git diff --name-only --diff-filter=U\n    cp = cherry-pick\n    default = !git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'\n    delete = branch -d\nDiscard changes to a file\n    discard = checkout --\n    ec = config --global -e\nList the changed files in the current or specified commit.\n    files = \"!f() { git diff-tree --no-commit-id --name-only -r ${1-HEAD}; }; f\"\n    find = \"!git ls-files | grep -i\"\n    graph = log --graph -10 --branches --remotes --tags  --format=format:'%Cgreen%h %Creset %<(75,trunc)%s (%cN, %cr) %Cred%d' --date-order\n    grep = grep -Ii\n    hist = log --graph --abbrev-commit --decorate --format=format:'%C(bold blue)%h%C(reset) - %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)' --all\n    history = log --graph --abbrev-commit --decorate --date=relative --format=format:'%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)' --all\nShows the commit message and files changed from the latest commit\n    latest = \"!git ll -1\"\n    lds = log --pretty=format:\"%C(yellow)%h\\\\ %ad%Cred%d\\\\ %Creset%s%Cblue\\\\ [%cn]\" --decorate --date=short\n    lost = fsck --lost-found\nA better git log.\n    ls = log --pretty=format:\"%C(yellow)%h%Cred%d\\\\ %Creset%s%Cblue\\\\ [%cn]\" --decorate\n    ls-subtrees = !\"git log | grep git-subtree-dir | awk '{ print $2 }'\"\nMoves a set of commits from the current branch to another\n    migrate = \"!f(){ DEFAULT=$(git default); CURRENT=$(git symbolic-ref --short HEAD); git checkout -b $1 && git branch --force $CURRENT ${3-$CURRENT@{u}} && git rebase --onto ${2-$DEFAULT} $CURRENT; }; f\"\n    new = !git init && git symbolic-ref HEAD refs/heads/main\n    open = \"!f(){ URL=$(git config remote.origin.url); open ${URL%.git}; }; f\"\n    pr = \"!f(){ URL=$(git config remote.origin.url); open ${URL%.git}/compare/$(git rev-parse --abbrev-ref HEAD); }; f\"\n    publish = \"!f() { git push origin $1 && git push drafts :$1 && git browse; }; f\"\n    pushf = push --force-with-lease --force-if-includes\n    rba = rebase --abort\n    rbc = \"!f(){ git add -A && git rebase --continue; }; f\"\n    re = \"!f(){ DEFAULT=$(git default); git fetch origin && git rebase origin/${1-$DEFAULT}; }; f\"\n    remotes = remote -v\n    restore = \"!f(){ git add -A && git commit -qm 'RESTORE SAVEPOINT'; git reset $1 --hard; }; f\"\n    ri = \"!f(){ DEFAULT=$(git default); git fetch origin && git rebase --interactive origin/${1-$DEFAULT}; }; f\"\n    save = !git add -A && git commit -m 'SAVEPOINT'\n    set-origin = remote set-url origin\n    set-upstream = remote set-url upstream\n    st = status -s\n    stashes = stash list\n    sync = !git pull --rebase && git push\n    undo = reset HEAD~1 --mixed\nUnstage a file\n    unstage = reset -q HEAD --\n    up = !git pull --rebase --prune $@ && git submodule update --init --recursive\n    wip = commit -am \"WIP\"\n    wipe = \"!f() { rev=$(git rev-parse ${1-HEAD}); git add -A && git commit --allow-empty -qm 'WIPE SAVEPOINT' && git reset $rev --hard; }; f\"",
        "locale": "en",
        "slug": "technical-posts/git-install",
        "folder": ""
    },
    {
        "title": "JetBrains Rider License",
        "description": "JetBrains Rider License",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "Granted 29 October 2024\n\nHi Roger,\n\nYour free one-year Rider license for non-commercial use is ready and waiting for you to get started!\n\nManage your licenses\n\nYou can always check your license info and manage all your JetBrains subscriptions in yourJetBrains Account. Log in or create a new account using the email address where you received this email.\n\nPlease note that you can use the current license fornon-commercial development only. For more details, refer to theToolbox Subscription Agreement for Non-Commercial Use.\n\nGet started with Rider\n\nIf youre switching from Visual Studio, check out ourmigration guides. If youre completely new to Rider, readour getting started guide.\nWatch thisvideo series, where our developer advocates show their favorite tips and tricks in Rider.\nCheck out the.NET Tools blogfor the latest tips, tricks, and insights to help you get the most out of Rider.\n\nYour order information\n\nOrder number: A6821481\nOrder date: 29 Oct 2024\nLicense ID: OZ0B5E1HD2\nValid until: 28 Oct 2025\n\n\nKind regards,  \nThe JetBrains team\n\n_www.jetbrains.com  \nThe Drive to Develop_",
        "locale": "en",
        "slug": "technical-posts/jetbrains-rider",
        "folder": ""
    },
    {
        "title": "A Rust function return type for file IO operations",
        "description": "A Rust function return type for file IO operations",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-17T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "In Rust, the return type -> io::Result indicates that the function returns a Result type from the std::io module, specifically for I/O operations. Here's a breakdown of what this means:\n\nComponents\n\nio::Result:\n\n    Result is an enum in Rust that is used for error handling. It has two variants:\n        Ok(T): Indicates success and contains a value of type T.\n        Err(E): Indicates an error and contains a value of type E, which is typically an error type.\n    In the context of io::Result, E is usually an io::Error, which represents various I/O-related errors (like file not found, permission denied, etc.).\n\n():\n    This is the unit type in Rust, which is analogous to void in other programming languages. It indicates that the function does not return any meaningful value on success.\n    In the context of io::Result, it means that if the function succeeds, it will return Ok(()), signifying that it completed successfully but doesn't produce a value.\n\nUsage Example\n\nHere's a simple example of a function that uses io::Result as a return type:\n\nuse std::fs::File;\nuse std::io::{self, Write};\n\nfn write_to_file(file_path: &str, content: &str) -> io::Result {\n    dlet mut file = File::create(file_path)?; // Attempt to create the file\n    file.write_all(content.as_bytes())?; // Write content to the file\n    Ok(()) // Return Ok(()) on success\n}\n\nError Handling\n\nWhen you call a function that returns io::Result, you typically handle potential errors using pattern matching or the ? operator:\n\nfn main() {\n    match write_to_file(\"output.txt\", \"Hello, world!\") {\n        Ok(()) => println!(\"File written successfully.\"),\n        Err(e) => eprintln!(\"Failed to write to file: {}\", e),\n    }\n}\n\nOr using the ? operator for more concise error handling:\n\nfn main() -> io::Result {\n    write_to_file(\"output.txt\", \"Hello, world!\")?;\n    println!(\"File written successfully.\");\n    Ok(())\n}\n\nIn this case, if write_to_file returns an error, it will propagate that error up to the caller. If successful, it will proceed to print the success message.",
        "locale": "en",
        "slug": "technical-posts/rust-function-return-type-for-file-io-operations",
        "folder": ""
    },
    {
        "title": "Learn how to publish a package to npm with a complete setup including, TypeScript, Prettier, Vitest, GitHub Actions, and versioning with Changesets.",
        "description": "Learn how to publish a package to npm with a complete setup including, TypeScript, Prettier, Vitest, GitHub Actions, and versioning with Changesets.",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "clippings",
            "node",
            "typescript"
        ],
        "content": "In this guide, we'll go through every single step you need to take to publish a package to npm.\n\nThis is not a minimal guide. We'll be setting up a fully production-ready package from an empty directory. This will include:\n\nGit for version control\nTypeScript for writing our code and keeping it type-safe\nPrettier for formatting our code\n@arethetypeswrong/cli for checking our exports\ntsup for compiling our TypeScript code into CJS and ESM\nVitest for running our tests\nGitHub Actions for running our CI process\nChangesets for versioning and publishing our package\n\nIf you want to see the finished product, check out this demo repo.\n\nVideo\n\nIf you prefer video content, I've created a video walkthrough of this guide:\n\n1\\. Git\n\nIn this section, we'll create a new git repository, set up a .gitignore, create an initial commit, create a new repository on GitHub, and push our code to GitHub.\n\n1.1: Initialize the repo\n\nRun the following command to initialize a new git repository:\n\ngit init\n\n1.2: Set up a.gitignore\n\nCreate a .gitignore file in the root of your project and add the following:\n\nnode_modules\n\n1.3: Create an initial commit\n\nRun the following command to create an initial commit:\n\ngit add .\n\ngit commit -m \"Initial commit\"\n\n1.4: Create a new repository on GitHub\n\nUsing the GitHub CLI, run the following command to create a new repository. I've chosen the name tt-package-demo for this example:\n\ngh repo create tt-package-demo --source=. --public\n\n1.5: Push to GitHub\n\nRun the following command to push your code to GitHub:\n\ngit push --set-upstream origin main\n\n2: package.json\n\nIn this section, we'll create a package.json file, add a license field, create a LICENSE file, and add a README.md file.\n\n2.1: Create a package.json file\n\nCreate a package.json file with these values:\n\n{\n\n  \"name\": \"tt-package-demo\",\n\n  \"version\": \"1.0.0\",\n\n  \"description\": \"A demo package for Total TypeScript\",\n\n  \"keywords\": \"demo\", \"typescript\"],\n\n  \"homepage\": \"https://github.com/mattpocock/tt-package-demo\",\n\n  \"bugs\": {\n\n    \"url\": \"https://github.com/mattpocock/tt-package-demo/issues\"\n\n  },\n\n  \"author\": \"Matt Pocock  (https://totaltypescript.com)\",\n\n  \"repository\": {\n\n    \"type\": \"git\",\n\n    \"url\": \"git+https://github.com/mattpocock/tt-package-demo.git\"\n\n  },\n\n  \"files\": [\"dist\"],\n\n  \"type\": \"module\"\n\n}\n\nname is the name by which people will install your package. It must be unique on npm. You can create [organization scopes (such as @total-typescript/demo) for free, these can help make it unique.\nversion is the version of your package. It should follow semantic versioning: the 0.0.1 format. Each time you publish a new version, you should increment this number.\ndescription and keywords are short descriptions of your package. They're listed in searches in the npm registry.\nhomepage is the URL of your package's homepage. The GitHub repo is a good default, or a docs site if you have one.\nbugs is the URL where people can report issues with your package.\nauthor is you! You can add optionally add your email and website. If you have multiple contributors, you can specify them as an array of contributors with the same formatting.\nrepository is the URL of your package's repository. This creates a link on the npm registry to your GitHub repo.\nfiles is an array of files that should be included when people install your package. In this case, we're including the dist folder. README.md, package.json and LICENSE are included by default.\ntype is set to module to indicate that your package uses ECMAScript modules, not CommonJS modules.\n\nAdd a license field to your package.json. Choose a license here. I've chosen MIT.\n\n{\n\n  \"license\": \"MIT\"\n\n}\n\nCreate a file called LICENSE (no extension) containing the text of your license. For MIT, this is:\n\nMIT License\n\nCopyright (c) year\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\n\nof this software and associated documentation files (the \"Software\"), to deal\n\nin the Software without restriction, including without limitation the rights\n\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\ncopies of the Software, and to permit persons to whom the Software is\n\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\n\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\nSOFTWARE.\n\nChange the year] and [fullname] placeholders to the current year and your name.\n\n2.4: Add a README.md file\n\nCreate a README.md file with a description of your package. Here's an example:\n\ntt-package-demo\n\nA demo package for Total TypeScript.\n\nThis will be shown on the npm registry when people view your package.\n\n3: TypeScript\n\nIn this section, we'll install TypeScript, set up a tsconfig.json, create a source file, create an index file, set up a build script, run our build, add dist to .gitignore, set up a ci script, and configure our tsconfig.json for the DOM.\n\n3.1: Install TypeScript\n\nRun the following command to install TypeScript:\n\nnpm install --save-dev typescript\n\nWe add --save-dev to install TypeScript as a development dependency. This means it won't be included when people install your package.\n\n3.2: Set up a tsconfig.json\n\nCreate a tsconfig.json with the following values:\n\n{\n\n  \"compilerOptions\": {\n\n    /* Base Options: */\n\n    \"esModuleInterop\": true,\n\n    \"skipLibCheck\": true,\n\n    \"target\": \"es2022\",\n\n    \"allowJs\": true,\n\n    \"resolveJsonModule\": true,\n\n    \"moduleDetection\": \"force\",\n\n    \"isolatedModules\": true,\n\n    \"verbatimModuleSyntax\": true,\n\n    /* Strictness */\n\n    \"strict\": true,\n\n    \"noUncheckedIndexedAccess\": true,\n\n    \"noImplicitOverride\": true,\n\n    /* If transpiling with TypeScript: */\n\n    \"module\": \"NodeNext\",\n\n    \"outDir\": \"dist\",\n\n    \"rootDir\": \"src\",\n\n    \"sourceMap\": true,\n\n    /* AND if you're building for a library: */\n\n    \"declaration\": true,\n\n    /* AND if you're building for a library in a monorepo: */\n\n    \"declarationMap\": true\n\n  }\n\n}\n\nThese options are explained in detail in my [TSConfig Cheat Sheet.\n\n3.3: Configure your tsconfig.json for the DOM\n\nIf your code runs in the DOM (i.e. requires access to document, window, or localStorage etc), skip this step.\n\nIf your code doesn't require access to DOM API's, add the following to your tsconfig.json:\n\n{\n\n  \"compilerOptions\": {\n\n    // ...other options\n\n    \"lib\": \"es2022\"]\n\n  }\n\n}\n\nThis prevents the DOM typings from being available in your code.\n\nIf you're not sure, skip this step.\n\n3.4: Create A Source File\n\nCreate a src/utils.ts file with the following content:\n\nexport const add = (a: number, b: number) => a + b;\n\n3.5: Create An Index File\n\nCreate a src/index.ts file with the following content:\n\nexport { add } from \"./utils.js\";\n\nThe .js extension will look odd. [This article explains more.\n\n3.6: Set up a build script\n\nAdd a scripts object to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"build\": \"tsc\"\n\n  }\n\n}\n\nThis will compile your TypeScript code to JavaScript.\n\n3.7: Running Your Build\n\nRun the following command to compile your TypeScript code:\n\nnpm run build\n\nThis will create a dist folder with your compiled JavaScript code.\n\n3.8: Add dist to.gitignore\n\nAdd the dist folder to your .gitignore file:\n\ndist\n\nThis will prevent your compiled code from being included in your git repository.\n\n3.9: Set up a ci script\n\nAdd a ci script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"ci\": \"npm run build\"\n\n  }\n\n}\n\nThis gives us a quick shortcut for running all required operations on CI.\n\n4: Prettier\n\nIn this section, we'll install Prettier, set up a .prettierrc, set up a format script, run the format script, set up a check-format script, add the check-format script to our CI script, and run the CI script.\n\nPrettier is a code formatter that automatically formats your code to a consistent style. This makes your code easier to read and maintain.\n\n4.1: Install Prettier\n\nRun the following command to install Prettier:\n\nnpm install --save-dev prettier\n\n4.2: Set up a.prettierrc\n\nCreate a .prettierrc file with the following content:\n\n{\n\n  \"semi\": true,\n\n  \"singleQuote\": true,\n\n  \"trailingComma\": \"all\",\n\n  \"printWidth\": 80,\n\n  \"tabWidth\": 2\n\n}\n\nYou can add more options to this file to customize Prettier's behavior. You can find a full list of options here.\n\n4.3: Set up a format script\n\nAdd a format script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"format\": \"prettier --write .\"\n\n  }\n\n}\n\nThis will format all files in your project using Prettier.\n\n4.4: Run the format script\n\nRun the following command to format all files in your project:\n\nnpm run format\n\nYou might notice some files change. Commit them with:\n\ngit add .\n\ngit commit -m \"Format code with Prettier\"\n\n4.5: Set up a check-format script\n\nAdd a check-format script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"check-format\": \"prettier --check .\"\n\n  }\n\n}\n\nThis will check if all files in your project are formatted correctly.\n\n4.6: Adding to our CI script\n\nAdd the check-format script to your ci script in your package.json:\n\n{\n\n  \"scripts\": {\n\n    \"ci\": \"npm run build && npm run check-format\"\n\n  }\n\n}\n\nThis will run the check-format script as part of your CI process.\n\n5: exports, main and @arethetypeswrong/cli\n\nIn this section, we'll install @arethetypeswrong/cli, set up a check-exports script, run the check-exports script, set up a main field, run the check-exports script again, set up a ci script, and run the ci script.\n\n@arethetypeswrong/cli is a tool that checks if your package exports are correct. This is important because these are easy to get wrong, and can cause issues for people using your package.\n\n5.1: Install @arethetypeswrong/cli\n\nRun the following command to install @arethetypeswrong/cli:\n\nnpm install --save-dev @arethetypeswrong/cli\n\n5.2: Set up a check-exports script\n\nAdd a check-exports script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"check-exports\": \"attw --pack .\"\n\n  }\n\n}\n\nThis will check if all exports from your package are correct.\n\n5.3: Run the check-exports script\n\nRun the following command to check if all exports from your package are correct:\n\nnpm run check-exports\n\nYou should notice various errors:\n\n\n\n                    \"tt-package-demo\"    \n\n\n\n node10              Resolution failed \n\n\n\n node16 (from CJS)   Resolution failed \n\n\n\n node16 (from ESM)   Resolution failed \n\n\n\n bundler             Resolution failed \n\n\n\nThis indicates that no version of Node, or any bundler, can use our package.\n\nLet's fix this.\n\n5.4: Setting main\n\nAdd a main field to your package.json with the following content:\n\n{\n\n  \"main\": \"dist/index.js\"\n\n}\n\nThis tells Node where to find the entry point of your package.\n\n5.5: Try check-exports again\n\nRun the following command to check if all exports from your package are correct:\n\nnpm run check-exports\n\nYou should notice only one warning:\n\n\n\n                    \"tt-package-demo\"            \n\n\n\n node10                                        \n\n\n\n node16 (from CJS)   ESM (dynamic import only) \n\n\n\n node16 (from ESM)   (ESM)                     \n\n\n\n bundler                                       \n\n\n\nThis is telling us that our package is compatible with systems running ESM. People using CJS (often in legacy systems) will need to import it using a dynamic import.\n\n5.6 Fix The CJS Warning\n\nIf you don't want to support CJS (which I recommend), change the check-exports script to:\n\n{\n\n  \"scripts\": {\n\n    \"check-exports\": \"attw --pack . --ignore-rules=cjs-resolves-to-esm\"\n\n  }\n\n}\n\nNow, running check-exports will show everything as green:\n\n\n\n                    \"tt-package-demo\" \n\n\n\n node10                             \n\n\n\n node16 (from CJS)   (ESM)          \n\n\n\n node16 (from ESM)   (ESM)          \n\n\n\n bundler                            \n\n\n\nIf you prefer to dual publish CJS and ESM, skip this step.\n\n5.7: Adding to our CI script\n\nAdd the check-exports script to your ci script in your package.json:\n\n{\n\n  \"scripts\": {\n\n    \"ci\": \"npm run build && npm run check-format && npm run check-exports\"\n\n  }\n\n}\n\n6: Using tsup to Dual Publish\n\nIf you want to publish both CJS and ESM code, you can use tsup. This is a tool built on top of esbuild that compiles your TypeScript code into both formats.\n\nMy personal recommendation would be to skip this step, and only ship ES Modules. This makes your setup significantly simpler, and avoids many of the pitfalls of dual publishing, like Dual Package Hazard.\n\nBut if you want to, go ahead.\n\n6.1: Install tsup\n\nRun the following command to install tsup:\n\nnpm install --save-dev tsup\n\n6.2: Create a tsup.config.ts file\n\nCreate a tsup.config.ts file with the following content:\n\nimport { defineConfig } from \"tsup\";\n\nexport default defineConfig({\n\n  entryPoints: \"src/index.ts\"],\n\n  format: [\"cjs\", \"esm\"],\n\n  dts: true,\n\n  outDir: \"dist\",\n\n  clean: true,\n\n});\n\nentryPoints is an array of entry points for your package. In this case, we're using src/index.ts.\nformat is an array of formats to output. We're using cjs (CommonJS) and esm (ECMAScript modules).\ndts is a boolean that tells tsup to generate declaration files.\noutDir is the output directory for the compiled code.\nclean tells tsup to clean the output directory before building.\n\n6.3: Change the build script\n\nChange the build script in your package.json to the following:\n\n{\n\n  \"scripts\": {\n\n    \"build\": \"tsup\"\n\n  }\n\n}\n\nWe'll now be running tsup to compile our code instead of tsc.\n\n6.4: Add an exports field\n\nAdd an exports field to your package.json with the following content:\n\n{\n\n  \"exports\": {\n\n    \"./package.json\": \"./package.json\",\n\n    \".\": {\n\n      \"import\": \"./dist/index.js\",\n\n      \"default\": \"./dist/index.cjs\"\n\n    }\n\n  }\n\n}\n\nThe exports field tells programs consuming your package how to find the CJS and ESM versions of your package. In this case, we're pointing folks using import to dist/index.js and folks using require to dist/index.cjs.\n\nIt's also recommended to add ./package.json to the exports field. This is because certain tools need easy access to your package.json file.\n\n6.5: Try check-exports again\n\nRun the following command to check if all exports from your package are correct:\n\nnpm run check-exports\n\nNow, everything is green:\n\n\n\n                    \"tt-package-demo\" \n\n\n\n node10                             \n\n\n\n node16 (from CJS)   (CJS)          \n\n\n\n node16 (from ESM)   (ESM)          \n\n\n\n bundler                            \n\n\n\n6.6: Turn TypeScript into a linter\n\nWe're no longer running tsc to compile our code. And tsup doesn't actually check our code for errors - it just turns it into JavaScript.\n\nThis means that our ci script won't error if we have TypeScript errors in our code. Eek.\n\nLet's fix this.\n\n6.6.1: Add noEmit to tsconfig.json\n\nAdd a noEmit field to your tsconfig.json:\n\n{\n\n  \"compilerOptions\": {\n\n    // ...other options\n\n    \"noEmit\": true\n\n  }\n\n}\n\n6.6.2: Remove unused fields from tsconfig.json\n\nRemove the following fields from your tsconfig.json:\n\noutDir\nrootDir\nsourceMap\ndeclaration\ndeclarationMap\n\nThey are no longer needed in our new 'linting' setup.\n\n6.6.3: Change module to Preserve\n\nOptionally, you can now change module to Preserve in your tsconfig.json:\n\n{\n\n  \"compilerOptions\": {\n\n    // ...other options\n\n    \"module\": \"Preserve\"\n\n  }\n\n}\n\nThis means you'll no longer need to import your files with .js extensions. This means that index.ts can look like this instead:\n\nexport * from \"./utils\";\n\n6.6.4: Add a lint script\n\nAdd a lint script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"lint\": \"tsc\"\n\n  }\n\n}\n\nThis will run TypeScript as a linter.\n\n6.6.5: Add lint to your ci script\n\nAdd the lint script to your ci script in your package.json:\n\n{\n\n  \"scripts\": {\n\n    \"ci\": \"npm run build && npm run check-format && npm run check-exports && npm run lint\"\n\n  }\n\n}\n\nNow, we'll get TypeScript errors as part of our CI process.\n\n7: Testing with Vitest\n\nIn this section, we'll install vitest, create a test, set up a test script, run the test script, set up a dev script, and add the test script to our CI script.\n\nvitest is a modern test runner for ESM and TypeScript. It's like Jest, but better.\n\n7.1: Install vitest\n\nRun the following command to install vitest:\n\nnpm install --save-dev vitest\n\n7.2: Create a test\n\nCreate a src/utils.test.ts file with the following content:\n\nimport { add } from \"./utils.js\";\n\nimport { test, expect } from \"vitest\";\n\ntest(\"add\", () => {\n\n  expect(add(1, 2)).toBe(3);\n\n});\n\nThis is a simple test that checks if the hello function returns the correct value.\n\n7.3: Set up test script\n\nAdd a test script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"test\": \"vitest run\"\n\n  }\n\n}\n\nvitest run runs all tests in your project once, without watching.\n\n7.4: Run the test script\n\nRun the following command to run your tests:\n\nnpm run test\n\nYou should see the following output:\n\n src/utils.test.ts (1)\n\n    hello\n\n Test Files  1 passed (1)\n\n      Tests  1 passed (1)\n\nThis indicates that your test passed successfully.\n\n7.5: Set up dev script\n\nA common workflow is to run your tests in watch mode while developing. Add a dev script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"dev\": \"vitest\"\n\n  }\n\n}\n\nThis will run your tests in watch mode.\n\n7.6: Adding to our CI script\n\nAdd the test script to your ci script in your package.json:\n\n{\n\n  \"scripts\": {\n\n    \"ci\": \"npm run build && npm run check-format && npm run check-exports && npm run lint && npm run test\"\n\n  }\n\n}\n\n8\\. Set up our CI with GitHub Actions\n\nIn this section, we'll create a GitHub Actions workflow that runs our CI process on every commit and pull request.\n\nThis is a crucial step in ensuring that our package is always in a working state.\n\n8.1: Creating our workflow\n\nCreate a .github/workflows/ci.yml file with the following content:\n\nname: CI\n\non:\n\n  pull_request:\n\n  push:\n\n    branches:\n\n      main\n\nconcurrency:\n\n  group: ${{ github.workflow }}-${{ github.ref }}\n\n  cancel-in-progress: true\n\njobs:\n\n  ci:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      uses: actions/checkout@v4\n\n      name: Use Node.js\n\n        uses: actions/setup-node@v4\n\n        with:\n\n          node-version: \"20\"\n\n      name: Install dependencies\n\n        run: npm install\n\n      name: Run CI\n\n        run: npm run ci\n\nThis file is what GitHub uses as its instructions for running your CI process.\n\nname is the name of the workflow.\non specifies when the workflow should run. In this case, it runs on pull requests and pushes to the main branch.\nconcurrency prevents multiple instances of the workflow from running at the same time, using cancel-in-progress to cancel any existing runs.\njobs is a set of jobs to run. In this case, we have one job called ci.\nactions/checkout@v4 checks out the code from the repository.\nactions/setup-node@v4 sets up Node.js and npm.\nnpm install installs the project's dependencies.\nnpm run ci runs the project's CI script.\n\nIf any part of our CI process fails, the workflow will fail and GitHub will let us know by showing a red cross next to our commit.\n\n8.2: Testing our workflow\n\nPush your changes to GitHub and check the Actions tab in your repository. You should see your workflow running.\n\nThis will give us a warning on every commit made, and every PR made to the repository.\n\n9\\. Publishing with Changesets\n\nIn this section, we'll install @changesets/cli, initialize Changesets, make changeset releases public, set commit to true, set up a local-release script, add a changeset, commit your changes, run the local-release script, and finally see your package on npm.\n\nChangesets is a tool that helps you version and publish your package. It's an incredible tool that I recommend to anyone publishing packages to npm.\n\n9.1: Install @changesets/cli\n\nRun the following command to initialise Changesets:\n\nnpm install --save-dev @changesets/cli\n\n9.2: Initialize Changesets\n\nRun the following command to initialize Changesets:\n\nnpx changeset init\n\nThis will create a .changeset folder in your project, containing a config.json file. This is also where your changesets will live.\n\n9.3: Make changeset releases public\n\nIn .changeset/config.json, change the access field to public:\n\n// .changeset/config.json\n\n{\n\n  \"access\": \"public\"\n\n}\n\nWithout changing this field, changesets won't publish your package to npm.\n\n9.4: Set commit to true:\n\nIn .changeset/config.json, change the commit field to true:\n\n// .changeset/config.json\n\n{\n\n  \"commit\": true\n\n}\n\nThis will commit the changeset to your repository after versioning.\n\n9.5: Set up a local-release script\n\nAdd a local-release script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"local-release\": \"changeset version && changeset publish\"\n\n  }\n\n}\n\nThis script will run your CI process and then publish your package to npm. This will be the command you run when you want to release a new version of your package from your local machine.\n\n9.6 Run CI in prepublishOnly\n\nAdd a prepublishOnly script to your package.json with the following content:\n\n{\n\n  \"scripts\": {\n\n    \"prepublishOnly\": \"npm run ci\"\n\n  }\n\n}\n\nThis will automatically run your CI process before publishing your package to npm.\n\nThis is useful to separate from the local-release script in case a user accidentally runs npm publish without running local-release. Thanks to [Jordan Harband for the suggestion!\n\n9.7: Add a changeset\n\nRun the following command to add a changeset:\n\nnpx changeset\n\nThis will open an interactive prompt where you can add a changeset. Changesets are a way to group changes together and give them a version number.\n\nMark this release as a patch release, and give it a description like \"Initial release\".\n\nThis will create a new file in the .changeset folder with the changeset.\n\n9.8: Commit your changes\n\nCommit your changes to your repository:\n\ngit add .\n\ngit commit -m \"Prepare for initial release\"\n\n9.9: Run the local-release script\n\nRun the following command to release your package:\n\nnpm run local-release\n\nThis will run your CI process, version your package, and publish it to npm.\n\nIt will have created a CHANGELOG.md file in your repository, detailing the changes in this release. This will be updated each time you release.\n\n9.10: See your package on npm\n\nGo to:\n\nhttp://npmjs.com/package/\n\nYou should see your package there! You've done it! You've published to npm!\n\nSummary\n\nYou now have a fully set up package. You've set up:\n\nA TypeScript project with the latest settings\nPrettier, which both formats your code and checks that it's formatted correctly\n@arethetypeswrong/cli, which checks that your package exports are correct\ntsup, which compiles your TypeScript code to JavaScript\nvitest, which runs your tests\nGitHub Actions, which runs your CI process\nChangesets, which versions and publishes your package\n\nFor further reading, I'd recommend setting up the Changesets GitHub action and PR bot to automatically recommend contributors add changesets to their PR's. They are both phenomenal.\n\nAnd if you've got any more questions, let me know!## Cursor Rules for Better AI Development\n\n\n\nFinding existing community.cursor/rules for TypeScript lacking, I'm sharing my own set to hopefully kickstart a discussion on what makes effective AI coding guidance. These rules focus purely on TypeScript language features, documentation, structure (like Result types), teaching the AI specific nuances (like noUncheckedIndexedAccess), and practical habits, rather than specific frameworks. I also distinguish between shareable, project-specific Workspace Rules (versioned in Git) and personalized Global Rules (living in your IDE) to tailor the AI to your individual style and workflow. You can download my current ruleset using the link in the original post.\n\nMatt Pocock Matt Pocock\n\nShould You Declare Return Types?\n\nHere's a quick .cursor/rules addition you can make for handling return types in TypeScript.\n\nWhen declaring functions on the top-level of a module,\ndeclare their return types. This will help future AI\nassistants understand the function's purpose.\n\n\\\\\\`ts\nconst myFunc = (): string => {\nreturn \"hello\";\n};\n\\\\\\`\n\nOne exception to this is components which return JSX.\nNo need to\n\nMatt Pocock Matt Pocock## TypeScript Announces Go Rewrite, Achieves 10x Speedup\n\nMatt Pocock Matt Pocock## TypeScript 5.8 Ships --erasableSyntaxOnly To Disable Enums\n\nMatt Pocock Matt Pocock## Node.js Now Supports TypeScript By Default\n\nMatt Pocock Matt Pocock## This Crazy Syntax Lets You Get An Array Element's Type\n\nMatt Pocock Matt Pocock",
        "locale": "en",
        "slug": "technical-posts/how-to-create-an-npm-package",
        "folder": ""
    },
    {
        "title": "Sveltekit miscellaneous things",
        "description": "Sveltekit miscellaneous things",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Accessing a page's HEAD section\n\nThis code injects or overwrites an attribute tag in the head section:\n\n\n\tHome\n\nGet page url info\n\n\n    import { page } from '$app/stores'\n\n\nPosts\n\n\n    {JSON.stringify($page, null, 2)}\n\n\nProduces this\n\nURL {\n  href: 'http://localhost:5173/about',\n  origin: 'http://localhost:5173',\n  protocol: 'http:',\n  username: '',\n  password: '',\n  host: 'localhost:5173',\n  hostname: 'localhost',\n  port: '5173',\n  pathname: '/about',\n  search: '',\n  searchParams: URLSearchParams {},\n  hash: ''\n}\n\nSee this SO question for more info (and maybe a better way--see last answer)\n\nroger",
        "locale": "en",
        "slug": "technical-posts/sveltekit-miscellaneous-things",
        "folder": ""
    },
    {
        "title": "CSS Forms",
        "description": "CSS Forms",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Valid input outline indicator",
        "locale": "en",
        "slug": "technical-posts/css-forms",
        "folder": ""
    },
    {
        "title": "Rust mutable references",
        "description": "Rust mutable references",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "Question\n\nThe first call to get_file_lines needs to be like this:\n\nget_file_lines(&full_filename, &mut lines)?;\n\nbut the recursive call needs to be like this:\n\nget_file_lines(&include_filename, lines)?;\n\nwhy is the &mut omitted in the recursive call\n\nAnswer\n\nThe reason you omit the &mut in the recursive call is that Rust automatically handles the &mut borrow when passing the lines vector. Both calls are effectively the same, but since lines is already borrowed mutably (&mut Vec), you dont need to re-specify the &mut keyword in the recursive call.\n\nLets break it down:\n\n1. Initial Call to get_file_lines:\n\nWhen you first call get_file_lines like this:\n\nget_file_lines(&full_filename, &mut lines)?;\n\n&mut lines explicitly borrows the lines vector mutably for the first time. This is required because you're allowing the function to modify the vector (push new lines to it).\nAfter the first mutable borrow, lines is now being passed around as a mutable reference for as long as it is borrowed.\n\n2. Recursive Call:\n\nThe recursive call looks like this:\n\nget_file_lines(&include_filename, lines)?;\n\nNotice that you're passing lines without &mut. This works because lines is already a mutable reference. Inside the function, you're working with a &mut Vec, and Rust doesn't require you to re-borrow a mutable reference that is already being passed around.\n\nWhy does Rust allow this?\n\nWhen you pass lines without &mut, Rust still recognizes that it's a mutable reference (&mut Vec) because the function signature expects it.\nIf lines is already borrowed mutably, the compiler knows you're still working with that mutable reference and doesn't need you to explicitly mark it as &mut again.\n\nFunction Signature:\n\nfn get_file_lines(full_filename: &str, lines: &mut Vec) -> io::Result {\n\nThe function signature clearly expects lines to be a &mut Vec, so when you pass lines recursively, it already knows it's a mutable reference.\n\nExample:\n\nHeres a quick example to illustrate this point:\n\nfn main() {\n    let mut numbers = vec![1, 2, 3];\n    add_to_vec(&mut numbers);\n    println!(\"{:?}\", numbers);  // Output: [1, 2, 3, 4, 5]\n\n    // Recursive call, no need for &mut again\n    add_to_vec(numbers);\n}\n\nfn add_to_vec(v: &mut Vec) {\n    v.push(4);\n    // Recursive call, 'v' is already a mutable reference\n    if v.len() < 5 {\n        add_to_vec(v);\n    } else {\n        v.push(5);\n    }\n}\n\nKey Takeaways:\n\nInitial borrow: You need to use &mut to borrow the lines vector mutably for the first call.\nSubsequent uses: Once lines is borrowed mutably, you don't need to re-specify &mut in recursive calls because Rust knows you're passing a mutable reference.",
        "locale": "en",
        "slug": "technical-posts/passing-mutable-references",
        "folder": ""
    },
    {
        "title": "PowerShell approved verbs",
        "description": "PowerShell approved verbs",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "Add\nClear\nClose\nCompress\nConnect\nConvert\nConvertFrom\nConvertTo\nCopy\nDismount\nEnter\nExit\nExpand\nExport\nFind\nFormat\nGet\nGrant\nHide\nImport\nInitialize\nInstall\nInvoke\nJoin\nLock\nMeasure\nMount\nMove\nNew\nOpen\nOptimize\nPop\nPush\nRead\nReceive\nRegister\nRemove\nRename\nReset\nResize\nRestart\nRestore\nResume\nRevoke\nSave\nSearch\nSelect\nSet\nShow\nSkip\nSplit\nStart\nStep\nStop\nSubmit\nSuspend\nSwitch\nTest\nUnblock\nUndo\nUninstall\nUnlock\nUnregister\nUpdate\nUse\nWait\nWatch\nWrite",
        "locale": "en",
        "slug": "technical-posts/powershell-approved-verbs",
        "folder": ""
    },
    {
        "title": "mermaid-script-notes-for-asna-com-deploy",
        "description": "mermaid-script-notes-for-asna-com-deploy",
        "date_created": "2025-09-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-com"
        ],
        "content": "Here's the documentation for the Mermaid flowchart script:\n\nMermaid Flowchart Documentation\n\nOverview\nThis flowchart visualizes the complete pre-deployment pipeline that transforms raw markdown documentation into a production-ready multilingual documentation website with search functionality.\n\nNode Types and Color Coding\n\nclassDef inputFile fill:#e1f5fe\nclassDef processScript fill:#fff3e0\nclassDef outputFile fill:#e8f5e8\nclassDef finalOutput fill:#fce4ec\nclassDef liveSite fill:#f3e5f5\n\nInput Files (Light Blue #e1f5fe): Source materials that feed into the pipeline\nProcess Scripts (Light Orange #fff3e0): Node.js scripts that transform data\nOutput Files (Light Green #e8f5e8): Intermediate files created during processing\nLive Site Files (Light Purple #f3e5f5): Files directly used by the production website\nFinal Output (Light Pink #fce4ec): External services or final deployment targets\n\nHere's documentation explaining how the Mermaid script itself works:\n\nMermaid Script Structure Documentation\n\nFlowchart Declaration\nflowchart TD\nflowchart - Declares this as a flowchart diagram type\nTD - Sets direction as \"Top Down\" (vertical layout)\n\nNode Definitions\nA[Raw Markdown Files] --> B[\"Step 1: create-markdown-objects.js\"]\nA[text] - Creates a rectangular node with ID \"A\" containing the specified text\nB[\"text\"] - Square brackets with quotes allow multi-line text and special characters\n--> - Creates a directional arrow from node A to node B\n\nNode with Line Breaks\nH[\"tags-list.js(live site)\"]\n`` - HTML line break tag creates multi-line text within a node\nUseful for adding annotations or secondary information\n\nClass Definitions\nclassDef inputFile fill:#e1f5fe\nclassDef processScript fill:#fff3e0\nclassDef outputFile fill:#e8f5e8\nclassDef finalOutput fill:#fce4ec\nclassDef liveSite fill:#f3e5f5\nclassDef - Defines a CSS class that can be applied to nodes\nfill:#color - Sets the background color using hex values\nClass names are arbitrary (inputFile, processScript, etc.)\n\nClass Applications\nclass A,K,Q inputFile\nclass B,E,F,G,L,N,P processScript\nclass - Applies a previously defined class to specific nodes\nMultiple nodes can be assigned the same class using comma separation\nNode IDs (A, K, Q) must match those defined in the flowchart\n\nFlow Patterns Used\nSequential: A --> B --> C (one step after another)\nBranching: B --> C and B --> D (one input, multiple outputs)  \nConverging: J --> P and M --> P (multiple inputs, one output)\nParallel: Multiple independent processing chains running simultaneously\n\nStyling Strategy\nThe script uses color coding to create a visual hierarchy:\nLight colors to avoid overwhelming the diagram\nDistinct hues for each processing stage\nSimilar colors grouped by function (all processing scripts are orange)\nSpecial highlighting for production-critical files (live site = purple)\n\nThis creates an intuitive visual flow from raw inputs through processing stages to final outputs.",
        "locale": "en",
        "slug": "technical-posts/mermaid-script-notes-for-asna-com-deploy",
        "folder": ""
    },
    {
        "title": "Sveltekit locals",
        "description": "Sveltekit locals",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "how to use http context api with sveltekit to pass data from one request to another\n\nWhile SvelteKit doesn't have a built-in \"HTTP Context\" API like some server-side frameworks, we can achieve similar functionality using locals within the SvelteKit server hooks, along with techniques like passing data through the URL, or temporary storage.\n\nHere's a breakdown of different approaches and how to implement them in your SvelteKit app:\n\nUnderstanding locals in SvelteKit\n\nlocals is an object available in your SvelteKit server-side hooks ( src/hooks.server.js or src/hooks.server.ts) that persists across a single request/response cycle.\nIt's ideal for passing data between different hooks within the same request or even passing information to your load functions.\nCrucially, locals is only available server-side, it's not directly accessible in your client-side components.\n\nCommon Approaches\n\nUsing locals for Request-Scoped Data\n\n    Scenario: You need to set a flag or some data in one hook, and use it in another hook or your load function during the same request.\n    Example:\n\n        // src/hooks.server.ts\n        import type { Handle } from \"@sveltejs/kit\";\n\n        export const handle: Handle = async ({ event, resolve }) => {\n            if (event.url.pathname.startsWith(\"/protected\")) {\n                event.locals.user = { id: 123, name: \"John Doe\" }; // Setting user on locals\n            }\n\n            const response = await resolve(event);\n            return response;\n        };\n\n        // src/routes/+page.server.ts\n        import type { PageServerLoad } from \"./$types\";\n\n        export const load: PageServerLoad = async ({ locals }) => {\n            if (locals.user) {\n                return {\n                    greeting: Hello, ${locals.user.name},\n                };\n            } else {\n                return {\n                    greeting: \"Hello Stranger\",\n                };\n            }\n        };\n\n        Explanation:\n            The handle hook checks if the route starts with /protected. If so, it adds a user object to event.locals.\n            The load function can access event.locals and use the user data.\n            This data is specific to that single request.\n\n    Limitations: Data set in locals will not carry over between different requests.\n\nPassing Data via the URL (Query Parameters)\n\n    Scenario: You need to persist data between requests, for example, redirecting after a form submit while carrying context information.\n    Example:\n\n        // src/routes/form/+page.server.ts\n        import { redirect } from \"@sveltejs/kit\";\n        import type { Actions } from \"./$types\";\n\n        export const actions: Actions = {\n            default: async ({ request }) => {\n                const formData = await request.formData();\n                const name = formData.get(\"name\");\n                // ... processing logic\n                throw redirect(303, /success?userName=${name});\n            },\n        };\n\n        // src/routes/form/+page.svelte\n        \n          \n          Submit\n        \n\n        // src/routes/success/+page.server.ts\n        import type { PageServerLoad } from \"./$types\";\n\n        export const load: PageServerLoad = async ({ url }) => {\n            const userName = url.searchParams.get(\"userName\");\n\n            return {\n                userName,\n                message: Thanks ${userName} for submitting!,\n            };\n        };\n\n        // src/routes/success/+page.svelte\n        {data.message}\n\n    Explanation:\n        After a form submission, we redirect to /success and include the name from the form as a query parameter.\n        The success route's load function can access the parameter using url.searchParams\n    Considerations:\n        This is suitable for small pieces of data that can be safely exposed in the URL.\n        Avoid exposing sensitive information.\n        Keep URL lengths within reasonable limits.\n\nTemporary Storage (e.g., Database, Redis, Session Storage):\n\n    Scenario: You need to preserve data across requests and potentially across multiple user sessions.\n    Example (using session storage with an in-memory adapter):\n\n        // src/hooks.server.ts\n        import type { Handle, HandleServerError } from \"@sveltejs/kit\";\n\n        interface UserData {\n            userId: number;\n            name: string;\n        }\n\n        const sessionStore = new Map();\n\n        function createSessionId() {\n            return crypto.randomUUID();\n        }\n\n        export const handle: Handle = async ({ event, resolve }) => {\n            let sessionId = event.cookies.get(\"sessionId\");\n            let userData: UserData | undefined = undefined;\n            if (!sessionId) {\n                sessionId = createSessionId();\n                event.cookies.set(\"sessionId\", sessionId);\n            }\n            if (sessionStore.has(sessionId)) {\n                userData = sessionStore.get(sessionId);\n                event.locals.user = userData;\n            }\n\n            if (event.url.pathname === \"/login\") {\n                if (!sessionId) {\n                    sessionId = createSessionId();\n                    event.cookies.set(\"sessionId\", sessionId);\n                }\n                const user = { userId: 1, name: \"John Doe\" } as UserData;\n                sessionStore.set(sessionId, user);\n                event.locals.user = user;\n                throw redirect(302, \"/\");\n            }\n            const response = await resolve(event);\n            return response;\n        };\n\n        export const handleError: HandleServerError = async ({\n            error,\n            event,\n        }) => {\n            console.error(\"Error in handle:\", error);\n            return {\n                message: \"A server error occurred.\",\n                error,\n            };\n        };\n\n        // src/routes/+page.server.ts\n        import type { PageServerLoad } from \"./$types\";\n\n        export const load: PageServerLoad = async ({ locals }) => {\n            return {\n                message: locals?.user\n                    ? Hello ${locals.user.name}\n                    : \"Hello anonymous user\",\n            };\n        };\n\n    Explanation:\n        We're using a simple Map as an in-memory store (for demonstration only, in production use a proper database, Redis, etc.).\n        We generate a session ID (UUID) and store it as a cookie.\n        When a request comes in:\n            If we find a valid session ID in cookies, we restore the session data from our storage.\n            If the path is /login, we can set the user into storage\n        We can use the locals to pass down the user data during the request\n    Considerations:\n        This approach is more complex but allows you to handle user sessions and persistent data.\n        Choose appropriate persistent storage based on your needs.\n        Implement a proper session management strategy.\n\nChoosing the Right Approach\n\nlocals: Ideal for short-lived data specific to a single request.\nURL parameters: Suitable for basic data to be passed between requests via redirects.\nTemporary storage: Necessary for persisting data across requests and sessions, consider using databases, Redis or more persistent forms of session storage in production\n\nImportant Notes:\n\nSecurity: Be mindful of security when passing data around. Don't expose sensitive data in URLs.\nScalability: If you need to handle many concurrent requests or persist data across multiple users, use a robust solution such as a database or Redis.\nSession Management: If you're dealing with user authentication and sessions, research secure session handling techniques.\n\nBy carefully considering the different techniques and choosing the right approach for your particular use case, you can implement a robust and efficient data passing system in your SvelteKit application.",
        "locale": "en",
        "slug": "technical-posts/sveltekit-locals",
        "folder": ""
    },
    {
        "title": "CSS Flex demystified",
        "description": "CSS Flex demystified",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css/flex"
        ],
        "content": "Okay, let's demystify the flex shorthand property in CSS. It's a powerful way to control how flex items (the children inside a flex container) size themselves.\n\nThe Core Idea:\n\nThe flex property is a shorthand for three individual properties that control the flexibility of a flex item:\n\nflex-grow: How much should the item grow if there's extra space?\nflex-shrink: How much should the item shrink if there isn't enough space?\nflex-basis: What's the item's ideal starting size before growing or shrinking?\n\nSyntax:\n\n.flex-item {\n    /* flex:   ; */\n    flex: 0 1 auto; /* This is the default value */\n}\n\nLet's break down the components:\n\n\n1. flex-grow (The first value)\n\nWhat it does: Dictates the proportion of available extra space in the flex container that this item should take up.\nValue: A unitless, non-negative number (e.g., 0, 1, 2).\nDefault: 0 (meaning the item won't grow by default).\nHow it works: If you have extra space in the container, it's distributed among items with flex-grow greater than 0.\n    If one item has flex-grow: 1 and others have flex-grow: 0, that one item gets all the extra space.\n    If Item A has flex-grow: 1 and Item B has flex-grow: 2, Item B will get twice as much of the extra space as Item A.\n\n\n2. flex-shrink (The second value)\n\nWhat it does: Dictates the proportion of overflow space that should be removed from this item if the items collectively are too big for the container.\nValue: A unitless, non-negative number (e.g., 0, 1, 2).\nDefault: 1 (meaning the item will shrink proportionally by default if needed).\nHow it works: If items need to shrink, those with a higher flex-shrink value will shrink more relative to others.\n    flex-shrink: 0 means the item will not shrink below its flex-basis size. It becomes inflexible in terms of shrinking.\n    If Item A has flex-shrink: 1 and Item B has flex-shrink: 2, Item B will shrink twice as much as Item A when space is tight (relative to their flex-basis).\n\n\n3. flex-basis (The third value)\n\nWhat it does: Defines the \"ideal\" or \"hypothetical\" starting size of the item before any growing (flex-grow) or shrinking (flex-shrink) calculations are applied.\nValue: Can be a length (px, em, %, etc.) or the keyword auto or content.\nDefault: auto (meaning \"look at my width or height property first, otherwise size me based on my content\").\nHow it works: This is the size the browser tries to give the item. Then, it checks if there's extra space (apply flex-grow) or not enough space (apply flex-shrink) compared to the sum of all flex-basis values.\n    If set to 0 (or 0%), the item's content size is ignored for the initial space calculation, and space distribution relies entirely on flex-grow factors.\n\n\nCommon Shorthand Values:\n\nYou'll often see single-value keywords used with flex:\n\nflex: initial;\n    Equivalent to: flex: 0 1 auto; (The default)\n    Doesn't grow, shrinks if needed, basis is determined automatically. Item sizes itself based on its width/height or content.\nflex: auto;\n    Equivalent to: flex: 1 1 auto;\n    Grows to fill space, shrinks if needed, basis is determined automatically. It's flexible both ways, respecting its initial size.\nflex: none;\n    Equivalent to: flex: 0 0 auto;\n    Doesn't grow, doesn't shrink. Item is inflexible, sticking to its auto basis (usually its content or explicit width/height).\nflex: ; (e.g., flex: 1;, flex: 2;)\n    Equivalent to: flex:  1 0%;\n    This is very common! It means the item grows proportionally (`), shrinks normally (1), and has a flex-basis of 0. The 0 basis is key  it makes items share space relative only to their flex-grow factor, ignoring their initial content size. This is great for creating equal-width columns (flex: 1` on all items).\n\n\nExample: Equal Width Columns\n\n\n    Item 1\n    Item 2 has more content\n    Item 3\n\n\n.container {\n    display: flex;\n    border: 1px solid black;\n}\n\n.item {\n    flex: 1; /* This is flex: 1 1 0%; */\n    border: 1px solid red;\n    padding: 10px;\n}\n\nResult: All three items will have equal width, regardless of their content. Why?\n\nflex-basis: 0%: Initially, each item is considered to have zero width.\nflex-grow: 1: All the container's space is considered \"extra space\". Since all items have flex-grow: 1, they each get an equal share (1/3) of that total space.\nflex-shrink: 1: If the window gets too small, they will shrink proportionally (though this often doesn't happen if basis is 0 unless you have min-widths).\n\n\nKey Takeaway:\n\nThe flex shorthand is your primary tool for controlling how flex items size themselves within their container, balancing their ideal size (flex-basis) with their willingness to grow (flex-grow) or shrink (flex-shrink) based on available space. Understanding the three components and the common keyword shortcuts (initial, auto, none, 1) will get you very far!",
        "locale": "en",
        "slug": "technical-posts/css-flex-demystified",
        "folder": ""
    },
    {
        "title": "Creating a symbolic link with DOS mklink",
        "description": "Creating a symbolic link with DOS mklink",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows",
            "utility"
        ],
        "content": "This command creates a symbolic link at c:\\Users\\thumb\\Documents\\projects\\link that points to the c:\\Users\\thumb\\Documents\\projects\\original folder:\n\nmklink /D \"c:\\Users\\thumb\\Documents\\projects\\link\" \"c:\\Users\\thumb\\Documents\\projects\\original\"\n\nYou can work with folders from either directory. There is only one physical copy of the files and they are in the orginal folder. The link folder must not exist.\n\nTo delete the symbolic link, delete the linked folder. Do not delete the original!\n\nIn this example, the top is the original folder and the linked folder is below. Linked folders are shown in the File Explorer with a shortcut icon.\n\n![[image-20.png|399x97]]\n\nExample\n\nFor this Sveltekit project:\n\nC:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\n\nThis mklink\n\nmklink /D \"C:\\Users\\thumb\\Documents\\projects\\svelte\\rp-blog\\src\\markdown\\obs-tech\" \"C:\\Users\\thumb\\Documents\\resilio-envoy\\Obsidian\\brainiac\\technical-posts\"\n\ncreated a symbolic for the technical-posts folder in the Obsidian Brainiac vault that makes it look like the Obsidian content lives in the SvelteKit project's folder structure:\n\n![[image-21.png]]\n\nThe obs-tech folder points back to the Obsidian folder. This enables publishing that Obsidian content to the web and adding a search engine for it.\n\n[!danger]\nRemember that any changes made to the content in the obs-tech folder also changes the Obsidian source content. This includes adding and deleting files.",
        "locale": "en",
        "slug": "technical-posts/creating-a-symbolic-link-with-dos-mklink",
        "folder": ""
    },
    {
        "title": "How to read return data with use:enhance and form actions",
        "description": "How to read return data with use:enhance and form actions",
        "date_created": "2025-07-08T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "Question: with use:enhance, how do I read the result from the form action\n\nrogerpence: in the hooks.server.ts (using the MSSqlConnect.ts moduel) I am successfully connecting to the mssql connection pool. But in +page.server.ts I am getting the error:  RequestError: No connection is specified for that request.\n\nI have confirmed that the SQL Server credentials do work.\n\nGitHub Copilot: The error RequestError: No connection is specified for that request. means the ms.sql.Request() in your +page.server.ts is not associated with an active connection or pool.\n\nWhy this happens:  \nIn the mssql library, a Request must be created from a connected ConnectionPool instance, not just from the sql module.  \nYou are doing:\nconst request = new ms.sql.Request();\nBut you should do:\nconst request = new ms.pool.request();\nOr, if using the mssql library's naming:\nconst request = ms.pool.request();\nThis ensures the request is tied to the open pool.\n\nHow to fix\n\nUpdate your +page.server.ts like this:\n\n// ...existing code...\n\ttry {\n\t\tconst request = ms.pool.request(); //  response.json())\n  .then(data => {\n    console.log('Response:', data);\n  })\n  .catch(error => {\n    console.error('Error:', error);\n  });\n\nYou can use this code in your Svelte component or any JavaScript file in your project.\n\nSimilar code found with 1 license type\n\nrogerpence: At line 69 I have some stubbed in to call the assignCategoryRule action in +page.server.ts but I don't think that form-based code is correct. I want to pass the categoryData object to the form actcion and then have the assignCategoryRule  form action call the post API at /api/update-category\n\nGitHub Copilot: You're correct: the current form setup is using SvelteKit's form actions, which only send form data (not your categoryData object) to the assignCategoryRule action. If you want to send the categoryData object to your /api/update-category endpoint via the form action, you need to:\n\nSend the data from the client to the form action (as JSON or as form fields).\nHave the form action call the API using fetch on the server side.\n\nHere's how you can do it:\n\n1. Pass categoryData to the form\n\nSince HTML forms only send string fields, you can serialize your categoryData as JSON in a hidden input:\n\n\n {\n    return async ({ result, formData }) => {\n      if (result.type === 'success') {\n        console.log(result);\n        console.log(formData);\n      }\n      await applyAction(result);\n    };\n  }}\n  \n  \n  Update\n\n2. Parse and forward the data in the form action\n\nUpdate your assignCategoryRule action to parse the JSON and call the API:\n\n// ...existing code...\nassignCategoryRule: async ({ request, fetch }) => {\n  const rawFormData = Object.fromEntries(await request.formData());\n  let categoryData;\n\n  try {\n    categoryData = JSON.parse(rawFormData.categoryData as string);\n  } catch (err) {\n    return fail(400, { message: 'Invalid category data' });\n  }\n\n  try {\n    // Call the API endpoint on the server\n    const apiResponse = await fetch('/api/update-category', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify(categoryData)\n    });\n\n    const apiResult = await apiResponse.json();\n\n    if (!apiResult.success) {\n      return fail(500, { message: apiResult.message });\n    }\n\n    return { success: true, apiResult };\n  } catch (err: any) {\n    return fail(500, { message: err.message || 'API call failed' });\n  }\n}\n// ...existing code...\n\n\nSummary:  \nSerialize your object as a hidden input in the form.\nParse it in the form action and forward it to your API.\n\nLet me know if you want a more dynamic example for each row, or if you want to use a pure fetch-based approach instead of SvelteKit form actions!\n\nrogerpence: the 'avail' variable on line 26 is the currently selected value of the select element. How can i make 'avail' available to the parent component. I am using sveltekit 2.x and svelte 5.5\n\nGitHub Copilot: To make the avail variable (the currently selected value) available to the parent component in Svelte 5, you should use the export let avail; pattern and bind it in the parent. In Svelte 5, you can also use the bind:avail directive in the parent component.\n\nHow to do it:\n\n1. In your Select.svelte component\n\nAdd export let avail; at the top, and remove the local let avail = ... declaration.  \nInstead, initialize avail if not provided:\n\n\n\texport interface ListItem {\n\t\tvalue: string;\n\t\ttext: string;\n\t}\n\tinterface ComponentProps {\n\t\tdomName: string;\n\t\tlistItems: ListItem];\n\t\tselectedValue?: string | undefined | null;\n\t\tshowInvalidValueError?: boolean;\n\t\tform?: string;\n\t}\n\n\texport function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n\t\treturn listItems.some((element) => element.value === selectedValue);\n\t}\n\n\tconst { domName, listItems, selectedValue, showInvalidValueError, form }: ComponentProps = $props();\n\n\texport let avail = selectedValue && isSelectValueInList(selectedValue)\n\t\t? selectedValue\n\t\t: listItems[0].value;\n\n\n\n\t\n\t\t{#each listItems as listItem}\n\t\t\t{listItem.text}\n\t\t{/each}\n\t\n\t{#if !isSelectValueInList(selectedValue) && showInvalidValueError}\n\t\t\n\t\t\t{Current value of '${selectedValue}' is not in the list}\n\t\t\n\t{/if}\n\n2. In the parent component\n\nBind to avail like this:\n\n\n\nNow, selectedCategory in the parent will always reflect the current value of the select in Select.svelte.\n\n\nSummary:  \nUse export let avail in the child.\nUse bind:avail={yourVar} in the parent.\n\nThis is the Svelte 5+ idiomatic way to expose a value for two-way binding.\n\nrogerpence: export let is the svelte wayu to do things. I need to use a rune for this I think\n\nGitHub Copilot: You're correct! In Svelte 5, to expose a reactive value from a child component to the parent using runes, you should use the $export rune.\n\nHeres how you can do it with your avail variable:\n\n\n\timport { $export } from 'svelte';\n\n\texport interface ListItem {\n\t\tvalue: string;\n\t\ttext: string;\n\t}\n\tinterface ComponentProps {\n\t\tdomName: string;\n\t\tlistItems: ListItem[];\n\t\tselectedValue?: string | undefined | null;\n\t\tshowInvalidValueError?: boolean;\n\t\tform?: string;\n\t}\n\n\texport function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n\t\treturn listItems.some((element) => element.value === selectedValue);\n\t}\n\n\tconst { domName, listItems, selectedValue, showInvalidValueError, form }: ComponentProps = $props();\n\n\tconst selectValueInList = isSelectValueInList(selectedValue);\n\n\tlet avail = $state(selectedValue && selectValueInList ? selectedValue : listItems[0].value);\n\n\t$export('avail', avail); // \n\n\n\t\n\t\t{#each listItems as listItem}\n\t\t\t{listItem.text}\n\t\t{/each}\n\t\n\t{#if !selectValueInList && showInvalidValueError}\n\t\t\n\t\t\t{Current value of '${selectedValue}' is not in the list}\n\t\t\n\t{/if}\n\n\nUsage in parent:\n\n\nThis is the Svelte 5 rune-based way to expose a variable for two-way binding!\n\nrogerpence: there isn't a $export rune\n\nGitHub Copilot: You're rightthere is no $export rune in Svelte 5. The correct rune for exposing a value to the parent for two-way binding is export let (the classic Svelte way), but in Svelte 5 with runes, you use the @export directive in the `` block.\n\nHow to expose a value for two-way binding in Svelte 5:\n\n1. Use @export with your variable\n\nUpdate your script block like this:\n\n\n\t@export let avail = selectedValue && isSelectValueInList(selectedValue)\n\t\t? selectedValue\n\t\t: listItems[0].value;\n\n\texport interface ListItem {\n\t\tvalue: string;\n\t\ttext: string;\n\t}\n\tinterface ComponentProps {\n\t\tdomName: string;\n\t\tlistItems: ListItem[];\n\t\tselectedValue?: string | undefined | null;\n\t\tshowInvalidValueError?: boolean;\n\t\tform?: string;\n\t}\n\n\texport function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n\t\treturn listItems.some((element) => element.value === selectedValue);\n\t}\n\n\tconst { domName, listItems, selectedValue, showInvalidValueError, form }: ComponentProps = $props();\n\n\tconst selectValueInList = isSelectValueInList(selectedValue);\n\n2. Use bind:avail in the parent\n\n\n\n\nSummary:  \nUse @export let avail = ... in the child.\nUse bind:avail={...} in the parent.\n\nThis is the Svelte 5+ idiomatic way for two-way binding with runes.  \nLet me know if you need a full example!\n\nrogerpence: I think you are halluicnating. There isn't a @export directive\n\nGitHub Copilot: You are correctthere is no @export directive or $export rune in Svelte 5.  \nThe official and only way to expose a variable for two-way binding to a parent component is still to use export let avail in your `` block.\n\nIf you want the parent to bind to the select value, update your component like this:\n\n\n\texport interface ListItem {\n\t\tvalue: string;\n\t\ttext: string;\n\t}\n\tinterface ComponentProps {\n\t\tdomName: string;\n\t\tlistItems: ListItem[];\n\t\tselectedValue?: string | undefined | null;\n\t\tshowInvalidValueError?: boolean;\n\t\tform?: string;\n\t}\n\n\texport function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n\t\treturn listItems.some((element) => element.value === selectedValue);\n\t}\n\n\tconst { domName, listItems, selectedValue, showInvalidValueError, form }: ComponentProps = $props();\n\n\tconst selectValueInList = isSelectValueInList(selectedValue);\n\n\texport let avail = selectedValue && selectValueInList\n\t\t? selectedValue\n\t\t: listItems[0].value;\n\n\n\n\t\n\t\t{#each listItems as listItem}\n\t\t\t{listItem.text}\n\t\t{/each}\n\t\n\t{#if !selectValueInList && showInvalidValueError}\n\t\t\n\t\t\t{Current value of '${selectedValue}' is not in the list}\n\t\t\n\t{/if}\n\n\nUsage in parent:\n\n\nSummary:  \nUse export let avail in the child.\nUse bind:avail={...} in the parent.\n\nThank you for catching that!\n\nrogerpence: how can I export the 'avail' value to the parent component with svelte 5\n\nGitHub Copilot: In Svelte 5 with runes, there are two ways to expose the avail value to the parent component:\n\nOption 1: Emit an event when avail changes\n\n\n    // ...existing code...\n    \n    // Create a dispatcher to emit events\n    import { createEventDispatcher } from 'svelte';\n    const dispatch = createEventDispatcher();\n    \n    // Watch for changes to avail and emit an event\n    $effect(() => {\n        dispatch('change', avail);\n    });\n\n\nThen in the parent:\n selectedCategory = e.detail} />\n\nOption 2: Create a reactive prop with a default value\n\n\n    // ...existing code...\n    \n    // Remove the $state declaration\n    // let avail = $state(selectedValue && selectValueInList ? selectedValue : listItems[0].value);\n    \n    // Instead, export it as a prop with a default value\n    export let avail = selectedValue && selectValueInList ? selectedValue : listItems[0].value;\n\n\nThen in the parent:\n\n\nOption 2 is generally simpler and follows Svelte conventions. Just use export let avail in the child component to enable two-way binding with the parent.\n\nrogerpence: export let is deprecated in svelte 5\n\nGitHub Copilot: You're right - in Svelte 5 with runes, export let has been replaced with $props(). To make a value available to the parent component, you'll need to use the createEventDispatcher approach since two-way binding works differently with runes.\n\nHere's how to expose the avail value to the parent in Svelte 5 with runes:\n\n\n\timport { createEventDispatcher } from 'svelte';\n\tconst dispatch = createEventDispatcher();\n\n\texport interface ListItem {\n\t\tvalue: string;\n\t\ttext: string;\n\t}\n\tinterface ComponentProps {\n\t\tdomName: string;\n\t\tlistItems: ListItem[];\n\t\tselectedValue?: string | undefined | null;\n\t\tshowInvalidValueError?: boolean;\n\t\tform?: string;\n\t}\n\n\texport function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n\t\treturn listItems.some((element) => element.value === selectedValue);\n\t}\n\n\tconst { domName, listItems, selectedValue, showInvalidValueError, form }: ComponentProps = $props();\n\n\tconst selectValueInList = isSelectValueInList(selectedValue);\n\n\tlet avail = $state(selectedValue && selectValueInList ? selectedValue : listItems[0].value);\n\t\n\t// Send the value to the parent whenever it changes\n\t$effect(() => {\n\t\tdispatch('valueChange', avail);\n\t});\n\n\n\n\t\n\t\t{#each listItems as listItem}\n\t\t\t{listItem.text}\n\t\t{/each}\n\t\n\t{#if !selectValueInList && showInvalidValueError}\n\t\t\n\t\t\t{Current value of '${selectedValue}' is not in the list}\n\t\t\n\t{/if}\n\n\nThen in your parent component:\n\n\n  let selectedValue = $state('initial value');\n  \n  function handleValueChange(event) {\n    selectedValue = event.detail;\n  }\n\n\n\n\nThis is the recommended approach for component communication in Svelte 5 with runes.\n\nrogerpence: createEventDispatcher is also depreated in svelte 5\n\nGitHub Copilot: You're right - createEventDispatcher is also deprecated in Svelte 5 with runes. Instead, you should use the new createEventDispatchers() function and $event() rune for events.\n\nHere's how to update your Select.svelte component:\n\n\n\texport interface ListItem {\n\t\tvalue: string;\n\t\ttext: string;\n\t}\n\tinterface ComponentProps {\n\t\tdomName: string;\n\t\tlistItems: ListItem[];\n\t\tselectedValue?: string | undefined | null;\n\t\tshowInvalidValueError?: boolean;\n\t\tform?: string;\n\t}\n\n\texport function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n\t\treturn listItems.some((element) => element.value === selectedValue);\n\t}\n\n\tconst { domName, listItems, selectedValue, showInvalidValueError, form }: ComponentProps = $props();\n\n\t// The 'avail' variable is necessary because bound values can't const.\n\t// Further, when you bind a value that can be updated (like with bind:value),\n\t// the variable needs to be declared with $state() to make it reactive.\n\tconst selectValueInList = isSelectValueInList(selectedValue);\n\n\tlet avail = $state(selectedValue && selectValueInList ? selectedValue : listItems[0].value);\n\n\t// Create a valueChange event\n\tconst valueChange = $event();\n\t\n\t// Trigger the event when avail changes\n\t$effect(() => {\n\t\tvalueChange(avail);\n\t});\n\n\n\n\t\n\t\t{#each listItems as listItem}\n\t\t\t{listItem.text}\n\t\t{/each}\n\t\n\t{#if !selectValueInList && showInvalidValueError}\n\t\t\n\t\t\t{Current value of '${selectedValue}' is not in the list}\n\t\t\n\t{/if}\n\n\nThen in your parent component:\n\n\n  let selectedValue = $state('initial value');\n\n\n selectedValue = event}\n/>\n\nNote: With the new event system in Svelte 5, the event handler receives the value directly, not wrapped in a detail property.\n\nrogerpence: what type is ms in the handle function\n\nGitHub Copilot: In your [hooks.server.ts]hooks.server.ts ) file, ms is of type MSSqlConnect, which is the class you're importing from '$lib/app/MSSqlConnect'.\n\nconst ms = new MSSqlConnect(sqlConfig);\n\nThe MSSqlConnect class (defined in [src/lib/app/MSSqlConnect.ts) encapsulates:\nA SQL Server connection pool\nMethods to connect to and close the pool\nAccess to the mssql library\n\nIf you wanted to add a type annotation for better clarity, you could write:\n\nconst ms: MSSqlConnect = new MSSqlConnect(sqlConfig);\n\nLater, you make this instance available to all endpoints by adding it to event.locals:\n\nevent.locals.ms = ms;\n\nThis allows all your server-side endpoints to access the SQL connection pool through locals.ms.\n\nrogerpence: I'm assigning the MSSqlConnect type to 'ms' at line 27, but also creating a new instance of MSqlConnect in that same line. Is it OK to use MSSqlConnect as a type and a concrete object in this case\n\nGitHub Copilot: Yes, it's perfectly fine and actually good practice to use MSSqlConnect both as a type annotation and as a concrete class in the same line:\n\nconst ms: MSSqlConnect = new MSSqlConnect(sqlConfig);\n\nThis pattern is common in TypeScript and provides several benefits:\n\nType Safety: The type annotation ensures that ms will always be of type MSSqlConnect, preventing accidental reassignment to a different type.\n\nCode Clarity: It makes it explicit what type of object ms is supposed to be, improving code readability.\n\nEditor Support: IDEs and code editors can provide better autocompletion and type checking based on the explicit type.\n\nThe type annotation and the class instantiation serve different purposes:\nThe type annotation (ms: MSSqlConnect) is for the TypeScript compiler and is removed during compilation.\nThe instantiation (new MSSqlConnect(sqlConfig)) creates the actual runtime object.\n\nThis is standard TypeScript practice and doesn't cause any performance issues or conflicts.\n\nrogerpence: create the CSS (using nesting) for the table.compact-table that provides a compact row height\n\nGitHub Copilot: Here's the CSS for a compact table with reduced row height:\n\n.compact-table {\n  border-collapse: collapse;\n  width: 100%;\n  font-size: 0.875rem; /* 14px */\n  \n  & thead {\n    background-color: #f8f9fa;\n    \n    & th {\n      padding: 0.5rem 0.75rem;\n      text-align: left;\n      font-weight: 600;\n      border-bottom: 2px solid #dee2e6;\n      font-size: 0.8125rem; /* 13px */\n    }\n  }\n  \n  & tbody {\n    & tr {\n      border-bottom: 1px solid #e9ecef;\n      \n      &:hover {\n        background-color: #f8f9fa;\n      }\n      \n      & td {\n        padding: 0.375rem 0.75rem; /* Compact padding */\n        vertical-align: middle;\n        line-height: 1.2;\n        \n        /* Style form elements within cells */\n        & select,\n        & inputtype=\"text\"] {\n          padding: 0.25rem 0.5rem;\n          font-size: 0.875rem;\n          border: 1px solid #ced4da;\n          border-radius: 0.25rem;\n          width: 100%;\n          max-width: 200px;\n        }\n        \n        & button {\n          padding: 0.25rem 0.5rem;\n          font-size: 0.8125rem;\n          background-color: #007bff;\n          color: white;\n          border: none;\n          border-radius: 0.25rem;\n          cursor: pointer;\n          \n          &:hover {\n            background-color: #0056b3;\n          }\n        }\n      }\n    }\n  }\n}\n\nThis CSS provides:\nCompact row height with reduced padding\nSmaller font sizes for a more condensed look\nProper styling for form elements within table cells\nHover effects for better user experience\nResponsive form controls with appropriate sizing\n\nrogerpence: how to set select tag backgroung\n\nGitHub Copilot: To set the background color for your select elements, you need to modify the .select class in your CSS. Looking at your current styles, you have:\n\n.select {\n\t/* ...existing styles... */\n\tbackground-color: #fff;\n\tbackground-image: linear-gradient(to top, #f9f9f9, #fff 33%);\n}\n\nTo change the select background, update these properties:\n\n.select {\n\tdisplay: grid;\n\tgrid-template-areas: \"select\";\n\talign-items: center;\n\tposition: relative;\n\tmin-width: 15ch;\n\tmax-width: 30ch;\n\tborder: 1px solid var(--select-border);\n\tborder-radius: 0.25em;\n\tpadding: 0.25em 0.5em;\n\tfont-size: 1rem;\n\tcursor: pointer;\n\tline-height: 1.1;\n\tbackground-color: #2c3e50; /* Dark background to match your table */\n\tbackground-image: none; /* Remove the gradient */\n\tcolor: #d5dbdb; /* Light text color */\n}\n\n/* Also update the select element itself */\nselect {\n\tappearance: none;\n\tbackground-color: transparent;\n\tborder: none;\n\tpadding: 0 1em 0 0;\n\tmargin: 0;\n\twidth: 100%;\n\tfont-family: inherit;\n\tfont-size: inherit;\n\tcursor: inherit;\n\tline-height: inherit;\n\tz-index: 1;\n\toutline: none;\n\tcolor: inherit; /* Inherit text color from parent */\n}\n\n/* Style the arrow to match the dark theme */\n:root {\n\t--select-border: #566573;\n\t--select-focus: #1f618d;\n\t--select-arrow: #d5dbdb;\n}\n\nThis will give your select elements a dark background that matches your table theme with light text for better contrast.\n\nSimilar code found with 2 license types\n\nrogerpence: how to set the color of the text in a select tag\n\nGitHub Copilot: To set the color of the text in a select tag, you need to add the color property to your .select class. Looking at your CSS, you can update it like this:\n\n.select {\n\tdisplay: grid;\n\tgrid-template-areas: \"select\";\n\talign-items: center;\n\tposition: relative;\n\tmin-width: 15ch;\n\tmax-width: 30ch;\n\tborder: 1px solid var(--select-border);\n\tborder-radius: 0.25em;\n\tpadding: 0.25em 0.5em;\n\tfont-size: 1rem;\n\tcursor: pointer;\n\tline-height: 1.1;\n\tbackground-color: #566573; \n\tcolor: var(--text-color); /* Add this line to set text color */\n\t/* background-image: linear-gradient(to top, #f9f9f9, #fff 33%); */\n}\n\nSince you already have --text-color: red; defined in your :root, this will make the select text red. If you want a different color for the select text specifically, you can either:\n\nChange the CSS custom property value:\n:root {\n\t--select-border: #777;\n\t--select-focus: blue;\n\t--select-arrow: var(--select-border);\n\t--text-color: #d5dbdb; /* Light gray for better contrast */\n}\n\nOr use a specific color directly:\n.select {\n\t/* ...existing styles... */\n\tcolor: #d5dbdb; /* Light gray text */\n}\n\nThe text color will be inherited by the select element inside the .select container since you have color: inherit; in your select styles.\n\nSimilar code found with 2 license types\n\nrogerpence: that doesn't work\n\nGitHub Copilot: You're right, that approach doesn't work reliably. The issue is that select elements and their option elements have their own styling behavior that can override parent styles.\n\nTry this more direct approach in your CSS:\n\nselect {\n\tappearance: none;\n\tbackground-color: transparent;\n\tborder: none;\n\tpadding: 0 1em 0 0;\n\tmargin: 0;\n\twidth: 100%;\n\tfont-family: inherit;\n\tfont-size: inherit;\n\tcursor: inherit;\n\tline-height: inherit;\n\tz-index: 1;\n\toutline: none;\n\tcolor: #d5dbdb; /* Add this line directly to select */\n}\n\n/* Also style the option elements */\nselect option {\n\tcolor: #2c3e50; /* Dark text for dropdown options */\n\tbackground-color: #fff; /* White background for options */\n}\n\n/* For the select container */\n.select {\n\tdisplay: grid;\n\tgrid-template-areas: \"select\";\n\talign-items: center;\n\tposition: relative;\n\tmin-width: 15ch;\n\tmax-width: 30ch;\n\tborder: 1px solid var(--select-border);\n\tborder-radius: 0.25em;\n\tpadding: 0.25em 0.5em;\n\tfont-size: 1rem;\n\tcursor: pointer;\n\tline-height: 1.1;\n\tbackground-color: #566573;\n\tcolor: #d5dbdb; /* This sets the text color for the container */\n}\n\nThis directly targets the select element and its option elements to ensure the text color is applied correctly.\n\nSimilar code found with 2 license types\n\nrogerpence: that also doesn't work\n\nGitHub Copilot: You're right, styling select elements can be tricky because browsers have their own default styling. Try adding the color property directly to the select element and use !important to override browser defaults:\n\nselect {\n\tappearance: none;\n\tbackground-color: transparent;\n\tborder: none;\n\tpadding: 0 1em 0 0;\n\tmargin: 0;\n\twidth: 100%;\n\tfont-family: inherit;\n\tfont-size: inherit;\n\tcursor: inherit;\n\tline-height: inherit;\n\tz-index: 1;\n\toutline: none;\n\tcolor: red !important; /* Add this line with !important */\n}\n\nselect option {\n\tcolor: red !important; /* Also add !important here */\n\tbackground-color: white !important;\n}\n\n.select {\n\tdisplay: grid;\n\tgrid-template-areas: \"select\";\n\talign-items: center;\n\tposition: relative;\n\tmin-width: 15ch;\n\tmax-width: 30ch;\n\tborder: 1px solid var(--select-border);\n\tborder-radius: 0.25em;\n\tpadding: 0.25em 0.5em;\n\tfont-size: 1rem;\n\tcursor: pointer;\n\tline-height: 1.1;\n\tbackground-color: #566573; \n\tcolor: red !important; /* Add !important here too */\n}\n\nThe !important declaration should override the browser's default styling for select elements. This is one of the cases where !important is actually justified because browser defaults for form elements are notoriously difficult to override.\n\nSimilar code found with 2 license types\n\nrogerpence: I'm tyring to do a full outer join with a where clause. This isn't working:\nSELECT distinct [rpb].[description], [category].[rule], [category].[value], [category].[category]\nFROM rpbudget as rpb\nwhere [rpb].[description] NOT like 'ACH%' AND \n      [rpb].[description] NOT like 'ACCTVERIFY%' AND\n      [rpb].[description] NOT like 'ATM %' \nfull outer join categoryrules as category \n   ON rpb.description = category.description\n\nGitHub Copilot: The issue is with the placement of your WHERE clause. In SQL, the WHERE clause must come after the FROM clause and any JOIN clauses. Here's the corrected syntax:\n\nSELECT DISTINCT [rpb].[description], [category].[rule], [category].[value], [category].[category]\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE [rpb].[description] NOT LIKE 'ACH%' AND \n      [rpb].[description] NOT LIKE 'ACCTVERIFY%' AND\n      [rpb].[description] NOT LIKE 'ATM %'\n\nSQL clause order:\nSELECT\nFROM\nJOIN (any type)\nWHERE\nGROUP BY\nHAVING\nORDER BY\n\nNote: Be careful with FULL OUTER JOIN and WHERE clauses. If you filter on columns from the left table (rpb), you might eliminate rows that only exist in the right table (category). If you want to preserve all rows from the category table, consider using conditions in the ON clause instead:\n\nSELECT DISTINCT [rpb].[description], [category].[rule], [category].[value], [category].[category]\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\n   AND [rpb].[description] NOT LIKE 'ACH%' \n   AND [rpb].[description] NOT LIKE 'ACCTVERIFY%' \n   AND [rpb].[description] NOT LIKE 'ATM %'\n\n\nrogerpence: is there a way to do that query and provide default values for null columns not found in the join\n\nGitHub Copilot: Yes! You can use COALESCE or ISNULL to provide default values for NULL columns in your join. Here are a few approaches:\n\nOption 1: Using COALESCE (works in most SQL databases)sql\nSELECT DISTINCT \n    COALESCE([rpb].[description], [category].[description]) AS description,\n    COALESCE([category].[rule], 'includes') AS rule,\n    COALESCE([category].[value], [rpb].[description]) AS value,\n    COALESCE([category].[category], 'uncategorized') AS category\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE (\n    [rpb].[description] NOT LIKE 'ACH%' AND \n    [rpb].[description] NOT LIKE 'ACCTVERIFY%' AND\n    [rpb].[description] NOT LIKE 'ATM %'\n) OR [rpb].[description] IS NULL\nOption 2: Using ISNULL (SQL Server specific)sql\nSELECT DISTINCT \n    ISNULL([rpb].[description], [category].[description]) AS description,\n    ISNULL([category].[rule], 'includes') AS rule,\n    ISNULL([category].[value], [rpb].[description]) AS value,\n    ISNULL([category].[category], 'uncategorized') AS category\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE (\n    [rpb].[description] NOT LIKE 'ACH%' AND \n    [rpb].[description] NOT LIKE 'ACCTVERIFY%' AND\n    [rpb].[description] NOT LIKE 'ATM %'\n) OR [rpb].[description] IS NULL\nOption 3: Using CASE statements for more complex logicsql\nSELECT DISTINCT \n    COALESCE([rpb].[description], [category].[description]) AS description,\n    CASE \n        WHEN [category].[rule] IS NULL THEN 'includes'\n        ELSE [category].[rule]\n    END AS rule,\n    CASE \n        WHEN [category].[value] IS NULL THEN [rpb].[description]\n        ELSE [category].[value]\n    END AS value,\n    CASE \n        WHEN [category].[category] IS NULL THEN 'uncategorized'\n        ELSE [category].[category]\n    END AS category\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE (\n    [rpb].[description] NOT LIKE 'ACH%' AND \n    [rpb].[description] NOT LIKE 'ACCTVERIFY%' AND\n    [rpb].[description] NOT LIKE 'ATM %'\n) OR [rpb].[description] IS NULL\nKey points:\nCOALESCE returns the first non-NULL value from the list\nISNULL (SQL Server) takes exactly two parameters and returns the second if the first is NULL\nThe WHERE clause includes OR [rpb].[description] IS NULL to preserve rows that only exist in the category table\nYou can set different default values for each column based on your business logic\n\nChoose the approach that best fits your needs and database system!\n\nrogerpence: is there a way to do a LIKE query like this with many conditions so that the conditions can be supplied as a list\n\nGitHub Copilot: \n\nrogerpence: is there a way to do a LIKE query like this with many conditions so that the conditions can be supplied as a list:\n\nSELECT DISTINCT [rpb].[description]\nFROM rpbudget AS rpb\nWHERE [rpb].[description] NOT LIKE 'ACH%' AND \n      [rpb].[description] NOT LIKE 'ACCTVERIFY%' AND\n      [rpb].[description] NOT LIKE 'ATM %' AND\n      [rpb].[description] NOT like 'AUTOPAY%' AND\n      [rpb].[description] NOT like 'Bill Paid%'\n\nGitHub Copilot: Yes! You can use SQL's NOT EXISTS with a subquery or create a more dynamic approach. Here are several options:\n\nOption 1: Using NOT EXISTS with VALUES (SQL Server)typescript\nconst excludePatterns = [\n    'ACH%',\n    'ACCTVERIFY%', \n    'ATM %',\n    'AUTOPAY%',\n    'Bill Paid%',\n    'CHECK #%',\n    'DBT CRD%',\n    'FROM CHECKING%',\n    'DDA REGULAR%',\n    'MONEYLINK %',\n    'FROM SAVINGS%',\n    'ONLINE PMT%',\n    'INTERNET BILL PAYMENT%',\n    'INTERNET CHECK%',\n    'INST XFER PAYPAL%',\n    'PAYMENT AMALGAMATED%',\n    'PAYPAL TRANSFER',\n    '%ASNA, INC. PPD%',\n    'PAYROLL ASNA%',\n    'POS CRE%',\n    'POS DEB%',\n    'PRE AUTH%',\n    'TRM#%',\n    'TRSF FROM%',\n    'USATAX%',\n    'WITHDRAWAL%',\n    'ZELLE TRANSFER%',\n    'ROGER MEDI%',\n    'REGULAR%'\n];\n\nexport const sql = `\nSELECT DISTINCT \n    COALESCE([rpb].[description], [category].[description]) AS description,\n    COALESCE([category].[rule], 'equals') AS [rule],\n    COALESCE([category].[value], [rpb].[description]) AS value,\n    COALESCE([category].[category], 'uncategorized') AS category\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE (\n    NOT EXISTS (\n        SELECT 1 FROM (VALUES \n            ${excludePatterns.map(pattern => ('${pattern}')).join(',\\n            ')}\n        ) AS exclude_patterns(pattern)\n        WHERE [rpb].[description] LIKE exclude_patterns.pattern\n    )\n) OR [rpb].[description] IS NULL`;\nOption 2: Using parameterized queries (Recommended)typescript\nexport const excludePatterns = [\n    'ACH%',\n    'ACCTVERIFY%', \n    'ATM %',\n    'AUTOPAY%',\n    'Bill Paid%',\n    'CHECK #%',\n    'DBT CRD%',\n    'FROM CHECKING%',\n    'DDA REGULAR%',\n    'MONEYLINK %',\n    'FROM SAVINGS%',\n    'ONLINE PMT%',\n    'INTERNET BILL PAYMENT%',\n    'INTERNET CHECK%',\n    'INST XFER PAYPAL%',\n    'PAYMENT AMALGAMATED%',\n    'PAYPAL TRANSFER',\n    '%ASNA, INC. PPD%',\n    'PAYROLL ASNA%',\n    'POS CRE%',\n    'POS DEB%',\n    'PRE AUTH%',\n    'TRM#%',\n    'TRSF FROM%',\n    'USATAX%',\n    'WITHDRAWAL%',\n    'ZELLE TRANSFER%',\n    'ROGER MEDI%',\n    'REGULAR%'\n];\n\nexport function buildExcludeQuery(patterns: string[]): string {\n    const conditions = patterns.map((_, index) => [rpb].[description] NOT LIKE @pattern${index}).join(' AND ');\n    \n    return `\n    SELECT DISTINCT \n        COALESCE([rpb].[description], [category].[description]) AS description,\n        COALESCE([category].[rule], 'equals') AS [rule],\n        COALESCE([category].[value], [rpb].[description]) AS value,\n        COALESCE([category].[category], 'uncategorized') AS category\n    FROM rpbudget AS rpb\n    FULL OUTER JOIN categoryrules AS category \n       ON rpb.description = category.description\n    WHERE (${conditions}) OR [rpb].[description] IS NULL`;\n}\nOption 3: Using a temporary table approachtypescript\nexport const sql = `\nWITH exclude_patterns AS (\n    SELECT 'ACH%' AS pattern\n    UNION ALL SELECT 'ACCTVERIFY%'\n    UNION ALL SELECT 'ATM %'\n    UNION ALL SELECT 'AUTOPAY%'\n    UNION ALL SELECT 'Bill Paid%'\n    UNION ALL SELECT 'CHECK #%'\n    UNION ALL SELECT 'DBT CRD%'\n    UNION ALL SELECT 'FROM CHECKING%'\n    UNION ALL SELECT 'DDA REGULAR%'\n    UNION ALL SELECT 'MONEYLINK %'\n    UNION ALL SELECT 'FROM SAVINGS%'\n    UNION ALL SELECT 'ONLINE PMT%'\n    UNION ALL SELECT 'INTERNET BILL PAYMENT%'\n    UNION ALL SELECT 'INTERNET CHECK%'\n    UNION ALL SELECT 'INST XFER PAYPAL%'\n    UNION ALL SELECT 'PAYMENT AMALGAMATED%'\n    UNION ALL SELECT 'PAYPAL TRANSFER'\n    UNION ALL SELECT '%ASNA, INC. PPD%'\n    UNION ALL SELECT 'PAYROLL ASNA%'\n    UNION ALL SELECT 'POS CRE%'\n    UNION ALL SELECT 'POS DEB%'\n    UNION ALL SELECT 'PRE AUTH%'\n    UNION ALL SELECT 'TRM#%'\n    UNION ALL SELECT 'TRSF FROM%'\n    UNION ALL SELECT 'USATAX%'\n    UNION ALL SELECT 'WITHDRAWAL%'\n    UNION ALL SELECT 'ZELLE TRANSFER%'\n    UNION ALL SELECT 'ROGER MEDI%'\n    UNION ALL SELECT 'REGULAR%'\n)\nSELECT DISTINCT \n    COALESCE([rpb].[description], [category].[description]) AS description,\n    COALESCE([category].[rule], 'equals') AS [rule],\n    COALESCE([category].[value], [rpb].[description]) AS value,\n    COALESCE([category].[category], 'uncategorized') AS category\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE (\n    NOT EXISTS (\n        SELECT 1 FROM exclude_patterns \n        WHERE [rpb].[description] LIKE exclude_patterns.pattern\n    )\n) OR [rpb].[description] IS NULL`;\nOption 2 is recommended as it's the most maintainable and allows you to easily modify the exclude patterns without changing the SQL string.\n\nrogerpence: I want to put the code either 100% in sql or a stored procedure\n\nGitHub Copilot: Here are two approaches to move the logic entirely into SQL:\n\nOption 1: Pure SQL with CTE (Common Table Expression)sql\nexport const sql = `\nWITH exclude_patterns AS (\n    SELECT 'ACH%' AS pattern\n    UNION ALL SELECT 'ACCTVERIFY%'\n    UNION ALL SELECT 'ATM %'\n    UNION ALL SELECT 'AUTOPAY%'\n    UNION ALL SELECT 'Bill Paid%'\n    UNION ALL SELECT 'CHECK #%'\n    UNION ALL SELECT 'DBT CRD%'\n    UNION ALL SELECT 'FROM CHECKING%'\n    UNION ALL SELECT 'DDA REGULAR%'\n    UNION ALL SELECT 'MONEYLINK %'\n    UNION ALL SELECT 'FROM SAVINGS%'\n    UNION ALL SELECT 'ONLINE PMT%'\n    UNION ALL SELECT 'INTERNET BILL PAYMENT%'\n    UNION ALL SELECT 'INTERNET CHECK%'\n    UNION ALL SELECT 'INST XFER PAYPAL%'\n    UNION ALL SELECT 'PAYMENT AMALGAMATED%'\n    UNION ALL SELECT 'PAYPAL TRANSFER'\n    UNION ALL SELECT '%ASNA, INC. PPD%'\n    UNION ALL SELECT 'PAYROLL ASNA%'\n    UNION ALL SELECT 'POS CRE%'\n    UNION ALL SELECT 'POS DEB%'\n    UNION ALL SELECT 'PRE AUTH%'\n    UNION ALL SELECT 'TRM#%'\n    UNION ALL SELECT 'TRSF FROM%'\n    UNION ALL SELECT 'USATAX%'\n    UNION ALL SELECT 'WITHDRAWAL%'\n    UNION ALL SELECT 'ZELLE TRANSFER%'\n    UNION ALL SELECT 'ROGER MEDI%'\n    UNION ALL SELECT 'REGULAR%'\n)\nSELECT DISTINCT \n    COALESCE([rpb].[description], [category].[description]) AS description,\n    COALESCE([category].[rule], 'equals') AS [rule],\n    COALESCE([category].[value], [rpb].[description]) AS value,\n    COALESCE([category].[category], 'uncategorized') AS category\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE (\n    NOT EXISTS (\n        SELECT 1 FROM exclude_patterns \n        WHERE [rpb].[description] LIKE exclude_patterns.pattern\n    )\n    AND YEAR([rpb].[postdate]) = 2025\n) OR [rpb].[description] IS NULL\nORDER BY description`;\nOption 2: Stored Procedure (Recommended)\n\nFirst, create the stored procedure in your SQL Server database:sql\nCREATE PROCEDURE [dbo].[GetBudgetCategoryData]\n    @Year INT = 2025\nAS\nBEGIN\n    SET NOCOUNT ON;\n    \n    WITH exclude_patterns AS (\n        SELECT 'ACH%' AS pattern\n        UNION ALL SELECT 'ACCTVERIFY%'\n        UNION ALL SELECT 'ATM %'\n        UNION ALL SELECT 'AUTOPAY%'\n        UNION ALL SELECT 'Bill Paid%'\n        UNION ALL SELECT 'CHECK #%'\n        UNION ALL SELECT 'DBT CRD%'\n        UNION ALL SELECT 'FROM CHECKING%'\n        UNION ALL SELECT 'DDA REGULAR%'\n        UNION ALL SELECT 'MONEYLINK %'\n        UNION ALL SELECT 'FROM SAVINGS%'\n        UNION ALL SELECT 'ONLINE PMT%'\n        UNION ALL SELECT 'INTERNET BILL PAYMENT%'\n        UNION ALL SELECT 'INTERNET CHECK%'\n        UNION ALL SELECT 'INST XFER PAYPAL%'\n        UNION ALL SELECT 'PAYMENT AMALGAMATED%'\n        UNION ALL SELECT 'PAYPAL TRANSFER'\n        UNION ALL SELECT '%ASNA, INC. PPD%'\n        UNION ALL SELECT 'PAYROLL ASNA%'\n        UNION ALL SELECT 'POS CRE%'\n        UNION ALL SELECT 'POS DEB%'\n        UNION ALL SELECT 'PRE AUTH%'\n        UNION ALL SELECT 'TRM#%'\n        UNION ALL SELECT 'TRSF FROM%'\n        UNION ALL SELECT 'USATAX%'\n        UNION ALL SELECT 'WITHDRAWAL%'\n        UNION ALL SELECT 'ZELLE TRANSFER%'\n        UNION ALL SELECT 'ROGER MEDI%'\n        UNION ALL SELECT 'REGULAR%'\n    )\n    SELECT DISTINCT \n        COALESCE([rpb].[description], [category].[description]) AS description,\n        COALESCE([category].[rule], 'equals') AS [rule],\n        COALESCE([category].[value], [rpb].[description]) AS value,\n        COALESCE([category].[category], 'uncategorized') AS category\n    FROM rpbudget AS rpb\n    FULL OUTER JOIN categoryrules AS category \n       ON rpb.description = category.description\n    WHERE (\n        NOT EXISTS (\n            SELECT 1 FROM exclude_patterns \n            WHERE [rpb].[description] LIKE exclude_patterns.pattern\n        )\n        AND YEAR([rpb].[postdate]) = @Year\n    ) OR [rpb].[description] IS NULL\n    ORDER BY description;\nEND\nThen update your TypeScript to call the stored procedure:typescript\nexport const sql = EXEC [dbo].[GetBudgetCategoryData] @Year = 2025;\nBenefits of the stored procedure approach:\nCentralized business logic in the database\nBetter performance (compiled once, executed many times)\nEasier to maintain exclude patterns\nCan accept parameters (like year)\nBetter security (prevents SQL injection)\n\nOption 2 (stored procedure) is recommended for production use as it's more maintainable and performant.\n\nrogerpence: how can put those like conditions in a speical-case table. \n\nGitHub Copilot: You can create a special table to store the exclude patterns and use it in your query. Here's how:\n\nStep 1: Create the exclude patterns tablesql\n-- Create the table\nCREATE TABLE [dbo].[ExcludePatterns] (\n    [Id] INT IDENTITY(1,1) PRIMARY KEY,\n    [Pattern] NVARCHAR(255) NOT NULL,\n    [Description] NVARCHAR(500) NULL,\n    [IsActive] BIT DEFAULT 1,\n    [CreatedDate] DATETIME2 DEFAULT GETDATE()\n);\n\n-- Insert your patterns\nINSERT INTO [dbo].[ExcludePatterns, Description]) VALUES\n('ACH%', 'ACH transactions'),\n('ACCTVERIFY%', 'Account verification'),\n('ATM %', 'ATM transactions'),\n('AUTOPAY%', 'Auto pay transactions'),\n('Bill Paid%', 'Bill payment transactions'),\n('CHECK #%', 'Check transactions'),\n('DBT CRD%', 'Debit card transactions'),\n('FROM CHECKING%', 'Checking account transfers'),\n('DDA REGULAR%', 'DDA regular transactions'),\n('MONEYLINK %', 'MoneyLink transactions'),\n('FROM SAVINGS%', 'Savings account transfers'),\n('ONLINE PMT%', 'Online payments'),\n('INTERNET BILL PAYMENT%', 'Internet bill payments'),\n('INTERNET CHECK%', 'Internet check payments'),\n('INST XFER PAYPAL%', 'PayPal instant transfers'),\n('PAYMENT AMALGAMATED%', 'Amalgamated payments'),\n('PAYPAL TRANSFER', 'PayPal transfers'),\n('%ASNA, INC. PPD%', 'ASNA payroll deposits'),\n('PAYROLL ASNA%', 'ASNA payroll'),\n('POS CRE%', 'POS credit transactions'),\n('POS DEB%', 'POS debit transactions'),\n('PRE AUTH%', 'Pre-authorized transactions'),\n('TRM#%', 'Terminal transactions'),\n('TRSF FROM%', 'Transfer from transactions'),\n('USATAX%', 'Tax transactions'),\n('WITHDRAWAL%', 'Withdrawal transactions'),\n('ZELLE TRANSFER%', 'Zelle transfers'),\n('ROGER MEDI%', 'Roger medical payments'),\n('REGULAR%', 'Regular transactions');\nStep 2: Update your SQL query to use the tabletypescript\nexport const sql = `\nSELECT DISTINCT \n    COALESCE([rpb].[description], [category].[description]) AS description,\n    COALESCE([category].[rule], 'equals') AS [rule],\n    COALESCE([category].[value], [rpb].[description]) AS value,\n    COALESCE([category].[category], 'uncategorized') AS category\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE (\n    NOT EXISTS (\n        SELECT 1 FROM [dbo].[ExcludePatterns] ep\n        WHERE ep.[IsActive] = 1 \n        AND [rpb].[description] LIKE ep.[Pattern]\n    )\n    AND YEAR([rpb].[postdate]) = 2025\n) OR [rpb].[description] IS NULL\nORDER BY description`;\nStep 3: (Optional) Create a stored procedure for better performancesql\nCREATE PROCEDURE [dbo].[GetBudgetCategoryDataWithExclusions]\n    @Year INT = 2025\nAS\nBEGIN\n    SET NOCOUNT ON;\n    \n    SELECT DISTINCT \n        COALESCE([rpb].[description], [category].[description]) AS description,\n        COALESCE([category].[rule], 'equals') AS [rule],\n        COALESCE([category].[value], [rpb].[description]) AS value,\n        COALESCE([category].[category], 'uncategorized') AS category\n    FROM rpbudget AS rpb\n    FULL OUTER JOIN categoryrules AS category \n       ON rpb.description = category.description\n    WHERE (\n        NOT EXISTS (\n            SELECT 1 FROM [dbo].[ExcludePatterns] ep\n            WHERE ep.[IsActive] = 1 \n            AND [rpb].[description] LIKE ep.[Pattern]\n        )\n        AND YEAR([rpb].[postdate]) = @Year\n    ) OR [rpb].[description] IS NULL\n    ORDER BY description;\nEND\nBenefits of this approach:\n\nMaintainable: Add/remove patterns without changing code\nFlexible: Can activate/deactivate patterns with the IsActive flag\nDocumented: Each pattern can have a description\nAuditable: Track when patterns were created\nPerformance: Database can optimize the query better\nReusable: Other queries can use the same exclude patterns\n\nManaging the patterns:\n\nYou can now easily manage your exclude patterns:sql\n-- Add a new pattern\nINSERT INTO [dbo].[ExcludePatterns, Description]) \nVALUES ('NEW_PATTERN%', 'Description of new pattern');\n\n-- Disable a pattern temporarily\nUPDATE [dbo].[ExcludePatterns] \nSET [IsActive] = 0 \nWHERE [Pattern] = 'PATTERN_TO_DISABLE%';\n\n-- View all patterns\nSELECT * FROM [dbo].[ExcludePatterns] ORDER BY [Pattern];\nThis approach is much cleaner and more maintainable than hardcoding the patterns in your SQL query.\n\nrogerpence: Tell me more about this WHERE clause... it is working and it is amazing. I also don't understand it!\n\nGitHub Copilot: \n\nrogerpence: Tell me more about this WHERE clause... it is working and it is amazing. I also don't understand it!\n\nWHERE (\n    NOT EXISTS (\n        SELECT 1 FROM [dbo].[ExcludePatterns] ep\n        WHERE ep.[IsActive] = 1 \n        AND [rpb].[description] LIKE ep.[Pattern]\n    )\n\nGitHub Copilot: Great question! The NOT EXISTS clause is indeed amazing and powerful. Let me break it down step by step:\n\nWhat NOT EXISTS does:\n\nThe NOT EXISTS clause returns TRUE when the subquery returns no rows. It's like asking: \"Show me records where this condition does NOT exist.\"\n\nBreaking down your WHERE clause:sql\nWHERE (\n    NOT EXISTS (\n        SELECT 1 FROM [dbo].[ExcludePatterns] ep\n        WHERE ep.[IsActive] = 1 \n        AND [rpb].[description] LIKE ep.[Pattern]\n    )\n    AND YEAR([rpb].[postdate]) = 2025\n) OR [rpb].[description] IS NULL\nStep-by-step explanation:\n\n1. The subquery runs for each row in your main query:sql\nSELECT 1 FROM [dbo].[ExcludePatterns] ep\nWHERE ep.[IsActive] = 1 \nAND [rpb].[description] LIKE ep.[Pattern]\nFor each rpb.description (like \"WALMART PURCHASE\"), this subquery:\nLooks at all active patterns in the ExcludePatterns table\nChecks if \"WALMART PURCHASE\" matches ANY of those patterns\nIf it finds a match, it returns 1 (or any value - the actual value doesn't matter)\nIf no match is found, it returns no rows\n\n2. The NOT EXISTS evaluates the subquery result:\nIf the subquery returns any rows  NOT EXISTS = FALSE  exclude this record\nIf the subquery returns no rows  NOT EXISTS = TRUE  include this record\n\n3. Visual example:\n\nLet's say you have these patterns in your ExcludePatterns table:\nACH%\nATM %\nPAYPAL%\n\nAnd your rpbudget has these descriptions:\n\"WALMART PURCHASE\"  Subquery finds no matching patterns  NOT EXISTS = TRUE  INCLUDE\n\"ACH DEPOSIT\"  Subquery finds ACH% pattern  NOT EXISTS = FALSE  EXCLUDE\n\"ATM WITHDRAWAL\"  Subquery finds ATM % pattern  NOT EXISTS = FALSE  EXCLUDE\n\"GROCERY STORE\"  Subquery finds no matching patterns  NOT EXISTS = TRUE  INCLUDE\n\nWhy this is amazing:\n\nDynamic: Add/remove patterns without changing your query\nEfficient: SQL Server can optimize this very well\nReadable: The logic is clear - \"exclude if it matches any forbidden pattern\"\nMaintainable: All your business rules are in one table\n\nThe OR [rpb].[description] IS NULL part:\n\nThis preserves rows from your FULL OUTER JOIN where there's no matching rpb record (only category rules exist). Without this, you'd lose category-only records.\n\nAlternative way to think about it:\n\nYour old approach was like:sql\nWHERE description NOT LIKE 'ACH%' \n  AND description NOT LIKE 'ATM %' \n  AND description NOT LIKE 'PAYPAL%'\nThe NOT EXISTS approach is like:sql\nWHERE description doesn't match any pattern in my forbidden list\nIt's essentially the same logic, but infinitely more flexible!\n\nrogerpence: how is this, by selecting only 1 row, looking at all active patterns:\n SELECT 1 FROM [dbo].[ExcludePatterns] ep\n        WHERE ep.[IsActive] = 1 \n        AND [rpb].[description] LIKE ep.[Pattern]\n\nGitHub Copilot: Great question! You're thinking about this correctly, but there's a key concept to understand about how NOT EXISTS works with subqueries.\n\nThe subquery doesn't select \"only 1 row\" - it potentially checks ALL patterns!\n\nLet me explain what actually happens:\n\nHow the subquery executes:sql\nSELECT 1 FROM [dbo].[ExcludePatterns] ep\nWHERE ep.[IsActive] = 1 \nAND [rpb].[description] LIKE ep.[Pattern]\nThis subquery:\nScans through ALL active patterns in the ExcludePatterns table\nFor each pattern, it checks if [rpb].[description] LIKE ep.[Pattern]\nAs soon as it finds ONE match, it returns 1 and stops looking\nIf no patterns match, it returns no rows\n\nVisual example:\n\nLet's say your ExcludePatterns table has:\nACH% (Active)\nATM % (Active)\nPAYPAL% (Active)\nOLD_PATTERN% (Inactive)\n\nAnd you're checking [rpb].[description] = \"ATM WITHDRAWAL\":sql\n-- The subquery effectively does this:\n-- Check pattern 1: \"ATM WITHDRAWAL\" LIKE \"ACH%\"  FALSE\n-- Check pattern 2: \"ATM WITHDRAWAL\" LIKE \"ATM %\"  TRUE! \n-- Found a match! Return 1 and stop checking\n-- (Never checks PAYPAL% or OLD_PATTERN%)\nThe SELECT 1 part:\n\nThe 1 is arbitrary - it could be SELECT 'found' or SELECT  or SELECT ep.Pattern. The important thing is that *any result** means \"a match was found.\"sql\n-- These are all equivalent:\nSELECT 1 FROM [dbo].[ExcludePatterns] ep WHERE...\nSELECT 'MATCH' FROM [dbo].[ExcludePatterns] ep WHERE...\nSELECT ep.Pattern FROM [dbo].[ExcludePatterns] ep WHERE...\nWhy NOT EXISTS is efficient:\n\nShort-circuit evaluation: Stops as soon as it finds one match\nNo data transfer: Only cares if rows exist, not what they contain\nIndex friendly: Can use indexes on the Pattern column\n\nYour old approach vs. NOT EXISTS:\n\nOld approach (your commented code):sql\nWHERE [description] NOT LIKE 'ACH%' \n  AND [description] NOT LIKE 'ATM %' \n  AND [description] NOT LIKE 'PAYPAL%'\nAlways checks all patterns even after finding a match\nHardcoded - must change code to add patterns\n\nNOT EXISTS approach:sql\nWHERE NOT EXISTS (\n    SELECT 1 FROM [dbo].[ExcludePatterns] ep\n    WHERE ep.[IsActive] = 1 \n    AND [rpb].[description] LIKE ep.[Pattern]\n)\nStops checking as soon as it finds a match\nDynamic - add patterns to table without changing code\nMore efficient for large pattern lists\n\nSo the subquery does look at all active patterns, but it's smart about it - it stops as soon as it finds a match!\n\nrogerpence: how do I change the query in the try to call the 'GetBudgetCategoryDataWithExclusions' with msql \n\nGitHub Copilot: To call the stored procedure GetBudgetCategoryDataWithExclusions instead of the raw SQL query, you need to use the execute method instead of query. Here's how to update your code:typescript\nexport const load: PageServerLoad = async ({ params, locals, fetch }) => {\n\tconst ms = locals.ms; // Get the pool from locals (setup in hooks.server.ts)\n\n\ttry {\n\t\tconst request = ms.pool.request();\n\t\t\n\t\t// Add input parameter for the year\n\t\trequest.input('Year', ms.sql.Int, 2025);\n\t\t\n\t\t// Execute the stored procedure instead of a raw query\n\t\tconst result = await request.execute('dbo.GetBudgetCategoryDataWithExclusions');\n\n\t\tms.closePool(); // Close the pool after the query\n\n\t\tconsole.log('result', result);\n\n\t\treturn {\n\t\t\tdatarows: result.recordset\n\t\t};\n\t} catch (err: any) {\n\t\tconsole.error('Error loading rpBudget:', err);\n\t\treturn error(500, { message: Could not load rpBudget. ${err.message || ''} });\n\t}\n};\nKey changes:\nUse request.execute() instead of request.query()\nAdd input parameter for the year using request.input('Year', ms.sql.Int, 2025)\nRemove the import of sql from './sql' since you're no longer using it\n\nAlternative approach - if you want to make the year dynamic:typescript\nexport const load: PageServerLoad = async ({ params, locals, fetch, url }) => {\n\tconst ms = locals.ms; // Get the pool from locals (setup in hooks.server.ts)\n\n\ttry {\n\t\tconst request = ms.pool.request();\n\t\t\n\t\t// Get year from URL parameter or default to current year\n\t\tconst year = url.searchParams.get('year') ? parseInt(url.searchParams.get('year')!) : new Date().getFullYear();\n\t\t\n\t\t// Add input parameter for the year\n\t\trequest.input('Year', ms.sql.Int, year);\n\t\t\n\t\t// Execute the stored procedure\n\t\tconst result = await request.execute('dbo.GetBudgetCategoryDataWithExclusions');\n\n\t\tms.closePool(); // Close the pool after the query\n\n\t\tconsole.log('result', result);\n\n\t\treturn {\n\t\t\tdatarows: result.recordset\n\t\t};\n\t} catch (err: any) {\n\t\tconsole.error('Error loading rpBudget:', err);\n\t\treturn error(500, { message: Could not load rpBudget. ${err.message || ''} });\n\t}\n};\nThis way you can call your page with different years like: /?year=2024 or /?year=2025.\n\nrogerpence: how can I type the result to be an array of the Rules type\n\nGitHub Copilot: To type the result as an array of the Rules type, you need to update your load function's return type and cast the result. Here's how:typescript\nimport type { PageServerLoad, Actions } from './$types';\nimport { error, fail, redirect } from '@sveltejs/kit';\nimport { type CategoryRules } from '$lib/types/CategoryRules';\nimport type { MSSqlConnect } from '$lib/app/MSSqlConnect';\nimport { sql } from './sql';\n\ntype Rules = {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n};\n\nexport const load: PageServerLoad = async ({ params, locals, fetch }): Promise => {\n\tconst ms = locals.ms; // Get the pool from locals (setup in hooks.server.ts)\n\n\ttry {\n\t\tconst request = ms.pool.request();\n\t\t\n\t\t// Add input parameter for the year\n\t\trequest.input('Year', ms.sql.Int, 2025);\n\n\t\t// Execute the stored procedure instead of a raw query\n\t\tconst result = await request.execute('dbo.GetBudgetCategoryDataWithExclusions');\n\n\t\tconsole.log(result);\n\n\t\tms.closePool(); // Close the pool after the query\n\n\t\tconsole.log('result', result);\n\n\t\t// Cast the result to Rules array\n\t\tconst datarows: Rules[] = result.recordset as Rules[];\n\n\t\treturn {\n\t\t\tdatarows\n\t\t};\n\t} catch (err: any) {\n\t\tconsole.error('Error loading rpBudget:', err);\n\t\treturn error(500, { message: Could not load rpBudget. ${err.message || ''} });\n\t}\n};\nKey changes:\n\nAdded explicit return type: Promise\nTyped the result: const datarows: Rules[] = result.recordset as Rules[];\nUsed the typed variable: return { datarows }\n\nAlternative approach - if you want runtime validation:typescript\nfunction isValidRule(obj: any): obj is Rules {\n\treturn (\n\t\ttypeof obj.description === 'string' &&\n\t\ttypeof obj.rule === 'string' &&\n\t\ttypeof obj.value === 'string' &&\n\t\ttypeof obj.category === 'string'\n\t);\n}\n\nexport const load: PageServerLoad = async ({ params, locals, fetch }): Promise => {\n\tconst ms = locals.ms;\n\n\ttry {\n\t\tconst request = ms.pool.request();\n\t\trequest.input('Year', ms.sql.Int, 2025);\n\t\tconst result = await request.execute('dbo.GetBudgetCategoryDataWithExclusions');\n\n\t\tms.closePool();\n\n\t\t// Validate and type the result\n\t\tconst datarows: Rules[] = result.recordset.filter(isValidRule);\n\n\t\treturn {\n\t\t\tdatarows\n\t\t};\n\t} catch (err: any) {\n\t\tconsole.error('Error loading rpBudget:', err);\n\t\treturn error(500, { message: Could not load rpBudget. ${err.message || ''} });\n\t}\n};\nThis approach provides both TypeScript type safety and runtime validation to ensure your data matches the expected structure.\n\nrogerpence: is it better to use an interface or type in this case\n\nGitHub Copilot: In this case, interface would be better than type for your Rules definition. Here's why:\n\nUse interface for your Rules:typescript\ninterface Rules {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n}\nWhy interface is better here:\n\n1. Object shape definition\nRules represents a simple object structure\ninterface is the idiomatic choice for object shapes in TypeScript\n\n2. Extensibility\nYou can extend interfaces later if needed:typescript\ninterface ExtendedRules extends Rules {\n\tid?: number;\n\tcreatedDate?: Date;\n}\n3. Better error messages\nTypeScript provides clearer error messages with interfaces\nBetter IDE support and autocomplete\n\n4. Consistency\nYou're already using interface for CategoryRules imported from $lib/types/CategoryRules\nConsistency across your codebase is important\n\nWhen to use type vs interface:\n\nUse interface for:\nObject shapes (like your Rules)\nWhen you might need to extend later\nPublic APIs\nReact props, function parameters\n\nUse type for:\nUnion types: type Status = 'loading' | 'success' | 'error'\nComputed types: type Keys = keyof SomeObject\nComplex type manipulations\nFunction signatures: type Handler = (data: string) => void\n\nUpdated recommendation:typescript\nimport type { PageServerLoad, Actions } from './$types';\nimport { error, fail, redirect } from '@sveltejs/kit';\nimport { type CategoryRules } from '$lib/types/CategoryRules';\nimport type { MSSqlConnect } from '$lib/app/MSSqlConnect';\n\ninterface Rules {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n}\n\nexport const load: PageServerLoad = async ({\n\tparams,\n\tlocals,\n\tfetch\n}): Promise => {\n\t// ...existing code...\n};\nThis makes your code more consistent and follows TypeScript best practices.\n\nrogerpence: can I use omit (or is it pick) to create the type I need from CategoryRules\n\nGitHub Copilot: Yes! You can use Omit to create the Rules type from CategoryRules. Since CategoryRules has nullable fields and includes an id, you can create a cleaner type like this:typescript\nimport type { PageServerLoad, Actions } from './$types';\nimport { error, fail, redirect } from '@sveltejs/kit';\nimport { type CategoryRules } from '$lib/types/CategoryRules';\nimport type { MSSqlConnect } from '$lib/app/MSSqlConnect';\n\n// Create Rules type by omitting 'id' and making fields non-nullable\ntype Rules = Omit & {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n};\n\n// Alternative approach - if you want to keep it simpler:\n// type Rules = Required>;\nDifferent approaches:\n\n1. Omit + intersection (recommended):typescript\ntype Rules = Omit & {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n};\n2. Pick + intersection:typescript\ntype Rules = Pick & {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n};\n3. Required + Omit:typescript\ntype Rules = Required>;\nWhich to choose:\n\nOption 1 (Omit + intersection) is most explicit and clear\nOption 3 (Required>) is more concise but assumes you want all fields required\n\nThe first option is recommended because:\nIt's explicit about removing the id field\nIt's clear about making nullable fields non-nullable\nIt's easy to understand and maintain\n\nYour updated code would look like:typescript\nimport type { PageServerLoad, Actions } from './$types';\nimport { error, fail, redirect } from '@sveltejs/kit';\nimport { type CategoryRules } from '$lib/types/CategoryRules';\nimport type { MSSqlConnect } from '$lib/app/MSSqlConnect';\n\ntype Rules = Omit & {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n};\n\nexport const load: PageServerLoad = async ({\n\tparams,\n\tlocals,\n\tfetch\n}): Promise => {\n\t// ...existing code...\n};\nThis way you maintain a single source of truth for your data structure while creating the specific type you need.\n\nrogerpence: Is there a good convention to indicate that a type that defines a physical table as opposed to a runtime type  \n\nGitHub Copilot: Yes! There are several good conventions to distinguish between database/physical table types and runtime application types:\n\nCommon Conventions:\n\n1. Suffix Convention (Most Popular):typescript\nimport { z } from 'zod';\n\nexport const categoryRulesSchema = z.object({\n\tid: z.number().int().nullable(),\n\tdescription: z.string().nullable(),\n\trule: z.string().nullable(),\n\tvalue: z.string().nullable(),\n\tcategory: z.string().nullable()\n});\n\n// Database/Table type\nexport type CategoryRulesEntity = z.infer;\n\n// Runtime/Application types\nexport type CategoryRulesDto = Omit & {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n};\n\nexport type CategoryRulesRequest = Omit;\nexport type CategoryRulesResponse = Required;\n2. Prefix Convention:typescript\n// Database types\nexport type DbCategoryRules = z.infer;\nexport type DbUser = { id: number; email: string | null; /* ... */ };\n\n// Application types\nexport type CategoryRules = Required>;\nexport type User = { id: number; email: string; /* ... */ };\n3. Namespace/Module Convention:typescript\nexport namespace DB {\n\texport type CategoryRules = z.infer;\n}\n\nexport namespace App {\n\texport type CategoryRules = Required>;\n}\n\n// Usage:\n// const dbRecord: DB.CategoryRules = ...;\n// const appData: App.CategoryRules = ...;\n4. Folder Structure Convention:\nsrc/lib/types/\n entities/          # Database table types\n    CategoryRules.ts\n    User.ts\n dtos/             # Data transfer objects\n    CategoryRulesDto.ts\n    UserDto.ts\n api/              # API request/response types\n     CategoryRulesApi.ts\n     UserApi.ts\nRecommended Approach (Suffix Convention):typescript\nimport { z } from 'zod';\n\nexport const categoryRulesSchema = z.object({\n\tid: z.number().int().nullable(),\n\tdescription: z.string().nullable(),\n\trule: z.string().nullable(),\n\tvalue: z.string().nullable(),\n\tcategory: z.string().nullable()\n});\n\n// Database entity (matches physical table)\nexport type CategoryRulesEntity = z.infer;\n\n// Application DTOs (Data Transfer Objects)\nexport type CategoryRulesDto = Required>;\nexport type CategoryRulesCreateDto = Omit;\nexport type CategoryRulesUpdateDto = Partial> & { id: number };\nCommon Suffixes:\nEntity - Database table representation\nDto - Data Transfer Object (API/runtime)\nModel - Domain model\nRequest - API request payload\nResponse - API response payload\nCreate - For creation operations\nUpdate - For update operations\n\nUsage in your code:typescript\nimport type { CategoryRulesEntity, CategoryRulesDto } from '$lib/types/CategoryRules';\n\n// Database query returns entities\nconst dbResult: CategoryRulesEntity[] = result.recordset;\n\n// Transform to DTOs for the frontend\nconst datarows: CategoryRulesDto[] = dbResult.map(entity => ({\n\tdescription: entity.description || '',\n\trule: entity.rule || '',\n\tvalue: entity.value || '',\n\tcategory: entity.category || ''\n}));\nThis convention makes it immediately clear whether you're working with database data (nullable, includes IDs) or application data (processed, non-nullable).\n\nrogerpence: what is required doing here:\nexport type CategoryRulesResponse = Required;\nGitHub Copilot: The Required utility type in TypeScript removes the optional and nullable modifiers from all properties of a type, making them all required and non-nullable.\n\nHowever, in your specific case:typescript\nexport type CategoryRulesResponse = Required;\nThis won't work as expected because your CategoryRulesEntity properties are defined as nullable (e.g., string | null), not optional (e.g., string | undefined).\n\nWhat Required does:\n\nRemoves ? (optional): { name?: string }  { name: string }\nDoes NOT remove | null: { name: string | null }  { name: string | null }\n\nYour current types:typescript\n// Your CategoryRulesEntity looks like this:\ntype CategoryRulesEntity = {\n  id: number | null;\n  description: string | null;\n  rule: string | null;\n  value: string | null;\n  category: string | null;\n}\n\n// Required would still be:\ntype CategoryRulesResponse = {\n  id: number | null;        // Still nullable!\n  description: string | null; // Still nullable!\n  rule: string | null;       // Still nullable!\n  value: string | null;      // Still nullable!\n  category: string | null;   // Still nullable!\n}\nWhat you probably want:typescript\n// To remove nullability, you need a custom utility type:\ntype NonNullable = {\n  [P in keyof T]: NonNullable;\n};\n\nexport type CategoryRulesResponse = NonNullable;\n// Result:\n// {\n//   id: number;\n//   description: string;\n//   rule: string;\n//   value: string;\n//   category: string;\n// }\nBetter approach for your use case:typescript\nimport { z } from 'zod';\n\nexport const categoryRulesSchema = z.object({\n  id: z.number().int().nullable(),\n  description: z.string().nullable(),\n  rule: z.string().nullable(),\n  value: z.string().nullable(),\n  category: z.string().nullable()\n});\n\n// Database entity (matches physical table with nullable fields)\nexport type CategoryRulesEntity = z.infer;\n\n// DTO without id and with required fields\nexport type CategoryRulesDto = {\n  description: string;\n  rule: string;\n  value: string;\n  category: string;\n};\n\n// Response type with all fields required (for API responses)\nexport type CategoryRulesResponse = {\n  id: number;\n  description: string;\n  rule: string;\n  value: string;\n  category: string;\n};\nThis approach is clearer and more explicit about what each type represents.\n\nrogerpence: how does formData have a descriptionrules input from this form\n\nGitHub Copilot: Looking at your code, the formData does NOT have a descriptionrules input from this form. Here's why:\n\nWhat's actually in your form:svelte\n\n    \n    \n    \n    \n    Update\n\nThe Select component issue:\n\nYour Select component has form={formId} but is outside the `` tag:svelte\n\n\n    form={formId}\n    listItems={descriptionRules}\n\nWhat formData actually contains:javascript\n// formData will have:\n{\n  \"description\": \"WALMART PURCHASE\",\n  \"rule\": \"includes\",           // from currentDescriptionRule[index]\n  \"value\": \"WALMART PURCHASE\",  // from currentValue[index]\n  \"category\": \"grocery\"         // from currentCategory[index]\n}\n\n// formData will NOT have:\n// \"descriptionrule\" - because the Select is outside the form\nHow the Select contributes to the form:\n\nEven though the Select is outside the form, it can still contribute to the form because:\n\nThe Select has form={formId} attribute\nThe Select renders a ` with name=\"descriptionrule\"`\nHTML5 allows form controls outside a form to be associated with it using the form attribute\n\nTo see the descriptionrule value:\n\nIf the Select is properly associated with the form, you should see:javascript\n// Expected formData:\n{\n  \"description\": \"WALMART PURCHASE\",\n  \"descriptionrule\": \"includes\",  // From the Select component\n  \"rule\": \"includes\",             // From hidden input\n  \"value\": \"WALMART PURCHASE\",\n  \"category\": \"grocery\"\n}\nDebug steps:\n\nCheck the rendered HTML in browser dev tools to see if the Select has the correct form attribute\nConsole log formData to see what's actually being sent:svelte\nuse:enhance={() => {\n    return async ({ result, formData }) => {\n        console.log('FormData entries:', Object.fromEntries(formData.entries()));\n        // ...\n    };\n}}\nThe issue might be that the Select component isn't properly rendering the form attribute on the underlying `` element.\n\nrogerpence: when +page.svelte calls the assignCategoryRule action, formdata has a 'categoryrule' value. Where did that come from in +page.svelte's form\n\nGitHub Copilot: Looking at your code, I don't see where categoryrule would come from in the form data. Based on your current form structure, the formData should only contain these fields:javascript\n// Expected formData from your current form:\n{\n  \"description\": \"...\",\n  \"rule\": \"...\",        // from hidden input\n  \"value\": \"...\",       // from hidden input  \n  \"category\": \"...\",    // from hidden input\n  \"descriptionrule\": \"...\", // from Select component (if properly associated)\n}\nThere is no categoryrule field in your current form.\n\nPossible sources of categoryrule:\n\n1. Typo in console output\nYou might be seeing categoryrule when it's actually category or descriptionrule.\n\n2. Hidden in your actual code\nThere might be another hidden input or Select component that you haven't shown in the excerpt.\n\n3. From a different form submission\nThe console log might be from a different form or previous submission.\n\n4. Browser dev tools confusion\nCheck the actual Network tab in browser dev tools to see what's being sent.\n\nTo debug this:\n\nAdd this to your action to see exactly what's being received:typescript\nexport const actions: Actions = {\n    assignCategoryRule: async ({ locals, request, params }) => {\n        const rawFormData = Object.fromEntries(await request.formData());\n        \n        console.log('=== FORM DATA DEBUG ===');\n        console.log('All form fields received:');\n        Object.entries(rawFormData).forEach(([key, value]) => {\n            console.log(  ${key}: ${value});\n        });\n        console.log('=== END DEBUG ===');\n        \n        // ... rest of your code\n    }\n};\nCheck your Select component:\n\nMake sure your Select component is rendering the correct name attribute:svelte\n\n\n    \n\nIf domName=\"category\", then you should see category in formData, not categoryrule.\n\nCan you share the exact console output you're seeing? That would help identify where categoryrule is actually coming from.\n\nrogerpence: am I calling the assignCateogoryRule correctly here\n\nGitHub Copilot: Looking at your code, you're not calling the assignCategoryRule function at all. Here's what's happening:\n\nCurrent situation:\n\nThe function assignCategoryRule() is defined but never called\nThe form action ?/assignCategoryRule is being called instead\nThese are two completely different things\n\nWhat you have:svelte\n\n\nfunction assignCategoryRule() {\n    fetch('/api/update-category', { ... }); // This never runs\n}\n\nThe flow that's actually happening:\n\nUser clicks \"Update\" button\nForm submits to server action ?/assignCategoryRule\nServer action in +page.server.ts runs\nuse:enhance callback runs after server responds\nYour JavaScript assignCategoryRule() function is never called\n\nTo fix this, you have two options:\n\nOption 1: Call the function in the enhance callbacksvelte\n {\n        return async ({ result, formData }) => {\n            if (result.type === 'success') {\n                // Now call your JavaScript function\n                assignCategoryRule();\n                console.log(result);\n                console.log(formData);\n            }\n            await applyAction(result);\n        };\n    }}\nOption 2: Remove the server action and use only the JavaScript functionsvelte\n {\n        // Call your JavaScript function directly\n        assignCategoryRule();\n    }}\nOption 3: Remove the JavaScript function (recommended)\n\nSince you're already using SvelteKit's form actions, you probably don't need the separate JavaScript function:svelte\n\n// Remove this function entirely\n// function assignCategoryRule() { ... }\n\n\n\n    \n\nWhich approach do you want to use? The server action approach (Option 3) is generally preferred in SvelteKit applications.\n\nrogerpence: the assignCategoryRule is unnecessary\n\nGitHub Copilot: You're absolutely right! The assignCategoryRule function is unnecessary since you're already using SvelteKit's form actions. Here's the cleaned up code:svelte\n\n\timport { enhance, applyAction } from '$app/forms';\n\timport type { PageData, ActionData } from './$types'; // Use generated types\n\timport { descriptionRules, categories } from '$lib/data/budget-meta-data';\n\timport Select from '$lib/components/Select.svelte';\n\n\tconst { data, form } = $props();\n\n\tlet currentDescriptionRule: string[] | undefined = $state([]);\n\tlet currentCategory: string[] | undefined = $state([]);\n\tlet currentValue: string[] | undefined = $state([]);\n\n\tfunction handleValueChange(index: number, value: string) {\n\t\tcurrentValue[index] = value;\n\t}\n\n\tdata.datarows.map((row: any) => {\n\t\tcurrentValue.push(row.description);\n\t});\n\n\n\n\t\n\t\t\n\t\t\tDescription\n\t\t\tRule\n\t\t\tValue\n\t\t\tCategory\n\t\t\tAction\n\t\t\n\t\n\t\n\t\t{#each data.datarows as row, index}\n\t\t\t{@const formId = form-item-${crypto.randomUUID()}}\n\n\t\t\t\n\t\t\t\t{row.description}\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\t handleValueChange(index, e.currentTarget.value)}\n\t\t\t\t\t/>\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\t {\n\t\t\t\t\t\t\treturn async ({ result, formData }) => {\n\t\t\t\t\t\t\t\tif (result.type === 'success') {\n\t\t\t\t\t\t\t\t\tconsole.log('Form submitted successfully:', result);\n\t\t\t\t\t\t\t\t\tconsole.log('Form data:', formData);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tawait applyAction(result);\n\t\t\t\t\t\t\t};\n\t\t\t\t\t\t}}\n\t\t\t\t\t>\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\tUpdate\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t{/each}\n\t\n\nRemoved:\nassignCategoryRule() function\ncategoryData variable\nUnused import { type CategoryRules }\n\nBenefits:\nCleaner code\nSingle responsibility: the server action handles the API call\nNo duplicate logic\nFollows SvelteKit best practices\n\nThe server action in +page.server.ts already handles the database update, so the client-side function was redundant.\n\nrogerpence: what are the domname attributes doing\n\nGitHub Copilot: The domName attributes are setting the name attribute on the HTML form elements that get rendered by your Select component.\n\nWhat happens:\n\nIn your Svelte code:svelte\n\n    form={formId}\n    listItems={descriptionRules}\nIn your Select component:svelte\n\n     -->\n\nFinal rendered HTML:html\n\n    Includes\n    Equals\nWhy this matters:\n\nWhen the form is submitted, the name attribute determines the key in the form data:javascript\n// Form submission creates FormData with these keys:\n{\n    \"description\": \"WALMART PURCHASE\",           // from hidden input\n    \"descriptionrule\": \"includes\",               // from Select with domName=\"descriptionrule\"\n    \"rule\": \"includes\",                          // from hidden input\n    \"value\": \"WALMART PURCHASE\",                 // from hidden input\n    \"category\": \"grocery\",                       // from Select with domName=\"category\"\n}\nYour current setup:\n\ndomName=\"descriptionrule\"  creates name=\"descriptionrule\"  formData key: \"descriptionrule\"\ndomName=\"category\"  creates name=\"category\"  formData key: \"category\"\n\nThe form attribute:\n\nThe form={formId} attribute associates the select element with the form even though it's outside the `` tag. This is HTML5 functionality that allows form controls to be anywhere in the DOM but still belong to a specific form.svelte\n\n\n\n\n\n    Update\n\nSo domName is essentially the \"field name\" that will appear in your form submission data.\n\nrogerpence: formData has description, value, category, and rule keys. How is not compliant with CategoryRulesDto \n\nGitHub Copilot: The issue is that your CategoryRulesDto type and your form data structure don't match. Here's the problem:\n\nYour CategoryRulesDto type:typescript\n// From your types file\nexport type CategoryRulesDto = {\n  description: string;\n  rule: string;\n  value: string;\n  category: string;\n};\nYour actual form data:javascript\n// What you're actually receiving\n{\n  \"description\": \"WALMART PURCHASE\",\n  \"rule\": \"includes\",           // This comes from hidden input\n  \"value\": \"WALMART PURCHASE\",  // This comes from hidden input  \n  \"category\": \"grocery\"         // This comes from hidden input\n}\nThe casting issue:typescript\nconst formdata: CategoryRulesDto = rawFormData; // This is unsafe casting!\nYou're telling TypeScript that rawFormData is a CategoryRulesDto, but you haven't validated that the data actually matches the structure.\n\nSolutions:\n\nOption 1: Proper validation with Zodtypescript\nimport { categoryRulesSchema } from '$lib/types/CategoryRules';\n\nexport const actions: Actions = {\n    assignCategoryRule: async ({ locals, request, params }) => {\n        const rawFormData = Object.fromEntries(await request.formData());\n        const ms = locals.ms;\n\n        console.log('rawFormData', rawFormData);\n\n        try {\n            // Validate the form data\n            const validatedData = categoryRulesSchema.parse({\n                id: null, // Since this is a create/update\n                description: rawFormData.description,\n                rule: rawFormData.rule,\n                value: rawFormData.value,\n                category: rawFormData.category\n            });\n\n            const result = await upsertCategoryRule(ms, validatedData);\n\n            return {\n                success: true,\n                data: result\n            };\n        } catch (err: any) {\n            console.error('Validation error:', err);\n            return fail(400, {\n                data: rawFormData,\n                errors: err.errors || null,\n                message: 'Invalid form data'\n            });\n        }\n    }\n};\nOption 2: Manual transformationtypescript\nexport const actions: Actions = {\n    assignCategoryRule: async ({ locals, request, params }) => {\n        const rawFormData = Object.fromEntries(await request.formData());\n        const ms = locals.ms;\n\n        console.log('rawFormData', rawFormData);\n\n        try {\n            // Transform form data to match CategoryRulesEntity\n            const categoryRuleEntity: CategoryRulesEntity = {\n                id: null, // For new records\n                description: rawFormData.description as string,\n                rule: rawFormData.rule as string,\n                value: rawFormData.value as string,\n                category: rawFormData.category as string\n            };\n\n            const result = await upsertCategoryRule(ms, categoryRuleEntity);\n\n            return {\n                success: true,\n                data: result\n            };\n        } catch (err: any) {\n            console.error('Error processing form:', err);\n            return fail(400, {\n                data: rawFormData,\n                errors: null,\n                message: 'Failed to process form data'\n            });\n        }\n    }\n};\nOption 3: Fix the function signature\n\nYour upsertCategoryRule function expects CategoryRulesEntity but you're passing CategoryRulesDto:typescript\n// Change this:\nconst result = await upsertCategoryRule(ms, formdata);\n\n// To this:\nconst categoryRuleEntity: CategoryRulesEntity = {\n    id: null,\n    description: rawFormData.description as string,\n    rule: rawFormData.rule as string,\n    value: rawFormData.value as string,\n    category: rawFormData.category as string\n};\n\nconst result = await upsertCategoryRule(ms, categoryRuleEntity);\nThe main issue is the unsafe type casting. You should validate/transform the data before using it, rather than just casting it to the expected type.\n\nrogerpence: the following is a sql server stored procedure that provides upsert functionality for a table:\nUSE [rp]\nGO\n/* Object:  StoredProcedure [dbo].[rp_categoryrulesUpsert]    Script Date: 7/5/2025 12:09:59 PM */\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\nALTER   PROC [dbo].[rp_categoryrulesUpsert] -- Renamed to your specific name if needed, e.g., rp_categoryrulesUpsert2\n    @id INT = NULL,\n    @description VARCHAR(150),\n    @rule VARCHAR(50),\n    @value VARCHAR(500),\n    @category VARCHAR(150)\nAS\nBEGIN\n    SET NOCOUNT ON;\n    SET XACT_ABORT ON;\n\n    DECLARE @ActionStatus VARCHAR(10);\n    DECLARE @OutputId INT;\n\n    BEGIN TRY\n        BEGIN TRAN;\n\n        IF @id IS NOT NULL AND @id > 0 -- Check for a valid, positive ID indicating an update attempt\n        BEGIN\n            -- Attempt to update\n            UPDATE [dbo].[categoryrules]\n            SET    [description] = @description,\n                   [rule] = @rule,\n                   [value] = @value,\n                   [category] = @category\n            WHERE  [id] = @id;\n\n            IF @@ROWCOUNT > 0 -- Check if any row was actually updated\n            BEGIN\n                SET @ActionStatus = 'UPDATED';\n                SET @OutputId = @id;\n\n                SELECT\n                    cr.[id],\n                    cr.[description],\n                    cr.[rule],\n                    cr.[value],\n                    cr.[category],\n                    @ActionStatus AS ActionStatus\n                FROM\n                    [dbo].[categoryrules] cr\n                WHERE\n                    cr.[id] = @OutputId;\n            END\n            ELSE\n            BEGIN\n                -- No row found with the given @id to update.\n                SET @ActionStatus = 'NOT_FOUND';\n                SET @OutputId = @id; -- The ID that was attempted\n\n                SELECT\n                    @OutputId AS id,\n                    CAST(NULL AS VARCHAR(150)) AS description,\n                    CAST(NULL AS VARCHAR(50)) AS [rule], -- Changed 'AS rule' to 'AS [rule]'\n                    CAST(NULL AS VARCHAR(500)) AS value,\n                    CAST(NULL AS VARCHAR(150)) AS category,\n                    @ActionStatus AS ActionStatus;\n            END\n        END\n        ELSE\n        BEGIN\n            -- Insert new row\n            INSERT INTO [dbo].[categoryrules, rule], [value], [category])\n            VALUES (@description, @rule, @value, @category);\n\n            SET @OutputId = SCOPE_IDENTITY(); -- Get the newly inserted ID\n            SET @ActionStatus = 'INSERTED';\n\n            SELECT\n                cr.[id],\n                cr.[description],\n                cr.[rule],\n                cr.[value],\n                cr.[category],\n                @ActionStatus AS ActionStatus\n            FROM\n                [dbo].[categoryrules] cr\n            WHERE\n                cr.[id] = @OutputId;\n        END\n\n        COMMIT TRAN;\n    END TRY\n    BEGIN CATCH\n        IF @@TRANCOUNT > 0\n            ROLLBACK TRAN;\n\n        THROW;\n    END CATCH;\nEND\n\ncreate a template from that I can to easily write upsert procs for other tables\n\nGitHub Copilot: Here's a reusable template for creating upsert stored procedures:sql\nUSE [YourDatabaseName]\nGO\n\n/* Object:  StoredProcedure [dbo].[TableName_Upsert] */\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE OR ALTER PROC [dbo].[TableName_Upsert]\n    @id INT = NULL,\n    -- Add your table-specific parameters here\n    @column1 VARCHAR(150),\n    @column2 VARCHAR(50),\n    @column3 VARCHAR(500),\n    @column4 VARCHAR(150)\n    -- Add more parameters as needed\nAS\nBEGIN\n    SET NOCOUNT ON;\n    SET XACT_ABORT ON;\n\n    DECLARE @ActionStatus VARCHAR(10);\n    DECLARE @OutputId INT;\n\n    BEGIN TRY\n        BEGIN TRAN;\n\n        IF @id IS NOT NULL AND @id > 0 -- Check for a valid, positive ID indicating an update attempt\n        BEGIN\n            -- Attempt to update\n            UPDATE [dbo].[YourTableName]\n            SET    [column1] = @column1,\n                   [column2] = @column2,\n                   [column3] = @column3,\n                   [column4] = @column4\n                   -- Add more columns as needed\n            WHERE  [id] = @id;\n\n            IF @@ROWCOUNT > 0 -- Check if any row was actually updated\n            BEGIN\n                SET @ActionStatus = 'UPDATED';\n                SET @OutputId = @id;\n\n                SELECT\n                    t.[id],\n                    t.[column1],\n                    t.[column2],\n                    t.[column3],\n                    t.[column4],\n                    -- Add more columns as needed\n                    @ActionStatus AS ActionStatus\n                FROM\n                    [dbo].[YourTableName] t\n                WHERE\n                    t.[id] = @OutputId;\n            END\n            ELSE\n            BEGIN\n                -- No row found with the given @id to update.\n                SET @ActionStatus = 'NOT_FOUND';\n                SET @OutputId = @id; -- The ID that was attempted\n\n                SELECT\n                    @OutputId AS id,\n                    CAST(NULL AS VARCHAR(150)) AS column1,\n                    CAST(NULL AS VARCHAR(50)) AS column2,\n                    CAST(NULL AS VARCHAR(500)) AS column3,\n                    CAST(NULL AS VARCHAR(150)) AS column4,\n                    -- Add more columns as needed with appropriate CAST types\n                    @ActionStatus AS ActionStatus;\n            END\n        END\n        ELSE\n        BEGIN\n            -- Insert new row\n            INSERT INTO [dbo].[YourTableName, column2], [column3], [column4])\n            VALUES (@column1, @column2, @column3, @column4);\n\n            SET @OutputId = SCOPE_IDENTITY(); -- Get the newly inserted ID\n            SET @ActionStatus = 'INSERTED';\n\n            SELECT\n                t.[id],\n                t.[column1],\n                t.[column2],\n                t.[column3],\n                t.[column4],\n                -- Add more columns as needed\n                @ActionStatus AS ActionStatus\n            FROM\n                [dbo].[YourTableName] t\n            WHERE\n                t.[id] = @OutputId;\n        END\n\n        COMMIT TRAN;\n    END TRY\n    BEGIN CATCH\n        IF @@TRANCOUNT > 0\n            ROLLBACK TRAN;\n\n        THROW;\n    END CATCH;\nEND\nGO\nHow to use this template:\n\n1. Replace placeholders:\nYourDatabaseName  Your actual database name\nTableName_Upsert  Your table name + _Upsert (e.g., Users_Upsert)\nYourTableName  Your actual table name\ncolumn1, column2, etc.  Your actual column names\n\n2. Update data types:\nChange VARCHAR(150), VARCHAR(50), etc. to match your column types\nUse appropriate types like INT, DATETIME2, DECIMAL, NVARCHAR, etc.\n\n3. Example for a Users table:sql\nUSE [MyApp]\nGO\n\nCREATE OR ALTER PROC [dbo].[Users_Upsert]\n    @id INT = NULL,\n    @email NVARCHAR(255),\n    @firstName NVARCHAR(100),\n    @lastName NVARCHAR(100),\n    @isActive BIT,\n    @createdDate DATETIME2 = NULL\nAS\nBEGIN\n    SET NOCOUNT ON;\n    SET XACT_ABORT ON;\n\n    DECLARE @ActionStatus VARCHAR(10);\n    DECLARE @OutputId INT;\n\n    BEGIN TRY\n        BEGIN TRAN;\n\n        IF @id IS NOT NULL AND @id > 0\n        BEGIN\n            UPDATE [dbo].[Users]\n            SET    [email] = @email,\n                   [firstName] = @firstName,\n                   [lastName] = @lastName,\n                   [isActive] = @isActive,\n                   [updatedDate] = GETDATE()\n            WHERE  [id] = @id;\n\n            IF @@ROWCOUNT > 0\n            BEGIN\n                SET @ActionStatus = 'UPDATED';\n                SET @OutputId = @id;\n\n                SELECT\n                    u.[id],\n                    u.[email],\n                    u.[firstName],\n                    u.[lastName],\n                    u.[isActive],\n                    u.[createdDate],\n                    u.[updatedDate],\n                    @ActionStatus AS ActionStatus\n                FROM [dbo].[Users] u\n                WHERE u.[id] = @OutputId;\n            END\n            ELSE\n            BEGIN\n                SET @ActionStatus = 'NOT_FOUND';\n                SET @OutputId = @id;\n\n                SELECT\n                    @OutputId AS id,\n                    CAST(NULL AS NVARCHAR(255)) AS email,\n                    CAST(NULL AS NVARCHAR(100)) AS firstName,\n                    CAST(NULL AS NVARCHAR(100)) AS lastName,\n                    CAST(NULL AS BIT) AS isActive,\n                    CAST(NULL AS DATETIME2) AS createdDate,\n                    CAST(NULL AS DATETIME2) AS updatedDate,\n                    @ActionStatus AS ActionStatus;\n            END\n        END\n        ELSE\n        BEGIN\n            INSERT INTO [dbo].[Users, [firstName], [lastName], [isActive], [createdDate])\n            VALUES (@email, @firstName, @lastName, @isActive, COALESCE(@createdDate, GETDATE()));\n\n            SET @OutputId = SCOPE_IDENTITY();\n            SET @ActionStatus = 'INSERTED';\n\n            SELECT\n                u.[id],\n                u.[email],\n                u.[firstName],\n                u.[lastName],\n                u.[isActive],\n                u.[createdDate],\n                u.[updatedDate],\n                @ActionStatus AS ActionStatus\n            FROM [dbo].[Users] u\n            WHERE u.[id] = @OutputId;\n        END\n\n        COMMIT TRAN;\n    END TRY\n    BEGIN CATCH\n        IF @@TRANCOUNT > 0\n            ROLLBACK TRAN;\n\n        THROW;\n    END CATCH;\nEND\nGO\nKey features of this template:\n\nConsistent return format - Always returns the affected row with ActionStatus\nTransaction safety - Uses transactions with proper rollback\nError handling - Comprehensive try/catch blocks\nFlexible ID handling - Handles both insert (NULL ID) and update (valid ID)\nStatus reporting - Returns 'INSERTED', 'UPDATED', or 'NOT_FOUND'\nSCOPE_IDENTITY() - Properly captures new IDs for inserts\n\nThis template makes it easy to create consistent upsert procedures across your database!\n\nrogerpence: why does 'body' on line 20 say no overload matches this call\n\nGitHub Copilot: The error occurs because the body property in the fetch options expects a string, but you're passing a CategoryRulesEntity object directly.\n\nThe Problem:typescript\nlet categoryData: CategoryRulesEntity = {\n    id: 0,\n    description: \"Carl's Tavern\",\n    rule: 'includes',\n    value: 'yyyxxxyy',\n    category: 'fast food'\n};\n\nfetch('/api/update-category', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: categoryData  //  This is an object, not a string\n})\nThe Solution:\n\nYou need to convert the object to a JSON string using JSON.stringify():typescript\nfetch('/api/update-category', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify(categoryData)  //  Convert object to JSON string\n})\nComplete fix:svelte\n\n    // ... other imports and code ...\n\n    function assignCategoryRule() {\n        fetch('/api/update-category', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify(categoryData)  // Convert to JSON string\n        })\n            .then((response) => response.json())\n            .then((data) => {\n                //console.log('Response:', data);\n            })\n            .catch((error) => {\n                console.error('Error:', error);\n            });\n    }\n\n    // ... rest of your code ...\n\nWhy this happens:\n\nThe fetch API's body property accepts:\nstring (most common for JSON)\nFormData\nBlob\nArrayBuffer\nURLSearchParams\n\nBut it doesn't accept plain JavaScript objects. When sending JSON data, you must stringify the object first.\n\nrogerpence: how can I see if selectedValue is equal to the value properry of one of the listItems elements\n\nGitHub Copilot: You can check if selectedValue is equal to one of the listItems values in several ways:\n\nOption 1: Use your existing function (simplest)svelte\n\n    // You already have this function!\n    export function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n        return listItems.some((element) => element.value === selectedValue);\n    }\n\n    // Use it anywhere in your component\n    const isValueInList = isSelectValueInList(selectedValue);\n    console.log('Is selected value in list?', isValueInList);\n\nOption 2: Create a reactive variablesvelte\n\n    // ... your existing code ...\n\n    // This will automatically update when selectedValue or listItems change\n    $: isSelectedValueInList = listItems.some(item => item.value === selectedValue);\n\n    // Use it in your template or logs\n    console.log('Selected value in list:', isSelectedValueInList);\n\n\n\n{#if isSelectedValueInList}\n     Valid selection\n{:else}\n     Invalid selection\n{/if}\nOption 3: Find the matching item (gets the full object)svelte\n\n    // ... your existing code ...\n\n    // Get the actual matching item (or undefined if not found)\n    $: matchingItem = listItems.find(item => item.value === selectedValue);\n\n    // Check if it exists\n    $: hasMatch = !!matchingItem;\n\n    console.log('Matching item:', matchingItem);\n    console.log('Has match:', hasMatch);\n\nOption 4: Use a derived state in Svelte 5svelte\n\n    // ... your existing code ...\n\n    // Svelte 5 approach with $derived\n    const isValueValid = $derived(\n        listItems.some(item => item.value === selectedValue)\n    );\n\n    const matchingItem = $derived(\n        listItems.find(item => item.value === selectedValue)\n    );\n\nYour existing code issue:\n\nYou already have the logic, but there are some issues in your component:svelte\n\n    // Fix this line - it's incomplete\n    const selectValueIsValid = isSelectValueInList(selectedValue); // Add the function call\n\n    // This variable name is confusing - rename it\n    const selectValueInList = isSelectValueInList(selectedValue);\n\n    // Clean version:\n    const isValidSelection = isSelectValueInList(selectedValue);\n\nRecommended approach:svelte\n\n    // ... your existing props and interfaces ...\n\n    // Clean, reactive check\n    $: isValidSelection = listItems.some(item => item.value === selectedValue);\n    \n    // Or use your existing function\n    $: isValidSelection = isSelectValueInList(selectedValue);\n\n    // Use it in your error display\n\n\n\n    \n        {#each listItems as listItem}\n            {listItem.text}\n        {/each}\n    \n    \n    {#if !isValidSelection && showInvalidValueError}\n        \n            Current value of '{selectedValue}' is not in the list\n        \n    {/if}\n\nThe $: reactive statement will automatically re-run whenever selectedValue or listItems changes, keeping your validation up to date.\n\nrogerpence: this is svelte 5\n\nGitHub Copilot: Your Svelte 5 code has several issues. Here's the cleaned up version:svelte\n\n\texport interface ListItem {\n\t\tvalue: string;\n\t\ttext: string;\n\t}\n\t\n\tinterface ComponentProps {\n\t\tdomName: string;\n\t\tlistItems: ListItem[];\n\t\tselectedValue?: string | undefined | null;\n\t\tshowInvalidValueError?: boolean;\n\t\tform?: string;\n\t}\n\n\texport function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n\t\treturn listItems.some((element) => element.value === selectedValue);\n\t}\n\n\tlet {\n\t\tdomName,\n\t\tlistItems,\n\t\tselectedValue = $bindable(),\n\t\tshowInvalidValueError,\n\t\tform\n\t}: ComponentProps = $props();\n\n\t// Svelte 5: Use $derived for reactive computations\n\tconst selectValueInList = $derived(isSelectValueInList(selectedValue));\n\t\n\t// Initialize avail with a valid value\n\tlet avail = $state(\n\t\tselectedValue && selectValueInList ? selectedValue : listItems[0]?.value\n\t);\n\n\t// Sync selectedValue when avail changes\n\t$effect(() => {\n\t\tselectedValue = avail;\n\t});\n\n\t// Sync avail when selectedValue changes from parent\n\t$effect(() => {\n\t\tif (selectedValue !== avail) {\n\t\t\tavail = selectedValue && selectValueInList ? selectedValue : listItems[0]?.value;\n\t\t}\n\t});\n\n\n\n\t\n\t\t{#each listItems as listItem}\n\t\t\t{listItem.text}\n\t\t{/each}\n\t\n\t\n\t{#if !selectValueInList && showInvalidValueError}\n\t\t\n\t\t\tCurrent value of '{selectedValue}' is not in the list\n\t\t\n\t{/if}\n\nKey changes for Svelte 5:\n\n1. Use $derived instead of reactive statementssvelte\n//  Old way (Svelte 4)\n$: selectValueInList = isSelectValueInList(selectedValue);\n\n//  New way (Svelte 5)  \nconst selectValueInList = $derived(isSelectValueInList(selectedValue));\n2. Removed broken/incomplete codesvelte\n//  These were broken\nconst selectValueIsValid = // incomplete\nconst localSelectValue = selectedValue ?? listItems[0].value; // unused\nconst selectedValueInList = selectedValue && selectValueInList ? selectedValue : listItems[0].value; // wrong logic\n3. Proper two-way binding with $effectsvelte\n// Sync both directions\n$effect(() => {\n\tselectedValue = avail; // avail  selectedValue\n});\n\n$effect(() => {\n\tif (selectedValue !== avail) {\n\t\tavail = selectedValue && selectValueInList ? selectedValue : listItems[0]?.value;\n\t}\n}); // selectedValue  avail\n4. Safe array accesssvelte\n//  Could break if listItems is empty\nlistItems[0].value\n\n//  Safe with optional chaining\nlistItems[0]?.value\nAlternative simpler approach:\n\nIf you want to keep it simpler, you could also do:svelte\n\n\t// ... interfaces and props ...\n\n\t// Simple reactive validation\n\tconst isValid = $derived(\n\t\tlistItems.some(item => item.value === selectedValue)\n\t);\n\n\t// Use selectedValue directly if valid, otherwise first item\n\tconst effectiveValue = $derived(\n\t\tisValid ? selectedValue : listItems[0]?.value\n\t);\n\n\t// Single state variable\n\tlet avail = $state(effectiveValue);\n\n\t// Sync back to parent\n\t$effect(() => {\n\t\tselectedValue = avail;\n\t});\n\n\n\n\t\n\t\t{#each listItems as listItem}\n\t\t\t{listItem.text}\n\t\t{/each}\n\t\n\t\n\t{#if !isValid && showInvalidValueError}\n\t\t\n\t\t\tCurrent value of '{selectedValue}' is not in the list\n\t\t\n\t{/if}\n\nThis approach is cleaner and follows Svelte 5 patterns better.\n\nrogerpence: line 32 gets this error\n\nGitHub Copilot: \n\nrogerpence: line 32 gets this error\nThis reference only captures the initial value of selectValueInList. Did you mean to reference it inside a derived instead?\n\nGitHub Copilot: The error occurs because you're using selectValueInList (a derived value) inside $state() initialization, which only captures the initial value. Here's the fix:\n\nProblem:svelte\n//  This captures only the initial value of selectValueInList\nlet avail = $state(\n    selectedValue && selectValueInList ? selectedValue : listItems[0]?.value\n);\nSolution - Use $derived.by():svelte\n\n    export interface ListItem {\n        value: string;\n        text: string;\n    }\n    \n    interface ComponentProps {\n        domName: string;\n        listItems: ListItem[];\n        selectedValue?: string | undefined | null;\n        showInvalidValueError?: boolean;\n        form?: string;\n    }\n\n    export function isSelectValueInList(selectedValue?: string | undefined | null): boolean {\n        return listItems.some((element) => element.value === selectedValue);\n    }\n\n    let {\n        domName,\n        listItems,\n        selectedValue = $bindable(),\n        showInvalidValueError,\n        form\n    }: ComponentProps = $props();\n\n    // Svelte 5: Use $derived for reactive computations\n    const selectValueInList = $derived(isSelectValueInList(selectedValue));\n    \n    //  Use $derived.by for complex reactive initialization\n    let avail = $derived.by(() => {\n        return selectedValue && selectValueInList ? selectedValue : listItems[0]?.value;\n    });\n\n    // Sync selectedValue when avail changes\n    $effect(() => {\n        selectedValue = avail;\n    });\n\n\n\n    \n        {#each listItems as listItem}\n            {listItem.text}\n        {/each}\n    \n    \n    {#if !selectValueInList && showInvalidValueError}\n        \n            Current value of '{selectedValue}' is not in the list\n        \n    {/if}\n\nAlternative approach - Separate state and derived:svelte\n\n    // ... interfaces and props ...\n\n    const selectValueInList = $derived(isSelectValueInList(selectedValue));\n    \n    // Simple state for the select element\n    let avail = $state(selectedValue || listItems[0]?.value);\n    \n    // Derived value that handles validation\n    const validatedValue = $derived(\n        selectedValue && selectValueInList ? selectedValue : listItems[0]?.value\n    );\n\n    // Sync avail with validated value when it changes\n    $effect(() => {\n        avail = validatedValue;\n    });\n\n    // Sync selectedValue when avail changes\n    $effect(() => {\n        selectedValue = avail;\n    });\n\nEven simpler approach:svelte\n\n    // ... interfaces and props ...\n\n    const selectValueInList = $derived(isSelectValueInList(selectedValue));\n    \n    // Use selectedValue directly with fallback\n    const effectiveValue = $derived(\n        selectedValue && selectValueInList ? selectedValue : listItems[0]?.value\n    );\n    \n    let avail = $state('');\n    \n    // Initialize and sync avail\n    $effect(() => {\n        avail = effectiveValue || '';\n    });\n\n    // Sync back to parent\n    $effect(() => {\n        selectedValue = avail;\n    });\n\nThe key point is that $state() only captures initial values, while $derived() and $derived.by() are reactive and update when dependencies change.\n\nrogerpence: In the HTML table produced on this page, each row has a form used to update that row. How can I collect the current values of the row's columns of for the form to submit\n\nGitHub Copilot: You have a mismatch between what you're displaying and what you're submitting. Here are a few approaches to fix this:\n\nOption 1: Use the Select component's binding (Recommended)\n\nUpdate your Select components to bind to the state arrays:svelte\n\n    // ... existing code ...\n\n    let currentDescriptionRule: string[] = $state([]);\n    let currentCategory: string[] = $state([]);\n    let currentValue: string[] = $state([]);\n\n    // Initialize arrays with data from the server\n    $effect(() => {\n        currentDescriptionRule = data.datarows.map(row => row.rule || '');\n        currentCategory = data.datarows.map(row => row.category || '');\n        currentValue = data.datarows.map(row => row.value || '');\n    });\n\n    function handleValueChange(index: number, value: string) {\n        currentValue[index] = value;\n    }\n\n\n\n    {#each data.datarows as row, index}\n        {@const formId = form-item-${crypto.randomUUID()}}\n        \n            {row.description}\n            \n                \n            \n            \n                \n            \n            \n                \n            \n            \n                \n                    \n                    \n                    \n                    \n                    Update\n                \n            \n        \n    {/each}\n\nOption 2: Remove hidden inputs and rely on form association\n\nSince your Select components have form={formId}, they should automatically be included in the form submission:svelte\n\n    {#each data.datarows as row, index}\n        {@const formId = form-item-${crypto.randomUUID()}}\n        \n            {row.description}\n            \n                \n            \n            \n                \n            \n            \n                \n            \n            \n                \n                    \n                    \n                    Update\n                \n            \n        \n    {/each}\n\nOption 3: Collect values dynamically in the enhance functionsvelte\n {\n        return async ({ result, formData }) => {\n            // The formData should automatically include all form-associated elements\n            console.log('Form data collected:', Object.fromEntries(formData.entries()));\n            \n            if (result.type === 'success') {\n                console.log('Success:', result);\n            }\n            await applyAction(result);\n        };\n    }}\n    \n    Update\n\nOption 4: Clean up your initialization code\n\nYour current initialization has issues. Here's a cleaner approach:svelte\n\n    // ... other imports ...\n\n    let currentDescriptionRule: string[] = $state([]);\n    let currentCategory: string[] = $state([]);\n    let currentValue: string[] = $state([]);\n\n    // Clean initialization\n    $effect(() => {\n        if (data.datarows.length > 0) {\n            currentDescriptionRule = data.datarows.map(row => row.rule || '');\n            currentCategory = data.datarows.map(row => row.category || '');\n            currentValue = data.datarows.map(row => row.value || '');\n        }\n    });\n\n    function handleValueChange(index: number, value: string) {\n        currentValue[index] = value;\n    }\n\nI recommend Option 1 as it gives you the most control and makes the data flow clear. The key is to:\n\nInitialize your state arrays from the server data\nBind the form elements to these state arrays\nUse the state arrays in your hidden inputs\n\nThis way, any changes in the UI will be reflected in the form submission.\n\nrogerpence: with use:enhance, how do I read the result from the form action\n\nGitHub Copilot: With use:enhance, you can read the result from the form action in the callback function. Here's how to access different parts of the result:\n\nBasic Result Accesssvelte\n {\n        return async ({ result, formData }) => {\n            console.log('Result type:', result.type);\n            console.log('Full result:', result);\n            console.log('Form data:', Object.fromEntries(formData.entries()));\n            \n            if (result.type === 'success') {\n                console.log('Success data:', result.data);\n                // Handle success case\n            } else if (result.type === 'failure') {\n                console.log('Failure data:', result.data);\n                console.log('Status:', result.status);\n                // Handle failure case\n            } else if (result.type === 'error') {\n                console.log('Error:', result.error);\n                // Handle error case\n            }\n            \n            await applyAction(result);\n        };\n    }}\nResult Types and Their Properties\n\n1. Success Result (result.type === 'success')typescript\n{\n    type: 'success',\n    status: number,        // HTTP status code (200, 201, etc.)\n    data?: any            // Data returned from your action\n}\n2. Failure Result (result.type === 'failure')typescript\n{\n    type: 'failure', \n    status: number,        // HTTP status code (400, 422, etc.)\n    data?: any            // Data from fail() function\n}\n3. Error Result (result.type === 'error')typescript\n{\n    type: 'error',\n    status?: number,       // HTTP status code (500, etc.)\n    error: Error          // Error object\n}\n4. Redirect Result (result.type === 'redirect')typescript\n{\n    type: 'redirect',\n    status: number,        // 3xx status code\n    location: string       // Redirect URL\n}\nReading Data from Your Server Action\n\nBased on your server action, you can access the returned data:svelte\n {\n        return async ({ result, formData }) => {\n            if (result.type === 'success') {\n                // Access the data returned from your action\n                console.log('Server returned:', result.data);\n                \n                // If your action returns { bob: 'bob' }\n                console.log('Bob value:', result.data?.bob);\n                \n                // You can also access any other data you return\n                if (result.data?.updatedRecord) {\n                    console.log('Updated record:', result.data.updatedRecord);\n                }\n                \n                // Update UI or show success message\n                alert('Update successful!');\n                \n            } else if (result.type === 'failure') {\n                // Handle validation errors or other failures\n                console.error('Validation errors:', result.data?.errors);\n                console.error('Error message:', result.data?.message);\n                \n                // Show error to user\n                alert(Error: ${result.data?.message || 'Unknown error'});\n            }\n            \n            await applyAction(result);\n        };\n    }}\nComplete Example with Your Server Action\n\nUpdate your server action to return useful data:typescript\n// +page.server.ts\nexport const actions: Actions = {\n    assignCategoryRule: async ({ locals, request, params }) => {\n        const rawFormData = Object.fromEntries(await request.formData());\n        const ms = locals.ms;\n\n        try {\n            const categoryRuleEntity: CategoryRulesEntity = {\n                id: rawFormData.id ? Number(rawFormData.id) : null,\n                description: rawFormData.description as string,\n                rule: rawFormData.rule as string,\n                value: rawFormData.value as string,\n                category: rawFormData.category as string\n            };\n\n            const result = await upsertCategoryRule(ms, categoryRuleEntity);\n\n            return {\n                success: true,\n                message: 'Category rule updated successfully',\n                updatedRecord: categoryRuleEntity,\n                actionResult: result\n            };\n        } catch (err: any) {\n            return fail(400, {\n                success: false,\n                message: err.message || 'Failed to update category rule',\n                formData: rawFormData,\n                errors: err.errors || null\n            });\n        }\n    }\n};\nThen in your Svelte component:svelte\n {\n        return async ({ result, formData }) => {\n            console.log('Form submission result:', result);\n            \n            if (result.type === 'success') {\n                console.log('Success message:', result.data.message);\n                console.log('Updated record:', result.data.updatedRecord);\n                \n                // Maybe refresh the data or show a toast\n                // You could also update the local state here\n                \n            } else if (result.type === 'failure') {\n                console.error('Error message:', result.data?.message);\n                console.error('Form data that failed:', result.data?.formData);\n                \n                // Show error to user\n                alert(Error: ${result.data?.message});\n            }\n            \n            await applyAction(result);\n        };\n    }}\nAccessing Specific Return Values\n\nIf your action returns specific data:typescript\n// Server action returns:\nreturn {\n    bob: 'bob',\n    updatedId: 123,\n    timestamp: new Date().toISOString()\n};\nsvelte\n// Client can access:\nif (result.type === 'success') {\n    console.log('Bob:', result.data.bob);              // 'bob'\n    console.log('Updated ID:', result.data.updatedId); // 123\n    console.log('Timestamp:', result.data.timestamp);  // ISO string\n}\n`\n\nThe key is that anything you return from your server action will be available in result.data when result.type === 'success'.",
        "locale": "en",
        "slug": "technical-posts/returning-data-to-use-enhance-form-action",
        "folder": ""
    },
    {
        "title": "Create a Sveltekit fallback route",
        "description": "Create a Sveltekit fallback route",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Question to ChatGPT:\n\nHow can I provide a fallback route to a route not found in sveltekit\n\nNote this part of the answer... seems helpful.\n\nif ($session.error) {\n    // Redirect to a custom 404 page or any other fallback route\n    // You can use the goto function to navigate programmatically\n    goto(\"/custom-404-page\");\n}\n\nChatGPT answer:\n\nTo provide a fallback route for a route not found in SvelteKit, you can use the src/routes/__layout.svelte file to catch and handle the not found routes. The __layout.svelte file acts as a layout component and is applied to all pages in your SvelteKit app.\n\nHere's how you can implement a fallback route for handling page not found scenarios:\n\nCreate the __layout.svelte file if it doesn't exist in the src/routes directory.\n\nInside __layout.svelte, use the slot element to define the main content area of the layout:\n\n\n\n    // You can have any layout-specific logic here\n\n\n\n    \n\n\n\n    /* Add your layout styles here */\n\n\nNow, in the main section of __layout.svelte, you can check for the existence of the current route using the SvelteKit's built-in $session store. If the route is not found ($session.error is defined), you can redirect the user to a custom 404 page or any other fallback route you want:\n\n\n\n    import { onMount } from \"svelte\";\n\n    onMount(() => {\n        // This function will be called once the component is mounted\n        if ($session.error) {\n            // Redirect to a custom 404 page or any other fallback route\n            // You can use the goto function to navigate programmatically\n            goto(\"/custom-404-page\");\n        }\n    });\n\n\n\n    \n\n\n\n    /* Add your layout styles here */\n\n\nRemember to create the custom 404 page or the fallback route in your src/routes directory, so when the user is redirected, they will see the fallback content.\n\nThat's it! Now, whenever a user navigates to a route that doesn't exist in your SvelteKit app, they will be redirected to the custom 404 page or any other fallback route you have defined.",
        "locale": "en",
        "slug": "technical-posts/fallback-route-with-sveltekit",
        "folder": ""
    },
    {
        "title": "Get table schema",
        "description": "Get table schema",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres",
            "sql",
            "schema"
        ],
        "content": "This SQL gets the schema for a Postgres table.\n\nSELECT\n    column_name,\n    ordinal_position,\n    column_default,\n    is_identity,\n    data_type,\n    '' as dotnet_data_type,\n    character_maximum_length,\n    numeric_precision,\n    data_dictionary.description as description\nFROM\n    information_schema.columns\nLEFT JOIN data_dictionary\n\tON \"table\" = table_name\n\tAND \"column\" = column_name\nWHERE\n\ttable_name = 'family_backup'\n\nThe dotNetDataType column is later populated with:\n\nC:\\Users\\thumb\\Documents\\projects\\asna\\downloads\\downloads-schema-report\\get-schemas.js",
        "locale": "en",
        "slug": "technical-posts/get-table-schema",
        "folder": ""
    },
    {
        "title": "Rust function return type as string",
        "description": "Rust function return type as string",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "If you want a function to return a String, you would typically use the String type as the return type. Heres how you can define such a function:\n\nExample\n\nfn greeting(name: &str) -> String {\n    format!(\"Hello, {}!\", name)\n}\n\nfn main() {\n    let name = \"Alice\";\n    let message = greeting(name);\n    println!(\"{}\", message);\n}\n\nWhen to Use String\n\nYou would use a String return type in scenarios where:\n\nYou are generating or constructing a string value that will be used later.\nThe function is expected to return a string representation of some data or computation.\n\nConsiderations for Error Handling\n\nIf the function might encounter errors while generating the string (for example, when reading from a file or performing operations that can fail), it's often better to use a Result return type. This way, you can indicate success or failure while also providing an error message.\n\nExample with Result\n\nuse std::fs;\nuse std::io::{self, ErrorKind};\n\nfn read_file_contents(file_path: &str) -> Result {\n    let contents = fs::read_to_string(file_path)?;\n    Ok(contents)\n}\n\nfn main() {\n    match read_file_contents(\"some_file.txt\") {\n        Ok(contents) => println!(\"File contents:\\n{}\", contents),\n        Err(e) => eprintln!(\"Error reading file: {}\", e),\n    }\n}\n\nSummary\n\nUse String as the return type when the function is simply returning a string value.\nUse Result when you need to handle potential errors along with returning a string, allowing for more robust error handling and better feedback to the caller.",
        "locale": "en",
        "slug": "technical-posts/rust-function-return-type-as-string",
        "folder": ""
    },
    {
        "title": "Split the DataGate family from its parent family to create a separate DataGate release set",
        "description": "Split the DataGate family from its parent family to create a separate DataGate release set",
        "date_created": "2025-06-03T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "downloads"
        ],
        "content": "This PowerShell script splits the DG-related markdown change files and puts them in their own separate release set folder. The DG-related markdown files are removed from the parent family.\n\nRun this after downloading the markdown change files for the parent family--before creating the HTML files.  This operation is listed as \"Split the DG family from parent family\" in the Downloads Task Runner.\n\nC:\\Users\\thumb\\Documents\\projects\\asna\\downloads\\release-notes\\scripts\\split-dg-release-notes-changes.ps1\nExample\n\nBefore running this script:\n\n2025-06-04-nancy-beta\n\tavr-setup 16.0.92.0.md\n\tbrowser terminal 16.0.92.0.md\n\tdcs-setup 16.0.91.0.md\n\tdg400 16.0.40.0.md\n\tdgmonitor 16.0.67.0.md\n\tdgstudio 16.0.106.0.md\n\tdss-setup 16.0.91.0.md\n\tmobilerpg-setup 10.0.96.0.md\n\tmonarch nomad 16.0.90.0.md\n\tmonarch-setup 10.0.93.0.md\n\twings-setup 10.0.97.0.md\n\nRun the script:\n\nsplit-dg-release-note-changes -family_path 2025-06-04-nancy-beta\n\nAfter running this script: \n\n2025-06-04-nancy-beta\n\tavr-setup 16.0.92.0.md\n\tbrowser terminal 16.0.92.0.md\n\tmobilerpg-setup 10.0.96.0.md\n\tmonarch nomad 16.0.90.0.md\n\tmonarch-setup 10.0.93.0.md\n\twings-setup 10.0.97.0.md\n2025-06-04-datagate16-beta\n\tdcs-setup 16.0.91.0.md\n\tdg400 16.0.40.0.md\n\tdgmonitor 16.0.67.0.md\n\tdgstudio 16.0.106.0.md\n\tdss-setup 16.0.91.0.md\n\nA warning is shown if DataGate for the IBM is missing from the original markdown changes. If it is, and you're creating a beta release set, you generally want to find the the last DataGate for IBM i beta release and include it here.",
        "locale": "en",
        "slug": "technical-posts/split-datagate-from-parent-family",
        "folder": ""
    },
    {
        "title": "how-does-microsoft-define-individual-or-consumer-user",
        "description": "how-does-microsoft-define-individual-or-consumer-user",
        "date_created": "2025-09-16T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "microsoft"
        ],
        "content": "in what document or with what URL does microsoft define \"individual\" or \"consumer\" user in the context of the Extended Security Update for Windows 10\n\n\n\nCheck this video for the user/consumer answer\n\nGoogle Gemini\n\nIn the context of Microsoft's Extended Security Updates (ESU) for Windows 10, when they refer to \"individuals\" or \"consumers\" eligible for the $30 price, they are typically defining users who meet one of the following criteria:\n\nNon-Commercial Use: The ESU license is intended for personal use on a device that is not part of a business, organization, or enterprise network. This means your home PC, your family's laptop, etc.\nNo Active Microsoft 365 Subscription for Business/Enterprise: If you have a business or enterprise-level Microsoft 365 subscription (like Microsoft 365 Business Standard, Microsoft 365 E3, etc.) that would typically include Windows Pro or Enterprise licenses, you'd likely fall under commercial terms. The $30 ESU is aimed at those without such comprehensive commercial agreements.\nHome Users/Smallest Scale: Essentially, it's for the everyday person using Windows 10 at home, who isn't managing a large fleet of computers or part of an organization that would typically procure software licenses through volume licensing agreements.\n\nKey things to understand:\n\nPer Device, Per Year: The $30 price is per device, per year. The ESU program lasts for three years, with the price increasing each year. So, year one is $30, year two is $60, and year three is $120.\nNot for Enterprises: Larger businesses and organizations have different ESU pricing and requirements, often involving volume licensing agreements and potentially higher costs per device. The $30 option is specifically to address the needs of home users who might not want to upgrade to Windows 11 immediately but still need security updates.\nActivation via Microsoft Account: For individuals, the ESU will likely be purchased and linked to their personal Microsoft account, making activation and management simpler for a single user or a few devices.\n\nIn essence, if you're a regular person using Windows 10 at home, not for a business or organization, you're the \"individual\" or \"consumer\" they're targeting with this price.",
        "locale": "en",
        "slug": "technical-posts/how-does-microsoft-define-individual-or-consumer-user",
        "folder": ""
    },
    {
        "title": "japanese-dressing",
        "description": "Japanese dressing",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-10T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "recipe"
        ],
        "content": "For the Dressing:\n  1/4 Cup Sweet Onion\n  2 Piece Celery (2 Tbsp)\n  1 Piece Fresh Ginger (1 Tbsp)\n  2 Piece Carrot (2 Tbsp)\n  1 Tbsp Soy Sauce\n  1+1/2 tsp Tomato Paste\n  1+1/2 tsp Sugar\n  1 tsp Lemon Juice\n  1/2 tsp Salt\n  2 Tbsp Rice Wine Vinegar\n  1 Tbsp Water\n  1/4 Cup Peanut Oil . \n  For the Salad:\n  Icebery Lettuce, torn bite sized\n  Carrot SticksTomato .\n  1 Blend together all ingredients until smooth; serve over salad and enjoy! \n  2 Store extra dressing in a mason jar in the fridge; it will stay fresh for 3-4 days . \n5 minute prep time + 5 minute cook time . Recipe makes 4 servings\n\n=AZbOvnmUhQU3s9JY9tAYMnRAOyVKoiFYz97tupfvzCHg0tT3S5n3THrehVyPP46AHIx0Pr7xGNVROq9sQBNeLxGhPdC4gL1afMDYeoTUo2Yng67-PFDqE2eaGMkBKJK_KiX2QfJDclAZTAOUXtTvQu0Wg3yqNGsZgYiC7donMPe1_12mu2yBdCmtQi7w4xXh8zeZoNfUx04uSM-XWiihLE5Q&tn=KF)#fblifestyle=AZbOvnmUhQU3s9JY9tAYMnRAOyVKoiFYz97tupfvzCHg0tT3S5n3THrehVyPP46AHIx0Pr7xGNVROq9sQBNeLxGhPdC4gL1afMDYeoTUo2Yng67-PFDqE2eaGMkBKJK_KiX2QfJDclAZTAOUXtTvQu0Wg3yqNGsZgYiC7donMPe1_12mu2yBdCmtQi7w4xXh8zeZoNfUx04uSM-XWiihLE5Q&tn=KF)\n\nSee less\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "locale": "en",
        "slug": "technical-posts/japanese-dressing",
        "folder": ""
    },
    {
        "title": "using-javascript-promises",
        "description": "using-javascript-promises",
        "date_created": "2025-05-27T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript",
            "typescript"
        ],
        "content": "Okay, here's a simple TypeScript example of a function that returns a Promise, simulating work with setTimeout, and then examples of how to call it using both traditional .then().catch() and the more modern async/await.\n\n// 1. The function that returns a Promise\nfunction simulateAsyncTask(taskName: string, durationMs: number): Promise {\n    console.log(Starting task: ${taskName} (will take ${durationMs}ms));\n\n    // Return a new Promise\n    return new Promise((resolve, reject) => {\n        // Simulate asynchronous work using setTimeout\n        setTimeout(() => {\n            // For this example, we'll always resolve successfully.\n            // In a real scenario, you might have conditions to reject.\n            if (durationMs  {\n        // This block executes when the promise is resolved (successful)\n        console.log(\"Success (then):\", message);\n        // You can chain more async operations here\n        return simulateAsyncTask(\"Data Processing\", 1000);\n    })\n    .then((nextMessage) => {\n        console.log(\"Success (second then):\", nextMessage);\n    })\n    .catch((error) => {\n        // This block executes if the promise is rejected (error)\n        console.error(\"Error (catch):\", error.message);\n    })\n    .finally(() => {\n        // This block executes regardless of success or failure\n        console.log(\"Operation attempted (finally).\\n\");\n    });\n\n// Example of a task that might fail (to demonstrate .catch)\nsimulateAsyncTask(\"Risky Operation\", -500)\n    .then(message => console.log(\"This won't be reached:\", message))\n    .catch(error => console.error(\"Error from Risky (catch):\", error.message))\n    .finally(() => console.log(\"Risky operation attempted (finally).\\n\"));\n\n\n// 3. Example of calling the function using async/await\n// We need an async function to use 'await'\nasync function runTasksWithAsyncAwait() {\n    console.log(\"--- Using async/await ---\");\n\n    try {\n        console.log(\"Calling first task with await...\");\n        const result1 = await simulateAsyncTask(\"Image Loading\", 1500);\n        // Code here waits for simulateAsyncTask to resolve\n        console.log(\"Success (await 1):\", result1);\n\n        console.log(\"Calling second task with await...\");\n        const result2 = await simulateAsyncTask(\"Rendering UI\", 500);\n        console.log(\"Success (await 2):\", result2);\n\n        console.log(\"Calling a potentially failing task with await...\");\n        const result3 = await simulateAsyncTask(\"Another Risky Op\", -100);\n        console.log(\"This line won't be reached if the above fails:\", result3);\n\n    } catch (error: any) { // Explicitly type error or use 'unknown' and check type\n        // Any rejected promise from an 'await' call will be caught here\n        console.error(\"Error (await catch):\", error.message);\n    } finally {\n        // This block executes regardless of success or failure within the try block\n        console.log(\"Async/await operations attempted (finally).\\n\");\n    }\n}\n\n// Call the async function to execute the async/await examples\n// We need to wait a bit for the .then/.catch examples to run before starting async/await\n// or they will interleave their console logs.\nsetTimeout(() => {\n    runTasksWithAsyncAwait();\n}, 4000); // Wait 4 seconds to let the previous examples mostly finish printing\n\n\n// To run this:\n// 1. Save as promise_example.ts\n// 2. Compile: tsc promise_example.ts\n// 3. Run: node promise_example.js\n\nExplanation:\n\nsimulateAsyncTask(taskName: string, durationMs: number): Promise\n    This function takes a taskName (for logging) and durationMs (to simulate work).\n    It returns Promise, meaning it promises to eventually produce a string value.\n    new Promise((resolve, reject) => { ... }): This is the core of creating a promise.\n        resolve: A function you call when the asynchronous operation is successful. The argument passed to resolve (e.g., result) will be the value the promise fulfills with.\n        reject: A function you call when the asynchronous operation fails. The argument passed to reject (e.g., new Error(...)) will be the reason for failure.\n    setTimeout(() => { ... }, durationMs): This standard JavaScript function simulates a delay. After durationMs, the callback function inside setTimeout is executed.\n    Inside the setTimeout callback, we check if durationMs is valid. If not, we reject the promise. Otherwise, we resolve it with a success message.\n\nCalling with .then().catch().finally()\n    simulateAsyncTask(...).then(successCallback): The successCallback function is executed if and when the promise resolves. It receives the value passed to resolve().\n    .catch(errorCallback): The errorCallback function is executed if and when the promise is rejected. It receives the value passed to reject().\n    .finally(cleanupCallback): The cleanupCallback is executed after the promise is settled (either resolved or rejected). It's useful for cleanup tasks.\n    Chaining: You can chain .then() calls. If a .then() callback returns a new promise, the next .then() in the chain will wait for that new promise to resolve.\n\nCalling with async/await\n    async function runTasksWithAsyncAwait() { ... }: The async keyword before a function declaration allows you to use await inside it. An async function implicitly returns a Promise.\n    const result = await simulateAsyncTask(...): The await keyword pauses the execution of runTasksWithAsyncAwait until the simulateAsyncTask promise settles.\n        If the promise resolves, await returns the resolved value (which is then assigned to result).\n        If the promise rejects, await throws the rejected value (error).\n    try...catch...finally: This standard JavaScript error handling mechanism is used with async/await to handle rejected promises.\n        The try block contains the await calls.\n        The catch block catches any errors thrown by await (i.e., rejected promises).\n        The finally block works just like with synchronous code.\n\nasync/await is generally preferred for writing asynchronous code as it often makes the code look and behave more like synchronous code, which can be easier to read and reason about, especially for complex sequences of asynchronous operations.+",
        "locale": "en",
        "slug": "technical-posts/using-javascript-promises",
        "folder": ""
    },
    {
        "title": "Putting the Pino logger object in the locals object for Sveltekit",
        "description": "Putting the Pino logger object in the locals object for Sveltekit",
        "date_created": "2025-06-07T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "logging",
            "pino",
            "svelte"
        ],
        "content": "Using Pool in Both Locals and Context in SvelteKit\n\nYou cannot directly use Svelte's context API (setContext/getContext) in your hooks.server.ts file. The Svelte context API is component-based and only available within the component tree, while hooks.server.ts operates outside the component system.\n\nHere are your options:\n\nOption 1: Enhanced Local Logger (Recommended)\n\nCreate a request-specific logger in your handle hook that already has the pool injected:\n\nexport const handle: Handle = async ({ event, resolve }) => {\n  // Make the pool available to all server-side endpoints\n  event.locals.pool = pool;\n  \n  // Create an enhanced logger with pool pre-bound\n  event.locals.logger = {\n    ...logger,\n    // Pre-bound methods that don't need pool to be passed each time\n    info: (message: string, data?: any) => logger.info(message, data, pool),\n    error: (message: string, data?: any) => logger.error(message, data, pool),\n    warn: (message: string, data?: any) => logger.warn(message, data, pool),\n    debug: logger.debug, // No change needed since debug doesn't use pool\n    // Keep original methods available too\n    withPool: logger\n  };\n  \n  // Continue processing the request\n  const response = await resolve(event);\n  \n  return response;\n};`\n\nThen in your route handlers:\n\nexport const load = async ({ locals }) => {\n  // No need to pass the pool\n  locals.logger.info('Page loaded', { page: 'read' });\n  \n  // ...rest of your code\n};\n\nOption 2: Singleton Module Pattern\n\nIf you want the pool available across your application without passing it, create a singleton module:\n\nimport type { Pool } from 'pg';\nimport { PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE } from '$env/static/private';\nimport pkg from 'pg';\n\nconst { Pool: PgPool } = pkg;\n\n// Singleton instance\nlet pool: Pool | null = null;\n\n// Initialize the pool\nexport function getPool(): Pool {\n  if (!pool) {\n    pool = new PgPool({\n      host: PG_HOST,\n      port: Number(PG_PORT) || 5432,\n      user: PG_USER,\n      password: PG_PASSWORD,\n      database: PG_DATABASE,\n      ssl: false\n    });\n    \n    // Set up error handler\n    pool.on('error', (err) => {\n      console.error('Unexpected error on idle database client', err);\n    });\n  }\n  \n  return pool;\n}\n\n// Cleanup function for graceful shutdown\nexport async function closePool(): Promise {\n  if (pool) {\n    await pool.end();\n    pool = null;\n    console.log('Database pool closed');\n  }\n}`\n\nThen modify your pino-logger to use this:\n\nimport { getPool } from '$lib/server/db';\n\n// In your logger methods\ninfo: (message: string, data?: any, explicitPool?: Pool) => {\n  pinoLogger.info(data || {}, message);\n  \n  const pool = explicitPool || getPool();\n  if (!dev) {\n    logToDB(pool, 'info', message, data).catch((e) =>\n      pinoLogger.error('Failed to write to DB: ' + e.message)\n    );\n  }\n}\n\nOption 3: For Server Components\n\nIn actual server components (not hooks), you can use the Svelte context API:\n\n\n\n  import { setContext } from 'svelte';\n  import { writable } from 'svelte/store';\n  \n  export let data;\n  \n  // Make pool available via context if needed in server components\n  setContext('pool', data.pool);\n\n\n\n\nThen in server components (not client components):\n\n\n  import { getContext } from 'svelte';\n  import { logger } from '$lib/pino-logger';\n  \n  const pool = getContext('pool');\n  \n  // Now you can use the pool with logger\n  logger.info('Component mounted', { component: 'MyServerComponent' }, pool);\n\n\nOption 1 is the most SvelteKit-idiomatic approach and avoids globals while maintaining type safety.\n\n\n\nThis approach creates a request-specific enhanced logger that has the database pool pre-bound to the logging methods. Let me break down how it works and why it's useful:\n\nHow It Works\n\nevent.locals.logger = {\n  ...logger,\n  info: (message: string, data?: any) => logger.info(message, data, pool),\n  error: (message: string, data?: any) => logger.error(message, data, pool),\n  warn: (message: string, data?: any) => logger.warn(message, data, pool),\n  debug: logger.debug,\n  withPool: logger\n};\n\nObject Spread (...logger): \n   Copies all properties and methods from your original logger\n   Ensures you don't lose any existing functionality\n\nMethod Overrides:\n   Replaces the default methods with new functions\n   These new functions call the original methods but automatically include the pool\n   Creates a partial application/currying of the original functions\n\nUnchanged Methods:\n   debug doesn't need the pool, so it's directly referenced\n   No wrapper function needed\n\nOriginal Logger Access:\n   withPool: logger keeps a reference to the original logger\n   Useful if you need the original behavior in some cases\n\nBenefits\n\nSimplified Usage: You no longer need to pass the pool parameter every time:\n   // Instead of this:\n   locals.logger.info('User logged in', { userId: 123 }, locals.pool);\n   \n   // You can do this:\n   locals.logger.info('User logged in', { userId: 123 });\n\nFewer Errors: Eliminates forgetting to pass the pool parameter\n\nBetter Encapsulation: Implementation details are hidden from route handlers\n\nType Safety: TypeScript still enforces the correct parameter types\n\nFlexibility: Original logger still available via locals.logger.withPool\n\nImplementation in Your Code\n\nTo update your current code:\n\nexport const handle: Handle = async ({ event, resolve }) => {\n  // Make the pool available to all server-side endpoints\n  event.locals.pool = pool;\n\n  // Enhanced logger with pre-bound pool\n  event.locals.logger = {\n    ...logger,\n    info: (message: string, data?: any) => logger.info(message, data, pool),\n    error: (message: string, data?: any) => logger.error(message, data, pool),\n    warn: (message: string, data?: any) => logger.warn(message, data, pool),\n    debug: logger.debug,\n    testDbLogging: () => logger.testDbLogging(pool),\n    withPool: logger // Original logger\n  };\n\n  // Continue processing the request\n  const response = await resolve(event);\n\n  return response;\n};\n\nThis technique is a form of the \"Dependency Injection\" pattern where you're pre-configuring dependencies at a higher level to simplify usage at lower levels.\n\nObject Spread Operator in the Enhanced Logger Configuration\n\nThe ...logger part in the enhanced logger configuration is using JavaScript's object spread syntax to copy all properties and methods from your original logger into the new enhanced logger object.\n\nWhat It Does\n\nWhen you use ...logger inside a new object literal:\n\nevent.locals.logger = {\n  ...logger,\n  // other properties and methods\n};\n\nIt effectively:\n\nCreates a shallow copy of all the original logger's properties and methods\nPreserves all the existing functionality of your original logger\nSets up the foundation before you override or add specific methods\n\nIn Your Enhanced Logger Context\n\nevent.locals.logger = {\n  ...logger,  //  logger.info(message, data, pool),\n  // other methods\n};\n\nWithout the ...logger spread, your new logger object would only have the methods you explicitly define (info, error, warn) and would lose any other methods or properties from the original logger (like db, testDbLogging, etc.).\n\nBy spreading the original logger first, you ensure that any methods you don't explicitly redefine remain accessible in your enhanced logger.\n\nThis pattern gives you the best of both worlds:\nAll the original functionality remains available\nSelected methods are enhanced with the pre-bound pool parameter\n\nreleasing the pool\n\nIn this code, where 'pool' came from 'locals.pool,' you don't need to release the pool\n\nasync function logToDB(pool: Pool, level: string, message: string, data?: any) {\n\ttry {\n\t\tawait pool.query(\n\t\t\t'INSERT INTO app_logs (level, message, data, timestamp) VALUES ($1, $2, $3, NOW())',\n\t\t\t[level, message, data ? JSON.stringify(data) : null]\n\t\t);\n\t} catch (err) {\n\t\tpinoLogger.error({ err }, 'Failed to write log to database');\n\t}\n}\n\nHowever, if you do this, you need to release the client (the above method is the best pattern.)\n\n//  This pattern requires manual release\nasync function logToDB(pool: Pool, level: string, message: string, data?: any) {\n    const client = await pool.connect(); // Get a specific client\n    try {\n        await client.query(\n            'INSERT INTO app_logs (level, message, data, timestamp) VALUES ($1, $2, $3, NOW())',\n            [level, message, data ? JSON.stringify(data) : null]\n        );\n    } finally {\n        client.release(); // Must manually release\n    }\n}",
        "locale": "en",
        "slug": "technical-posts/using-logger-in-locals-with-sveltekit",
        "folder": ""
    },
    {
        "title": "PowerPoint design tips and examples",
        "description": "PowerPoint design tips and examples",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powerpoint"
        ],
        "content": "Questions\n\nWhy is this slide important\nWhat is the slide trying to say\nWhat can be simplified\nWhat should get attention\nWhat looks intentional\n\nConventions\n\nOne slide per message\nUsing images\nIf text is needed, use keywords, not text\nWhat is the presentation's call to action\nShow, don't tell\nDon't overuse animations\nUse consistent design\nUse contrast, size, and color to set audience focus\nNever more than six objects on a slide\nUse a dark background\nStructure\n    Why\n    Clear structure\n    Rehearse\nStructure\n    Compelling intro\n    Content\n        Three key messages\n    Conclusion\n        Summarize the three key messages\nSCQA\n    Situation\n    Complication\n    Question\n    Answer\n\nGood slide examples\n\nThree column table\nFive points around a circle--framework slide\nFive kinds of slides\nWhen to use chart types\nSlide examples\n\nSlide types\n\nTable slide\nChart slide\nSubtitle slide\nFramework slide\nVisual slide",
        "locale": "en",
        "slug": "technical-posts/powerpoint-design-tips-and-examples",
        "folder": ""
    },
    {
        "title": "Passing server-side data to the client in Sveltekit",
        "description": "Passing server-side data to the client in Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-svelte"
        ],
        "content": "This file forwards a page's locale and its URL (without the querystring) as a pageInfo object.\n\n+layout.server.js\n\nexport const load = async (request) => {\n\tlet locale = request.url.pathname.slice(1, 3);\n\t// This is the URL only--it doesn't include the query string.\n\tlet url = request.url.pathname;\n\n\treturn {\n\t\tpageInfo: {\n\t\t\tlocale: locale,\n\t\t\turl: url\n\t\t}\n\t};\n};\n\nWith no +layout.js file\n\nIf the route does not have a +layout.js file this data is forwarded to +page.svelte like this:\n\n+page.svelte\n\n\n\nexport let data\n\n\n\nwhere data value is:\n\ndata: { pageInfo: { locale: 'en', url: '/en/test' } }\n\nWith +layout.js file\n\n+layout.js can forward its own data (otherData) in this case and the data from the +layout.server.js file by spreading its data object.\n\n+layout.js\n\nexport async function load(data) {\n\treturn {\n\t\t...data,\n\t\totherData: {\n\t\t\tname: 'George',\n\t\t\tgroup: 'Beatles'\n\t\t}\n\t};\n}\n\n`+page.svelte\n\nexport data\n\nconst pageInfo = data.data.pageInfo\nconst otherData = data.otherData\n\nFetching the data from both +layout.server.js and +layout.js files requires an looking data.data construct. Considering the page order, it sorta makes sense. The first data object comes from the immediately preceding +layout.js file and then the data.data from +layout.server.js which precedes +layout.js,\n\nVisual it like this:\n\n+layout.server.js               sends 'data' with a 'pageInfo' property\n  +layout.js                    sends 'data' with an 'otherData' property\n    +page.svelte                receives 'data.otherData' and 'data.data.pageInfo'\n\nFor what it's worth\n\nI tried several variations like the one below to collapse the data in the +layout.js file and none of them worked.\n\n+layout.js\n\nexport async function load(data) {\n\treturn {\n\t\tpageInfo: {\n\t\t  locale: data.pageInfo.locale,\n\t\t  url: data.pageInfo.url\n\t\t}\n\t\totherData: {\n\t\t\tname: 'George',\n\t\t\tgroup: 'Beatles'\n\t\t}\n\t};\n}\n\nIt's mysterious to me why the above code didn't work. Dev tools shows that the data object comes into the load function with the pageInfo property.\n\n![[svelte-something.png]]",
        "locale": "en",
        "slug": "technical-posts/passing-data-from-server-to-client-with-sveltekit",
        "folder": ""
    },
    {
        "title": "Merge text files with PowerShell",
        "description": "Merge text files with PowerShell",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "This PowerShell script merges \"include\" files into the main file.\n\nmainfile.txt\n\nThis is line 1.\n\n#include otherfile.txt\n\nThis is line 2.\n\notherfile.txt\n\nThis is text from the other file\n\noutput\n\nThis is line 1.\n\nThis is text from the other file\n\nThis is line 2.\n\nPowerShell script\n\n(This script has a hardwired output file directory.)\n\nparam (\n    Parameter(Mandatory) $input_file\n)\n\n$global:linesCollection = @()\n\nfunction read-file {\n    param (\n        [string] $input_file\n    )\n\nCheck if the file exists\n    if (Test-Path $input_file) {\n        write-host reading\nRead the file line by line\n        Get-Content -Path $input_file | ForEach-Object {\nProcess each line\n            $line = $_\n\n            if ($line -match '^\\s*#include') {\n                $include_file = ($line -replace '^\\s*#include', '').trim()\n                Write-Host $include_file -ForegroundColor yellow\n                read-file -input_file $include_file\n            }\n            else {\nAdd the line to the collection\n                $global:linesCollection += $line\n            }\n        }\n    } else {\n        Write-Output \"File not found at: $input_file\"\n        [Environment]::Exit(1)\n    }\n}\n\npush-location\n\nset-location ..\\release-notes-base-templates\n\n$files = get-childitem *.md\n\nforeach ($file in $files) {\n    read-file -input_file $file.Name\n    $stringArray = [string[]]$linesCollection\n    $stringArray | Set-Content -Path \"..\\tester\\$($file.name)\"\n    $global:linesCollection = @()\n}\n\npop-location",
        "locale": "en",
        "slug": "technical-posts/merge-text-files-with-powershell",
        "folder": ""
    },
    {
        "title": "redefine console.log",
        "description": "redefine console.log",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "const originalLog = console.log;\n\n    console.log = function() {\n      const stack = new Error().stack;\n      const callerLine = stack?.split('\\n')[2];\n      //const parts = callerLine?.match(/at (.) \\((.):(\\d+):(\\d+)\\)/);\n      const parts = extractInfo(callerLine)\n\n      //let prefix = '';\n      let prefix = ${parts.filename}:${parts.position}\\n;\n\n      originalLog.apply(console, [prefix, ...arguments]);\n    };\n\n  }",
        "locale": "en",
        "slug": "technical-posts/redefine-console-log",
        "folder": ""
    },
    {
        "title": "Creating a packaged C# EXE for deployment/publish",
        "description": "Creating a packaged C# EXE for deployment/publish",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "csharp"
        ],
        "content": "With self-contained deployment it should bundle all the .NET FW stuff up--but I've had trouble with that. And it's probably silly if you are going to deploy more than one app on the target PC. I am using the other option below now.\n![[cs-profile-settings-for-single-file-deployment.png|497]]\n\nThis is a pretty cool option. If the target .NET FW isn't installed on the target PC, you are prompted to install it. This worked well deploying things to Mothra.\n![[Creating a packaged Csharp EXE for deployment-1.png|500]]",
        "locale": "en",
        "slug": "technical-posts/creating-a-packaged-csharp-exe-for-deployment",
        "folder": ""
    },
    {
        "title": "Recursion in Rust",
        "description": "Recursion in Rust",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "Does Rust support recursion\n\nYes, Rust supports recursion. You can define functions that call themselves to solve problems. Recursion in Rust works similarly to other languages, but Rust's emphasis on safety and performance means there are some considerations to keep in mind.\n\nKey Points About Recursion in Rust:\n\nFunction Definition:\n\n    You can define a recursive function just like in other languages. The function calls itself with different arguments until a base case is reached.\n\nBase Case:\n\n    Like in other languages, recursion needs a base case to terminate. Without a base case, recursion can lead to infinite loops and stack overflow errors.\n\nStack Size:\n\n    Rust does not automatically optimize recursion as some languages do (e.g., tail-call optimization). This means that deep recursion can lead to a stack overflow if the recursion depth is too great. For very deep recursion, consider iterative solutions or use Rust's std::collections::VecDeque for a stack-like structure.\n\nTail Recursion:\n    Rust does not guarantee tail-call optimization (TCO), which means that even if your recursive call is in the tail position, it might not be optimized away. However, writing tail-recursive functions is still a good practice for readability and can help if you decide to use TCO in a language that supports it.\n\nExample of Recursion:\n\nFactorial Function:\n\nHeres a simple example of a recursive function to calculate the factorial of a number:\n\nfn factorial(n: u64) -> u64 {\n    if n == 0 {\n        1 // Base case: factorial of 0 is 1\n    } else {\n        n * factorial(n - 1) // Recursive case\n    }\n}\n\nfn main() {\n    let number = 5;\n    println!(\"Factorial of {} is {}\", number, factorial(number));\n}\n\nIn this example:\n\nThe base case is when n is 0, where the function returns 1.\nFor other values, the function calls itself with n - 1 and multiplies the result by n.\n\nHandling Deep Recursion:\n\nFor problems that may require deep recursion, consider these alternatives:\n\nIterative Solutions: Convert the recursive solution to an iterative one, which often avoids stack overflow issues.\nManual Stack Management: Use data structures like Vec or VecDeque to manage the stack manually.\n\nExample of an Iterative Approach:\n\nHere's an iterative approach to the factorial problem:\n\nfn factorial_iterative(n: u64) -> u64 {\n    let mut result = 1;\n    for i in 1..=n {\n        result *= i;\n    }\n    result\n}\n\nfn main() {\n    let number = 5;\n    println!(\"Factorial of {} is {}\", number, factorial_iterative(number));\n}\n\nThis approach uses a loop to compute the factorial, which avoids potential issues with recursion depth.\n\nSummary:\n\nRust supports recursion and you can use it to solve problems naturally.\nEnsure you have a base case to prevent infinite recursion.\nFor deep recursion, consider using iterative solutions or manually managing recursion with stack-like structures to avoid stack overflow.",
        "locale": "en",
        "slug": "technical-posts/recursion-in-rust",
        "folder": ""
    },
    {
        "title": "Good looking web pages",
        "description": "Good looking web pages",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "design",
            "css"
        ],
        "content": "https://dbschema.com/\nhttps://sendgrid.com/en-us\nhttps://codingwithpatrik.dev/",
        "locale": "en",
        "slug": "technical-posts/inspiration-for-good-web-pages",
        "folder": ""
    },
    {
        "title": "SvelteKit global error handling",
        "description": "SvelteKit global error handling",
        "date_created": "2025-06-14T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "Of course! This is an excellent question, as robust error handling is crucial for any production application.\n\nIn SvelteKit 2 (which uses Svelte 5 by default), the \"best way\" to add global error handling is a layered approach that combines three key features:\n\n+error.svelte: The user-facing page that displays when an error occurs.\nhooks.server.js: A server-side hook to intercept, log, and process errors before they are sent to the user. This is your primary tool for server-side global handling.\nhooks.client.js: A client-side hook to catch errors that happen exclusively in the browser after hydration.\n\nLet's break down how to use them together for a complete solution.\n\nThe Core Strategy\n\n| File | Purpose | When it Runs |\n| :--- | :--- | :--- |\n| src/routes/+error.svelte | Displays the error to the user. | When an error is thrown on the server, or you navigate to a non-existent route. |\n| src/hooks.server.js | Catches & Logs all unhandled server-side errors. | On the server, before +error.svelte is rendered. Ideal for logging to services like Sentry, Axiom, etc. |\n| src/hooks.client.js | Catches & Logs all unhandled client-side errors. | In the browser, for errors that occur after the initial page load (e.g., in an event handler). |\n\nPart 1: The User-Facing Error Page (+error.svelte)\n\nThis is the component your users will see. You should create a root-level one to catch all errors across your application.\n\nsrc/routes/+error.svelte\n\n\n\timport { page } from '$app/stores';\n\n\t// The $page store contains the status code and error object\n\tconst status = $page.status;\n\tconst error = $page.error;\n\n\n\n\t{status}\n\n\t{#if error?.message}\n\t\t{error.message}\n\t{:else}\n\t\tSomething went wrong.\n\t{/if}\n\n\t{#if status === 404}\n\t\tWe couldn't find the page you were looking for.\n\t{:else if status >= 500}\n\t\tOur team has been notified of the problem. Please try again later.\n\t{/if}\n\n\tGo to Homepage\n\n\n\n\t.error-container {\n\t\ttext-align: center;\n\t\tmargin-top: 5rem;\n\t}\n\th1 {\n\t\tfont-size: 4rem;\n\t\tcolor: #ff3e00;\n\t}\n\n\nThis component automatically receives the status and error object through the $page store.\n\nPart 2: Global Server-Side Handling (hooks.server.js)\n\nThis is the most important part for global backend error handling. The handleError hook runs for any unhandled error on the server (in load functions, form actions, API routes, or rendering).\n\nThis is the perfect place to:\nLog the full error details (including stack trace) to a logging service.\nScrub sensitive information from the error before it's sent to the client.\nSend notifications to your team (e.g., via Slack or email).\n\nsrc/hooks.server.js (or .ts)\n\n// @ts-check\nimport { dev } from '$app/environment';\n\n/**\n This function is the global error handler for the server.\n @type {import('@sveltejs/kit').HandleServerError}\n */\nexport async function handleError({ error, event }) {\n\t// error is the actual error object\n\t// event is the SvelteKit event object\n\tconst errorId = crypto.randomUUID();\n\n\t// 1. Log the error\n\t// In a real app, you'd send this to Sentry, LogRocket, etc.\n\tconsole.error('Unhandled Server Error:', {\n\t\terrorId,\n\t\terror,\n\t\turl: event.url.pathname,\n\t\t// You can add user info if available:\n\t\t// userId: event.locals.user?.id\n\t});\n\n\t// 2. Return a \"sanitized\" error object for the client\n\t// This prevents leaking sensitive implementation details.\n\t// The message property will be available in $page.error.message\n\t// on the +error.svelte page.\n\tconst publicError = {\n\t\tmessage: An unexpected error occurred. Please quote this ID if you contact support: ${errorId}.,\n\t\terrorId\n\t};\n\n\t// For developers, still show the full error in development\n\tif (dev) {\n\t\treturn {\n\t\t\t...publicError,\n\t\t\tmessage: /** @type {Error} */ (error).message,\n\t\t\tstack: /** @type {Error} */ (error).stack\n\t\t};\n\t}\n\n\treturn publicError;\n}\n\nNow, any unexpected server crash will be logged, and the user will see the friendly message from publicError on your +error.svelte page instead of a scary stack trace.\n\nPart 3: Global Client-Side Handling (hooks.client.js)\n\nThis hook catches errors that happen only in the browser after the application has been hydrated. These errors do not trigger the +error.svelte page because the app is already running.\n\nThis is the place to report client-side issues to your logging service.\n\nsrc/hooks.client.js (or .ts)\n\n// @ts-check\n\n/** @type {import('@sveltejs/kit').HandleClientError} */\nexport async function handleError({ error, event }) {\n\tconst errorId = crypto.randomUUID();\n\n\t// In a real app, you'd send this to a client-side error tracker\n\tconsole.error('Unhandled Client Error:', {\n\t\terrorId,\n\t\terror,\n\t\turl: event.url.pathname\n\t});\n\n\t// You can optionally show a toast notification or a modal to the user\n\t// For example, using a custom store:\n\t// import { userAlerts } from '$lib/stores/alerts';\n\t// userAlerts.show('An error occurred. Please refresh the page.');\n\n\treturn {\n\t\tmessage: A client-side error occurred. Ref: ${errorId},\n\t\terrorId\n\t};\n}\n\nHow to Intentionally Trigger Errors (The \"Right\" Way)\n\nWhen you need to stop execution and show an error page for predictable reasons (like \"Not Found\" or \"Permission Denied\"), don't just throw new Error(). Instead, use the error helper from SvelteKit.\n\nThis signals to SvelteKit that it's an expected error, and it will gracefully render your +error.svelte page with the correct status code.\n\nExample in a load function:\n\nsrc/routes/posts/[slug]/+page.server.js\n\nimport { error } from '@sveltejs/kit';\nimport { getPost } from '$lib/server/database';\n\n/** @type {import('./$types').PageServerLoad} */\nexport async function load({ params, locals }) {\n\t// Example: Permission check\n\tif (!locals.user) {\n\t\t// 401 Unauthorized\n\t\tthrow error(401, 'You must be logged in to view this post.');\n\t}\n\n\tconst post = await getPost(params.slug);\n\n\tif (!post) {\n\t\t// 404 Not Found\n\t\tthrow error(404, 'Post not found');\n\t}\n\n\treturn { post };\n}\n\nKey Difference:\nthrow error(404, ...): Expected error. Skips handleError hook, goes directly to +error.svelte. This is correct for 4xx status codes.\nthrow new Error(...): Unexpected error. Is caught by handleError hook, then goes to +error.svelte. This is what happens when your code truly breaks.\n\nSummary: The Complete Workflow\n\nCreate src/routes/+error.svelte: Design your beautiful, user-friendly error page.\nCreate src/hooks.server.js: Implement the handleError function to log all unexpected backend crashes and sanitize the error message for the user.\nCreate src/hooks.client.js: (Optional but recommended) Implement handleError to catch and log errors that only happen in the browser.\nUse throw error(status, message) in your load functions and form actions for all expected error conditions like 404s, 403s, etc.\nLet your hooks.server.js automatically catch all other unexpected exceptions.\n\nThis layered approach ensures that you have complete, robust, and secure error handling for your SvelteKit 2 application.",
        "locale": "en",
        "slug": "technical-posts/sveltekit-global-error-handling",
        "folder": ""
    },
    {
        "title": "Extending a VM's disk size",
        "description": "Extending a VM's disk size",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "hyper-v"
        ],
        "content": "These instructions worked well\n\nWhen you extend a VM disk, you first change its disk size properties from the host. This doesn't allocate the new diskspace, it only stakes a claim to it.\nAfter changing the disk size value, start the VM and open DISKMGMT.MSC to see the new partition. If there is a \"Recovery\" partition against the primary partition and the new one, follow the steps below. (other wise, just extending the volume).\nbut...\nIf there is a \"Health (Recovery Partition),\" the newly added volume of the disk will be after that recovery partition.\nIn the image below, the new, unallocated disk volume appeared after the recovery partition.\n![[Extending a VM's disk size.png|500]]\nRecovery partitions can be deleted. But you need to use the override option to do so.\nList the partitions to show their numbers.\nSelect the recovery partition number.\nThen delete the recovery partition.\n\ndelete partition override\n\nThat should make the existing volume and the new volume contiguous. Right-click the new volume to extend it.",
        "locale": "en",
        "slug": "technical-posts/extending-a-vms-disk-size",
        "folder": ""
    },
    {
        "title": "Postgres SQL",
        "description": "Postgres SQL",
        "date_created": "2025-06-14T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "CREATE TABLE downloads (\n  id INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  date_requested timestamp,\n  name varchar(120),\n  company varchar(120),\n  country varchar(60),\n  email_address varchar(120),\n  product varchar(120),\n  family varchar(60)\n);\n\n-- To set the starting value for the auto-incrementing ID:\nALTER SEQUENCE downloads_id_seq RESTART WITH 1023;",
        "locale": "en",
        "slug": "technical-posts/postgres-sql",
        "folder": ""
    },
    {
        "title": "Restore old context menu on Windows 11",
        "description": "Restore old context menu on Windows 11",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows"
        ],
        "content": "Restore old context menu on Windows 11\n\nPut this is a text file with a .reg extension and double-click it.\n\nWindows Registry Editor Version 5.00\n\n[HKEY_CURRENT_USER\\Software\\Classes\\CLSID\\{86ca1aa0-34aa-4e8b-a509-50c905bae2a2}\\InprocServer32]\n@=\"\"",
        "locale": "en",
        "slug": "technical-posts/restore-old-context-menu-in-windows-11",
        "folder": ""
    },
    {
        "title": "ASNA DataGate Component Suite DCS examples",
        "description": "ASNA DataGate Component Suite DCS examples",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna"
        ],
        "content": "E:\\jerrybutler-disk\\Contents\\Devo\\VSProjects\\CS\\Other\\DCSReadRecords\nE:\\jerrybutler-disk\\Contents\\Devo\\VSProjects\\VR\\Other\\DCSAutoCompleteItem\nE:\\jerrybutler-disk\\Archives\\C\\RollingRock\\F_Code\\ClassArchive\\avradvancedUK\\StateListDynamic",
        "locale": "en",
        "slug": "technical-posts/examples",
        "folder": ""
    },
    {
        "title": "get podman running",
        "description": "get podman running",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres",
            "containers"
        ],
        "content": "Get podman running\n\nHelpful YouTube video\nhttps://www.youtube.com/watch?v=sqJLvXzLgA8\n\nSet-VMProcessor -VMName  -ExposeVirtualizationExtensions $true\n\nCreate a named volume with Podman\n\npodman volume create postgres_data\n\nOn Windows, containers won't run with bind mounted folders because of permissions:\nUsing a named volume resolves this issue. because the postgres user inside the container will have the necessary permissions to chmod the data directory within the Podman-managed volume. Your data will still be persistent across container restarts, it just won't be directly visible under your Windows user folders. See [[podman named volumes]] for more info.\nUsing the named value, this starts the Postgres container\n\npodman run --rm -d -p 5432:5432 -v postgres_data:/var/lib/postgresql/data --name postgresdb -e POSTGRES_PASSWORD=roger postgres\n\nadd this at the end to persist logging info\n\n-c logging_collector=on\n\nStart psql command line\n\npodman exec -it -u postgres postgresdb psql\n\nStop and remove containers\n\npodman stop postgresdb\npodman rm postgresdb\n\nShow all containers\n\npodman ps -a\n\n\nuse exit command then \\q to quit.\n\ncomposer file\n\nThis isn't tested but converts the Run command to a composer file.\n\nversion: '3.8' # Specify Compose file version (use 3.8 or newer for best features)\n\nservices:\n  postgresdb: # This is the service name\n    image: postgres:latest # Specifies the image to use (\n    container_name: postgresdb\n    ports:\n      \"5432:5432\" # Maps host port 5432 to container port 5432 (maps to -p)\n    volumes:\n      postgres_data:/var/lib/postgresql/data # Mounts the named volume (maps to -v)\n    environment:\n      POSTGRES_PASSWORD: roger # Sets environment variables (maps to -e)\n\nvolumes:\n  postgres_data: # Defines the named volume used in the service\n    driver: local # Specifies the volume driver (optional, 'local' is the default)\n\npodman compose -f compose-file-name.yaml\n\nTo connect\n\nhost: localhost\ndatabase: postgres\nuser: postgres\npassword: roger\nport: 5432",
        "locale": "en",
        "slug": "technical-posts/get-podman-running",
        "folder": ""
    },
    {
        "title": "Creating a class in Rust",
        "description": "Creating a class in Rust",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "In Rust, you create a new class using the struct keyword, which defines a structure that can contain fields. You can then implement methods for the struct using the impl keyword. Here's a basic example of creating a new struct (class) in Rust:\n\nExample: Creating a Struct\n\nLet's create a simple Rectangle struct with fields for width and height, and implement methods to calculate the area and perimeter.\n\n// Define the Rectangle struct\nstruct Rectangle {\n    width: f64,\n    height: f64,\n}\n\n// Implement methods for Rectangle\nimpl Rectangle {\n    // Method to calculate area\n    fn area(&self) -> f64 {\n        self.width * self.height\n    }\n\n    // Method to calculate perimeter\n    fn perimeter(&self) -> f64 {\n        2.0 * (self.width + self.height)\n    }\n}\n\nfn main() {\n    // Create a new instance of Rectangle\n    let rect = Rectangle {\n        width: 5.0,\n        height: 3.0,\n    };\n\n    // Call the methods\n    println!(\"Area: {}\", rect.area());\n    println!(\"Perimeter: {}\", rect.perimeter());\n}\n\nExplanation\n\nStruct Definition:\n\n    The struct Rectangle defines a new data structure with two fields: width and height, both of type f64.\n\nImplementing Methods:\n\n    The impl Rectangle block is where we define methods associated with the Rectangle struct.\n    The &self parameter is a reference to the instance of the struct, allowing access to its fields.\n\nCreating an Instance:\n\n    In the main function, we create an instance of Rectangle with specific width and height values.\n\nUsing Methods:\n    We call the area and perimeter methods on the rect instance to get the area and perimeter of the rectangle.\n\nSummary\n\nIn Rust, structs serve a similar purpose to classes in other languages, allowing you to group related data together. You can define methods on structs to encapsulate behavior, promoting code organization and reusability.",
        "locale": "en",
        "slug": "technical-posts/creating-a-class-in-rust",
        "folder": ""
    },
    {
        "title": "read-excel-column-schema-with-csharp",
        "description": "read-excel-column-schema-with-csharp",
        "date_created": "2025-10-20T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "excel",
            "csharp"
        ],
        "content": "using ClosedXML.Excel;\nusing System;\nusing System.Linq;\n\npublic class SpreadsheetReader\n{\n    public static void ReadSpreadsheetColumns(string spreadsheetName)\n    {\n        try\n        {\n            using (var workbook = new XLWorkbook(spreadsheetName))\n            {\n                // Assuming you want to read the first worksheet\n                var worksheet = workbook.Worksheet(1); \n\n                // Get the used range to iterate through columns that have data\n                var usedRange = worksheet.RangeUsed();\n\n                Console.WriteLine($\"Reading spreadsheet: {spreadsheetName}\");\n                Console.WriteLine(\"-------------------------------------\");\n\n                // Iterate through columns in the used range\n                foreach (var column in usedRange.ColumnsUsed())\n                {\n                    // Get the column number\n                    int columnNumber = column.ColumnNumber();\n\n                    // Get the header cell value (assuming the first row is the header)\n                    // If your spreadsheet doesn't have a header, you might want to use a different way to name columns.\n                    string columnName = worksheet.Cell(1, columnNumber).GetValue();\n\n                    // Get a cell from this column to inspect its data type.\n                    // We'll pick the first cell with a value after the header, if it exists.\n                    var firstDataCell = column.CellsUsed(c => c.Address.RowNumber != 1).FirstOrDefault();\n\n                    string excelDataType = \"N/A\";\n                    if (firstDataCell != null)\n                    {\n                        // ClosedXML provides the DataType property which maps to Excel's data types\n                        excelDataType = firstDataCell.DataType.ToString();\n                    }\n\n                    Console.WriteLine($\"Column Name: {columnName}, Excel Data Type: {excelDataType}\");\n                }\n            }\n        }\n        catch (Exception ex)\n        {\n            Console.WriteLine($\"An error occurred: {ex.Message}\");\n        }\n    }\n\n    public static void Main(string[] args)\n    {\n        // Replace \"YourSpreadsheet.xlsx\" with the actual path to your spreadsheet file\n        string spreadsheetName = \"YourSpreadsheet.xlsx\"; \n\n        // Create a dummy spreadsheet for testing if you don't have one\n        CreateDummySpreadsheet(spreadsheetName);\n\n        ReadSpreadsheetColumns(spreadsheetName);\n    }\n\n    private static void CreateDummySpreadsheet(string filename)\n    {\n        using (var workbook = new XLWorkbook())\n        {\n            var worksheet = workbook.Worksheet(\"Sheet1\");\n\n            // Add headers\n            worksheet.Cell(\"A1\").Value = \"Name\";\n            worksheet.Cell(\"B1\").Value = \"Age\";\n            worksheet.Cell(\"C1\").Value = \"Date Joined\";\n            worksheet.Cell(\"D1\").Value = \"Is Active\";\n            worksheet.Cell(\"E1\").Value = \"Salary\";\n            worksheet.Cell(\"F1\").Value = \"\"; // An empty column\n\n            // Add data\n            worksheet.Cell(\"A2\").Value = \"Alice\";\n            worksheet.Cell(\"B2\").Value = 30;\n            worksheet.Cell(\"C2\").Value = new DateTime(2020, 1, 15);\n            worksheet.Cell(\"D2\").Value = true;\n            worksheet.Cell(\"E2\").Value = 50000.50;\n\n            worksheet.Cell(\"A3\").Value = \"Bob\";\n            worksheet.Cell(\"B3\").Value = 25;\n            worksheet.Cell(\"C3\").Value = new DateTime(2021, 5, 20);\n            worksheet.Cell(\"D3\").Value = false;\n            worksheet.Cell(\"E3\").Value = 45000;\n\n            workbook.SaveAs(filename);\n            Console.WriteLine($\"Dummy spreadsheet '{filename}' created for testing.\");\n        }\n    }\n}",
        "locale": "en",
        "slug": "technical-posts/read-excel-column-schema-with-csharp",
        "folder": ""
    },
    {
        "title": "Using Zod with forms",
        "description": "Using Zod with forms",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "This is a good link to how to use Zod with JS Doc\nhttps://blog.jim-nielsen.com/2023/types-in-jsdoc-with-zod/\n\nI used it like this at the of a +page.svelte file that has a form:\n\nI'm not sure it buys me anything!\n\n\n\timport { superForm } from 'sveltekit-superforms/client';\n\timport SuperDebug from 'sveltekit-superforms/client/SuperDebug.svelte';\n\n\timport { z } from 'zod';\n\n\tconst contactSchema = z.object({\n\t\tname: z.string().min(3),\n\t\tcompany: z.string().min(3),\n\t\taddress: z.string().email(),\n\t\tsendupdates: z.boolean().default(false)\n\t});\n\n\t// Extract the inferred type as a JSDoc type\n\t/** @typedef { z.infer data } */\n\n\texport let data;\n\n\tconst { form } = superForm(data.form);\n",
        "locale": "en",
        "slug": "technical-posts/using-forms",
        "folder": ""
    },
    {
        "title": "Find all occurrences of 'console.log' that aXren't commented out",
        "description": "Find all occurrences of 'console.log' that aXren't commented out",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "vs-code"
        ],
        "content": "Find all occurrences of console.log that aren't commented out.\n\n^\\s*console.log\n\nworkbench.editor.labelformat to show a file's directory in the editor tab.\n\nAdd a new tag with a value with VS Code to all markdown files\n\n![[Pasted image 20240112121554.png]]",
        "locale": "en",
        "slug": "technical-posts/visual-studio-code-search-technique",
        "folder": ""
    },
    {
        "title": "mdsvex-markdown-sveltekit",
        "description": "Using mdsved with SvelteKit",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "Yes, MDsveX is still considered the best and most popular way to use Markdown with SvelteKit 2. It integrates seamlessly with SvelteKit's preprocessor system, allowing you to write Svelte components directly within your Markdown files and use front matter for metadata.\n\nHere's why it remains a top choice:\n\nSvelte in Markdown: MDsveX allows you to embed Svelte components directly into your Markdown, making it incredibly powerful for creating interactive content within static pages.\nFront Matter Support: It has built-in support for YAML front matter, which is essential for defining metadata like title, date, author, and more for your Markdown pages.\nSyntax Highlighting: Integrates well with remark-prism or other syntax highlighting libraries to make code blocks look great.\nExtensibility: Being built on top of remark and rehype, you can extend its functionality with a vast ecosystem of plugins for things like table of contents generation, image optimization, and more.\nActive Maintenance: It's well-maintained and keeps up with SvelteKit's advancements.\n\nWhile there might be other experimental or niche solutions, MDsveX is the most mature, feature-rich, and widely adopted choice for this specific use case.\n\nIf you're looking to set it up, you'll typically:\n\nInstall mdsvex and svelte-preprocess.\nConfigure svelte.config.js to include the MDsveX preprocessor.\nCreate .mdsvex files in your src/routes directory.\n\nHere's a quick example of a simple svelte.config.js setup for MDsveX:\n\nimport adapter from '@sveltejs/adapter-auto';\nimport { vitePreprocess } from '@sveltejs/vite-plugin-svelte';\nimport { mdsvex } from 'mdsvex';\n\n/** @type {import('@sveltejs/kit').Config} */\nconst config = {\n    extensions: ['.svelte', '.md', '.svx'], // Add .md and .svx extensions\n\n    preprocess: [\n        vitePreprocess(),\n        mdsvex({\n            extensions: ['.md', '.svx'],\n            layout: {\n                blog: './src/lib/layouts/blog-layout.svelte' // Example for a layout\n            }\n        })\n    ],\n\n    kit: {\n        adapter: adapter()\n    }\n};\n\nexport default config;\n\nAnd then an example .md file:\n\ntitle: My Awesome Blog Post\ndate: 2023-10-27\nauthor: AI Assistant\n\n\nThis is a paragraph in my SvelteKit blog post.\n\n\n    import MyComponent from '$lib/components/MyComponent.svelte';\n\n\n\n\nHere's some more content.\n\nWould you like me to generate a simple visual representation of how MDsveX processes a Markdown file into a SvelteKit page?",
        "locale": "en",
        "slug": "technical-posts/mdsvex-markdown-sveltekit",
        "folder": ""
    },
    {
        "title": "Passing async-fetched data to a component",
        "description": "Passing async-fetched data to a component",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "This fixes the issue I had passing async-fetched to a component.\n\n\n\n    // @ts-nocheck\n\n    import { onMount } from 'svelte';\n    import {get_release_set} from '$lib/data-repo.js'\n    import FilenameCompare2 from './FilenameCompare2.svelte';\n    import { writable } from 'svelte/store';\n\n    let release_set_rows = []\n    let prev_release_set_rows= []\n    let prev_rows_store = writable([]);\n\n    async function get_data(family_id, release_date,prev_family_id, prev_release_date) {\n        release_set_rows = await get_release_set(family_id, release_date)\n\n        const prev_release_set_rows = await get_release_set(prev_family_id, prev_release_date)\n        //console.log('prev_release_set_rows', prev_release_set_rows)\n\n        const rows = [...prev_release_set_rows.reduce((acc, row) => {\n            acc.push({id: row.id,\n                    family_id: row.family_id,\n                    release_date: row.release_date,\n                    product_id: row.product_id,\n                    binary_file: row.binary_filename,\n                    readme_file: row.readme_filename})\n            return acc\n        }, [])]\n\n        $prev_rows_store = rows\n        // alternatively:\n        //prev_rows_store.set(rows);\n    }\n\n    async function button_one() {\n        await get_data(132, '2024-08-04', 132, '2024-07-17')\n    }\n\n    async function button_two() {\n        await get_data(139, '2024-05-09', 139, '2024-03-25')\n    }\n\n    \n\n    Test Page\n\n    ONE | TWO\n\n    {#each release_set_rows as row, index (row.product_id)}\n        {row.id} {row.product_name} {row.binary_filename ?? ''}\n        \n        \n    {/each}\n\n\n\timport YesNoColumn from \"$lib/components/YesNoColumn.svelte\";\n    import { onMount } from 'svelte';\n    import FilenameCompare2 from './FilenameCompare2.svelte';\n    import { get } from 'svelte/store';\n\n    //export let export_rows\n    export let filename\n    export let filetype = 'binary'\n    export let product_id\n    export let prev_rows_store\n\n    let prev_filename = ''\n\n    function show_rows(prev_rows) {\n        const prev_row = prev_rows.find(row => row.product_id == product_id)\n        if (prev_row == undefined) {\n            return ''\n        }\n\n        if (filetype != 'binary' ) {\n            return prev_row.readme_file ?? ''\n        }\n\n        return prev_row.binary_file ?? ''\n    }\n\n    // This is weird and I hope runes makes it better.\n    // this is like a change event that is raised when the\n    // prev_rows_store changes. In this case, the parent\n    // component is changes.\n    $: {\n        // if prev_filename is empty:\n        //   no change\n        // if prev_filename is not empty and prev_filename != filename\n        //   file changed\n        // else\n        //   no change\n\n        prev_filename = show_rows($prev_rows_store)\n        // alternatively:\n        // prev_rows_store.subscribe(row => {\n        //     prev_filename = show_rows(row)\n        // });\n    }\n\n    onMount( () => {\n    })\n\n\n\n    {prev_filename}\n",
        "locale": "en",
        "slug": "technical-posts/passing-async-fetched-data-to-a-component",
        "folder": ""
    },
    {
        "title": "A Vite search-and-replace plugin",
        "description": "A Vite search-and-replace plugin",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-17T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "vite"
        ],
        "content": "Two options for a Vite plugin that performs a simple search and replace during render time:\n\n/**\n Vite plugin for search and replace during build/dev\n @param {Object} options - Configuration options\n @param {Array} options.replacements - Array of search/replace pairs\n @param {Array} options.include - File patterns to include (default: all)\n @param {Array} options.exclude - File patterns to exclude\n @returns {import('vite').Plugin}\n */\nexport function searchReplacePlugin(options = {}) {\n\tconst {\n\t\treplacements = [],\n\t\tinclude = [/\\.(svelte|js|ts|html|css)$/],\n\t\texclude = [/node_modules/]\n\t} = options;\n\n\treturn {\n\t\tname: 'vite-plugin-search-replace',\n\t\t\n\t\tenforce: 'pre', // Run before other plugins\n\t\t\n\t\ttransform(code, id) {\n\t\t\t// Check if file should be processed\n\t\t\tconst shouldInclude = include.some(pattern => \n\t\t\t\tpattern instanceof RegExp ? pattern.test(id) : id.includes(pattern)\n\t\t\t);\n\t\t\t\n\t\t\tconst shouldExclude = exclude.some(pattern =>\n\t\t\t\tpattern instanceof RegExp ? pattern.test(id) : id.includes(pattern)\n\t\t\t);\n\t\t\t\n\t\t\tif (!shouldInclude || shouldExclude) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\t\n\t\t\t// Perform replacements\n\t\t\tlet transformedCode = code;\n\t\t\tlet hasChanges = false;\n\t\t\t\n\t\t\tfor (const { search, replace } of replacements) {\n\t\t\t\tconst originalCode = transformedCode;\n\t\t\t\t\n\t\t\t\tif (search instanceof RegExp) {\n\t\t\t\t\ttransformedCode = transformedCode.replace(search, replace);\n\t\t\t\t} else {\n\t\t\t\t\ttransformedCode = transformedCode.replaceAll(search, replace);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tif (originalCode !== transformedCode) {\n\t\t\t\t\thasChanges = true;\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t\t// Return transformed code with source map\n\t\t\tif (hasChanges) {\n\t\t\t\treturn {\n\t\t\t\t\tcode: transformedCode,\n\t\t\t\t\tmap: null // Set to null for simplicity, or generate proper source map\n\t\t\t\t};\n\t\t\t}\n\t\t\t\n\t\t\treturn null;\n\t\t}\n\t};\n}\n\nUsage in vite.config.js:\n\nimport { sveltekit } from '@sveltejs/kit/vite';\nimport { defineConfig } from 'vite';\nimport { searchReplacePlugin } from './vite-plugins/search-replace.js';\n\nexport default defineConfig({\n\tplugins: [\n\t\tsearchReplacePlugin({\n\t\t\treplacements: [\n\t\t\t\t{\n\t\t\t\t\tsearch: 'IBM&nbsp;i',\n\t\t\t\t\treplace: 'IBM i'\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tsearch: /ASNA's/g,\n\t\t\t\t\treplace: 'ASNA\\'s'\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tsearch: '{{YEAR}}',\n\t\t\t\t\treplace: new Date().getFullYear().toString()\n\t\t\t\t}\n\t\t\t],\n\t\t\tinclude: [/\\.svelte$/, /\\.js$/],\n\t\t\texclude: [/node_modules/, /\\.config\\./]\n\t\t}),\n\t\tsveltekit()\n\t]\n});`\n\nAlternative: Simpler version with just string replacements:\n\nexport function simpleSearchReplace(replacements) {\n\treturn {\n\t\tname: 'simple-search-replace',\n\t\t\n\t\ttransform(code, id) {\n\t\t\t// Only process Svelte files\n\t\t\tif (!id.endsWith('.svelte')) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\t\n\t\t\tlet result = code;\n\t\t\t\n\t\t\t// Apply all replacements\n\t\t\tObject.entries(replacements).forEach(([search, replace]) => {\n\t\t\t\tresult = result.replaceAll(search, replace);\n\t\t\t});\n\t\t\t\n\t\t\t// Only return if changes were made\n\t\t\tif (result !== code) {\n\t\t\t\treturn { code: result, map: null };\n\t\t\t}\n\t\t\t\n\t\t\treturn null;\n\t\t}\n\t};\n}\n\nSimple usage:\n\nimport { sveltekit } from '@sveltejs/kit/vite';\nimport { defineConfig } from 'vite';\nimport { simpleSearchReplace } from './vite-plugins/search-replace-simple.js';\n\nexport default defineConfig({\n\tplugins: [\n\t\tsimpleSearchReplace({\n\t\t\t'IBM&nbsp;i': 'IBM i',\n\t\t\t'{{COMPANY}}': 'ASNA',\n\t\t\t'{{YEAR}}': new Date().getFullYear().toString()\n\t\t}),\n\t\tsveltekit()\n\t]\n});\n\nFeatures:\n\nBuild-time replacement: Runs during both dev and build\nFile filtering: Can include/exclude specific file types\nRegExp support: Supports both string and regex patterns\nNo runtime overhead: Replacements happen at build time\nSource map compatible: Can be extended to maintain source maps\n\nThe plugin runs during the transform phase, which means it processes files as they're being bundled, making it efficient for both development and production builds.",
        "locale": "en",
        "slug": "technical-posts/vite-search-and-replace-plugin",
        "folder": ""
    },
    {
        "title": "A Rust function that returns a Boolean",
        "description": "A Rust function that returns a Boolean",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "If you want a function to return a boolean value indicating success or failure (or some condition), you would typically use the bool type as the return type. Here's how you can define such a function:\n\nExample\n\nfn is_positive(number: i32) -> bool {\n    number > 0\n}\n\nfn main() {\n    let num = 5;\n    if is_positive(num) {\n        println!(\"{} is positive.\", num);\n    } else {\n        println!(\"{} is not positive.\", num);\n    }\n}\n\nWhen to Use bool\n\nYou would use a bool return type in cases where:\n\nYou need to check a condition, such as whether a value meets certain criteria (e.g., whether a number is positive, whether a string is empty, etc.).\nThe function's purpose is simply to evaluate a condition rather than perform an operation that may fail.\n\nConsiderations\n\nHowever, if the function might encounter errors or require more nuanced feedback, it's often better to use a Result type. For example, if you have a function that checks if a file exists, you might want to indicate success or failure while also providing an error message:\n\nuse std::fs;\n\nfn file_exists(file_path: &str) -> Result {\n    match fs::metadata(file_path) {\n        Ok(_) => Ok(true),\n        Err(e) if e.kind() == std::io::ErrorKind::NotFound => Ok(false),\n        Err(e) => Err(e),\n    }\n}\n\nfn main() {\n    match file_exists(\"some_file.txt\") {\n        Ok(exists) => {\n            if exists {\n                println!(\"The file exists.\");\n            } else {\n                println!(\"The file does not exist.\");\n            }\n        },\n        Err(e) => eprintln!(\"Error checking file: {}\", e),\n    }\n}\n\nSummary\n\nUse bool for simple checks that return true or false.\nUse Result when you need to handle potential errors alongside a boolean result, providing more context about failure conditions.",
        "locale": "en",
        "slug": "technical-posts/rust-function-return-type-as-boolean",
        "folder": ""
    },
    {
        "title": "Specifying function return types in Rust",
        "description": "Specifying function return types in Rust",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "In Rust, you specify the return value of a function by declaring the return type after the function signature using the -> symbol. The value that you want to return is typically placed as the last expression in the function body, without a semicolon. If you include a semicolon, it turns the expression into a statement, which does not return a value.\n\nBasic Syntax:\n\nfn function_name(arguments) -> ReturnType {\n    // Function body\n\n    // The last expression (without a semicolon) is the return value\n    value_to_return\n}\n\nExample:\n\nfn add(a: i32, b: i32) -> i32 {\n    // This returns the sum of a and b\n    a + b // No semicolon here, so this is the return value\n}\n\n-> i32: This specifies that the return type of the function is an i32.\na + b: This is the last expression, and since it doesn't have a semicolon, it becomes the return value of the function.\n\nExplicit Return with return:\n\nYou can also explicitly return a value using the return keyword. This is more common in conditional or early return cases but can be used at any point in the function.\n\nfn add(a: i32, b: i32) -> i32 {\n    return a + b; // Explicitly returning a value using return\n}\n\nExample with Conditional Return:\n\nfn check_value(x: i32) -> bool {\n    if x > 10 {\n        return true; // Early return\n    }\n\n    false // Implicit return (last expression)\n}\n\nIn this case, true is returned if x > 10, and if the condition is not met, false is returned as the last expression of the function.\n\nFunctions with No Return Value:\n\nIf a function doesn't return a value, its return type is () (unit), which is the default return type if none is specified.\n\nfn greet() {\n    println!(\"Hello, world!\"); // No return type needed\n}\n\nIn this case, the function doesn't return anything, so you don't need to explicitly specify a return type.\n\nWhat does Rust's match statement do\n\nIn Rust, the match statement is used for pattern matching, allowing you to handle different possible values and states of an enum or other types. In the provided code, match is used to handle the result of reading each line from the reader.\n\nHere's what match does in this context:\n\nfor line in reader.lines() {\n    // Match on the result of reading the line\n    match line {\n        // If successful, push the line's content into the vector\n        Ok(content) => lines.push(content),\n        // If there's an error, print an error message to standard error\n        Err(e) => eprintln!(\"Error reading line: {}\", e),\n    }\n}\n\nBreakdown:\n\nreader.lines():\n\n    This returns an iterator where each item is a Result.\n    Result is an enum that can be either:\n        Ok(String): Indicates a successful read operation, where String is the content of the line.\n        Err(io::Error): Indicates an error during reading, where io::Error describes what went wrong.\n\nmatch line:\n\n    match is used to destructure and handle the different possible states of the Result.\n\nPatterns:\n    Ok(content): This pattern matches when the line is a successful result. Here, content is the string value read from the line. In this case, the code executes lines.push(content), which adds the line to the lines vector.\n    Err(e): This pattern matches when the line is an error. Here, e is the io::Error describing what went wrong. The code executes eprintln!(\"Error reading line: {}\", e), which prints an error message to standard error.\n\nSummary:\n\nThe match statement is used to handle both possible outcomes of reading a line:\n    On Success (Ok): Add the line to the lines vector.\n    On Error (Err): Print an error message to the standard error stream.\n\nThis approach ensures that both successful and unsuccessful attempts to read lines are appropriately handled.",
        "locale": "en",
        "slug": "technical-posts/specifying-function-return-types-in-rust",
        "folder": ""
    },
    {
        "title": "Sveltekit - getting data from server to client. This article discusses PageServerLoad versus explicit typing for load functions with +page.server and +page.svelte.",
        "description": "Sveltekit - getting data from server to client. This article discusses PageServerLoad versus explicit typing for load functions with +page.server and +page.svelte.",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "The right way\n\n[!tip]\nDo this instead\n\nWhen the load return typed explicitly:\n\n+page.server.ts\n\nexport async function load(): Promise {\n    const todos: Custom.Todo[] = getTodos();\n    console.log('todos', todos);\n    return { todos };\n}\n\n[!info]\nSee [[Using TypeScript with Svelte]] for defining custom types like Custom.Todo (as above)\n\nThe data value in the corresponding +page.svelte is typed correctly.\n\n![[Sveltekit - getting data from server to client -1.png|500]]\n\n+page.svelte\n\n\n    import type { PageData } from './$types';\n\n    export let data: PageData;\n\nThe wrong way\n\nThe docs say to type a load function in +page.server.ts as type PageServerLoad--as shown below:\n\n[!danger]\nDo not do this\n\n+page.server.ts\n\nimport { getTodos } from \"$lib/server/database\";\nimport type { PageServerLoad } from \"./$types\";\n\nexport async function load(): Promise {\n    const todos: Custom.Todo[] = getTodos();\n    console.log(\"todos\", todos);\n    return { todos };\n}\n\nHowever, that presents the TypeScript warning that todos isn't declared on type PageServerLoad. And, in the corresponding +page.svelte the incoming data isn't typed.\n\n![[Sveltekit - getting data from server to client 2.png|500]]",
        "locale": "en",
        "slug": "technical-posts/sveltekit-getting-data-from-server-to-client-",
        "folder": ""
    },
    {
        "title": "Resize VHDX",
        "description": "Resize VHDX",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows"
        ],
        "content": "https://www.nakivo.com/blog/increase-disk-size-hyper-v-complete-guide/\nBefore you can start the process of expanding a Hyper-V disk, you should note the following:\n\nIf your VM is running, shut it down. As you know, some settings cannot be modified when the VM is in the running or saved state.\nThe VM should not have any snapshots. After modifying virtual hard disks, the associated snapshots might become invalid.\nIn Hyper-V, you cannot expand disks belonging to a differencing disk chain. Such virtual hard disks have child virtual hard disks associated with them, and any attempt to edit them might result in data loss. Using Hyper-V functionality, you can increase the disk size of only dynamically expanding or fixed virtual hard disks.\n\nIf everything is fine, you are ready to start expanding the disks. As mentioned above, this process includes two main steps:\n\nExpand the virtual hard disk using the Edit Virtual Hard Disk Wizard.\nExtend the volume size through launching the Disk Management utility inside the VM.\n\nBelow, I am going to break down each step separately to avoid confusion.\n\nStep 1: How to expand Hyper-V VMs hard disk\n\nThe first step of the process is conducted in the following way:\n\nOpen Hyper-V Manager. Right-click the VM and select Settings.Opening Hyper-V Manager (Increase Disk Size in Hyper-V)\nSelect Hard Drivein the Hardware pane and click This action will automatically launch the Edit Virtual Hard Disk wizard.Virtual Hard Drive (Increase Disk Size in Hyper-V)\nThis action automatically launches the Edit Virtual Hard Disk wizard. Click Nextto skip the Locate Disk step.Locate Virtual Hard Disk (Increase Disk Size in Hyper-V)\nIn the Choose Action step, select Expandto expand the storage capacity of the virtual hard disk. ClickNext.Choose Action (Increase Disk Size in Hyper-V)\nIn the Configure Disk step, you can specify by how much you wish to expand the disk.Configure Disk (Increase Disk Size in Hyper-V)\nThe next step is to look through the changes you are about to implement. Click Finishto complete the action and close the wizard.Summary (Increase Disk Size in Hyper-V)\nTo verify that you have succeeded in expanding the capacity of the virtual hard disk, right-click the VM, select Settings,findHard Drive,and press theInspectbutton. This way, you can get access to the virtual hard disk properties and check whether the maximum disk size has actually increased.Inspect Disk (Increase Disk Size in Hyper-V)\n\nStep 2: How to extend the volume inside the VM\n\nThe next step is to extend the volume that resides on the virtual hard disk. This operation is performed in the following way:\n\nRight-click the VM and select Connect.Start the virtual machine and log in.Connecting to the VM (Increase Disk Size in Hyper-V)\nEnter DISKMGMT.MSC in the search bar to open the Disk Management Utility on your VM. Disk Management demonstrates the existing volumes and the amount of unallocated space we would use to extend the C: drive.Disk Management Utility (Increase Disk Size in Hyper-V)\nRight-click the C: drive and select Extend Volume.This action launches the Extend Volume wizard.Extend Volume (Increase Disk Size in Hyper-V)\nSelect the available amount of space by which you want to extend the volume. Click Next.Extend Volume Wizard (Increase Disk Size in Hyper-V)\nCheck that you have selected the correct settings. Then, click Finishto close the wizard.Completing the Wizard (Increase Disk Size in Hyper-V)\nAs you can see from the screenshot below, the size of the C: drive has successfully increased after extending the volume.Extended Drive Volume (Increase Disk Size in Hyper-V)",
        "locale": "en",
        "slug": "technical-posts/resize-vhdx",
        "folder": ""
    },
    {
        "title": "Managing Postgres connection pooling",
        "description": "Managing Postgres connection pooling",
        "date_created": "2025-06-13T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "This discusses this Postgres error\n\nencountered during dev work with Sveltekit.\n\nThis is a very common warning in Node.js applications, especially during development with tools like SvelteKit that have hot-reloading. Let's break it down.\n\nThe Short Answer (TL;DR)\n\nThis warning means your application is repeatedly setting up \"shutdown\" listeners for your database connection (or something similar) without ever cleaning up the old ones. This typically happens in a SvelteKit development environment because the server restarts on every file change (Hot Module Replacement), but the old database connection isn't being properly closed.\n\nYou are likely creating a new database connection pool on every server reload, and each one is adding a listener to know when the app shuts down. You're leaking these listeners.\n\nThe fix is not to increase the listener limit. The fix is to ensure you only have one instance of your database client/pool for the entire application lifetime and to properly close it when the application terminates.\n\nDetailed Breakdown of the Error Message\n\nLet's dissect the message itself: (node:5232) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 SIGTERM listeners added to [process]. MaxListeners is 10. Use emitter.setMaxListeners() to increase limit\n\nEventEmitter: This is a core building block in Node.js. Many objects, like network connections, file streams, and the process object itself, \"emit\" events. You can listen for these events using methods like .on() or .addListener().\n\nprocess: This is a global Node.js object that represents the current running application. It's an EventEmitter.\n\nSIGTERM: This is a \"signal\" that can be sent to a process to request its termination (a graceful shutdown). When your SvelteKit dev server restarts, or when a production environment manager (like Docker or PM2) stops your app, it often sends a SIGTERM signal. Your code can listen for this signal to perform cleanup tasks, like closing database connections.\n\nMaxListenersExceededWarning: By default, Node.js will warn you if you add more than 10 listeners for the same event on the same EventEmitter. This is a safety feature because it's a strong indicator of a \"memory leak.\" You're allocating resources (the listeners) but never releasing them.\n\nWhy This Happens in a SvelteKit + Postgres App\n\nThe most common cause is how you're initializing your Postgres client (like node-postgres i.e., pg, or an ORM like Drizzle ORM or Prisma).\n\nLet's look at a common incorrect pattern:javascript\n// src/routes/some-route/+server.js or a hook\nimport pg from 'pg';\n\n//  ANTI-PATTERN: This code runs every time the server reloads in dev mode.\nconst pool = new pg.Pool({\n  connectionString: process.env.DATABASE_URL,\n});\n\n// Many database clients automatically add a SIGTERM listener\n// to gracefully close the pool on shutdown.\n// So, on every file save:\n// 1. Vite reloads the server module.\n// 2. A NEW pool is created.\n// 3. A NEW SIGTERM listener is added to the process object.\n// 4. The OLD pool and its listener are never cleaned up.\n// After 10 reloads, you get the warning.\n\nexport async function GET() {\n  const result = await pool.query('SELECT NOW()');\n  // ...\n}\nThe Correct Solution: The Singleton Pattern\n\nYou need to ensure that only one instance of your database connection pool is ever created. A common way to do this is to create it in a dedicated module and cache it.\n\nStep 1: Create a dedicated database client module.\n\nSvelteKit's server-only modules are perfect for this. Create a file like src/lib/server/db.js.javascript\n// src/lib/server/db.js\n\nimport pg from 'pg';\nimport { PG_DATABASE, PG_HOST, PG_PASSWORD, PG_PORT, PG_USER } from '$env/static/private';\n\n// This is the key: declare the pool variable outside the function.\nlet pool: pg.Pool | null = null;\n\n/**\n Returns the singleton instance of the Postgres connection pool.\n */\nexport function getDbPool() {\n  if (!pool) {\n    console.log('Creating new PostgreSQL connection pool...');\n    pool = new pg.Pool({\n\t\thost: PG_HOST,\n\t\tport: Number(PG_PORT) || 5432, // Ensure port is a number\n\t\tuser: PG_USER,\n\t\tpassword: PG_PASSWORD,\n\t\tdatabase: PG_DATABASE,\n\t\tssl: false\n\t});\n\n    // This is the graceful shutdown logic.\n    // It's added ONCE when the pool is created.\n    process.on('SIGTERM', () => {\n      console.log('SIGTERM signal received. Closing DB pool.');\n      pool.end();\n    });\n\n    // Optional: Also listen for SIGINT (Ctrl+C in terminal)\n    process.on('SIGINT', () => {\n      console.log('SIGINT signal received. Closing DB pool.');\n      pool.end();\n    });\n  }\n  return pool;\n}\nStep 2: Use the singleton in your routes and hooks.\n\nNow, instead of creating a new pool everywhere, you import and use your getter function.javascript\n// src/routes/some-route/+server.js\n\nimport { getDbPool } from '$lib/server/db.js';\n\nconst pool = getDbPool(); // Gets the existing pool, or creates it if it's the first time.\n\nexport async function GET() {\n  const client = await pool.connect();\n  try {\n    const result = await client.query('SELECT NOW()');\n    return new Response(JSON.stringify(result.rows[0]));\n  } finally {\n    // Always release the client back to the pool\n    client.release();\n  }\n}\nWhy This Works\n\nThe pool variable is created only once because Node.js caches modules. The first time getDbPool() is called, it creates the pool. Subsequent calls will see that pool already exists and will just return the cached instance.\nBecause the pool is only created once, the process.on('SIGTERM', ...) listener is also only attached once.\nThis completely solves the listener leak, and you get the added benefit of efficient connection pooling across your entire application.\n\nWhat About emitter.setMaxListeners()?\n\nThe error message suggests this as a \"solution.\" Almost always, this is a bad idea. It's like turning off a smoke alarm because you don't like the noise. It hides the symptom (the warning) but doesn't fix the underlying problem (the memory leak). Only use it if you are absolutely certain you have a valid reason to have more than 10 listeners for a single event. In this SvelteKit/DB scenario, you do not.\n\nFetching the pool object from locals\n\nThe code above says to call getDbPool() elsewhere in the app. I am putting the pool in the locals object and wanted to make sure that didn't defeat the purpose of the singleton object.\n\nAnswer\n\nThe short answer is: No, this does not defeat the purpose of the singleton pattern. In fact, what you are doing is the idiomatic, recommended way to handle shared resources like database connections in SvelteKit.\n\nLet's break down why this works perfectly and is considered a best practice.\n\nWhy Your Pattern is Correct\n\nYour code is leveraging two separate concepts that work together beautifully:\n\nModule Scope (for the Singleton): Node.js caches modules. When you do import { getDbPool } from '$lib/server/db.js'; and then const pool = getDbPool(); at the top level of your hooks.server.js file, that code runs once when your server process starts. The pool variable in your hooks file holds the single, shared instance of the database pool for the entire lifetime of the server. This correctly implements the singleton pattern.\n\nRequest Scope (for event.locals): The handle hook runs for every incoming request. The event object, including event.locals, is created fresh for each request and destroyed after the response is sent.\n\nWhen you do event.locals.pool = pool;, you are not creating a new pool. You are simply taking the single, shared pool object that already exists in your module's memory and attaching a reference to it to the current request's locals object.\n\nThink of it like this:\nYou have one central library in a city (the singleton pool instance).\nEvery person who visits the city (an incoming request) gets a library card (event.locals) that points them to the one and only library.\nYou aren't building a new library for every person. You're just giving them a cheap, easy way to access the existing one.\n\nThe Advantages of Your Approach (Dependency Injection)\n\nThis pattern is a form of Dependency Injection, and it's highly beneficial:\n\nDecoupling: Your route handlers (+server.js, +page.server.js) don't need to know how to create or get the database connection. They are \"dumb\" in that respect. They just know to expect it on event.locals. This makes your code cleaner and easier to reason about.\n\nTestability: This pattern makes testing your endpoints much easier. You can create a mock database pool in your test setup and just place it on the event.locals object, without your endpoint code ever knowing the difference.\n\nCentralized Logic: All the logic for initializing and managing the database connection is in one place (hooks.server.js and lib/server/db.js). If you need to add logging, monitoring, or other logic around the pool, you have a single place to do it.\n\nPro-Tip: Add Type-Safety\n\nTo make this pattern even better and get full autocompletion in your editor, you should declare the shape of event.locals in your src/app.d.ts file.\n\nsrc/app.d.tstypescript\n// See https://kit.svelte.dev/docs/types#app\n// for information about these interfaces\nimport type { Pool } from 'pg'; // Or the type from your specific DB driver/ORM\n\ndeclare global {\n\tnamespace App {\n\t\t// interface Error {}\n\t\tinterface Locals {\n\t\t\tpool: Pool; // Add your property here\n\t\t}\n\t\t// interface PageData {}\n\t\t// interface Platform {}\n\t}\n}\n\nexport {};\nNow, when you type event.locals. inside any of your server-side SvelteKit files, your editor will know that a pool property exists and what its type is.\n\nComplete Example Flow\n\nHere's how your excellent pattern looks from start to finish:\n\n1. The Singleton (src/lib/server/db.js)javascript\nimport pg from 'pg';\nimport { DATABASE_URL } from '$env/static/private';\n\nlet pool;\n\nexport function getDbPool() {\n  if (!pool) {\n    pool = new pg.Pool({ connectionString: DATABASE_URL });\n  }\n  return pool;\n}\n2. The Hook (src/hooks.server.js)javascript\nimport { getDbPool } from '$lib/server/db';\nimport type { Handle } from '@sveltejs/kit';\n\n// This runs ONCE when the server starts\nconst pool = getDbPool();\n\nexport const handle: Handle = async ({ event, resolve }) => {\n\t// This runs for EVERY request\n\t// It attaches a REFERENCE to the single pool instance\n\tevent.locals.pool = pool;\n\n\tconst response = await resolve(event);\n\treturn response;\n};\n3. The Endpoint (src/routes/api/users/+server.js)javascript\nimport { json } from '@sveltejs/kit';\n\nexport async function GET({ locals }) {\n\t// Get the pool from locals (no imports needed!)\n\tconst { pool } = locals;\n\n\tconst result = await pool.query('SELECT * FROM users');\n\n\treturn json(result.rows);\n}\n`\n\nConclusion: Keep doing what you're doing. It is the correct, efficient, and maintainable way to manage shared resources like a database pool within a SvelteKit application. You have successfully avoided the listener leak while implementing a clean, testable architecture.",
        "locale": "en",
        "slug": "technical-posts/postgres-pooling-management",
        "folder": ""
    },
    {
        "title": "Insert a row into a table based on another row",
        "description": "Insert a row into a table based on another row",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sql"
        ],
        "content": "Create a new family row based on another.\n\nIn this example, family 155 is being copied as family 258\n\nINSERT INTO family (\n             id,\n             created_at,\n             name,\n             description,\n             visual_studio_version,\n             availability_id,\n             group_id,\n             sort_order,\n             download_page_section_heading,\n             download_page_order,\n             release_date)\nSELECT 158,  <-- hardcode new family id\n       created_at,\n       name, <-- change\n       description,\n       visual_studio_version,\n       availability_id,<-- change\n       group_id,\n       sort_order, <-- change\n       download_page_section_heading, <-- change\n       download_page_order, <-- change\n       release_date\nFROM\n\tfamily as f2\nWHERE\n\tf2.id = 155\n\nChange: id, name, page_section_heading, download_page_order, and sort_order (and possibility the availabilty_id\n\nWhat is the template_set table?\n\nThe template_set table was originally intended to provide the base list of a products that a family owns. The theory at the time is that this template set would be used to populate the first release_set for a new family.\nHowever, it's generally easier to copy a release set from a similar family.",
        "locale": "en",
        "slug": "technical-posts/insert-a-row-into-a-table-based-on-another-row",
        "folder": ""
    },
    {
        "title": "example svelte.config.js",
        "description": "example svelte.config.js",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "import adapter from '@sveltejs/adapter-auto';\n//import adapter from '@sveltejs/adapter-vercel';\n\nimport { vitePreprocess } from '@sveltejs/kit/vite';\n\nimport { mdsvex } from 'mdsvex';\n\n/** @type {import('mdsvex').MdsvexOptions} */\nconst mdsvexOptions = {\n\textensions: ['.md']\n};\n\n/** @type {import('@sveltejs/kit').Config} */\nconst config = {\n\t// Consult https://kit.svelte.dev/docs/integrations#preprocessors\n\t// for more information about preprocessors\n\textensions: ['.svelte', '.md'],\n\tpreprocess: [vitePreprocess(), mdsvex(mdsvexOptions)],\n\n\tkit: {\n\t\tadapter: adapter(),\n\t\talias: {\n\t\t\t$components: 'src/components',\n\t\t\t$cmdline: 'src/lib/cmd-line',\n\t\t\t$data: 'src/lib/data',\n\t\t\t$routes: 'src/routes'\n\t\t}\n\t}\n};\n\nexport default config;",
        "locale": "en",
        "slug": "technical-posts/example-svelte-config",
        "folder": ""
    },
    {
        "title": "Read a text file with Rust -- with lots of Rust notes",
        "description": "Read a text file with Rust -- with lots of Rust notes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "I need to read a text file and wanted to do it with Rust. I asked ChatGPT for help. This is what followed:\n\n// Import necessary modules from the standard library\nuse std::fs::File; // To work with files (opening, reading, etc.)\nuse std::io::{self, BufRead}; // For I/O operations: 'self' for standard I/O, 'BufRead' for buffered reading (to read efficiently)\nuse std::path::Path; // To work with file system paths\n\n// Define a function that reads lines from a file\nfn read_lines_from_file(file_path: &str) -> io::Result> {\n    // Try to open the file at the provided file path\n    let file = File::open(file_path)?; // '?' propagates errors if file can't be opened\n\n    // Create a buffered reader for more efficient file reading\n    let reader = io::BufReader::new(file);\n\n    // Create an empty vector to hold lines from the file\n    let mut lines = Vec::new();\n\n    // Iterate through each line in the buffered reader\n    for line in reader.lines() {\n        // Handle the result of reading each line (Result)\n        match line {\n            // If successful, store the line's content in the vector\n            Ok(content) => lines.push(content),\n            // If an error occurs, print an error message\n            Err(e) => eprintln!(\"Error reading line: {}\", e),\n        }\n    }\n\n    // Return the vector of lines wrapped in an Ok result (io::Result)\n    Ok(lines)\n}\n\n// Main function: program's entry point\nfn main() -> io::Result {\n    // Define the path to the text file you want to read (update this to your file's path)\n    let path = \"C:\\\\Users\\\\thumb\\\\Documents\\\\data\\\\download-readmes\\\\papa\\\\papa-readme.md\";\n\n    // Call the read_lines_from_file function and handle its result\n    match read_lines_from_file(path) {\n        // If successful, retrieve the vector of lines\n        Ok(lines) => {\n            // Iterate through each line and print it\n            for line in lines {\n                println!(\"{}\", line); // Print each line to the console\n            }\n        }\n        // If there was an error, print an error message\n        Err(e) => eprintln!(\"Failed to read lines: {}\", e),\n    }\n\n    // Return Ok(()) to indicate successful execution of the program\n    Ok(())\n}\n\nKey Concepts:\n\nstd::fs::File: Used to handle file operations (like opening files).\nstd::io::{self, BufRead}: Used for input/output operations (io::Result handles potential errors, and BufRead allows efficient reading).\n?: A shortcut for error propagation. If an error occurs, the function returns the error immediately.\nmatch: Handles the success (Ok) or failure (Err) of operations like reading lines from a file.\nVec: A dynamic array (vector) used to store lines from the file.\n\nLet me know if you have any questions or need more clarification!\n\nWhat is Ok(lines) doing?\n\n[[Rust's OK result variant]]\n\nThe line Ok(lines) in Rust is returning a successful result (Ok) containing the value lines, which is a Vec (a vector of strings).\n\nExplanation:\n\nOk(...): This is a variant of the Result enum that indicates a successful operation. In Rust, the Result enum is used to handle operations that may either succeed or fail. It has two variants:\n\n    Ok(T): Represents success and holds a value of type T.\n    Err(E): Represents an error and holds a value of type E (usually an error type).\n\nlines: In this context, lines is a variable that likely holds a Vec (a dynamic array of strings), which was populated earlier in the function by reading lines from a file.\n\nWhat the line does:\n\nThe function is returning an Ok result containing the lines vector.\nSince the function's return type is io::Result>, Ok(lines) matches that type, meaning the function successfully returns a vector of strings.\n\nFor example, if you have a function like this:\n\nfn read_lines_from_file(file_path: &str) -> io::Result> {\n    // Assume lines is a vector of strings containing the file's content\n    let lines = vec![\"Line 1\".to_string(), \"Line 2\".to_string()];\n\n    // Returning a successful result (Ok) with the lines\n    Ok(lines)\n}\n\nIn this case, the Ok(lines) means that the function has successfully read the lines and is returning them in a vector wrapped in an Ok, signaling to the caller that the operation was successful. If the caller sees an Ok, it can then access the lines without worrying about errors.",
        "locale": "en",
        "slug": "technical-posts/read-a-text-file-with-rust",
        "folder": ""
    },
    {
        "title": "Using FlexSearch with SvelteKit with a service worker",
        "description": "Using FlexSearch with SvelteKit with a service worker",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "rogerpence: what does the following line of javascript do:\ntype === 'ready' && (search = 'ready')\n\nGitHub Copilot: Let me explain this line step by step:\n\nThis is a shorthand conditional expression using logical AND (&&)\nIt combines a comparison and an assignment\nIt works like this:\n    First evaluates type === 'ready'\n    If true, executes search = 'ready'\n    If false, stops evaluation\n\nIt's equivalent to this more verbose if statement:\n\nif (type === \"ready\") {\n    search = \"ready\";\n}\n\nThis pattern is common in reactive UI frameworks like Svelte for concise conditional state updates. The code checks if type equals 'ready' and if so, sets the search variable to 'ready'.\n\nThis is a terse way to write a conditional assignment, though some developers prefer the more explicit if statement for clarity.\n\nrogerpence: When an HTML popover is displayed, the page behind it isn't inert\n\nGitHub Copilot: Let's add an overlay and make the background inert when the popover is shown.\n\nPlan:\n\nAdd CSS for a backdrop overlay\nAdd JavaScript to handle showing/hiding overlay\nMake background inert using inert attribute\nToggle these when popover shows/hides\n\n\n// ...existing code...\n\n// Add popover state management\nlet isPopoverOpen = $state(false)\n\nonMount(() => {\n    // Add popover event listeners\n    const popover = document.getElementById('search-results')\n    popover?.addEventListener('beforetoggle', (event: Event) => {\n        const e = event as ToggleEvent\n        isPopoverOpen = e.newState === 'open'\n        document.body.toggleAttribute('inert', isPopoverOpen)\n    })\n})\n// ...existing code...\n\n\n\n    {#if isPopoverOpen}\n     document.getElementById('search-results')?.hidePopover()}\n    >\n    {/if}\n    \n\n\n\n    .backdrop {\n        position: fixed;\n        inset: 0;\n        background: rgba(0, 0, 0, 0.5);\n        z-index: 998; /* Below popover */\n    }\n\n    :global([inert]) {\n        pointer-events: none\n// ...existing code...\n\n// Add popover state management\nlet isPopoverOpen = $state(false)\n\nonMount(() => {\n    // Add popover event listeners\n    const popover = document.getElementById('search-results')\n    popover?.addEventListener('beforetoggle', (event: Event) => {\n        const e = event as ToggleEvent\n        isPopoverOpen = e.newState === 'open'\n        document.body.toggleAttribute('inert', isPopoverOpen)\n    })\n})\n// ...existing code...\n\n\n\n    {#if isPopoverOpen}\n     document.getElementById('search-results')?.hidePopover()}\n    >\n    {/if}\n    \n\n\n\n    .backdrop {\n        position: fixed;\n        inset: 0;\n        background: rgba(0, 0, 0, 0.5);\n        z-index: 998; /* Below popover */\n    }\n\n    :global([inert]) {\n        pointer-events: none\n\nrogerpence: how can I set the correct data type for the event varaible in line 38. The beforetoggle event is from the HTML popover api\n\nGitHub Copilot: Let's add the correct type for the beforetoggle event in the event listener. The HTML Popover API's ToggleEvent needs to be defined as it's not included in the default TypeScript DOM types yet.\n\nHere's how to add the type:\n\nFirst, add the ToggleEvent interface at the top of the script\nThen use it in the event listenersvelte\n\n    // Add this interface near the top of your script\n    interface ToggleEvent extends Event {\n        newState: 'open' | 'closed';\n        oldState: 'open' | 'closed';\n    }\n\n    // ...existing code...\n\n    popoverElement?.addEventListener('beforetoggle', (event: ToggleEvent) => {\n        if (event.newState === \"open\") {\n            //document.body.setAttribute('inert', 'true')\n            searchTerm = ''\n        } else {\n            //document.body.removeAttribute('inert')\n        }\n    })\n\n    // ...existing code...\n\nThis defines the ToggleEvent interface with the properties specified in the HTML Popover API specification and properly types the event parameter.\n\nrogerpence: that still cases the typescript error that no overload matches the call\n\nGitHub Copilot: Let's solve this step by step:\n\nResearch the HTML Popover API spec\nCreate a more specific type for the event\nUse type assertion to handle the DOM event\n\nHere's the corrected code:typescript\n// Update the interface definition\ninterface ToggleEvent {\n    newState: \"open\" | \"closed\";\n    oldState: \"open\" | \"closed\";\n}\n\n// Change the event listener\npopoverElement?.addEventListener(\"beforetoggle\", ((event: Event) => {\n    const toggleEvent = event as unknown as { newState: \"open\" | \"closed\" };\n    if (toggleEvent.newState === \"open\") {\n        //document.body.setAttribute('inert', 'true')\n        searchTerm = \"\";\n    } else {\n        //document.body.removeAttribute('inert')\n    }\n}) as EventListener);\nThis solution:\n\nCreates a separate interface for the toggle properties\nUses type assertion to handle the DOM event type\nMaintains type safety while working with the experimental Popover API\n\nrogerpence: would this also be a good way to do that:\nif ((event as unknown as { newState: 'open' | 'closed' }).newState === \"open\") {\nGitHub Copilot: No, that's not the best approach. Here's a better step-by-step solution:\n\nCreate a proper type for the Popover API toggle event\nCast the event once at the start of the function\nUse the typed variable throughout the functionsvelte\ninterface PopoverToggleEvent extends Event {\n    newState: 'open' | 'closed';\n    oldState: 'open' | 'closed';\n}\n\n// ...existing code...\n\npopoverElement?.addEventListener('beforetoggle', (event: Event) => {\n    const toggleEvent = event as PopoverToggleEvent;\n\n    if (toggleEvent.newState === \"open\") {\n        //document.body.setAttribute('inert', 'true')\n        searchTerm = ''\n    } else {\n        //document.body.removeAttribute('inert')\n    }\n});\nThis is better because:\n\nIt's more type-safe\nMore readable\nReuses the type definition\nAvoids inline type assertions\n\nrogerpence: what do you think of this:\nif ((event as ToggleEvent).newState === \"open\") {\nGitHub Copilot: Let's plan a better approach for handling the Popover toggle event:\n\nCast event once at start of handler function\nUse typed variable throughout\nKeep code clean and maintainablesvelte\npopoverElement?.addEventListener('beforetoggle', (event: Event) => {\n    const toggleEvent = event as ToggleEvent;\n\n    if (toggleEvent.newState === \"open\") {\n        searchTerm = ''\n    } else {\n        //document.body.removeAttribute('inert')\n    }\n});\nThis improves:\n\nType safety\nCode readability\nReduces repeated type assertions\nFollows TypeScript best practices\n\nrogerpence: Explain how the service worker in search-worker.ts is used with the SearchButton.svelte component\n\nGitHub Copilot: # Web Worker Integration in Search Functionality\n\nThe search-worker.ts file creates a Web Worker that handles search operations off the main thread, providing better performance for the search functionality in SearchButton.svelte. Here's how they work together:\n\nHow the Web Worker Works\n\nInitialization:\n\n    In SearchButton.svelte component the worker is created with new SearchWorker()\n    When SearchButton.svelte is mounted it sends a load message to initialize the worker--which rebuilds the index.\n\nCommunication Flow:\n    SearchButton                         search-worker.ts\n                         \n     Create                       \n     Worker                                            \n                  {type: 'load'}       Fetch data      \n                  Create index    \n                                                       \n     Set          {type: 'ready'}                      \n     search=ready                 \n                                                       \n     User types   {type: 'search',                     \n     searchTerm    payload: {          Process search  \n                    searchTerm }}      with FlexSearch \n                                  \n                                                       \n     Update UI    {type: 'results',                    \n     with results  payload: {                          \n                    results,                           \n                    searchTerm }}                      \n                                  \n                         \n    Key Interactions:\n    Worker loads search data and creates index when receiving load message\n    Component listens for ready and results messages from worker\n    Search requests are triggered reactively when searchTerm changes\n\nBenefits of This Approach\n\nPerformance: Search operations run in a separate thread, preventing UI freezes\nResponsiveness: UI remains interactive during complex search operations\nSeparation of Concerns: Search logic is isolated from UI rendering\n\nThis pattern is particularly effective for search implementations that might handle large datasets or complex search algorithms.\n\n\nFlexSearch example\n\nSearch schema\nexport type Content = {\n    locale: string;\n    content: string;\n    slug: string;\n    title: string;\n    description: string;\n    tags_list: string;\n    isPage: boolean;\n};\n`",
        "locale": "en",
        "slug": "technical-posts/using-flexsearch-with-a-service-worker",
        "folder": ""
    },
    {
        "title": "How to set the locale at ASNA.com",
        "description": "How to set the locale at ASNA.com",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "asna-com"
        ],
        "content": "Setting the locale\n\nThe current local is hardcoded in the +layout.server.js file in each locale's root folder. The +layout.server.js is shown below for the en locale.\n\n+layout.server.js\n\n// @ts-nocheck\nimport { superValidate } from 'sveltekit-superforms/server';\nimport contactFormSchema from '$components/contact-form/contact-form-schema.js';\n\nexport const prerender = true;\nexport const csr = true;\n\nconst user = {\n\tshowContactUsForm: false,\n\tcontactUsFormTitle: '',\n\tcontactUsFormSource: '',\n\tcontactUsTargetUrl: '',\n\temailKey: '',\n\tlocale: 'en',\n\tshowBeta: false,\n\tsaveDownloadInfo: false,\n\tshowFlash: false\n};\n\nexport const load = async (request, event) => {\n\tconst form = await superValidate(contactFormSchema);\n\n\treturn {\n\t\tform,\n\t\tuser\n\t};\n};\n\nGetting the locale from userState\n\nThe current locale is then available from the userState store.\n\nimport { setUserState, getUserState } from '$lib/userState';\n\nconst state = getUserState()\nconst locale = $state.locale",
        "locale": "en",
        "slug": "technical-posts/get-locale-and-url-for-page",
        "folder": ""
    },
    {
        "title": "Creating marketing emails with mjml",
        "description": "Creating marketing emails with mjml",
        "date_created": "2025-06-11T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "marketing",
            "email"
        ],
        "content": "I think mjml is pretty much a dead product. But it might have a little value in seeing the HTML its abstracted components produce. \n\nTo install:\n\nnpm init -y\npnpm add mjml\n\nAdd this as the first test email: test.mjml\n\nimport mjml2html from 'mjml' /* Compile an mjml string */ const htmlOutput =\nmjml2html(`\n\n    \n        \n            \n                 Hello World!dd \n            \n        \n    \n\n`, options) /* Print the responsive HTML generated and MJML errors if any */\nconsole.log(htmlOutput)\n\nStart two terminal tabs at project root: \nOpen live server in one\n\nTo compile test.mjml to test.html\n\nnpx mjml test.mjml -o test.html\n\nTo watch for changes and see them with live server:\n\nnpx mjml -w test.mjml",
        "locale": "en",
        "slug": "technical-posts/marketing-emails-mjml",
        "folder": ""
    },
    {
        "title": "How to page through a Json object",
        "description": "A 'Pager' class that fetches a page of objects from an array of Json objects",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript"
        ],
        "content": "A note on immutabilty\n\nThis pager doesn't explicitly protect against mutability of the input arr objects.\nconstructor(arr: T[], pageSize: number) {\n\n\t// Change this line\n    this.arr = [...arr\n    \n    // to this to do a deep copy using structuredClone\n    this.arr = structuredClone(arr);\n\nFor the Pager class, it's better to use the shallow copy:\nMuch better performance\nPagination typically doesn't modify data\nIf users need immutability, they should freeze their objects or use immutable data structures",
        "locale": "en",
        "slug": "technical-posts/paging-through-json-doc",
        "folder": ""
    },
    {
        "title": "Zod technical notes and code fragments for creating interfaces and types with Zod.",
        "description": "Zod technical notes and code fragments for creating interfaces and types with Zod.",
        "date_created": "2025-05-21T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "zod"
        ],
        "content": "[[svelte-forms-technical-notes|See this article for why you need Zod]]\nZod versioning\n\nEffective with Zod 3.25.0, the Zod binary includes both 3.x and 4.x versions.  I want to use v4 but asna.com is currently using v3.22. \n\nImport 3.x:\n\nimport { z } from \"zod\";\n\nimport 4.x\n\nimport { z } from \"zod/v4\";\n\nCreate a type or interface from a Zod object\n\nAn example Zod object for an HTML form (this is one is for the ASNA Download's Family table). \n\nNote how you can create a type from the Zod object. This implies that you shouldn't explicitly create a type or interface for a form; rather, first create the Zod object and then derive the form type from that. You can't create a Zod object from a type or an interface.\n\nNote that created_at is declared to Zod as optional. created_at needs to be the Zod object so that the inferred Family type includes created_at. It's optional because it won't be part of the form and shouldn't be necessary for form validation. \n\nexport const FamilyZodSchema = z.object({\n    id: z.string(), // Could add .uuid() if it's a UUID, or .regex(/^\\d+$/) if numeric string\n    created_at: z.coerce.date().optional(), // Coerces string/number to Date object\n    name: z.string(),\n    description: z.string(),\n    visual_studio_version: z.string(),\n    availability_id: z.string().nullable(), // .regex(/^\\d+$/).nullable() if numeric string\n    group_id: z.string().nullable(),       // .regex(/^\\d+$/).nullable() if numeric string\n    sort_order: z.number(),                // Could add .int().min(0) for example\n    download_page_section_heading: z.string(),\n    download_page_order: z.number(),       // Could add .int().min(0)\n    release_date: z.coerce.date().nullable(), // Coerces string to Date, or allows null\n});\n\nexport type Family = z.infer;\n\nexport type FamilyFormData = Omit;",
        "locale": "en",
        "slug": "technical-posts/zod-technical-notes",
        "folder": ""
    },
    {
        "title": "An explanation of using Rust unusual match function call.",
        "description": "An explanation of using Rust unusual match function call.",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "Rust has unusual function calls.\n\nmatch extract_numbers_from_filename(filename) {\n\tSome(result) => println!(\"Extracted: {}\", result),\n\tNone => eprintln!(\"No valid number series found in the file name\"),\n}\n\nThe function call you are referring to is a match expression in Rust. It is used to handle different possible outcomes of the extract_numbers_from_filename(filename) function, which returns an Option (either Some(String) or None). Here's a breakdown of the components and how it works:\n\n1. Function Call: extract_numbers_from_filename(filename)\n\nThis calls the function extract_numbers_from_filename, passing the filename (a string slice &str) as an argument.\nThe function tries to extract a series of numbers from the filename using a regular expression.\nIt returns an Option, which is an enum that can either be:\n    Some(String)  representing success, where the String contains the extracted and formatted numbers.\n    None  representing failure, where no valid number series was found in the file name.\n\n2. Match Expression: match extract_numbers_from_filename(filename)\n\nThe match expression in Rust is used to branch the control flow based on the value returned by extract_numbers_from_filename.\nThe expression takes the return value (Option) and checks which variant it matches: Some or None.\n\n3. Matching on Option:\n\nThe return value of the function is an Option. Heres what happens in detail:\n\nSome(result) =>\n\nIf the function returns Some(result), this means the regex was successful, and result is the String containing the extracted number sequence.\nIn this case, the result (the string inside the Some variant) is bound to the variable result.\nThe arrow => directs the program to execute the following block:\n    println!(\"Extracted: {}\", result);\n    This line prints the extracted number sequence, replacing {} with the value of result. For example:\n    Extracted: 12.45.678.99\n\nNone =>\n\nIf the function returns None, it means that no valid number sequence was found in the filename.\nIn this case, the None variant is matched, and the arrow => directs the program to execute the following block:\n    eprintln!(\"No valid number series found in the file name\");\n    This prints an error message to the standard error stream (i.e., it prints an error message to the terminal). eprintln! is similar to println!, but it writes to the standard error.\n\n4. Putting It All Together:\n\nHeres the complete breakdown of the match expression:\n\nmatch extract_numbers_from_filename(filename) {\n    Some(result) => println!(\"Extracted: {}\", result),\n    None => eprintln!(\"No valid number series found in the file name\"),\n}\n\nextract_numbers_from_filename(filename): Calls the function to extract numbers from the filename.\nmatch: The returned value is matched against the two possible variants of Option:\n    Some(result): If the function successfully extracted the numbers, result contains the formatted string, and its printed using println!.\n    None: If the function failed to extract numbers, it matches None, and an error message is printed using eprintln!.\n\nWhy Use Option?\n\nRust uses Option as a type-safe way of dealing with cases where a value might be absent. It forces you to explicitly handle both the Some and None cases, ensuring that you dont accidentally use a value that might not exist. The match expression is the common way of unwrapping an Option and handling both possible outcomes.\n\nExample:\n\nSuppose you run the program with the filename \"file 0012.0034.0567.0789.txt\", the extract_numbers_from_filename function might return Some(\"12.34.567.789\".to_string()). In that case, the match expression would execute the first arm (Some(result)) and print:\n\nExtracted: 12.34.567.789\n\nIf the filename doesnt contain the number pattern (e.g., \"file.txt\"), the function would return None, and the match expression would execute the second arm (None) and print an error message:\n\nNo valid number series found in the file name\n\nAlternatives to using match to call Rust functions\n\nIn Rust, the primary way to execute code is by calling functions. However, there are several alternative ways of defining and executing code besides traditional function calls, depending on the context and what you're trying to achieve. Let's explore some alternatives:\n\n1. Closures (Anonymous Functions)\n\nClosures are anonymous functions that can capture variables from their surrounding environment. You can define and execute a closure without explicitly defining a named function.\n\nExample of a closure:\n\nfn main() {\n    let add = |x: i32, y: i32| x + y;\n    let result = add(5, 10); // This is calling the closure\n    println!(\"The result is: {}\", result);\n}\n\nIn this case, add is a closure that behaves like a function but is defined inline without a name.\n\nClosures are commonly used in Rust for things like iterators, functional programming patterns, and passing behavior around (e.g., passing a closure to another function).\n\n2. Macros\n\nMacros in Rust allow you to define code that writes other code. Theyre a way to abstract over code patterns, and they are invoked differently from functions, using !.\n\nExample of a macro:\n\nfn main() {\n    println!(\"Hello, world!\"); // println! is a macro, not a function\n}\n\nYou can define your own macros as well:\n\nmacro_rules! say_hello {\n    () => {\n        println!(\"Hello, world!\");\n    };\n}\n\nfn main() {\n    say_hello!(); // This invokes the macro\n}\n\nMacros are powerful because they work on the abstract syntax tree level, meaning they can generate complex code at compile time.\n\n3. Inline Code Blocks ({})\n\nYou can execute a block of code inline without needing a named function. Code blocks {} return the last expression implicitly.\n\nExample:\n\nfn main() {\n    let result = {\n        let x = 5;\n        let y = 10;\n        x + y // This value is returned from the block\n    };\n\n    println!(\"The result is: {}\", result);\n}\n\nIn this case, the block { let x = 5; let y = 10; x + y } behaves similarly to a function but is evaluated inline and returns a value.\n\n4. Methods on Structs or Enums\n\nInstead of calling a free-standing function, you might define a method on a struct or enum and call it as an associated method.\n\nExample:\n\nstruct Point {\n    x: i32,\n    y: i32,\n}\n\nimpl Point {\n    fn distance_from_origin(&self) -> f64 {\n        ((self.x.pow(2) + self.y.pow(2)) as f64).sqrt()\n    }\n}\n\nfn main() {\n    let p = Point { x: 3, y: 4 };\n    println!(\"Distance: {}\", p.distance_from_origin()); // Method call\n}\n\nHere, distance_from_origin is a method associated with the Point struct. You call it using the instance p as p.distance_from_origin().\n\n5. Trait Implementations\n\nInstead of calling a specific function, you can invoke a method or behavior through trait implementations, which allow polymorphism.\n\nExample:\n\ntrait Greet {\n    fn greet(&self);\n}\n\nstruct Person {\n    name: String,\n}\n\nimpl Greet for Person {\n    fn greet(&self) {\n        println!(\"Hello, {}!\", self.name);\n    }\n}\n\nfn main() {\n    let person = Person { name: \"Alice\".to_string() };\n    person.greet(); // Calls the method via the trait\n}\n\nHere, greet is a method from the Greet trait, and it's called on an instance of Person through polymorphism.\n\n6. Pattern Matching as an Alternative\n\nIn some cases, instead of calling a function, you can handle logic using pattern matching. This can replace function calls that are based on conditional logic.\n\nExample:\n\nenum Message {\n    Hello,\n    Goodbye,\n}\n\nfn main() {\n    let message = Message::Hello;\n\n    match message {\n        Message::Hello => println!(\"Hello!\"),\n        Message::Goodbye => println!(\"Goodbye!\"),\n    }\n}\n\nIn this example, instead of calling a separate function to handle each message, pattern matching is used to directly handle the logic inline.\n\nSummary\n\nWhile traditional functions are the most common way to encapsulate and reuse code in Rust, you have several alternatives, such as:\n\nClosures: Inline anonymous functions that can capture their environment.\nMacros: Used to generate code patterns at compile time.\nInline code blocks: Self-contained blocks that return values.\nMethods: Functions associated with structs or enums.\nTraits: Providing polymorphic behavior.\nPattern Matching: Handling logic directly in match expressions.\n\nThese alternatives give Rust a lot of flexibility in how you structure and execute code, depending on the specific context.",
        "locale": "en",
        "slug": "technical-posts/rust-function-calls-with-match",
        "folder": ""
    },
    {
        "title": "Vary holy grail aside and article columns widths (clamp and minmax)",
        "description": "Vary holy grail aside and article columns widths (clamp and minmax)",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Yes, absolutely! You can use minmax() and clamp() to make the column widths of your aside and article elements responsive within your CSS grid. Here's how you can approach it, along with explanations:\n\nUnderstanding the Tools\n\nminmax(min, max):\n    Used within grid-template-columns or grid-template-rows.\n    Sets the minimum and maximum size of a grid track (column or row).\n    The track will try to be as large as it can, but won't go smaller than the min value or larger than the max value.\nclamp(min, preferred, max):\n    Used with width, height, font-size and other size related properties\n    Locks a value between min and max, using preferred when it is between those values.\n    This allows you to define a size that responds to the available space, but has limits.\n\nImplementation\n\nLet's assume your initial grid structure looks something like this:\n\n\n    Header\n    Aside\n    Article\n    Footer\n\n\n.grid-container {\n    display: grid;\n    grid-template-columns: repeat(2, 1fr); /* 2 columns, equal width */\n    grid-template-rows: auto 1fr auto; /* Header, content area, footer */\n    gap: 10px; /* Optional gap */\n}\n\nheader,\nfooter {\n    grid-column: 1 / -1; /* Spans from the first to the last column */\n}\n\naside {\n    /* set initial width for the aside */\n}\n\narticle {\n    /* set initial width for the article */\n}\n\nHere's how you can use minmax() and clamp() to make the aside and article columns responsive:\n\nUsing minmax()\n\n.grid-container {\n    display: grid;\n    grid-template-columns: minmax(200px, 20%) minmax(500px, 1fr); /* Responsive width of each column */\n    grid-template-rows: auto 1fr auto;\n    gap: 10px;\n}\n\nheader,\nfooter {\n    grid-column: 1 / -1;\n}\n\nExplanation:\n    grid-template-columns: minmax(200px, 20%) minmax(500px, 1fr);\n        First column (aside): minmax(200px, 20%) - The aside column will try to be 20% of the container's width, but it will not shrink smaller than 200px. This prevents it from becoming too small on very narrow screens.\n        Second column (article): minmax(500px, 1fr) - The article column will try to fill remaining space of the container (1fr), but it will not shrink smaller than 500px This ensure the content has enough space even on smaller screens.\n\nUsing clamp()\n\n.grid-container {\n    display: grid;\n    grid-template-columns: clamp(100px, 20%, 300px) clamp(400px, 80%, 1fr); /*Responsive width of each column */\n    grid-template-rows: auto 1fr auto;\n    gap: 10px;\n}\n\nheader,\nfooter {\n    grid-column: 1 / -1;\n}\n\nExplanation:\n    grid-template-columns: clamp(100px, 20%, 300px) clamp(400px, 80%, 1fr)\n        First column (aside): clamp(100px, 20%, 300px) - The aside column will aim to be 20% of the container's width, but will not shrink smaller than 100px, or grow larger than 300px.\n        Second column (article): clamp(400px, 80%, 1fr) - The article column will aim to be 80% of the container's width, but will not shrink smaller than 400px, or grow larger than 1 fraction of available free space.\n\nAdvantages of minmax() and clamp():\n\nResponsiveness: Your layout adapts gracefully to different screen sizes.\nContent Preservation: Avoids making columns too narrow, which can cause content to become unreadable.\nFlexibility: You can fine-tune the minimum and maximum sizes to suit your design.\nNo Media Queries: Can drastically reduce the number of media queries you need.\n\nChoosing Between minmax() and clamp():\n\nminmax() is generally used inside grid context, to set the sizing boundaries of columns and rows.\nclamp() is generally used when targeting a specific element size outside grid, and is a more generic tool which provides more control, allowing you to lock a size between a minimum, preferred, and maximum.\n\nImportant Considerations\n\nContent Overflow: Be mindful of how content might overflow the aside or article elements if their minimum sizes are too small.\nTesting: Test your layout on various screen sizes to ensure it looks good and works well.\nUser Experience: Strive for a layout that's easy and comfortable for users to navigate on any device.\n\nSummary\n\nBy using minmax() or clamp() in grid-template-columns, you can create a fluid and responsive grid layout where the aside and article columns adapt to different screen widths while maintaining readability. Experiment with different values to achieve the best balance for your design. Let me know if you have any other questions!",
        "locale": "en",
        "slug": "technical-posts/vary-holy-grail-aside-and-article-columns-widths-(clamp-and-minmax)",
        "folder": ""
    },
    {
        "title": "Getting locale from Sveltekit URL",
        "description": "Getting locale from Sveltekit URL",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "javascript"
        ],
        "content": "Note how you can use destructuring's default value to force a value for an undefined argument.\n\n[sveltekit-get-page-locale]]\n\n\n    import {page} from'$app/stores' const {(lang = \"en\")} from $page.params\n",
        "locale": "en",
        "slug": "technical-posts/get-language-page-parm",
        "folder": ""
    },
    {
        "title": "Create a Zod object from a C# data model",
        "description": "Create a Zod object from a C# data model",
        "date_created": "2025-05-31T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "zod",
            "data-model"
        ],
        "content": "Given this C# data model:\n\nusing System;\n\nnamespace DataModels;\n\npublic partial class Categoryrules\n{\n    /*\n     | Model type: table\n     */\n    public int id { get; set; } \n    public string? description { get; set; } \n    public string? rule { get; set; } \n    public string? value { get; set; } \n    public string? category { get; set; }\n}\n\nHere is a corresponding Zod object:\n\nimport { z } from 'zod';\n\nexport const categoryRulesSchema = z.object({\n  // public int id { get; set; }\n  // C# 'int' is a non-nullable integer.\n  id: z.number().int(),\n\n  // public string? description { get; set; }\n  // C# 'string?' means the string can be null.\n  // In Zod, .nullable() means the value can be the type or null.\n  // If the key itself might be missing (undefined), you'd use .optional() or .nullish().\n  // Assuming if the property exists, it's either a string or null:\n  description: z.string().nullable(),\n\n  // public string? rule { get; set; }\n  rule: z.string().nullable(),\n\n  // public string? value { get; set; }\n  value: z.string().nullable(),\n\n  // public string? category { get; set; }\n  category: z.string().nullable(),\n});\n\n// If you want to infer a TypeScript type from this schema:\nexport type CategoryRules = z.infer;\n\n/*\n// Example Usage:\n\nconst validData: CategoryRules = {\n  id: 1,\n  description: \"Rule for electronics\",\n  rule: \"IF price > 1000\",\n  value: \"HighValue\",\n  category: \"Electronics\",\n};\n\nconst validDataWithNulls: CategoryRules = {\n  id: 2,\n  description: null,\n  rule: \"ALWAYS\",\n  value: \"Default\",\n  category: null,\n};\n\nconst invalidData = {\n  id: \"not-a-number\", // Fails id: z.number()\n  description: 123,      // Fails description: z.string()\n  // rule is missing, but since it's nullable, not optional, this would fail\n  // if you strictly parse. If keys can be missing, use .optional()\n  value: \"Some Value\",\n  category: \"Books\",\n};\n\ntry {\n  categoryRulesSchema.parse(validData);\n  console.log(\"validData is valid!\");\n\n  categoryRulesSchema.parse(validDataWithNulls);\n  console.log(\"validDataWithNulls is valid!\");\n\n  // This will throw a ZodError\n  // categoryRulesSchema.parse(invalidData);\n  // console.log(\"invalidData is valid (this won't be reached)\");\n} catch (e) {\n  if (e instanceof z.ZodError) {\n    console.error(\"Validation failed:\", e.errors);\n  } else {\n    console.error(\"An unexpected error:\", e);\n  }\n}\n\n// --- Consideration: Optional vs. Nullable ---\n// If the C# properties, when serialized to JSON, might be omitted entirely\n// (not just present with a null value), you might prefer .optional()\n// or .nullish() for the string fields.\n\n// Example with .optional() (keys can be missing, implying undefined)\nconst categoryRulesOptionalSchema = z.object({\n  id: z.number().int(),\n  description: z.string().optional(), // string | undefined\n  rule: z.string().optional(),\n  value: z.string().optional(),\n  category: z.string().optional(),\n});\n\n// Example with .nullish() (keys can be missing OR value can be null)\nconst categoryRulesNullishSchema = z.object({\n  id: z.number().int(),\n  description: z.string().nullish(), // string | null | undefined\n  rule: z.string().nullish(),\n  value: z.string().nullish(),\n  category: z.string().nullish(),\n});\n\n// The choice between .nullable(), .optional(), and .nullish() depends on\n// how your C# objects are serialized and what you expect on the consuming (JavaScript/TypeScript) side.\n// For a direct translation of C# string? where the property is always present in the object\n// (even if its value is null), .nullable() is the most accurate.\n*/\n\nExplanation of Choices:\n\nid: z.number().int():\n    C# int is a non-nullable 32-bit signed integer.\n    z.number() specifies it must be a number.\n    .int() adds a refinement that the number must be an integer.\n    It's not nullable because the C# int is not int?.\n\ndescription: z.string().nullable() (and similar for rule, value, category):\n    C# string? indicates that the description property can hold a string value or null.\n    z.string() specifies it must be a string if it's not null.\n    .nullable() allows the value to be null in addition to being a string.\n\nImportant Consideration: nullable() vs. optional() vs. nullish()\n\n.nullable(): The key must exist, but its value can be the specified type OR null.\n    { description: null } is valid.\n    { description: \"text\" } is valid.\n    {} (missing description key) is invalid.\n    This most closely matches C# string? if you assume that when the C# object is serialized (e.g., to JSON), null properties are explicitly included as key: null.\n\n.optional(): The key can be missing entirely (which means its value is undefined in JavaScript), OR if it exists, it must be the specified type. It does NOT allow null unless you chain it (e.g., .nullable().optional()).\n    { description: \"text\" } is valid.\n    {} (missing description key) is valid (value becomes undefined).\n    { description: null } is invalid (unless you did z.string().nullable().optional()).\n\n.nullish(): The key can be missing (undefined), OR its value can be null, OR it can be the specified type. It's shorthand for .optional().nullable() in terms of what it allows.\n    { description: \"text\" } is valid.\n    { description: null } is valid.\n    {} (missing description key) is valid.\n\nFor your C# model, z.string().nullable() is generally the most direct translation, assuming that if a C# string property is null, it will be serialized as key: null rather than the key being omitted entirely from the JSON/object. If your serialization process omits keys for null values, then z.string().nullish() or z.string().optional() (if null isn't a possible value when the key is present) would be more appropriate.",
        "locale": "en",
        "slug": "technical-posts/create-zod-object-from-cs-data-model",
        "folder": ""
    },
    {
        "title": "How to use Resilio to keep folders synced",
        "description": "How to use Resilio to keep folders synced",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities",
            "obsidian"
        ],
        "content": "To add a client to Resilio, you need to copy in the key from the machine with the source folder. Do not add the folder on the client.\n\nAlways keep selective sync turned off.\nFor me, the Envoy VM is the host computer. My powerderfinger and mothra devices are peers to that box.\nmy Resilio key is in my Google Drive (\"Resilio Sync.btskey\")\n\nOn the host:\n\nCreate a shared folder\nGet its key by clicking the Share button\n    ![[Resilio folder syncing.png|500]]\n\nOn the client:\n\nClick plus sign\nUse \"Enter key or link\" option\nYou'll be prompted for a target folder\n    This folder should be empty\n    The shared folder will be immediately copied from the host",
        "locale": "en",
        "slug": "technical-posts/resilio-folder-syncing",
        "folder": ""
    },
    {
        "title": "ozgar",
        "description": "ozgar AI vendor",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "ozgar"
        ],
        "content": "![[image-56.png]]\n\n![[image-57.png]]\n\n![[image-58.png]]\n\n![[image-59.png]]\n\n![[image-60.png]]\n\n![[image-61.png]]\n\n![[image-62.png]]\n\n![[image-63.png]]",
        "locale": "en",
        "slug": "technical-posts/ozgar",
        "folder": ""
    },
    {
        "title": "Indexed access types in TypeScript",
        "description": "Indexed access types in TypeScript",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "export type MarkdownParseResult = {\n    successful: MarkdownFileResult];\n    failed: Array;\n};\n\nexport async function getMarkdownObjects>(\n    folder: string\n): Promise> {\n    const fileInfo: fs.Dirent[] = getAllDirEntries(folder) ?? [];\n\n    const successful: MarkdownFileResult[] = [];\n    const failed: MarkdownParseResult[\"failed\"] = [];\n\n    await Promise.all(\n        fileInfo.map(async (fi) => {\n            const fullFilename = path.join(fi.parentPath, fi.name);\n            const result = await parseMarkdownFile(fullFilename);\n\n            if (result.success) {\n                successful.push({\n                    dirent: fi,\n                    markdownObject: result.data,\n                });\n            } else {\n                failed.push({\n                    filename: fullFilename,\n                    dirent: fi,\n                    error: result.error,\n                });\n            }\n        })\n    );\n\n    return { successful, failed };\n}\n\nIn the code above, this line:\n\nconst failed: MarkdownParseResult[\"failed\"] = [];\n\n[\"failed\"] Indexes into the MarkdownParseResult type to get the type of the[failedproperty. Doing this, failed is an array of \n\nfailed: Array;",
        "locale": "en",
        "slug": "technical-posts/indexed-access-types-in-typescript",
        "folder": ""
    },
    {
        "title": "Work flowfor creating React-Email React Email emails",
        "description": "Work flowfor creating React-Email React Email emails",
        "date_created": "2025-06-22T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "react-email"
        ],
        "content": "Workflow setup \n\nCreate a file named wrapper.html in the folder of your email project (copy it from the root of the react-email-starter project). If you're working on this file:\n\nemails/lakeb2b/1015-06-26/test.tsx\n\nCreate this file: emails/lakeb2b/1015-06-26/wrapper.html\n\nSet the iframe src to:\n../../../out/lakeb2b/2025-06-26/test.html\n\nThe preview URL for that email is:\n\nhttp://127.0.0.1:8080/emails/lakeb2b/2025-06-26/wrapper.html\nStart the preview\n``\nStart two terminal sessions at the root of the folder react-email-start\nStart live server in one and open a browser to the preview URL for your project.\nIn the other terminal, use  pnpm watch:export2 to start the 'onchange' file watcher. This launches the react-email 'export' command when file changes are saved.\nWhen changes are made to the file you're working, a second or two after saving the file, the browser refreshes to show you the latest changes. \n\nsend-test-email.js\n\nThis file:\n\nC:\\Users\\thumb\\Documents\\projects\\node\\send-email\\send-test-email.js\n\nSends a test email \n\nnode send-test-email.js   \n\nThe default 'to' email address is rp@asna.com",
        "locale": "en",
        "slug": "technical-posts/react-email-workflow",
        "folder": ""
    },
    {
        "title": "Sveltekit forms - request.FormData1",
        "description": "Sveltekit forms - request.FormData1",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "forms"
        ],
        "content": "Form data is sent to a target action through the event argument:\n\nexport const actions: Actions = {\n    create: async (event) => {\n        const formData = Object.fromEntries(await event.request.formData());\n\nThe formData object is essentially a set of values for each form input received:\n\n![image-2.png|362x424]]\nThis line\n\nconst formData = Object.fromEntries(await event.request.formData());\n\nis the canonical way to convert the form data into a JavaScript object. [Object.fromEntries transforms a list of key-value pairs into an object.",
        "locale": "en",
        "slug": "technical-posts/sveltekit-forms-request-formdata",
        "folder": ""
    },
    {
        "title": "Luther 2.0 backup docs (using the term loosely)",
        "description": "Luther 2.0 backup docs (using the term loosely)",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "The PowerShell script to run the backup is here:\n\n\"C:\\Users\\thumb\\Documents\\Projects\\general-utilities\\luther-backup.ps1\"\n\n.\\mount-all.ps1\n\nget-sqldatabase -serverinstance DelRay | where { $_.Name -ne 'tempdb' } | backup-sqldatabase\n\npush-location -path .\nset-location -path \"C:\\Users\\thumb\\Documents\\Projects\\cs\\cs-misc\\LutherBackup\\LutherBackup\\bin\\Debug\\net6.0\\\"\n.\\LutherBackup.exe run-backup --device *all\npop-location\n.\\dismount-all.ps1\n\ncmd /c pause\n\nC# LutherBackup program\n\nC:\\Users\\thumb\\Documents\\Projects\\cs\\cs-misc\\LutherBackup\n\nappsettings.json as of 26 Oct 2023\n\n{\n  \"Config\": {\n    \"RoboCopyArgs\": \"/mt:64 /mir /v /nfl\",\n    \"RoboCopyExcludeFiles\": \"/xf NTUSER.DAT* ntuser.ini *.gm2 *.gbp *.pst *.vhdx .dmp\",\n    \"RoboCopyExcludeFolders\": \"/xd node_modules AppData dat env site-packages .git Dropbox\",\n    \"RoboCopyLogArg\":  \"/log:\"\n  },\n  \"BackupDriveInfo\": [\n    {\n      \"DriveName\": \"rpwin10git\",\n      \"SourceDirectory\": [\n        \"users\\\\roger\\\\documents\"\n      ],\n      \"TargetDevice\": [\n        \"seagate-4tb-desktop\",\n        \"seagate-4tb-little\"\n      ]\n    },\n    {\n      \"DriveName\": \"win10rp-1809\",\n      \"SourceDirectory\": [\n        \"c:\\\\users\\\\roger\\\\documents\",\n        \"c:\\\\utilities\",\n        \"c:\\\\users\\\\roger\\\\downloads\",\n        \"C:\\\\Program Files\\\\Microsoft SQL Server\\\\MSSQL14.MSSQLSERVER\\\\MSSQL\\\\Backup\"\n      ],\n      \"TargetDevice\": [\n        \"seagate-4tb-little\",\n        \"seagate-4tb-desktop\"\n      ]\n    },\n    {\n      \"DriveName\": \"delray\",\n      \"SourceDirectory\": [\n        \"users\\\\thumb\\\\documents\",\n        \"users\\\\thumb\\\\zotero\"\n      ],\n      \"TargetDevice\": [\n        \"seagate-4tb-little\",\n        \"seagate-4tb-desktop\"\n      ]\n    },\n    {\n      \"DriveName\": \"win-11-01\",\n      \"SourceDirectory\": [\n        \"users\\\\thumb\\\\documents\"\n      ],\n      \"TargetDevice\": [\n        \"seagate-4tb-desktop\",\n        \"seagate-4tb-little\"\n      ]\n    }\n  ]\n}",
        "locale": "en",
        "slug": "technical-posts/luther-backup-notes",
        "folder": ""
    },
    {
        "title": "An example docker file",
        "description": "An example docker file",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "docker",
            "containers",
            "podman"
        ],
        "content": "An example Dockerfile for setting up a PostgreSQL container.\n\nImportant Consideration:\n\nUsing Official Images: For standard services like databases (PostgreSQL, MySQL, Redis, etc.), it's almost always better to use the official image directly rather than building your own Dockerfile from scratch unless you need very specific customizations during the build process (like installing custom extensions requiring compilation). The official images are well-maintained, secure, and handle initialization correctly.\nNamed Volumes are Runtime: You declare a volume mount point within the Dockerfile using the VOLUME instruction. However, you create and attach the named volume when you run the container (docker run ... -v your-volume-name:/path/in/container ...) or define it in a docker-compose.yml file. The Dockerfile itself doesn't create the named volume.\n\nTherefore, the simplest Dockerfile for PostgreSQL often just leverages the official image and its built-in mechanisms. You'd typically use a docker-compose.yml file or a docker run command to manage the named volume and environment variables.\n\nHowever, if you wanted a Dockerfile that could be used as a base, perhaps adding custom initialization scripts, it might look something like this:\n\nExample Dockerfile (Illustrative - Often not needed for basic setup)\n\nUsing a specific version is recommended over 'latest' for stability\nFROM postgres:15\n\n--- Environment Variables (Can also be set at runtime) ---\nSet the password for the default 'postgres' superuser\n!! IMPORTANT: Do NOT hardcode sensitive passwords here in production.\n!! Use environment variables passed during 'docker run' or secrets management.\nENV POSTGRES_PASSWORD=mysecretpassword\n\nOptional: Create a default database for your application\nENV POSTGRES_DB=app_db\n\nOptional: Create a default user for your application\nENV POSTGRES_USER=app_user\nIf POSTGRES_USER is set, POSTGRES_PASSWORD applies to this user too\n\n--- Custom Initialization (Optional) ---\nCopy custom SQL or shell scripts to be run on first initialization\nThese scripts run only when the database cluster is initialized (i.e., the volume is empty)\nCOPY ./my_init_scripts/ /docker-entrypoint-initdb.d/\n\n--- Data Volume ---\nDeclare the standard PostgreSQL data directory as a volume mount point.\nThis tells Docker that data in this directory should be persisted.\nWhen you run the container, you'll map a named volume to this path.\nVOLUME /var/lib/postgresql/data\n\n--- Port Exposure ---\nDocument that the container exposes the default PostgreSQL port\nThis doesn't publish the port; publishing happens during 'docker run -p ...'\nEXPOSE 5432\n\nThe CMD is inherited from the base postgres image, which starts the server.\nYou typically don't need to override CMD or ENTRYPOINT unless doing something advanced.\n\nHow to Build and Run (The important part for named volumes):\n\nSave: Save the content above into a file named Dockerfile in an empty directory (or a directory containing only your optional my_init_scripts folder if you uncommented that section).\n\nBuild (Optional, as you often just use the official image):\n\n    docker build -t my-custom-postgres .\n\n    (Note: If you didn't add any custom files or steps beyond ENV/VOLUME/EXPOSE, this custom image is functionally identical to the base postgres:15 image).\n\nCreate a Named Volume (If it doesn't exist):\n\n    docker volume create postgres_data\n\nRun the Container using the Named Volume:\n\n    Using the official image directly (Recommended):\n\n        docker run --name my-postgres-container -d \\\n          -e POSTGRES_PASSWORD=mysecretpassword \\\n          -e POSTGRES_DB=app_db \\\n          -e POSTGRES_USER=app_user \\\n          -p 5432:5432 \\\n          -v postgres_data:/var/lib/postgresql/data \\\n          postgres:15\n\n    Using your custom-built image (if you built one):\n        docker run --name my-postgres-container -d \\\n          -p 5432:5432 \\\n          -v postgres_data:/var/lib/postgresql/data \\\n          my-custom-postgres\n        (Note: The ENV variables set in the Dockerfile will be used unless overridden with -e here)\n\nExplanation of the docker run command:\n\ndocker run: The command to create and start a container.\n--name my-postgres-container: Assigns a name to the container for easy reference.\n-d: Runs the container in detached mode (in the background).\n-e POSTGRES_PASSWORD=...: Sets environment variables (overriding any in the image if needed). This is the secure way to pass passwords.\n-p 5432:5432: Publishes the container's port 5432 to the host machine's port 5432.\n-v postgres_data:/var/lib/postgresql/data: This is the key part. It maps the named volume postgres_data (which you created or Docker creates automatically if it doesn't exist) to the /var/lib/postgresql/data directory inside the container (the one declared with VOLUME in the Dockerfile). This ensures data persistence.\npostgres:15 or my-custom-postgres: The image to use.\n\nIn summary, while the Dockerfile declares the volume mount point (/var/lib/postgresql/data), the actual creation and attachment of the named volume (postgres_data) happens at runtime via the docker run -v command or a docker-compose.yml file. For standard PostgreSQL, using the official image directly with docker run or docker-compose is usually the best approach.",
        "locale": "en",
        "slug": "technical-posts/example-docker-file",
        "folder": ""
    },
    {
        "title": "Vanilla CSS for Sveltekit",
        "description": "Vanilla CSS for Sveltekit",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-15T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "css"
        ],
        "content": "This document shows how to use vanilla CSS without an preprocessing. Given how much CSS has improved over the last couple of years, it's hard to make a good case for PostCSS. \nThe Sveltekit build step (with Vite's help) automatically compresses CSS.\nCSS's @import (which may, or may now, impede performance) runs at SvelteKit so it's performance is a moot point.\n\n src/\n     css  /\n        components/\n           _components.css\n           button.css\n        elements/\n           _elements.css\n           select.css\n        global/\n           _global.css\n           reset.css\n           tags.css\n           vars.css\n        layouts/\n           _layouts.css\n        utilities/\n           _utilities.css\n        _style.css      \n     lib/\n        assets/\n           favicon.svg\n        routes/\n            +layout.svelte\n            +page.svelte    \n     routes/\n        ..      \n     style.css\n\n+layout.svelte\n\n\n\timport favicon from '$lib/assets/favicon.svg';\n\timport '../style.css';\n\n\tlet { children } = $props();\n\n\n\n\t\n\n\n{@render children?.()}\n\nsrc/css/style.css\n\n@layer global, layouts, elements, components, utilities;\n\n/* @import \"https://unpkg.com/open-props\" layer(global); */\n@import './global/_global.css' layer(global);\n@import './layouts/_layouts.css' layer(layouts);\n@import './elements/_elements.css' layer(elements);\n@import './components/_components.css' layer(components);\n@import './utilities/_utilities.css' layer(utiilties);\n\nsrc/style.css\n\n@import './css/_style.css';\n\nThis is the swindle that makes this all work. At build time (or runtime in dev mode), style.css imports the various CSS files. Then /css/style.css imports the rendered style.css contents. It's almost magic--CSS @imports are driving all of this. At build time, Sveltekit and Vite produce a concrete, compressed, version of /css/style.css. That single, static CSS file is produced at (where `` is the hash file name created.)\n\n.svelte-kit\\output\\client\\_app\\immutable\\assets\\.css\n\nBy the way, this CSS process occurs without any configuration! \n\nroutes/+layout.svelte\n\n\n\timport favicon from '$lib/assets/favicon.svg';\n\timport '../style.css';\n\n\tlet { children } = $props();\n\n\n\n\t\n\n\n{@render children?.()}\n\n[!info]\nI avoided using @import with the original ASNA.com because of the potential performance implications of using @import. However, that was silly because I didn't understand how powerful the Sveltekit/Vite build process is. \nImplementation details\n\nEach directory under css is a category of css that is assigned to a given layer. \n\n_global.css\n\n@import \"reset.css\";\n@import \"vars.css\";\n@import \"tags.css\";\n\nreset.css\n\n*,\n*::before,\n*::after {\n  box-sizing: border-box;\n}\n\n/* Prevent font size inflation */\nhtml {\n  -moz-text-size-adjust: none;\n  -webkit-text-size-adjust: none;\n  text-size-adjust: none;\n}\n\n/* Remove default margin in favour of better control in authored CSS */\nbody, h1, h2, h3, h4, p,\nfigure, blockquote, dl, dd {\n  margin-block-end: 0;\n} \n\n/* Remove list styles on ul, ol elements with a list role, which suggests default styling will be removed */\nul[role='list'],\nol[role='list'] {\n  list-style: none;\n}\n\nu, ol {\n  font-size: inherit;\n}\n\n/* Set core body defaults */\nbody {\n  min-height: 100vh;\n  line-height: 1.5;\n}\n\n/* Set shorter line heights on headings and interactive elements */\nh1, h2, h3, h4,\nbutton, input, label {\n  line-height: 1.1;\n}\n\n/* Balance text wrapping on headings */\n/* h1, h2,\nh3, h4 {\n  text-wrap: balance;\n}  */\n\n/* A elements that don't have a class get default styles */\na:not([class]) {\n  text-decoration-skip-ink: auto;\n  color: currentColor;\n}\n\n/* Make images easier to work with */\nimg,\npicture {\n  max-width: 100%;\n  display: block;\n}\n\n/* Inherit fonts for inputs and buttons */\ninput, button,\ntextarea, select {\n  font-family: inherit;\n  font-size: inherit;\n}\n\n/* Make sure textareas without a rows attribute are not tiny */\ntextarea:not([rows]) {\n  min-height: 10em;\n}\n\n/* Anything that has been anchored to should have extra scroll margin */\n:target {\n  scroll-margin-block: 5ex;\n}\n\n\nThe ?. is the optional chaining operator. It safely handles cases where children might be undefined or null.\n\nWhat it does\n\n{@render children?.()}\n\nThis means:\nIf children exists: Render it\n**If children is undefined or null: Do nothing (no error thrown)\n\nWithout optional chaining\n\n{@render children()}\n\nThis would throw an error if children is undefined.\n\nWhy it's needed\n\nIn SvelteKit layouts, the children snippet represents the content of child routes. There might be edge cases during:\nInitial rendering\nRoute transitions\nError states\n\nWhere children could temporarily be undefined.\n\nEquivalent to\n\n// In regular JavaScript\nchildren ? children() : undefined\n\nThe ?.() is specifically for optional function/snippet calls. There's also:\nobject?.property - optional property access\narray?.[index] - optional array indexing\n\nThis is a defensive programming practice to prevent runtime errors when the snippet might not be available.",
        "locale": "en",
        "slug": "technical-posts/vanilla-css-for-sveltekit",
        "folder": ""
    },
    {
        "title": "Using Flex search with ASNA.com for its search engine",
        "description": "Using Flex search with ASNA.com for its search engine",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-23T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "search",
            "asna-com"
        ],
        "content": "Creating Flex search input\n\nThere are two existing Json files that provide data to Algolia:\n\nsrc\\lib\\data\\search-index.json\nsrc\\lib\\data\\search-index-pages.json\n\n!info]\nSee the src\\lib\\cmd-line\\refresh-algolia-index.js script for how it uses the Json files above. All of the files specified here are relative to the Project Ocho Sveltekit project root.\n\n[!revisit]\nWrite a document that explains the difference between indexing markdown pages and HTML pages.\n\nIt was a little sloppy using these two Json files directly because a) they aren't shaped the same and and b) not all of the properties provided are necessary for search indexing.\n\n[Flex is an in-memory search engine. The index is recreated, at runtime, on users' browsers. To try to reduce as much friction as possible for Flex this Node script:\n\nsrc\\lib\\cmd-line\\create-flex-search-input.js\n\ncreates a Flex-specific input file (using the two Json files from above as input) named src\\lib\\data\\flex-search-input.json. This Json contains only the properties needed for indexing (and therefore every object in this array has the same shape). Being one file, it also minimizes file activity for Flex indexing.\n\nThe shape of the Flex indexing object is:\n\n{\n\t\"locale\": string;\n\t\"content\": string;\n\t\"slug\": string;\n\t\"title\": string;\n\t\"description\": string;\n\t\"tags_list\": string;\n\t\"isPage\": boolean;\n},\n\n[!danger]\nFor production use, I need to integrate this new Node script with Project Ocho's full-deploy.ps1 script.\n\nreadyPromise in the new code\n\nreadyPromise is a one-time gate that pauses performSearch() until the worker tells you it has finished building the index (i.e., it posts { type: 'ready' }).\n\nHow it works (step by step)\n\nWhen you start the worker, you create a Promise and capture its resolver:\n\nreadyPromise = new Promise((resolve) => (resolveReady = resolve));`\n\nreadyPromise is now a pending Promise.\nresolveReady is a function you can call later to fulfill that Promise.\n\nYou send load to the worker:\n\nsearchWorker.postMessage({ type: 'load' });`\n\nWhen the worker finishes indexing, it posts back { type: 'ready' }.\n   Your onWorkerMessage handler receives it and calls the resolver:\n\nresolveReady?.();\nresolveReady = null;`\n\nCalling resolveReady() transitions readyPromise from pending  resolved.\n\nperformSearch() awaits that promise:\n\nawait readyPromise;`\n\nIf the worker isnt ready yet, execution pauses here.\nOnce resolveReady() is called, the await continues and the code sends the search request.\n\nWhy this is useful\nIt prevents a race where you might do:\n\npostMessage('load')\nimmediately postMessage('search_en')\n\nbefore the index exists.\n\nSmall gotcha to be aware of\nTypeScript-wise, readyPromise can be null, but performSearch() does await readyPromise. In practice your code sets it before awaiting, but you can make it safer by ensuring its always defined (e.g., return the promise from startSearchEngine()), if you want stricter correctness.",
        "locale": "en",
        "slug": "technical-posts/using-flex-search-with-asna-com",
        "folder": ""
    },
    {
        "title": "Putting the Postgres connection pool in the SvelteKit context object",
        "description": "Putting the Postgres connection pool in the SvelteKit context object",
        "date_created": "2025-06-07T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "postgres"
        ],
        "content": "The database pool is placed into the locals context via the SvelteKit handle hook in your hooks.server.ts file. Let's examine how this works:\n\nThe Flow:\n\nPool Initialization\n   In hooks.server.ts, the PostgreSQL pool is created once when the server starts:\n   // Create the pool once when the server starts\n   const pool = new Pool({\n     host: PG_HOST,\n     port: Number(PG_PORT) || 5432,\n     user: PG_USER,\n     password: PG_PASSWORD,\n     database: PG_DATABASE,\n     ssl: false\n   });\n\nAdding to locals in the Handle Hook\n   The pool is then added to event.locals in the handle hook:\n   export const handle: Handle = async ({ event, resolve }) => {\n     // Make the pool available to all server-side endpoints\n     event.locals.pool = pool;\n     \n     event.locals.logger = logger;\n     \n     // Continue processing the request\n     const response = await resolve(event);\n     \n     return response;\n   };\n\nAccessing in Route Handlers\n   In your route handler files (like +page.server.ts), you access it via the locals parameter:\n   export const load: PageServerLoad = async ({ locals }) => {\n     const pool = locals.pool;\n     // Use the pool\n   };\n\nImportant Points:\n\nlocals is a server-only context object that exists per-request\nIt's different from Svelte's component context (via getContext/setContext)\nThe pool is created once at server startup and reused across all requests\nOnly server-side code can access locals (endpoints, +page.server.ts, +layout.server.ts, etc.)\nClient-side components cannot directly access the database pool\n\nThis pattern effectively gives you a singleton pool that's available throughout the server-side of your SvelteKit application without having to pass it around explicitly or create it multiple times.\n\nUsing Svelte 5 Runes for Postgres Pool\n\nYou can't directly put a Postgres pool in a Svelte 5 rune for universal access, as database connections should only exist on the server side. Here's why and what to do instead:\n\nWhy Not Use Runes for Database Connections\n\nSecurity: Client-side JavaScript should never have direct database access\nArchitecture: Postgres connections are server-only resources\nPerformance: Database connections are heavyweight and not appropriate for client-side code\n\nRecommended Approach\n\nInstead, create a server-only data access layer:\n\n1. Create a Server-Only API Service\n\n[!info]\nSee above for how to put the Postgres pool in the context object.\n\nimport { getContext } from 'svelte';\nimport type { Pool } from 'pg';\n\n// Server-only: Create typed functions to access database\nexport async function queryDatabase(sql: string, params: any[] = []): Promise {\n  // This works in server-side components, +page.server.ts, +layout.server.ts\n  const locals = getContext('locals');\n  const pool = locals?.pool as Pool;\n  \n  if (!pool) {\n    throw new Error('Database pool not available');\n  }\n  \n  const client = await pool.connect();\n  try {\n    const result = await client.query(sql, params);\n    return result.rows as T[];\n  } finally {\n    client.release();\n  }\n}\n\n2. For Server Components (Svelte 5)\n\n\n  import { queryDatabase } from '$lib/server/db-service';\n  import { onMount } from 'svelte';\n  \n  let families = $state([]);\n  let loading = $state(true);\n  let error = $state(null);\n  \n  // Server-only function\n  async function loadFamilies() {\n    try {\n      families = await queryDatabase('SELECT * FROM family ORDER BY sort_order');\n      loading = false;\n    } catch (e) {\n      error = e.message;\n      loading = false;\n    }\n  }\n  \n  onMount(loadFamilies);\n\n\n{#if loading}\n  Loading...\n{:else if error}\n  {error}\n{:else}\n  \n    {#each families as family}\n      {family.name}\n    {/each}\n  \n{/if}\n\n3. For Universal Components (Client + Server)\n\nFor components that need to run on both client and server, use SvelteKit's data loading pattern:\n\nexport const load = async ({ locals }) => {\n  const { pool } = locals;\n  \n  try {\n    const client = await pool.connect();\n    try {\n      const result = await client.query('SELECT * FROM family ORDER BY sort_order');\n      return { \n        families: result.rows \n      };\n    } finally {\n      client.release();\n    }\n  } catch (error) {\n    return {\n      families: [],\n      error: error.message\n    };\n  }\n};\n\n\n  export let data;\n  \n  const { families, error } = data;\n\n\n{#if error}\n  {error}\n{:else}\n  \n    {#each families as family}\n      {family.name}\n    {/each}\n  \n{/if}\n\nThis approach maintains the proper separation between client and server code while leveraging Svelte 5's runes when appropriate.",
        "locale": "en",
        "slug": "technical-posts/postgres-pool-object-context-api",
        "folder": ""
    },
    {
        "title": "Rust concept of borrowing a value",
        "description": "Rust concept of borrowing a value",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "In Rust, borrowing is a key concept that enables memory safety and concurrency without a garbage collector. It allows you to refer to a value without taking ownership of it, meaning you can access or use data without moving it. This is crucial for ensuring that multiple parts of a program can use the same data safely.\n\nKey Concepts of Borrowing\n\nReferences:\n\n    Borrowing is done through references, which are created using the & symbol for immutable references and &mut for mutable references.\n    An immutable reference allows read access to the value, while a mutable reference allows both read and write access.\n\nOwnership Rules:\n\n    You can have either multiple immutable references (&T) or a single mutable reference (&mut T) to a value at any time, but not both. This prevents data races at compile time.\n\nLifetime:\n    References have lifetimes, which are annotations that tell the Rust compiler how long a reference is valid. The compiler checks that references do not outlive the data they point to.\n\nExamples\n\nImmutable Borrowing\n\nHeres an example of immutable borrowing:\n\nfn main() {\n    let s = String::from(\"Hello, world!\");\n\n    let len = calculate_length(&s); // Borrowing s\n\n    println!(\"The length of '{}' is {}.\", s, len);\n}\n\nfn calculate_length(s: &String) -> usize {\n    s.len() // Read-only access to s\n}\n\nIn this example:\n\nThe calculate_length function borrows s as an immutable reference. The original owner of s (in main) can still use s after the borrowing.\n\nMutable Borrowing\n\nHeres an example of mutable borrowing:\n\nfn main() {\n    let mut s = String::from(\"Hello\");\n\n    change(&mut s); // Borrowing s as mutable\n\n    println!(\"{}\", s);\n}\n\nfn change(s: &mut String) {\n    s.push_str(\", world!\"); // Modifying the borrowed value\n}\n\nIn this example:\n\nThe change function borrows s as a mutable reference. After this borrowing, no other references (mutable or immutable) can access s until the borrowing ends.\n\nBenefits of Borrowing\n\nMemory Safety: By enforcing strict rules on ownership and borrowing, Rust prevents common bugs related to dangling pointers, data races, and memory leaks at compile time.\nPerformance: Borrowing avoids unnecessary data copying, allowing for efficient use of resources.\n\nConclusion\n\nBorrowing is a fundamental concept in Rust that allows for safe and efficient management of memory. By using references, Rust enforces strict ownership rules, ensuring that data is accessed and modified in a way that prevents bugs and enhances performance. Understanding borrowing is crucial for mastering Rust and writing safe, concurrent applications.",
        "locale": "en",
        "slug": "technical-posts/rust-concept-of-borrowing-a-value",
        "folder": ""
    },
    {
        "title": "Zod  refinements",
        "description": "Zod  refinements",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "zod"
        ],
        "content": "In Zod,.regex()and.transform()arerefinementson the base string schema. ThedateFormatStringis using a combination of Zod's schema methods:\n\nz.string()- Creates a base string schema\n.regex()- Adds a pattern refinement\n.transform()- (if used) Would transform the value after validation\n\nHere's the breakdown of Zod methods being used:\n\nconst dateFormatString = z\n    .string() // Base schema\n    .regex(\n        // Refinement\n        /^\\d{4}-\\d{2}-\\d{2}$/,\n        { message: \"Date must be in the format YYYY-MM-DD\" }\n    );\n\nIf you wanted to also transform the string into a Date, you would add:\n\nconst dateFormatString = z\n    .string() // Base schema\n    .regex(\n        // Refinement\n        /^\\d{4}-\\d{2}-\\d{2}$/,\n        { message: \"Date must be in the format YYYY-MM-DD\" }\n    )\n    .transform(\n        (\n            val // Transformation\n        ) => new Date(val)\n    );\n\nThe key differences:\n\n.regex()is arefinement- it validates but doesn't change the value\n.transform()is atransformation- it changes the value after validation\nBoth are part of Zod's \"schema composition\" pattern\n    in Zod, the.transform()method returns the transformed value as the new value for that schema. Here's an example:\n\nimport { z } from \"zod\";\n\nconst dateFormatString = z\n    .string()\n    .regex(/^\\d{4}-\\d{2}-\\d{2}$/, {\n        message: \"Date must be in the format YYYY-MM-DD\",\n    })\n    .transform((val) => {\n        const date = new Date(val);\n        if (isNaN(date.getTime())) {\n            throw new Error(\"Invalid date string\");\n        }\n        return date; // This becomes the new value\n    });\n\n// Using the schema:\nconst result = dateFormatString.parse(\"2023-04-16\");\nconsole.log(result); // Output: Date object for 2023-04-16\nconsole.log(typeof result); // Output: \"object\" (Date object)\n\n``\nThis means:\n\nInput string \"2023-04-16\" is validated by the regex\nIf valid, it's passed to the transform function\nTransform converts it to a Date object\nThe Date object becomes the final value\nThe type inference (z.infer) will show this asDateinstead ofstring",
        "locale": "en",
        "slug": "technical-posts/zod-refinements",
        "folder": ""
    },
    {
        "title": "Slots are deprecated",
        "description": "Slots are deprecated",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Joy of Code snippets video\nSvelte 5 docs  Multiple content placeholders\n+layout.svelte\n\nSvelte 4\n\n\n\nSlots are deprecated in Svelte 5; replaced with @render:\n\nSvelte 5\n\n\n    let {children}  = $props();\n\n\n\n    {@render children()}\n\n\nEverything that came in through the ` now comes in through the children` property.",
        "locale": "en",
        "slug": "technical-posts/slots-are-deprecated",
        "folder": ""
    },
    {
        "title": "React Email nuggets mso outlook",
        "description": "React Email nuggets mso outlook",
        "date_created": "2025-06-23T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "react-email",
            "nuggets"
        ],
        "content": "Use 'mso' special comments\n\n\n\t\t\n\t\t\t\n\t\t\t\t&nbsp;\n\t\t\t\n\t\t\n\t\t`,\n\t}}\n/>\n\nPass children to a component\n\ninterface MyComponentProps {\n  children: React.ReactNode;\n}\n\nconst MyComponent: React.FC = ({ children }) => {\n    return (\n            \n                {children}\n            \n        \n    );\n};\n\n\nchildren here\n\nTest for props being provided\n\nTo negate the test and do something if a value is falsy, you can use the logical NOT operator (!). Here's how you would modify your code to do something whenbulletData.introTextis falsy:\n\nTest for truthy\n\n{bulletData.introText && (\n    \n        Default intro text when no intro is provided\n    \n)}\n\nTest for falsy \n\n{!bulletData.introText && (\n    \n        Default intro text when no intro is provided\n    \n)}\n\nYou can also use a ternary operator to handle both cases (truthy and falsy):\n\n{bulletData.introText ? (\n    \n        {bulletData.introText}\n    \n) : (\n    \n        Default intro text when no intro is provided\n    \n)}\n\nThe ternary approach is especially useful when you want to render different content based on the condition, rather than simply showing or hiding a component.\n\nBoth approaches let you conditionally render components when a value is falsy (undefined, null, empty string, 0, or false).\n\nAn Outlook-specific spacer\n\nThis is wrapped in an HtmlComment component.\n\n\"\n\tmso=\"true\"\n/>\n``",
        "locale": "en",
        "slug": "technical-posts/react-email-nuggets",
        "folder": ""
    },
    {
        "title": "How to create and use a Rust crate",
        "description": "How to create and use a Rust crate",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "To package the find_matching_files function into its own reusable Rust crate, you can follow these steps:\n\n1. Create a New Crate\n\nFirst, you'll create a new Rust library crate. Run the following command:\n\ncargo new find_files --lib\n\nThis will create a directory named find_files with the following structure:\n\nfind_files\n Cargo.toml\n src\n     lib.rs\n\n2. Modify Cargo.toml\n\nOpen the Cargo.toml file and modify the package metadata if needed. You will also need to add the glob crate as a dependency:\n\npackage]\nname = \"find_files\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\nglob = \"0.3\"\n\n3. Implement the Function in lib.rs\n\nIn the src/lib.rs file, implement the find_matching_files function:\n\nuse glob::glob;\nuse std::path::Path;\n\npub fn find_matching_files(directory: &str, pattern: &str) -> Vec {\n    // Combine directory and pattern\n    let full_pattern = format!(\"{}/{}\", directory, pattern);\n\n    let mut files = Vec::new();\n\n    // Use glob to find matching files\n    for entry in glob(&full_pattern).expect(\"Failed to read glob pattern\") {\n        match entry {\n            Ok(path) => {\n                if let Some(path_str) = path.to_str() {\n                    files.push(path_str.to_string());\n                }\n            },\n            Err(e) => eprintln!(\"Error matching file: {}\", e),\n        }\n    }\n\n    files\n}\n\npub fn is_valid_directory(directory: &str) -> bool {\n    Path::new(directory).is_dir()\n}\n\nIn this version:\n\nThe find_matching_files function is public (pub) so that it can be reused in other projects.\nAdded is_valid_directory to check if a directory is valid, also made public (pub).\n\n4. Build the Crate\n\nTo make sure everything is set up correctly, run the following command inside the find_files directory:\n\ncargo build\n\n5. Use the Crate in Another Project\n\nYou can now use this crate in another project by adding a dependency in the Cargo.toml of your main project. If you want to use the crate locally, add it as a path dependency:\n\n[dependencies]\nfind_files = { path = \"../find_files\" }\n\nIf you want to specify a fully qualified path (absolute path) to the find_files crate in Cargo.toml, you can do so like this:\n\n[dependencies]\nfind_files = { path = \"/full/path/to/find_files\" }\n\nReplace \"/full/path/to/find_files\" with the actual path on your filesystem.\n\nExample Usage\n\nIn your main program, you can now use the find_files crate:\n\nuse find_files::{find_matching_files, is_valid_directory};\nuse std::env;\nuse std::fs::File;\nuse std::io::{self, Write};\n\nfn write_to_file(file_path: &str, files: &[String]) -> io::Result {\n    let mut file = File::create(file_path)?;\n\n    for file_name in files {\n        writeln!(file, \"{}\", file_name)?;\n    }\n\n    Ok(())\n}\n\nfn main() {\n    // Get command-line arguments\n    let args: Vec = env::args().collect();\n\n    if args.len() != 4 {\n        eprintln!(\"Usage: {}   \", args[0]);\n        return;\n    }\n\n    let directory = &args[1];\n    let pattern = &args[2];\n    let output_file = &args[3];\n\n    if !is_valid_directory(directory) {\n        eprintln!(\"Error: {} is not a valid directory\", directory);\n        return;\n    }\n\n    let files = find_matching_files(directory, pattern);\n\n    match write_to_file(output_file, &files) {\n        Ok(_) => println!(\"Successfully wrote file names to {}\", output_file),\n        Err(e) => eprintln!(\"Failed to write to file {}: {}\", output_file, e),\n    }\n}\n\n6. Publish the Crate (Optional)\n\nIf you want to publish this crate on [crates.io, follow these steps:\n\nCreate an account on crates.io and link it to your Cargo installation by running:\n\n    cargo login \n\nPublish your crate by running:\n    cargo publish\n\nThis will make your find_files crate publicly available for anyone to use!",
        "locale": "en",
        "slug": "technical-posts/create-a-rust-crate",
        "folder": ""
    },
    {
        "title": "Sveltekit server-side load event",
        "description": "Sveltekit server-side load event",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "In a server-side SvelteKit file (+page.server.js in this case), the event argument provides all kinds of information about the request .\n\nexport const load = async (event) => {\n\tconsole.log('test=>', event);\n\n\tconst form = await superValidate(event, contactFormSchema);\n\n\treturn {\n\t\tform\n\t};\n};\n\nThis the data available in the event argument. I think the arg is poorly named in this case, but still, this is the data that's available.\n\ncookies: {\n    get: [Function: get],\n    getAll: [Function: getAll],\n    set: [Function: set],\n    delete: [Function: delete],\n    serialize: [Function: serialize]\n  },\n  fetch: [Function: fetch],\n  getClientAddress: [Function: getClientAddress],\n  locals: { locale: 'en' },\n  params: {},\n  platform: undefined,\n  request: Request {\n    [Symbol(realm)]: { settingsObject: [Object] },\n    [Symbol(state)]: {\n      method: 'GET',\n      localURLsOnly: false,\n      unsafeRequest: false,\n      body: null,\n      client: [Object],\n      reservedClient: null,\n      replacesClientId: '',\n      window: 'client',\n      keepalive: false,\n      serviceWorkers: 'all',\n      initiator: '',\n      destination: '',\n      priority: null,\n      origin: 'client',\n      policyContainer: 'client',\n      referrer: 'client',\n      referrerPolicy: '',\n      mode: 'cors',\n      useCORSPreflightFlag: false,\n      credentials: 'same-origin',\n      useCredentials: false,\n      cache: 'default',\n      redirect: 'follow',\n      integrity: '',\n      cryptoGraphicsNonceMetadata: '',\n      parserMetadata: '',\n      reloadNavigation: false,\n      historyNavigation: false,\n      userActivation: false,\n      taintedOrigin: false,\n      redirectCount: 0,\n      responseTainting: 'basic',\n      preventNoCacheCacheControlHeaderModification: false,\n      done: false,\n      timingAllowFailed: false,\n      headersList: [HeadersList],\n      urlList: [Array],\n      url: URL {}\n    },\n    [Symbol(signal)]: AbortSignal { aborted: false },\n    [Symbol(headers)]: HeadersList {\n      cookies: null,\n      [Symbol(headers map)]: [Map],\n      [Symbol(headers map sorted)]: null\n    }\n  },\n  route: { id: '/en/test' },\n  setHeaders: [Function: setHeaders],\n  url: URL {\n    href: 'http://localhost:5173/en/test',\n    origin: 'http://localhost:5173',\n    protocol: 'http:',\n    username: '',\n    password: '',\n    host: 'localhost:5173',\n    hostname: 'localhost',\n    port: '5173',\n    pathname: '/en/test',\n    search: '',\n    searchParams: URLSearchParams {},\n    hash: ''\n  },\n  isDataRequest: false,\n  isSubRequest: false,\n  depends: [Function: depends],\n  parent: [AsyncFunction: parent]\n}\n\nFor example, to get the URL for a redirect to the current URL, use event.route.id",
        "locale": "en",
        "slug": "technical-posts/request-data-available-to-sveltekit-server-side",
        "folder": ""
    },
    {
        "title": "Get file inventory with PowerShell",
        "description": "Get file inventory with PowerShell",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "This folder containers some PowerShell work to get a file inventory:\n\nC:\\Users\\thumb\\Documents\\projects\\avr\n\nThe PowerShell script below produces an Excel spreadsheet like this:\n\n![Get file inventory with PowerShell.png|995]]\n\nYou must provide the directory names to exclude and the file extensions to include. If Excel isn't present on the PC where you are running this script, remove the -show argument from the Export-Excel cmdlet.\n\nparam (\n    [string]$Title = \"File listing.\"\n)\n\n$excludedDirectories = @(\"debug\", \"release\", \"packages\", \"properties\")\n$includedExtensions = @(\".vr\", \".dll\", \".sln\", \".aspx\", \".asmx\", \".ashx\", \"*.master\")\n\n$Title = \"$Title - $(Get-Date -Format 'yyyy-MM-dd')\"\n\nfunction Should-IncludeFile {\n    param (\n        [Parameter(Mandatory = $true, ValueFromPipeline = $true)$FileSystemInfo,\n        [string[]]$ExcludedDirectories,\n        [string[]]$IncludedExtensions\n    )\n\nCheck if it's a directory or a file\n    if ($FileSystemInfo -is [System.IO.DirectoryInfo]) {\n        #Exclude directories.  Function now only deals with files\n        return $false\n    }\n\nCheck Extension\n    $includeByExtension = $false  # Default to exclude\n    foreach ($FileExtension in $IncludedExtensions) {\n        if ($FileSystemInfo.Extension -eq $FileExtension) {\n            $includeByExtension = $true # Include if extension matches\n            break # Optimization: Exit loop as soon as we find a match\n        }\n    }\n\nCheck Directory\n    $includeByDirectory = $true  # Default to include\n    foreach ($ExcludedDirectory in $ExcludedDirectories) {\n        if ($FileSystemInfo.DirectoryName -match $ExcludedDirectory) {\n            $includeByDirectory = $false # Exclude if directory matches\n            break # Optimization: Exit loop as soon as we find a match\n        }\n    }\n\n    return $includeByExtension -and $includeByDirectory # Include only if BOTH conditions are true.\n\n}\n\n\nGet-ChildItem -Path \".\\.\" -recurse -File `\n| Where-Object { Should-IncludeFile -FileSystemInfo $_ -IncludedExtensions $includedExtensions -ExcludedDirectories $excludedDirectories } `\n| Select-object -property name, extension, directory, fullname, lastwritetime `\n| Sort-Object -Property extension, name `\n| Export-Excel test.xlsx -title $Title -titlebold -show -clearsheet -autosize -TableName maintable -TableStyle  Medium1\n\nThe ValueFromPipeline = $true attribute in the Should-IncludeFile function's parameter declaration is crucial for enabling the function to seamlessly integrate with the PowerShell pipeline. Here's a detailed explanation of why it's necessary and what it accomplishes:\n\nUnderstanding the PowerShell Pipeline\n\nThe PowerShell pipeline is a series of commands connected by the pipe operator (|). Each command in the pipeline receives input from the previous command and passes output to the next command. This allows you to chain together commands to perform complex operations.\n\nHow ValueFromPipeline = $true Works\n\n1. Automatic Input Binding: When ValueFromPipeline = $true is specified for a parameter, PowerShell automatically attempts to bind the input objects from the pipeline to that parameter. In other words, each object flowing through the pipeline is automatically passed to the function's specified parameter.\n\n2. Processing Each Object: The function is executed once for each object that it receives from the pipeline.  The ValueFromPipeline = $true attribute essentially tells PowerShell to treat the function as if it were being called repeatedly, once for each item in the pipeline.  In the Should-IncludeFile example, the function will be called once for every FileSystemInfo object that Get-ChildItem produces.\n\n3. Accessing the Current Object: Inside the function, the parameter ($FileSystemInfo in this case) will hold the current object from the pipeline.  This allows the function to process each file individually.\n\nWhy It's Needed in Should-IncludeFile\n\n* Filtering: The purpose of the Should-IncludeFile function is to decide whether to include or exclude a specific file based on certain criteria. To make this decision, the function must receive the FileSystemInfo (file or directory) object to examine.\n\n* Integration with Where-Object: The Where-Object cmdlet uses script blocks (or functions with ValueFromPipeline = $true) to filter objects. Where-Object calls the script block/function for each object coming through the pipeline. If the function returns $true, the object is passed on; if it returns $false, the object is filtered out.\n\n* Without ValueFromPipeline = $true: If you didn't have ValueFromPipeline = $true, the Where-Object cmdlet would try to pass the entire collection of FileSystemInfo objects to the Should-IncludeFile function at once. The function would not be able to process each file individually. It would not know how to iterate over all the files, and most likely the program would crash.\n\nIn Summary:\n\nValueFromPipeline = $true is essential because it tells PowerShell to pass each FileSystemInfo object from the Get-ChildItem cmdlet through the pipeline one at a time to the Should-IncludeFile function. This enables the function to inspect each file individually and determine whether to include or exclude it based on your filtering criteria. Without this attribute, the function would not work correctly within the pipeline.",
        "locale": "en",
        "slug": "technical-posts/get-file-inventory-with-powershell",
        "folder": ""
    },
    {
        "title": "Outlook ghost tables and other MSO related things",
        "description": "Outlook ghost tables and other MSO related things",
        "date_created": "2025-06-29T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "outlook",
            "email"
        ],
        "content": "Here are the major gotchas with Outlook MSO ghost tables that can trip you up:\n1. Width Calculation Issues\nGhost table width must account for parent padding/margins\nAlways calculate: ghostWidth = parentWidth - leftPadding - rightPadding\nUse integer values: width=\"536\" not width=\"536px\"\n\n2. Nested Table Problems\n\n\n\n  \n      \n    `\n\n3. Font Size/Line Height Conflicts\n\n\n  \n   `\n\n4. Alignment Inheritance Issues\n\n\n  \n   `\n\n5. Background Color Bleeding\n\n\n `\n\n6. Padding/Margin Double-Application\n\n\n  \n   `\n\n7. Missing role=\"presentation\"\n\n\n`\n\n8. Incomplete MSO Table Structure\n\n\ncontent\n\n\n\ncontent`\n\n9. Image Width Issues\n\n\n\n\n\n`\n\n10. Border Collapse Problems\n\n\n`\n\n11. Text Wrapping in Narrow Columns\n\n\n\nCol 1\nCol 2\n\n`\n\n12. Your JSON Structure Gotcha\nLooking at your JSON, watch out for:\n{\n  \"calloutss\": { // Typo: \"calloutss\" instead of \"callouts\"\n    \"spaceAfter\": \"0\" // String instead of number\n  }\n}`\n\n13. VML Objects Interference\n\n\n `\n\nPro Tips:\nTest in actual Outlook, not just browser dev tools\nUse Litmus or Email on Acid for comprehensive testing\nKeep ghost tables simple - avoid complex nesting\nAlways specify explicit widths on all td elements\nUse mso-line-height-rule: exactly for consistent spacing\n\nThe key is remembering that Outlook's Word rendering engine is very different from web browsers, and ghost tables are essentially creating a parallel rendering path specifically for Outlook's quirks.\n\nVML\n\nVML (Vector Markup Language) becomes a concern in email development specifically when dealing with Outlook 2007-2016 on Windows and certain advanced email design elements. Here's when you need to worry about it:\n\n1. Background Images in Outlook\n\n\n\n\n\n\n\n  Text over background\n\n\n\n\n`\n\n2. Rounded Corners/Buttons\n\n\n\n\n\n\n  Click Me\n\n\n\n\n`\n\n3. Your Email JSON Context - When You'd Need VML:\n\nLooking at your JSON structure, you'd need VML if you wanted to add:\n\nBackground Images Behind Text:\n{\n  \"heroSection\": {\n    \"backgroundImage\": \"https://nyc3.digitaloceanspaces.com/asna-assets/newsletter/summer.jpg\",\n    \"text\": \"Summer is here...\",\n    \"requiresVML\": true\n  }\n}`\n\nFancy Call-to-Action Buttons:\n{\n  \"callouts\": {\n    \"text\": \"ASNA support for Windows 10 ends on October 14th, 2025\",\n    \"buttonStyle\": \"rounded\", // This would need VML\n    \"imageUrl\": \"https://asna-assets.nyc3.digitaloceanspaces.com/newsletter/exclamation-point.png\"\n  }\n}`\n\n4. VML Gotchas with Ghost Tables:\n\nVML can interfere with your MSO ghost tables:\n\n\n\n\n\n\n\n\n  \n\n\nContent\n\n\n\n\n\n\n\n\n`\n\n5. When You DON'T Need VML (Your Current Case):\n\nBased on your JSON structure, you probably don't need VML because you're using:\nSimple text content\nStandard images (not backgrounds)\nBasic callouts with images\nStandard paragraph formatting\n\n6. VML Red Flags in Your Setup:\n\nYou'd need to worry about VML if you decide to enhance your design with:\nHero sections with text over background images\nRounded corner buttons/callouts \nComplex shaped elements\nGradient backgrounds\n\n7. VML Namespace Declaration:\n\nIf you do need VML, ensure your HTML includes:\n`\n\nBottom Line for Your Project:\n\nYour current email structure is simple and VML-free, which is actually good news! You're using:\nText content \nStandard images   \nBasic tables \nMSO ghost tables \n\nVML only becomes necessary when you want fancy visual effects that Outlook's Word rendering engine can't handle with standard HTML/CSS. For newsletters like yours, avoiding VML keeps things simpler and more reliable.\n\nKeep it simple = fewer headaches! \n\nMSO exceptions\n\nOutlook for Windows uses Microsoft Word's rendering engine (not a modern web browser engine like WebKit or Gecko), and it often ignores standard CSS. mso- properties are proprietary CSS extensions that speak directly to that Word engine.\n\nHere is a comprehensive list of the most common and useful mso- related exceptions, categorized by their function.\n\nThe Foundation: Conditional Comments\n\nBefore listing the properties, it's essential to know how to apply them. You almost always use them inside Outlook-specific conditional comments. This ensures that only Outlook reads these styles, while other email clients ignore them and use your standard CSS.\n\nThe basic structure:\n\n  \n    /* MSO-specific styles go here */\n1. Spacing, Padding, and Margins\n\nThis is the most common category of fixes.\n\nmso-padding-alt\nYou already know this one. It's used to force padding on elements like `, , and  where Outlook ignores the standard padding` property.\n.my-box {\n  padding: 20px; /* For modern clients */\n  mso-padding-alt: 20px 20px 20px 20px; /* For Outlook */\n}\n\nmso-margin-alt\nThe direct counterpart to mso-padding-alt. It forces margins on elements that Outlook would otherwise collapse or ignore.\n.some-div {\n  margin-bottom: 20px; /* For modern clients */\n  mso-margin-bottom-alt: 20px; /* For Outlook */\n}\n\nmso-line-height-rule: exactly;\nThis is a critical one. Outlook sometimes adds extra space above or below text, especially when line-height is set to a relative value (like 1.5) or a value smaller than the font size. This property forces Outlook to respect your line-height value precisely.\n.my-text {\n  font-size: 16px;\n  line-height: 20px;\n  mso-line-height-rule: exactly; /* Forces Outlook to use 20px */\n}\n\nmso-table-lspace and mso-table-rspace\nWhen you nest tables, Outlook can add unwanted space on the left (lspace) and right (rspace). Setting these to 0pt removes that ghost spacing. This is a must-have for any aligned or nested table structures.\n\n\n  table {\n    mso-table-lspace: 0pt;\n    mso-table-rspace: 0pt;\n  }\n\n2. Borders and Colors\n\nmso-border-alt\nSimilar to padding, this forces a border style on an element when the standard border property is being flaky. It's especially useful for buttons made from `` elements.\n.button-cell {\n  border: 1px solid #000000;\n  mso-border-alt: 1px solid #000000; /* Outlook fallback */\n}\nmso-color-alt\nForces a color for elements, most commonly used to ensure border colors render correctly.\n.colored-border {\n  border: 2px solid #5562a4;\n  mso-border-alt: 2px solid #5562a4;\n  mso-color-alt: #5562a4; /* Ensures border color is correct in Outlook */\n}\n\n3. Text and Font Control\n\nmso-font-alt\nProvides a fallback font for Outlook. If Outlook doesn't recognize or can't render your primary font (e.g., a web font), it will use the font specified here. It's a best practice to always include this when using custom fonts.\n.heading {\n  font-family: 'Open Sans', Arial, sans-serif; /* For modern clients */\n  mso-font-alt: 'Arial'; /* Fallback font for Outlook */\n}\n\nmso-text-raise\nThis property helps you control the vertical alignment of text. Outlook sometimes renders text slightly lower than other clients, especially with scaling issues (high DPI screens). You can use mso-text-raise with a negative value to pull the text up.\n.fix-alignment {\n  mso-text-raise: -2px; /* Pulls text up by 2 pixels in Outlook */\n}\n\n4. Visibility and Layout\n\nmso-hide: all;\nThis is extremely powerful. It completely hides an element from Outlook. It's the key to creating \"Outlook-only\" and \"non-Outlook\" content blocks.\n\nExample: The \"Ghost Table\"\nYou might have a `-based layout for modern clients but need a `-based layout for Outlook.\n\n\n  \n    \n      \n        OUTLOOK-ONLY CONTENT\n      \n    \n  \n\n\n\n  \n    NON-OUTLOOK CONTENT\n  \n\nIn this example, Outlook sees the table but not the div (because of mso-hide: all). All other clients see the div but not the table (because it's wrapped in a negative conditional comment).\n\n5. Advanced & Less Common Properties\n\nmso-ansi-font-size, mso-bidi-font-size: Force a specific font size in points (pt).\nmso-element-frame-width, mso-element-frame-height: Can be used to try and force dimensions on wrapper elements.\nmso-effects-shadow-color: Attempts to apply a text-shadow color.\nVector Markup Language (VML): While not a simple mso- property, VML is Microsoft's old XML-based language for vector graphics. It's the \"nuclear option\" for creating effects that Outlook doesn't support with CSS, most famously for background images on containers. This is a more advanced technique but is rooted in the same \"speaking to Microsoft's engine\" principle.\n\nSummary and Best Practices\n\nCode for Modern Clients First: Write your standard, clean HTML and CSS.\nTest in Outlook: Identify where it breaks (spacing, fonts, buttons, etc.).\nLayer on MSO Fixes: Use conditional comments (`) to add a  block in the  or inline mso-` properties to specifically target and fix the issues in Outlook.\nKey Properties to Remember:\n    mso-padding-alt and mso-margin-alt for spacing.\n    mso-line-height-rule: exactly; for text spacing.\n    mso-table-lspace: 0pt; and mso-table-rspace: 0pt; for nested tables.\n    mso-hide: all; for showing/hiding content.\n\nBy mastering these mso- properties, you gain precise control over how your emails render in one of the most challenging but widespread email clients.\n\nOutlook emails and CSS\n\nUnderstanding how different versions of Outlook handle CSS classes is key to building reliable emails.\n\nHeres a breakdown by Outlook version:\n\n1. Outlook for Windows (2007, 2010, 2013, 2016, 2019, 2021)\n\nThis is the version that causes the most trouble because it uses Microsoft Word's rendering engine.\n\nYes, it supports CSS classes. You can define a class in a ` block in the  of your email and apply it with class=\"my-class\"`.\nBIG CAVEAT: The support for CSS properties within those classes is very limited. The Word rendering engine ignores many modern CSS properties.\n\nWhat works with classes in Outlook for Windows:\nBasic font styling (font-family, font-size, color, font-weight).\nBasic table styling (border, background-color on ``s).\nLink styling (a:link, a:visited).\nmso- specific properties (this is a great use for classes!).\n\nWhat DOES NOT work well (or at all):\npadding and margin on `, , ` tags are often ignored or rendered inconsistently.\nwidth and height on ``s are unreliable.\nComplex selectors (e.g., div + p) are not supported.\nMost modern CSS like flexbox, grid, border-radius, box-shadow, and animations.\n\nWhy are inline styles so common then?\nBecause of these limitations, developers rely on inline styles (style=\"...\") for the most critical layout properties (width, padding, background-color, etc.) directly on ` and ` elements. This provides the highest level of cross-client compatibility and predictability, especially in Outlook for Windows.\n\n2. Outlook.com (Webmail), Outlook for Mac, and Outlook Mobile Apps\n\nThese versions are much better because they use modern, web-standard rendering engines (like WebKit, which also powers Safari and Chrome).\n\nYes, they have excellent support for CSS classes.\nYou can confidently define your styles in a ` block in the `, and they will be applied correctly.\nThey support a much wider range of CSS properties, including padding/margin on divs, max-width for responsive design, and even some CSS3 enhancements like border-radius.\nThis is where media queries (@media) work, allowing you to use classes to create a responsive layout for mobile.\n\nBest Practice: The Hybrid Approach\n\nBecause you have to code for the \"worst-case scenario\" (Outlook for Windows), the industry best practice is a hybrid approach that uses both CSS classes and inline styles.\n\n1. Use a ` block in the ` for:\nGlobal Resets: Basic styles for body, tables, and images.\nLink Styling: Setting colors for a:link, a:hover, etc. (Gmail requires this in the head).\nProgressive Enhancements: Styles that are \"nice to have\" but won't break the layout if Outlook ignores them (e.g., border-radius, font-smoothing).\nOutlook-Specific Fixes: Using classes within ` conditional comments to apply your mso-` fixes.\nResponsive Styles: Media queries for mobile clients.\n\n2. Use Inline Styles on HTML Tags (`, , `) for:\nCritical Layout Properties: width, height, background-color, padding, border.\nCritical Font Styles: font-family, font-size, line-height, color.\n\nThis \"belt-and-suspenders\" method ensures that the core structure and styling of your email hold up in every client, while allowing you to add more advanced styling for clients that can support it.\n\nCode Example of the Hybrid Approach\n\n\n\n\n  \n  My Email\n  \n  \n    /* This class is ONLY for Outlook */\n    .button-td-outlook {\n      mso-padding-alt: 12px 25px 12px 25px; /* Force padding */\n    }\n  \n  \n  \n    .button-link {\n      /* This styling is for modern clients */\n      background-color: #0078D4;\n      color: #ffffff;\n      padding: 12px 25px;\n      text-decoration: none;\n      border-radius: 5px; /* Will be ignored by Outlook Desktop */\n      display: inline-block;\n    }\n  \n\n\n  \n    \n      \n        \n        \n          \n            \n            \n              \n                Click Here\n              \n            \n          \n        \n      \n    \n  \n\n\n\nIn this example:\nModern Clients (Gmail, Apple Mail): See a blue button with rounded corners because they read the .button-link class and the inline padding.\nOutlook for Windows: Ignores border-radius. It also might ignore the padding on the ` tag. However, it gets its padding from the .button-td-outlook class (thanks to mso-padding-alt) applied to the parent . The bgcolor` and inline font styles ensure the button looks correct.",
        "locale": "en",
        "slug": "technical-posts/outlook-ghost-tables",
        "folder": ""
    },
    {
        "title": "Include HTML comments in Sveltekit build process",
        "description": "Include HTML comments in Sveltekit build process",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "By default, SvelteKit's build processes strips the HTML from SSG pages. This is generally what you want. However, I use a Sveltkit project to generate eblasts. The build process process HTML we pasted into Constant Contact and it works great.\nToday, though, I ran into a reason to need persisted HTML comments in the built HTML. I wanted to include some HTML that should be included only in emails rendered to the Windows version of Outlook. For example, I needed to include this special-case comment:\n\n\n\nto include a br tag to add whitespace for Outlook. Alas, the default build process stripped that out.\nAfter a little AI and Googling, I arrived at these settings to include HTML comments in Sveltekit's build process:\nvite.config.js\nThe esbuild directive tells Vite to include comments.\n\nimport { sveltekit } from '@sveltejs/kit/vite';\nimport { defineConfig } from 'vite';\n\nexport default defineConfig({\n\tplugins: [sveltekit()],\n\tesbuild: {\n\t\tlegalComments: 'inline'\n\t}\n});\n\nsvelte.config.js\nThe compilerOptions and preserveComments directives work with the Vite directive above to cause HTML includes to be included.\n\nimport adapter from '@sveltejs/adapter-auto';\n\n/** @type {import('@sveltejs/kit').Config} */\nconst config = {\n\tkit: {\n\t\tadapter: adapter(),\n\t\talias: {\n\t\t\t$components: 'src/components'\n\t\t},\n\t},\n\t// Include markup comments during the build process.\n\tcompilerOptions: {\n\t\tpreserveComments: true\n\t}\n};\n\nexport default config;",
        "locale": "en",
        "slug": "technical-posts/include-html-comments-in-sveltekit-build-process",
        "folder": ""
    },
    {
        "title": "Notes on using WSL2",
        "description": "Notes on using WSL2",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "containers",
            "wsl2"
        ],
        "content": "List distros and their status:\n\nwsl --list --verbose\nwsl -l -v",
        "locale": "en",
        "slug": "technical-posts/wsl2",
        "folder": ""
    },
    {
        "title": "Sveltekit forms - PageData and ActionData",
        "description": "Sveltekit forms - PageData and ActionData",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "forms"
        ],
        "content": "This code shows how to get data to and from a form.\n\nThis page provides the form and shows how to declare and use the data : PageData (initial data to the page) and form : ActionData (form data back to server) as well as show Zod-caught errors.\n\n+page.svelte\n\n\n    import type { PageData, ActionData } from './$types';\n\n    export let data: PageData | null | undefined = null;   // initial data to the page\n    export let form: ActionData | null | undefined = null; // data from the form\n\n\n\n    \n        \n            Company\n            \n            {form?.errors?.company?.0] ?? ''}\n        \n        \n            First name\n            \n            {form?.errors?.first_name?.[0] ?? ''}\n        \n        \n            Last name\n            \n            {form?.errors?.last_name?.[0] ?? ''}\n        \n        \n            Email address\n            \n            {form?.errors?.email?.[0] ?? ''}\n        \n        Log in\n    \n\n\nThis example uses Zod to validate the form. Zod is so unintrusive and works on the client and server, that's it crazy to not use it to validate form input.\n\nThis example sends a very basic object to the page as the initial data through the load function. In cases where the form isn't a CRUD form but a sign-up form, it's not necessary to use a load function. It's included here for completeness. In a production CRUD app, the load function would fetch data from a persistent store to initially populate the form.\n\n+page.server.ts\n\nimport type { PageServerLoad, Actions } from './$types';\nimport { fail } from '@sveltejs/kit';\nimport { gotoRegistrantFormSchema as formSchema } from '$lib/form-schema/gotoRegistrantFormSchema';\n\n// Initial page load.\nexport const load: PageServerLoad = async ({ cookies }) => {\n    let formData = {\n        company: 'Company Name',\n        first_name: 'First Name',\n        last_name: 'Last Name',\n        email: 'Email Address'\n    };\n\n    return { formData};\n};\n\n// The form pages posts to this action.\nexport const actions: Actions = {\n    create: async (event) => {\n        const formData = Object.fromEntries(await event.request.formData());\n\n        try {\n\t        // Have Zod validate the form with its schema.\n            const result = formSchema.parse(formData);\n\n\t\t\t// Redirect here on success.\n        } catch (err: any) {\n\t        // Fetch the Zod errors:\n            const { fieldErrors: errors } = err.flatten();\n\n            return {\n                data: formData,\n                errors\n            };\n        }\n\n\t\t// I don't think action should ever get here.\n        return { success: true };\n    }\n};\n\nIn a production app, the create action should provide a redirect to navigate the user to the next page.\n\nThe gotoRegistrantFormSchema.ts file define the form inputs for Zod.\n\nZod-related videos:\n\n[Learn Zod In 30 Minutes\nSvelteKit Form Validation with Zod\nZod Makes TypeScript Even Better\nSvelteKit Form Validation with Zod\n\ngotoRegistrantFormSchema.ts\n\nimport { z } from 'zod';\n\nexport const gotoRegistrantFormSchema = z.object({\n    company: z\n        .string({ required_error: 'Company is required' })\n        .min(1, { message: 'Company is required' })\n        .max(64, { message: 'Company must be less than 64 characters' })\n        .trim(),\n    first_name: z\n        .string({ required_error: 'First name is required' })\n        .min(1, { message: 'First name must be at least 6 characters' })\n        .max(32, { message: 'First name must be less than 32 characters' })\n        .trim(),\n    last_name: z\n        .string({ required_error: 'Last name is required' })\n        .min(1, { message: 'Last name must be at least 6 characters' })\n        .max(32, { message: 'Last name must be less than 32 characters' })\n        .trim(),\n    email: z\n        .string({ required_error: 'Email is required' })\n        .min(1, { message: 'Email is required' })\n        .max(64, { message: 'Email must be less than 64 characters' })\n        .email({ message: 'Email must be a valid email address' }),\n    permission: z.enum(['on'], { required_error: 'You must accept the terms and conditions' })\n});\n\nA note on the CSS (or lack thereof!)\n\nNo CSS is required to inhibit the display of errors if an error isn't present. If an error isn't present, the HTML rendered is:\n\n\n\nwhich intrinsically doesn't render anything. There is no need for CSS like this:\n\ndiv.error:empty {\n\tdisplay: none;\n}",
        "locale": "en",
        "slug": "technical-posts/sveltekit-forms-pagedata-and-actiondata",
        "folder": ""
    },
    {
        "title": "Querying Postgre meta data",
        "description": "Querying Postgre meta data",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "SELECT * FROM pg_tables WHERE tablename = 'family'",
        "locale": "en",
        "slug": "technical-posts/querying-postgres-meta-data",
        "folder": ""
    },
    {
        "title": "example docker compose file for a sveltekit app",
        "description": "example docker compose file for a sveltekit app",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "docker",
            "containers"
        ],
        "content": "I need to build a new web app using sveltekit 2.0. I want to build this app with docker containers; probably one for node js and one for postgres (using official images). the postgres container is for local development--at runtime the app will connect to supabase. I'll deploy this app to vercel. Show me an example docker compose file for this project.\n\nOkay, let's create a docker-compose.yml file for your SvelteKit 2.0 project, designed for local development with a Node.js service for the app and a separate PostgreSQL service mimicking Supabase.\n\nThis setup assumes:\n\nYou have Docker and Docker Compose installed.\nYou have already initialized your SvelteKit project (e.g., using npm create svelte@latest my-app).\nYour SvelteKit app will read database connection details from environment variables (a common practice).\n\nHere's the example docker-compose.yml file. Place this file in the root directory of your SvelteKit project.\n\nversion: \"3.8\" # Use a recent version of the Compose file format\n\nservices:\nSvelteKit Application Service (Node.js)\n    app:\n        container_name: sveltekit_app_dev\nUse an official Node.js image. Choose a version compatible with SvelteKit 2.0 (LTS recommended)\nAlpine versions are smaller.\n        image: node:20-alpine\nSet the working directory inside the container\n        working_dir: /app\nMount the current directory (.) on the host to /app in the container\nThis allows live code changes during development.\n':cached' can improve performance on macOS/Windows.\n        volumes:\n            .:/app:cached\nOptional: Use a named volume for node_modules to potentially speed up installs\nand avoid issues with host OS differences. Uncomment if needed.\n- node_modules:/app/node_modules\nExpose the SvelteKit/Vite development server port\n        ports:\n            \"5173:5173\" # Map host port 5173 to container port 5173 (Vite default)\nCommand to run inside the container:\n1. Install dependencies if node_modules doesn't exist (or is empty)\n2. Run the development server, making it accessible outside the container (--host)\n        command: sh -c \"npm install && npm run dev -- --host\"\nEnvironment variables needed by the SvelteKit app\n        environment:\n--- IMPORTANT ---\nThese variables should match the ones used in the 'db' service below\nYour SvelteKit app needs to be configured to use these for the DB connection\nExample using a standard PostgreSQL connection string:\n            DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}\nAdd any other environment variables your app needs for development\nVITE_SOME_API_KEY: your_dev_key\nEnsure Vite picks up env vars by prefixing with VITE_ if needed on the client-side\n        depends_on:\n            db:\nWait for the database service to be healthy before starting the app\n                condition: service_healthy\n        networks:\n            sveltekit_network\n\nPostgreSQL Database Service (for local development)\n    db:\n        container_name: postgres_dev\nUse an official PostgreSQL image\n        image: postgres:16-alpine # Use a specific version, Alpine is smaller\n        ports:\nMap a host port (e.g., 5433) to the default PostgreSQL port (5432) in the container\nUse 5433 to avoid conflicts if you have Postgres installed locally on 5432.\n            \"5433:5432\"\n        environment:\n--- IMPORTANT ---\nDefine the credentials for the local development database.\nThese MUST match the variables used in the DATABASE_URL for the 'app' service.\nUse strong passwords even for local dev, or manage them via .env file (see below)\n            POSTGRES_USER: ${POSTGRES_USER:-devuser} # Default to 'devuser' if not set in .env\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-devpassword} # Default to 'devpassword' if not set in .env\n            POSTGRES_DB: ${POSTGRES_DB:-devdb} # Default to 'devdb' if not set in .env\n        volumes:\nMount a named volume to persist database data across container restarts\n            postgres_data:/var/lib/postgresql/data\n        healthcheck:\nChecks if the PostgreSQL server is ready to accept connections\n            test:\n                [\"CMD-SHELL\", \"pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB\"]\n            interval: 10s\n            timeout: 5s\n            retries: 5\n            start_period: 10s # Give Postgres some time to start up initially\n        restart: unless-stopped # Restart the DB container unless manually stopped\n        networks:\n            sveltekit_network\n\nDefine named volumes\nvolumes:\n    postgres_data: # Volume for persisting PostgreSQL data\nnode_modules: # Uncomment if using the named volume for node_modules in the 'app' service\n\nDefine networks\nnetworks:\n    sveltekit_network:\n        driver: bridge\n\nExplanation:\n\nversion: '3.8': Specifies the Docker Compose file format version.\nservices:: Defines the different containers that make up your application stack.\n    app service (SvelteKit):\n        image: node:20-alpine: Uses the official Node.js 20 image (Alpine variant for smaller size).\n        working_dir: /app: Sets the default directory inside the container.\n        volumes: .:/app:cached: Mounts your project code into the container. Changes you make locally are reflected inside the container instantly (thanks to Vite's HMR). :cached can improve performance.\n        ports: - \"5173:5173\": Maps port 5173 on your host machine to port 5173 in the container, where the Vite dev server runs.\n        command: sh -c \"npm install && npm run dev -- --host\": First runs npm install (or yarn install if you use Yarn) to get dependencies, then starts the dev server using npm run dev. The -- --host part is crucial  it tells Vite to listen on 0.0.0.0 inside the container, making it accessible from your host machine via the mapped port.\n        environment: DATABASE_URL: ...: Crucially, this sets the database connection string your SvelteKit app will use. Notice it uses the service name db as the hostname. Docker Compose's internal networking handles resolving db to the correct container's IP address. It also references environment variables (${POSTGRES_USER}, etc.) which we'll define shortly.\n        depends_on: db: condition: service_healthy: Tells Docker Compose to start the app service only after the db service reports itself as healthy (passes its healthcheck).\n        networks: - sveltekit_network: Connects this service to a custom bridge network.\n    db service (PostgreSQL):\n        image: postgres:16-alpine: Uses the official PostgreSQL 16 image.\n        ports: - \"5433:5432\": Maps port 5433 on your host to the standard Postgres port 5432 inside the container. You'll connect to localhost:5433 from your local machine using a tool like DBeaver, pgAdmin, or psql.\n        environment:: Sets required environment variables for the Postgres container to initialize itself.\n            POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB: Define the user, password, and database name. The syntax ${VAR:-default} allows you to override these using a .env file (recommended) or uses the defaults provided.\n        volumes: - postgres_data:/var/lib/postgresql/data: Mounts a named volume called postgres_data to the location where Postgres stores its data files. This ensures your database data persists even if you stop and remove the container (docker compose down).\n        healthcheck:: Defines a command (pg_isready) Docker runs periodically to check if the database is responsive. This is used by the app service's depends_on condition.\n        restart: unless-stopped: Ensures the database container restarts automatically if it crashes, unless you explicitly stop it.\n        networks: - sveltekit_network: Connects this service to the same network as the app.\nvolumes:: Defines the named volumes used by the services (in this case, postgres_data).\nnetworks:: Defines the custom network. Using a custom network is generally good practice for isolating container communication.\n\nHow to Use:\n\nCreate a .env file: In the same directory as your docker-compose.yml, create a file named .env (make sure to add .env to your .gitignore file!). This file will securely store your environment variables. Docker Compose automatically loads variables from this file.\n\n.env file\n\nPostgreSQL Credentials for Local Development\n    POSTGRES_USER=myuser\n    POSTGRES_PASSWORD=mypassword123\n    POSTGRES_DB=mykitapp_dev\n\nAdd any other secrets or dev-specific variables here\nVITE_SOME_API_KEY=your_dev_key_here\n\n    Replace myuser, mypassword123, and mykitapp_dev with your desired local credentials.\n\nConfigure SvelteKit: Ensure your SvelteKit application (e.g., in src/hooks.server.js, src/lib/server/db.js, or wherever you initialize your database connection) reads the DATABASE_URL environment variable. You might use libraries like dotenv (though Docker Compose handles loading .env for the container) or rely on SvelteKit's built-in environment variable handling (e.g., using $env/static/private for server-side variables).\n\nRun Docker Compose: Open your terminal in the project root directory and run:\n\n    docker compose up -d\n\n    up: Creates and starts the containers.\n    -d: Runs the containers in detached mode (in the background).\n    The first time you run this, Docker will download the node and postgres images, which might take a few minutes. Subsequent runs will be much faster. It will also run npm install inside the app container.\n\nAccess Your App: Open your web browser and navigate to http://localhost:5173. You should see your SvelteKit development server running.\n\nAccess Your Database (Optional): You can connect to your local development PostgreSQL database using a database client tool:\n\n    Host: localhost\n    Port: 5433 (the host port you mapped)\n    Database: mykitapp_dev (or whatever you set in .env)\n    User: myuser (or whatever you set in .env)\n    Password: mypassword123 (or whatever you set in .env)\n\nView Logs: To see the logs from your containers:\n\n    docker compose logs -f # Follow logs for all services\n    docker compose logs -f app # Follow logs for the app service only\n    docker compose logs -f db  # Follow logs for the db service only\n\nStop Containers: When you're done:\n    docker compose down\n    This stops and removes the containers.\n    The postgres_data volume persists, so your database data will be there the next time you run docker compose up -d. If you want to remove the volume too (delete all DB data), use docker compose down -v.\n\nVercel Deployment Note:\n\nThis docker-compose.yml is specifically for local development. When you deploy to Vercel:\n\nVercel will build your SvelteKit app using its own build infrastructure (likely based on your package.json and potentially a vercel.json config). It doesn't use this Dockerfile or docker-compose setup directly for deployment builds.\nYou will configure Vercel's Environment Variables section in your project settings to point to your actual Supabase database URL and any other production secrets. The DATABASE_URL environment variable name should ideally be consistent between your local .env setup and your Vercel settings.\nYour SvelteKit code should fetch the DATABASE_URL from the environment, automatically using the local Postgres URL during docker compose up development and the Supabase URL when deployed on Vercel.\n\npodman run -it -v .:/var/roger alpine",
        "locale": "en",
        "slug": "technical-posts/example-docker-compose-file-for-a-sveltekit-app",
        "folder": ""
    },
    {
        "title": "Runes",
        "description": "Runes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "These runes make Svelte 5 highly modular, giving developers more control over reactivity and lifecycle management. As Svelte 5 evolves, additional runes may be introduced or refined to further enhance flexibility and reactivity. Let me know if youd like examples or additional details for any specific rune!\n\n$all: Runs code only after all dependencies in a group have been resolved; useful for grouped asynchronous code.\n$any: Triggers reactivity when any of the listed dependencies change, allowing for conditional updates.\n$context: Allows for setting and getting context values between components, streamlining inter-component communication.\n$derive: Creates computed properties derived from other values or stores, similar to $memo but geared toward properties that depend on external data.\n$effect: Runs a function with side effects whenever dependencies change, useful for tasks like interacting with the DOM or calling external APIs.\n$effectAsync: Handles asynchronous effects and reruns them as needed based on dependencies.\n$memo: Caches the result of a computation and only recomputes it when dependencies change, enabling derived state functionality.\n$onDestroy: Runs cleanup logic when a component is destroyed, like unsubscribing from stores or clearing timers.\n$run: Runs a function and recalculates whenever reactive dependencies change, similar to the $: statement in Svelte 3.\n$store: Provides a way to create reactive values within components, serving as a more granular alternative to Svelte's traditional stores.\n$tick: Defers updates until the DOM has been updated, allowing for waiting until the next render cycle.\n\nJoy of Code video about using Runes globally -- very good info here\nThat video also shows an interesting use of $derived.by\nRich Harris's discussion on using Runes globally\n\nLoad functions should be pure!",
        "locale": "en",
        "slug": "technical-posts/runes",
        "folder": ""
    },
    {
        "title": "Using Curl on Windows",
        "description": "Using Curl on Windows",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows",
            "utilities"
        ],
        "content": "curl -v --location 'https://api.getgo.com/G2W/rest/v2/organizers/200000000000313229/webinars/5372803598377463127/registrants?resendConfirmation=true' --header 'Content-Type: application/json' --header 'Authorization: Bearer eyJraWQiOiI2MjAiLCJhbGciOiJSUzUxMiJ9.eyJzYyI6ImNvbGxhYjoiLCJzdWIiOiI1MTIxNjQ4NjcwODUzOTI1ODkzIiwiYXVkIjoiZDc5YWNkYmEtYjVhZS00MDIwLWFlMDMtYTNmMDViMDZlZDNhIiwib2duIjoicGsiLCJ0eXAiOiJhIiwiZXhwIjoxNzQ2NjU5MTE4LCJpYXQiOjE3NDY2NTU1MTgsImp0aSI6IjY5ZjliODMwLTQ4YWYtNDUyYy04ZTA1LWE0OTRiNDIxNWY5OSJ9.J_pooXiQAuvTUzYQKWBsc4XbDd_IxwAzLSmQtDYcY4DvwT37xa2slyz-E0J8LKkwrout0e7EftE0v7SMl_bnyz1OXJRadbh5ijSx1dhTWLqlTaR2G4NkgzbRZVhmT48WPnXAXOgL-Vooxxv5JjDI3LaFY_melGBOexy2F6qizTPfEi_uON3SUlgT8LML0CjKrT21FQJMcQEKp6PQTlMaBsbjE7xXYeYrnSI59Qx_Vn4yUJi5w0AZIzVhoNWHt8AYUU6MOmb5fLW36low2PqN3XsZvEaARo7Q27wxjhq5LPdJcmmeeQTO3cmnlA6LirbT-4Rs1qXj7Xjzodl7oWtr4w' --data-raw '{    \"firstName\": \"Test User\",    \"lastName\": \"API case\",    \"email\": \"roger.pence@gmail.com\",    \"country\": \"United States\",    \"organization\": \"GoTo\"}'\n\nThe verbose (-v) flag generates detailed info on the request.\n\nHost api.getgo.com:443 was resolved.\nIPv6: (none)\nIPv4: 54.188.255.113, 44.235.156.16, 34.213.175.160\nTrying 54.188.255.113:443...\nschannel: disabled automatic use of client certificate\nALPN: curl offers http/1.1\nALPN: server accepted http/1.1\nConnected to api.getgo.com (54.188.255.113) port 443\nusing HTTP/1.x\nPOST /G2W/rest/v2/organizers/200000000000313229/webinars/5372803598377463127/registrants?resendConfirmation=true HTTP/1.1\nHost: api.getgo.com\nUser-Agent: curl/8.11.1\nAccept: /\nContent-Type: application/json\nAuthorization: Bearer eyJraWQiOiI2MjAiLCJhbGciOiJSUzUxMiJ9.eyJzYyI6ImNvbGxhYjoiLCJzdWIiOiI1MTIxNjQ4NjcwODUzOTI1ODkzIiwiYXVkIjoiZDc5YWNkYmEtYjVhZS00MDIwLWFlMDMtYTNmMDViMDZlZDNhIiwib2duIjoicGsiLCJ0eXAiOiJhIiwiZXhwIjoxNzQ2NjU5MTE4LCJpYXQiOjE3NDY2NTU1MTgsImp0aSI6IjY5ZjliODMwLTQ4YWYtNDUyYy04ZTA1LWE0OTRiNDIxNWY5OSJ9.J_pooXiQAuvTUzYQKWBsc4XbDd_IxwAzLSmQtDYcY4DvwT37xa2slyz-E0J8LKkwrout0e7EftE0v7SMl_bnyz1OXJRadbh5ijSx1dhTWLqlTaR2G4NkgzbRZVhmT48WPnXAXOgL-Vooxxv5JjDI3LaFY_melGBOexy2F6qizTPfEi_uON3SUlgT8LML0CjKrT21FQJMcQEKp6PQTlMaBsbjE7xXYeYrnSI59Qx_Vn4yUJi5w0AZIzVhoNWHt8AYUU6MOmb5fLW36low2PqN3XsZvEaARo7Q27wxjhq5LPdJcmmeeQTO3cmnlA6LirbT-4Rs1qXj7Xjzodl7oWtr4w\nContent-Length: 152\nupload completely sent off: 152 bytes\n< HTTP/1.1 201 Created\n< Server: nginx\n< Date: Wed, 07 May 2025 22:18:49 GMT\n< Content-Type: application/hal+json;charset=UTF-8\n< Content-Length: 148\n< Connection: keep-alive\n< x-amzn-Remapped-date: Wed, 07 May 2025 22:18:49 GMT\n< x-amzn-RequestId: 9acf8eea-8a4e-47fc-8836-7c96f5df061f\n< x-envoy-upstream-service-time: 327\n< x-amzn-Remapped-connection: keep-alive\n< x-amz-apigw-id: KN9bgGZePHcF50A=\n< X-Amzn-Trace-Id: Root=1-681bdc49-5747c42d10e730bb6fdf8249;Parent=51d05aeef9da6be2;Sampled=0;Lineage=1:738723aa:0\n< X-Cache: Miss from cloudfront\n< Via: 1.1 49b94a8674d6e86a841d6523f7dbaf14.cloudfront.net (CloudFront)\n< X-Amz-Cf-Pop: HIO50-C1\n< X-Amz-Cf-Id: DgwJojhaSoqjwOYyf7FuMKR9MNKEF1vRv5fNyu2Rl6yr8mowz3gsmQ==\n< Strict-Transport-Security: max-age=15768000\n< Access-Control-Allow-Origin: *\n< Access-Control-Allow-Headers: origin, x-requested-with, authorization, accept, content-type\n< Access-Control-Max-Age: 3628800\n< Access-Control-Allow-Methods: DELETE, GET, HEAD, OPTIONS, PATCH, POST, PUT\n<\n{\"registrantKey\":3689531560913815895,\"joinUrl\":\"https://global.gotowebinar.com/join/5372803598377463127/180717636\",\"status\":\"APPROVED\",\"asset\":true}* Connection #0 to host api.getgo.com left intact",
        "locale": "en",
        "slug": "technical-posts/using-curl-on-windows",
        "folder": ""
    },
    {
        "title": "ValueFromPipeline",
        "description": "ValueFromPipeline",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "param (\n    string]$Title = \"File listing.\"\n)\n\n$excludedDirectories = @(\"debug\", \"release\", \"packages\", \"properties\")\n$includedExtensions = @(\".vr\", \".dll\", \".sln\", \".aspx\", \".asmx\", \".ashx\", \"*.master\")\n\n$Title = \"$Title - $(Get-Date -Format 'yyyy-MM-dd')\"\n\n\nfunction Should-IncludeFile {\n    param (\n        [Parameter(Mandatory = $true, ValueFromPipeline = $true)$FileSystemInfo,\n        [string[]]$ExcludedDirectories,\n        [string[]]$IncludedExtensions\n    )\n\nCheck if it's a directory or a file\n    if ($FileSystemInfo -is [System.IO.DirectoryInfo]) {\n        #Exclude directories.  Function now only deals with files\n        return $false\n    }\n\nCheck Extension\n    $includeByExtension = $false  # Default to exclude\n    foreach ($FileExtension in $IncludedExtensions) {\n        if ($FileSystemInfo.Extension -eq $FileExtension) {\n            $includeByExtension = $true # Include if extension matches\n            break # Optimization: Exit loop as soon as we find a match\n        }\n    }\n\nCheck Directory\n    $includeByDirectory = $true  # Default to include\n    foreach ($ExcludedDirectory in $ExcludedDirectories) {\n        if ($FileSystemInfo.DirectoryName -match $ExcludedDirectory) {\n            $includeByDirectory = $false # Exclude if directory matches\n            break # Optimization: Exit loop as soon as we find a match\n        }\n    }\n\n    return $includeByExtension -and $includeByDirectory # Include only if BOTH conditions are true.\n\n}\n\n\nGet-ChildItem -Path \".\\.\" -recurse -File `\n| Where-Object { Should-IncludeFile -FileSystemInfo $_ -IncludedExtensions $includedExtensions -ExcludedDirectories $excludedDirectories } `\n| Select-object -property name, extension, directory, fullname, lastwritetime `\n| Sort-Object -Property extension, name `\n| Export-Excel test.xlsx -title $Title -titlebold -show -clearsheet -autosize -TableName maintable -TableStyle  Medium1\n\nValueFromPipeline\n\nThe ValueFromPipeline = $true attribute in the Should-IncludeFile function's parameter declaration is crucial for enabling the function to seamlessly integrate with the PowerShell pipeline. Here's a detailed explanation of why it's necessary and what it accomplishes:\n\nUnderstanding the PowerShell Pipeline\n\nThe PowerShell pipeline is a series of commands connected by the pipe operator (|). Each command in the pipeline receives input from the previous command and passes output to the next command. This allows you to chain together commands to perform complex operations.\n\nHow ValueFromPipeline = $true Works\n\nAutomatic Input Binding: When ValueFromPipeline = $true is specified for a parameter, PowerShell automatically attempts to bind the input objects from the pipeline to that parameter. In other words, each object flowing through the pipeline is automatically passed to the function's specified parameter.\n\nProcessing Each Object: The function is executed once for each object that it receives from the pipeline. The ValueFromPipeline = $true attribute essentially tells PowerShell to treat the function as if it were being called repeatedly, once for each item in the pipeline. In the Should-IncludeFile example, the function will be called once for every FileSystemInfo object that Get-ChildItem produces.\n\nAccessing the Current Object: Inside the function, the parameter ($FileSystemInfo in this case) will hold the current object from the pipeline. This allows the function to process each file individually.\n\nWhy It's Needed in Should-IncludeFile\n\nFiltering: The purpose of the Should-IncludeFile function is to decide whether to include or exclude a specific file based on certain criteria. To make this decision, the function must receive the FileSystemInfo (file or directory) object to examine.\n\nIntegration with Where-Object: The Where-Object cmdlet uses script blocks (or functions with ValueFromPipeline = $true) to filter objects. Where-Object calls the script block/function for each object coming through the pipeline. If the function returns $true, the object is passed on; if it returns $false, the object is filtered out.\n\nWithout ValueFromPipeline = $true: If you didn't have ValueFromPipeline = $true, the Where-Object cmdlet would try to pass the entire collection of FileSystemInfo objects to the Should-IncludeFile function at once. The function would not be able to process each file individually. It would not know how to iterate over all the files, and most likely the program would crash.\n\nIn Summary:\n\nValueFromPipeline = $true is essential because it tells PowerShell to pass each FileSystemInfo object from the Get-ChildItem cmdlet through the pipeline one at a time to the Should-IncludeFile function. This enables the function to inspect each file individually and determine whether to include or exclude it based on your filtering criteria. Without this attribute, the function would not work correctly within the pipeline.",
        "locale": "en",
        "slug": "technical-posts/valuefrompipeline",
        "folder": ""
    },
    {
        "title": "Avoiding Outlook's mysterious horizontal lines",
        "description": "Avoiding Outlook's mysterious horizontal lines",
        "date_created": "2025-06-26T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "outlook"
        ],
        "content": "To avoid Outlook's mysterious horizontal lines:\nThe HTML table structure with propercellPadding=\"0\" cellSpacing=\"0\" border=\"0\"attributes\nThe explicit border properties (borderTop: \"0\", borderBottom: \"0\")\nProper Outlook-specific properties (msoTableLspace,msoTableRspace)\nClean text styling with minimal margins and paddings\n\n\n\n\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\t\t{introData.gotoText}\n\t\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\t{introData.issue}\n\t\t\t\n\t\t\n\t\n",
        "locale": "en",
        "slug": "technical-posts/outlook-email-horizonal-lines",
        "folder": ""
    },
    {
        "title": "Create a custom HR tag",
        "description": "Create a custom HR tag",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Okay, here's how you can style an `` tag to be centered horizontally in the viewport and have a width of 50% of the viewport width using CSS.\n\nThe key is to set the width using viewport width units (vw) and then use margin: auto on the left and right sides to center it.\n\nHTML:\n\nIt's good practice to add a class to your ` tag so you can style it specifically without affecting other potential ` elements on your page.\n\nSome content above the line.\n\n\n\nSome content below the line.\n\nCSS:\n\nHere are a couple of common ways to style the hr itself after setting the width and centering:\n\nMethod 1: Using height and background-color (Recommended for solid lines)\n\nhr.centered-half {\n    width: 50vw; /* Set width to 50% of the viewport width */\n    margin-left: auto; /* Auto margin pushes it from the left */\n    margin-right: auto; /* Auto margin pushes it from the right */\n\n    /* --- Optional Styling for Appearance --- */\n    border: none; /* Remove the default 3D border */\n    height: 1px; /* Define the thickness of the line */\n    background-color: #ccc; /* Set the color of the line (e.g., light grey) */\n\n    /* Optional: Add some vertical space */\n    margin-top: 2em;\n    margin-bottom: 2em;\n}\n\n/* You can combine the margin properties like this: */\nhr.centered-half-shorthand {\n    width: 50vw;\n    margin: 2em auto; /* 2em top/bottom, auto left/right */\n\n    /* --- Optional Styling --- */\n    border: none;\n    height: 1px;\n    background-color: #333; /* Darker grey example */\n}\n\nMethod 2: Using border-top\n\nSometimes, people prefer styling the hr using its top border.\n\nhr.centered-half-border {\n    width: 50vw;\n    margin: 2em auto; /* Center horizontally, add vertical margin */\n\n    /* --- Styling using border --- */\n    border: none; /* Remove default borders */\n    border-top: 2px solid blue; /* Define line style using border-top */\n    /* Change '2px', 'solid', 'blue' as needed */\n}\n\nExplanation:\n\nwidth: 50vw;: This sets the width of the ` element to be exactly 50% of the current viewport's width. vw` stands for \"viewport width\".\nmargin-left: auto; and margin-right: auto; (or margin: auto horizontally): When a block-level element (like `) has a defined width that is less than its container's width, setting the left and right margins to auto` tells the browser to distribute the remaining horizontal space equally on both sides, effectively centering the element.\nborder: none;: Browsers often apply a default border (usually inset) to ` tags to give them a 3D look. Setting border: none;` removes this, allowing you to define your own appearance cleanly.\nheight: 1px; and background-color: #ccc;: (Method 1) After removing the border, the hr might become invisible. This gives it a specific height (thickness) and a solid color.\nborder-top: ...;: (Method 2) This achieves a similar visual result by applying a border only to the top edge of the (now borderless) hr element.\nmargin-top / margin-bottom (or vertical value in margin shorthand): These are optional but usually desired to add some spacing above and below the rule.\n\nChoose the CSS method you prefer and apply the class (centered-half, centered-half-shorthand, or centered-half-border) to your `` tag in the HTML.",
        "locale": "en",
        "slug": "technical-posts/create-a-custom-hr-tag",
        "folder": ""
    },
    {
        "title": "Generate SQL Server table schema in Json and CRUD stored procs",
        "description": "Generate SQL Server table schema in Json and CRUD stored procs",
        "date_created": "2025-05-31T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sql-server",
            "schema",
            "crud"
        ],
        "content": "The C# program at this folder can:\n\nCreate Json schema files for SQL Server tables and views (generally for use with Libretto)\nCreate CRUD stored products for SQL Server tables\nCreate C# data models\n\nC:\\Users\\thumb\\Documents\\projects\\cs\\sql-server-generator\n\nGenerate JSON schema for the tables in a SQL Server database\n\ncreate-json-data-models -d rp\n\nwhere -d is the database name.  \n\nThe Json schema files are created in the Libretto schemas\\sql-server-schemas\\[database name] folder.\n\nGenerate SQL Server CRUD stored procs for tables in a SQL Server database\n\ncreate-crud-procs -d rp\n\nwhere -d is the database name. \n\nThe CRUD procs are created and copied to the clipboard. They are not written out as files. \n\nCreate CS data models for a the tables in a SQL Server database\n\ncreate-cs-data-models -d asna-version-policy -n DataModels\n\nwhere -d is the database name and -n is the namespace name.\n\nC# models are created here where the last folder is the database name:\n\nC:\\Users\\thumb\\AppData\\Roaming\\DapperModels\\rp",
        "locale": "en",
        "slug": "technical-posts/generate-sql-server-schema-and-crud-procs",
        "folder": ""
    },
    {
        "title": "sveltekit-forms-handling",
        "description": "sveltekit-forms-handling",
        "date_created": "2025-05-25T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "forms"
        ],
        "content": "Define the form type and validation rules\n\n\nSimpleZodSchema \nSimple\nSimpleFormData\n\nimport { z } from 'zod';\n\n// All Zod fields are required by default -- but that means keys not values.\n// For example, use min()/max() to mark a string field value as required.\n\nexport const SimpleZodSchema = z.object({\n\tid: z.string().optional(),\n\tdescription: z.string().min(5),\n\tlocation: z.string()\n});\n\nexport type Simple = z.infer;\n\n// This type is used for form data, where the 'id' field is not included.\n// Omit others as needed.\nexport type SimpleFormData = Omit;\n\npage.server.ts\n\nimport { type Simple, SimpleZodSchema } from '$lib/types/Simple';\nimport type { PageServerLoad, Actions } from './$types';\nimport { error, fail, redirect } from '@sveltejs/kit';\n\nexport const load: PageServerLoad = async ({ params }) => {\n\tconst simple: Simple = {\n\t\tid: '1',\n\t\tdescription: 'This is a description',\n\t\tlocation: 'west'\n\t};\n\n\treturn {\n\t\tsimple\n\t};\n};\n\nexport const actions: Actions = {\n\tupdate: async ({ locals, request, params }) => {\n\t\tconst rawFormData = Object.fromEntries(await request.formData());\n\n\t\tconsole.log('rawFormData', rawFormData);\n\n\t\ttry {\n\t\t\tconst formData = SimpleZodSchema.parse(rawFormData) as Simple;\n\n\t\t\tconst id = formData.id;\n\t\t\tconst description = formData.description;\n\t\t} catch (err: any) {\n\t\t\tconst { fieldErrors: errors } = err.flatten();\n\n\t\t\treturn fail(400, {\n\t\t\t\tdata: rawFormData,\n\t\t\t\terrors,\n\t\t\t\tmessage: ''\n\t\t\t});\n\t\t}\n\n\t\t// Redirect back to the list page after successful update\n\t\t//throw redirect(303, /read?id=${id});\n\t}\n};\n\n+page.svelte\n\n\n\timport { enhance } from '$app/forms';\n\timport type { PageData, ActionData } from './$types'; // Use generated types\n\timport { type Simple } from '$lib/types/Simple';\n\n\tconst { data, form } = $props();\n\n\t// 'data' comes from the load function, and 'form' comes from the action.\n\t// On the page's initial load, 'form' is null. After a form submission, 'form' contains\n\t// the action result and data still has the initial data. The line below ensures that\n\t// we use the form data if available, otherwise we fall back to the initial data.\n\n\tconst formData: Simple = form?.data ?? data.simple;\n\n\n\n\tSimple Route\n\n\nExample Form\n\n\n\t{form?.message}\n\t{#if form?.errors?.general}{form.errors.general}{/if}\n\n\n\n\t\n\t\t\n\n\t\tDescription:\n\t\t\n\t\t{#if form?.errors?.description}{form.errors.description}{/if}\n\n\t\tLocation:\n\t\t\n\t\t{#if form?.errors?.location}{form.errors.location}{/if}\n\t\n\tUpdate Family\n",
        "locale": "en",
        "slug": "technical-posts/sveltekit-forms-handling",
        "folder": ""
    },
    {
        "title": "SvelteKit logging with Pino",
        "description": "SvelteKit logging with Pino",
        "date_created": "2025-06-07T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "pino",
            "svelte"
        ],
        "content": "Logging for Your SvelteKit Application\n\nFor your SvelteKit application that needs to log to Postgres, Pino is indeed a good choice, but there are some considerations:\n\nUsing Pino (Recommended)\n\nPino is an excellent choice because:\n\nPerformance: It's one of the fastest Node.js loggers available\nJSON output: Makes it easy to parse and store logs\nLow overhead: Minimal impact on your application\nTransport system: Allows you to send logs to different destinations\nMature ecosystem: Well-maintained with good community support\n\nFor your PostgreSQL integration, you can use:\n\nimport pino from 'pino';\nimport { pool } from '$lib/db'; // Your Postgres connection\n\n// Create the base logger\nconst logger = pino({\n  level: 'info',\n  timestamp: pino.stdTimeFunctions.isoTime,\n  // Customize log format as needed\n});\n\n// Add custom method to log to database\nconst dbLogger = logger.child({});\n\n// Extend with a method to save to DB\ndbLogger.db = async (level: string, message: string, data?: any) => {\n  try {\n    await pool.query(\n      'INSERT INTO app_logs (level, message, data, timestamp) VALUES ($1, $2, $3, NOW())',\n      level, message, data ? JSON.stringify(data) : null]\n    );\n  } catch (err) {\n    logger.error({ err }, 'Failed to write log to database');\n  }\n  \n  // Also log to console\n  dbLogger[level;\n};\n\nexport { dbLogger as logger };\n\nThen in your global-listener.svelte.ts:\n\nimport { browser } from '$app/environment';\nimport * as ListenerTypes from '$lib/types/listener-types';\nimport { logger } from '$lib/utils/logger';\n\n// Define the structure of our store's state\ntype GlobalListener = {\n\tshowContactForm: boolean;\n\tshowToast: boolean; // Optional: Add more properties as needed\n\tpayload?: ListenerTypes.DialogCargo | ListenerTypes.ToastCargo;\n};\n\n// Create the reactive state using $state\n// Initialize with a default value\nlet appStore = $state({\n\tshowContactForm: false,\n\tshowToast: false // Optional: Initialize other properties as needed\n});\n\n// Optional: Log when the store module is loaded\nif (browser) {\n\tlogger.info('Shared store module loaded (client)');\n} else {\n\tlogger.info('Shared store module loaded (server)');\n\t\n\t// Example of DB logging for important server events\n\tlogger.db('info', 'Store module initialized', { environment: 'server' });\n}\n\n// Action creators with logging\nexport function showToast(toast: ListenerTypes.ToastCargo) {\n    logger.db('info', 'Toast displayed', { type: toast.toastType, heading: toast.heading });\n    globalState.payload = toast;\n    globalState.showToast = true;\n}\n\nexport function showDialog(dialog: ListenerTypes.DialogCargo) {\n    logger.db('info', 'Dialog displayed', { type: dialog.dialogType, heading: dialog.heading });\n    globalState.payload = dialog;\n    globalState.showContactForm = true;\n}\n\nexport const globalState = appStore;\n\nWriting Your Own Logger?\n\nWriting your own basic logger is not silly if your needs are truly minimal. However:\n\nTime investment: Even a simple logger requires time to build, test, and maintain\nEdge cases: Production logging has many edge cases (like async context, error handling)\nFeatures: You'll likely want more features later (filtering, formatting, etc.)\n\nIf you really want a minimal DIY approach:\n\n// Simple DIY logger that writes to Postgres\nexport const logger = {\n  async log(level: string, message: string, data?: any) {\n    const timestamp = new Date().toISOString();\n    console.log(${timestamp}: ${message}, data || '');\n    \n    try {\n      await pool.query(\n        'INSERT INTO app_logs (level, message, data, timestamp) VALUES ($1, $2, $3, $4)',\n        level, message, data ? JSON.stringify(data) : null, timestamp]\n      );\n    } catch (err) {\n      console.error('Failed to write log to database:', err);\n    }\n  },\n  \n  info(message: string, data?: any) {\n    return this.log('info', message, data);\n  },\n  \n  error(message: string, data?: any) {\n    return this.log('error', message, data);\n  },\n  \n  warn(message: string, data?: any) {\n    return this.log('warn', message, data);\n  },\n  \n  debug(message: string, data?: any) {\n    return this.log('debug', message, data);\n  }\n};\n\nRecommendation\n\nI recommend starting with Pino - it gives you a solid foundation with minimal code and allows you to easily extend it as your needs grow. The time you save can be spent on your application's core features.\n\nRemember to create your Postgres table:\n\nCREATE TABLE app_logs (\n  id SERIAL PRIMARY KEY,\n  level VARCHAR(10) NOT NULL,\n  message TEXT NOT NULL,\n  data JSONB,\n  timestamp TIMESTAMP NOT NULL\n);\nCREATE INDEX idx_app_logs_level ON app_logs(level);\nCREATE INDEX idx_app_logs_timestamp ON app_logs(timestamp);\n\nMaking Pino globally available in SvelteKit components\n\n\nTo make the Pino logger globally available to all SvelteKit components, you need to set it up in a way that makes it easy to import and use throughout your application. Here's how to structure this:\n\n1. Create a Central Logger Module\n\nFirst, create a dedicated logger module:\n\nimport pino from 'pino';\nimport { dev } from '$app/environment';\nimport { pool } from '$lib/db'; // Your Postgres connection\n\n// Configure base logger with appropriate options\nconst pinoLogger = pino({\n  level: dev ? 'debug' : 'info',\n  transport: dev ? {\n    target: 'pino-pretty',\n    options: { colorize: true }\n  } : undefined,\n  timestamp: pino.stdTimeFunctions.isoTime,\n});\n\n// Create the logger with DB capabilities\nconst logger = {\n  // Base logging methods\n  info: (message: string, data?: any) => {\n    pinoLogger.info(data || {}, message);\n    if (!dev) logToDB('info', message, data).catch(e => \n      pinoLogger.error('Failed to write to DB: ' + e.message));\n  },\n  \n  error: (message: string, data?: any) => {\n    pinoLogger.error(data || {}, message);\n    if (!dev) logToDB('error', message, data).catch(e => \n      pinoLogger.error('Failed to write to DB: ' + e.message));\n  },\n  \n  warn: (message: string, data?: any) => {\n    pinoLogger.warn(data || {}, message);\n    if (!dev) logToDB('warn', message, data).catch(e => \n      pinoLogger.error('Failed to write to DB: ' + e.message));\n  },\n  \n  debug: (message: string, data?: any) => {\n    pinoLogger.debug(data || {}, message);\n    // Usually don't log debug messages to DB\n  },\n  \n  // Direct DB logging (when you only want to log to DB)\n  db: (level: string, message: string, data?: any) => {\n    pinoLogger[level;\n    return logToDB(level, message, data);\n  }\n};\n\n// Helper function to log to database\nasync function logToDB(level: string, message: string, data?: any) {\n  try {\n    await pool.query(\n      'INSERT INTO app_logs (level, message, data, timestamp) VALUES ($1, $2, $3, NOW())',\n      [level, message, data ? JSON.stringify(data) : null]\n    );\n  } catch (err) {\n    pinoLogger.error({ err }, 'Failed to write log to database');\n  }\n}\n\n// Export the logger\nexport { logger };\n\n2. Attach to the SvelteKit Context in Hooks\n\nTo make it available on every request, add it to the event.locals in your hooks file:\n\nimport type { Handle } from '@sveltejs/kit';\nimport { globalState } from '$lib/components/global-listener.svelte';\nimport { logger } from '$lib/utils/logger';\n\n// Extend the App.Locals interface to include our logger\ndeclare global {\n  namespace App {\n    interface Locals {\n      logger: typeof logger;\n    }\n  }\n}\n\nexport const handle: Handle = async ({ event, resolve }) => {\n  // Attach logger to event.locals so it's available in load functions and actions\n  event.locals.logger = logger;\n  \n  // Reset the contact form state on every request\n  globalState.showContactForm = false;\n  \n  // Log each request\n  logger.info(Request: ${event.request.method} ${event.url.pathname}, {\n    query: Object.fromEntries(event.url.searchParams),\n    userAgent: event.request.headers.get('user-agent')\n  });\n\n  const response = await resolve(event);\n  return response;\n};\n\n3. Use in Server Components and Load Functions\n\nIn server-side code, you can access the logger from event.locals:\n\n// In a +page.server.ts file\nexport const load = async ({ locals }) => {\n  locals.logger.info('Loading page data');\n  \n  // Your load function logic...\n  \n  return { /* data */ };\n};\n\n4. Create a Client-Side Wrapper\n\nFor client components, create a simplified client-side version:\n\nimport { browser } from '$app/environment';\n\n// Simplified client-side logger\nexport const logger = {\n  info: (message: string, data?: any) => {\n    if (browser) console.info([INFO] ${message}, data || '');\n  },\n  \n  error: (message: string, data?: any) => {\n    if (browser) console.error([ERROR] ${message}, data || '');\n  },\n  \n  warn: (message: string, data?: any) => {\n    if (browser) console.warn([WARN] ${message}, data || '');\n  },\n  \n  debug: (message: string, data?: any) => {\n    if (browser) console.debug([DEBUG] ${message}, data || '');\n  }\n};\n\n5. Update Your Global State to Use the Logger\n\nimport { browser } from '$app/environment';\nimport * as ListenerTypes from '$lib/types/listener-types';\nimport { logger } from '$lib/utils/client-logger'; // Use client logger for browser code\n\n// Define the structure of our store's state\ntype GlobalListener = {\n\tshowContactForm: boolean;\n\tshowToast: boolean;\n\tpayload?: ListenerTypes.DialogCargo | ListenerTypes.ToastCargo;\n};\n\nlet appStore = $state({\n\tshowContactForm: false,\n\tshowToast: false\n});\n\nif (browser) {\n\tlogger.info('Shared store module loaded (client)');\n} else {\n\t// This won't run in browser environments\n\tlogger.info('Shared store module loaded (server)');\n}\n\n// Action creators with logging\nexport function showToast(toast: ListenerTypes.ToastCargo) {\n    logger.info('Toast displayed', { type: toast.toastType, heading: toast.heading });\n    globalState.payload = toast;\n    globalState.showToast = true;\n}\n\nexport function showDialog(dialog: ListenerTypes.DialogCargo) {\n    logger.info('Dialog displayed', { type: dialog.dialogType, heading: dialog.heading });\n    globalState.payload = dialog;\n    globalState.showContactForm = true;\n}\n\nexport const globalState = appStore;\n\n6. Import and Use in Components\n\nNow you can use the logger in any component:\n\n\n    import { logger } from '$lib/utils/client-logger';\n    \n    function handleClick() {\n        logger.info('Button clicked');\n        // Do something\n    }\n\n\nClick me\n\nThis architecture provides:\n\nServer-side logging: Full Pino capabilities with DB integration\nClient-side logging: Simplified console logging\nConsistent API: Same method calls regardless of environment\nGlobal availability: Easy to import and use anywhere\nType safety: TypeScript interfaces for both environments\n\nFor optimal flexibility, you can also create a logging context that can be provided to components through Svelte's context API if you prefer that approach over imports.\n\nWhy logger.testDbLogging Uses Await But logger.info Doesn't\n\nThe difference in usage comes from their implementation and return values:\n\n1. logger.info Function\ninfo: (message: string, data?: any, pool?: Pool) => {\n    pinoLogger.info(data || {}, message);\n    if (!dev && pool)\n        logToDB(pool, 'info', message, data).catch((e) =>\n            pinoLogger.error('Failed to write to DB: ' + e.message)\n        );\n}\n\nThis function:\nDoesn't return a Promise: It returns undefined (void)\nUses fire-and-forget pattern: The database operation happens asynchronously\nHandles errors internally: Uses .catch() to capture errors within the function\nIs designed for regular logging: Not meant to block execution flow\n\n2. logger.testDbLogging Function\ntestDbLogging: async (pool: Pool) => {\n    pinoLogger.info('Testing DB logging');\n    \n    try {\n        await logToDB(pool, 'info', 'Test DB write from pino-logger', {\n            timestamp: new Date().toISOString()\n        });\n        pinoLogger.info('Successfully wrote test log to database');\n        return true;\n    } catch (err: any) {\n        pinoLogger.error({ err }, 'Failed to write test log to database');\n        return false;\n    }\n}\n\nThis function:\nIs declared as async: Returns a Promise\nReturns a value: true or false based on success\nWaits for completion: Uses await to ensure the database write completes\nIs designed for testing: Specifically to verify DB writing functionality\n\nBest Practice\n\nThis is intentional design. For normal logging functions, you typically don't want to await them since logging should not block your application flow. For test functions, you want to know the result, so you await the Promise to get the true/false result.\n\nIf you wanted to know when a log was successfully written to the database in your normal code flow, you would need to modify the regular logging functions to return Promises and then await them.",
        "locale": "en",
        "slug": "technical-posts/logging-in-sveltekit-with-pino",
        "folder": ""
    },
    {
        "title": "How to use WSL2 and Podman or Docker for the most performance",
        "description": "How to use WSL2 and Podman or Docker for the most performance",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "containers",
            "wsl2"
        ],
        "content": "Gettting WSL2 up and running\n\nEnsure you are using WSL2\n\nwsl --set-default-version 2\n\nInstall Ubuntu (which is the default install)\n\nwsl --install\n\nwsl.exe -d ubuntu\n\nInstall Podman on Unbuntu in WSL2\nsudo apt update\nsudo apt -y install podman\n\nTomorrow night install postgres image and run it with podman\n\n\n\nOkay, let's break down the performance considerations for containerized SvelteKit development with your setup. The key performance bottleneck in WSL2 development, especially for Node.js projects like SvelteKit with large node_modules directories and frequent file changes (HMR), is filesystem I/O performance.\n\nUnderstanding the Core Issue:\n\nFast: Accessing files within a WSL2 distro's native Linux filesystem (e.g., ext4) is very fast, near native Linux speed.\nSlow: Accessing files across the Windows/WSL2 boundary (e.g., accessing /mnt/c/... from within WSL, or \\\\wsl$\\... from Windows) is significantly slower due to the network filesystem protocol (9p) used for this boundary.\n\nAnalyzing Your Setup:\n\nPodman Engine Location: Podman Desktop runs the actual Podman engine (the service that manages containers) inside a dedicated WSL2 distro, which is podman-machine-default in your case.\nContainer Operations: When you build (podman build) or run (podman run) a container, the Podman engine in podman-machine-default needs to access the source files (the build context) and potentially mount volumes.\nPengwin: Your preferred development environment where you likely have your tools and customizations.\n\nEvaluating Development Strategies for Performance:\n\nOption 1: Code on Windows (C:\\...), Build/Run via Podman Desktop\n\n    Workflow: Edit code on Windows filesystem. Podman Desktop tells the engine in podman-machine-default to build/run using code from /mnt/c/....\n    Performance: Worst. The Podman engine inside podman-machine-default has to constantly read/write files across the slow WSL2/Windows boundary (/mnt/c). Builds will be slow, node_modules installation within the container (if mapped) will be slow, and Hot Module Replacement (HMR) will be significantly delayed. Avoid this.\n\nOption 2: Code in Pengwin (/home/user/...), Build/Run via Podman Desktop\n\n    Workflow: Edit code in Pengwin (e.g., using VS Code Remote - WSL connected to Pengwin). Use Podman Desktop (or the podman CLI on Windows) which talks to the engine in podman-machine-default. The engine needs to access files located in another WSL distro (\\\\wsl$\\Pengwin\\...).\n    Performance: Better than Option 1, but still not optimal. Accessing files between WSL distros is faster than accessing the Windows filesystem, but it still involves inter-distro communication that isn't as fast as accessing files within the same distro. Builds and HMR will be better than option 1, but not the fastest possible.\n\nOption 3: Code in podman-machine-default (/home/user/...), Build/Run via Podman Desktop\n\n    Workflow: Treat podman-machine-default as your development environment. Store your code inside it (e.g., /home/user/projects/my-svelte-app). Use VS Code Remote - WSL to connect directly to the podman-machine-default distro for editing. Use Podman Desktop (or the CLI within podman-machine-default) to build/run.\n    Performance: Excellent. The source code and the Podman engine accessing it reside within the same WSL2 distro. All file operations (reading build context, volume mounting for HMR) happen within the fast native Linux filesystem. Builds, dependency installs, and HMR will be significantly faster.\n    Downside: podman-machine-default is meant as an appliance. You'll need to install Node.js, git, and any other development tools you need directly into it. It won't have your Pengwin customizations.\n\nOption 4: Code in Pengwin (/home/user/...), Build/Run using Podman installed natively in Pengwin\n    Workflow: Install podman directly inside Pengwin (sudo apt update && sudo apt install podman or Pengwin's equivalent). Store and edit code in Pengwin (using VS Code Remote - WSL connected to Pengwin). Run podman build and podman run commands directly from the Pengwin terminal.\n    Performance: Excellent. Just like Option 3, the source code and the Podman engine accessing it are within the same WSL2 distro (Pengwin in this case). File I/O is fast.\n    Downside: Podman Desktop won't automatically manage this Podman instance. You'd primarily interact via the CLI in Pengwin. Podman Desktop would still manage the other engine in podman-machine-default, which you wouldn't be using for this workflow.\n\nConclusion: The Most Performant Ways\n\nThe most performant approaches are Option 3 and Option 4 because they keep the source code files within the same Linux filesystem as the Podman engine that's processing them.\n\nChoose Option 3 (Code in podman-machine-default) if:\n\n    You primarily want to use Podman Desktop for management.\n    You don't mind setting up your development tools (Node, git, etc.) within the podman-machine-default distro.\n    You want the tightest integration with the Podman Desktop ecosystem.\n    How: Connect VS Code Remote - WSL to podman-machine-default, clone/create your project there (/home/user/...), use Podman Desktop or the podman CLI within that distro. Use volume mounts like -v /home/user/projects/my-svelte-app:/app.\n\nChoose Option 4 (Code in Pengwin, Podman in Pengwin) if:\n    You strongly prefer using your customized Pengwin environment for development.\n    You are comfortable managing Podman primarily via the command line within Pengwin.\n    You don't need tight Podman Desktop GUI integration for this specific workflow.\n    How: Ensure podman is installed in Pengwin. Connect VS Code Remote - WSL to Pengwin, create/clone your project there (/home/user/projects/...), run podman build . and podman run -v /home/user/projects/my-svelte-app:/app ... directly in the Pengwin terminal.\n\nFor SvelteKit development where fast HMR and build times are crucial, avoiding the cross-boundary filesystem access (Options 1 and 2) is key.\n\nI have a Windows 11 PC with WSL2 and Pengwin installed. If I install PodMan in that Pengwin instance, can I create a container in it that maps a port from the container to the host Windows 11 PC? I want to develop web apps in that container and test them in the host's browser.\n\nYes, absolutely! You can definitely do this.\nHere's how it works and why:\n\nPodman inside WSL2: You install Podman within your Pengwin (or any other WSL2) distribution just like you would on a native Linux system.\nContainer Port Mapping: When you run a container using Podman, you use the -p or --publish flag to map ports. For example:\nInside Pengwin (WSL2)\n    podman run -d --name mywebapp -p 8080:80 docker.io/library/nginx\n    This command tells Podman:\n    Run a container in detached mode (-d).\n    Name it mywebapp.\n    Map port 8080 on the host (which in this context is your Pengwin/WSL2 instance) to port 80 inside the container.\n    Use the official Nginx image as an example.\nWSL2 Network Magic (localhostForwarding): This is the crucial part. WSL2 has a built-in feature often referred to as localhostForwarding. When a process inside your WSL2 distribution (like the Podman service managing the port forward) listens on 0.0.0.0: or [::]: (meaning it accepts connections from any interface within WSL2), Windows automatically forwards connections made to localhost: on your Windows host machine to that listening port inside the WSL2 instance.\nAccessing from Windows Browser: Because of this localhostForwarding, you can open your web browser on Windows 11 and navigate to:\n    http://localhost:8080\n    Windows will automatically route this request to port 8080 inside your Pengwin WSL2 instance, which Podman will then route to port 80 inside your container.\n\nIn Summary:\n\nYou install Podman in Pengwin (WSL2).\nYou run your container using podman run -p : .... The `` refers to the port on the WSL2 instance.\nWSL2's networking automatically makes services listening within WSL2 accessible via localhost: on the Windows host.\nYou access your web app from your Windows browser using http://localhost:.\n\nThis is a very common and effective workflow for developing applications using containers within WSL2 and accessing them from the Windows host.\n\nInstalling Docker in WSL2\n\n\nYou've hit on a critical point regarding WSL2 and file system performance! You are absolutely correct to identify that accessing files across the Windows/WSL2 boundary is significantly slower than accessing files purely within the WSL2 filesystem.\n\nThis insight fundamentally changes the recommendation for Docker installation if file I/O performance is a primary concern.\n\nRe-evaluation: Installing Docker Directly in WSL2 Ubuntu (and managing via CLI)\n\nGiven your concern about file access speed, installing Docker directly within your WSL2 Ubuntu distribution and managing it purely via the CLI (without relying on Docker Desktop's WSL2 integration for the Docker engine itself) becomes the superior approach for maximum performance and isolation.\n\nLet me clarify the distinction and why this is the case:\n\n1. Docker Desktop's WSL2 Backend (Typical Setup):\nWhen you install Docker Desktop on Windows and enable its WSL2 integration, Docker Desktop effectively runs its Docker daemon within a special, hidden WSL2 distribution that it manages.\nWhen you run docker commands from your Ubuntu WSL2 distribution, those commands are essentially forwarded to the Docker daemon running in Docker Desktop's managed WSL2 distro.\nThe key performance bottleneck here is volume mounting: If you map a Docker volume from a Windows path (e.g., /mnt/c/Users/YourUser/project:/app) to a container, or even from your user WSL2 distro's path that itself is backed by a Windows share, you encounter the slow cross-OS file access.\nWhile Docker Desktop tries to optimize this, any I/O that has to traverse the Windows/WSL2 boundary for container volumes will suffer.\n\n2. Docker Engine Installed Directly in Your WSL2 Ubuntu Distribution (CLI-Managed):\nIn this scenario, you install the docker-ce package directly into your user-facing Ubuntu WSL2 distro.\nThe Docker daemon runs as a service within that same Ubuntu distro.\nWhen you use docker run or docker-compose up within that Ubuntu terminal, the Docker daemon is local to that environment.\nCrucially, when you define Docker volumes, they should point to paths *within that same WSL2 Ubuntu distribution's filesystem* (e.g., /home/youruser/myproject/pgdata:/var/lib/postgresql/data).\nAdvantage: All file I/O for your PostgreSQL data and any application code mounted into containers happens entirely within the high-performance WSL2 ext4 filesystem. There is no cross-OS file access for container operations, which significantly boosts performance.\n\nRecommendation Update:\n\nYes, you should install the Docker Engine directly into your WSL2 Ubuntu image and manage it via the CLI.\n\nThis is the recommended approach when:\nFile I/O performance (especially for databases like PostgreSQL) is critical.\nYou want the best possible \"native Linux\" Docker experience within WSL2.\nYou're primarily working within the WSL2 environment for your development.\n\nSteps for Installing Docker Engine Directly in WSL2 Ubuntu:\n\nEnsure you *don't* have Docker Desktop running and integrated with your specific Ubuntu WSL2 distro, or at least disable that integration if it is. (It's generally fine to have Docker Desktop for other things, but your WSL2 Ubuntu will run its own Docker daemon).\n\nOpen your WSL2 Ubuntu terminal.\n\nUpdate your package list:\n    sudo apt update\n    sudo apt upgrade\n\nInstall necessary packages for Docker:\n    sudo apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common\n\nAdd Docker's official GPG key:\n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nVerify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88:\n    sudo apt-key fingerprint 0EBFCD88\n\nAdd the Docker APT repository:\n    sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n\nUpdate your package list again (to include Docker packages):\n    sudo apt update\n\nInstall Docker Engine, containerd, and Docker Compose:\n    sudo apt install docker-ce docker-ce-cli containerd.io docker-compose\n\nAdd your user to the docker group (so you don't need sudo for every Docker command):\n    sudo usermod -aG docker $USER\n    You'll need to close and reopen your WSL2 terminal for this change to take effect.\n\nStart the Docker service:\n    sudo service docker start\n    (Or sudo systemctl start docker if systemctl is available in your WSL2 setup, which it usually is in newer Ubuntu versions.)\n\nVerify Docker is running:\n    docker run hello-world\n    You should see a message indicating Docker is working.\n\nNow, when you use docker-compose for PostgreSQL, ensure your volumes section points to paths within your WSL2  directory (e.g., /my_project/pgdata).\n\nExample docker-compose.yml for this setup:\n\nversion: '3.8'\n\nservices:\n  db:\n    image: postgres:14-alpine\n    restart: always\n    environment:\n      POSTGRES_DB: my_app_db\n      POSTGRES_USER: dev_user\n      POSTGRES_PASSWORD: dev_password\n    volumes:\nThis volume path is relative to the docker-compose.yml location\nand will resolve to a directory within your WSL2 Ubuntu filesystem.\n      ./pgdata:/var/lib/postgresql/data\n    ports:\n      \"5432:5432\"\n\nvolumes:\n  pgdata: # This defines a named volume, which Docker creates/manages\ninternally within the WSL2 filesystem, leading to optimal performance.\nAlternatively, you can use a bind mount like - /home/youruser/pg_data:/var/lib/postgresql/data\nbut named volumes are generally preferred for persistence.\n\nBy installing Docker directly in your WSL2 Ubuntu image, you get the best of both worlds: the isolation and portability of Docker, combined with the native file system performance of WSL2's Linux environment.\n\nPodman alternative\n\nYes, Podman is absolutely a rational and often advantageous alternative to Docker in this specific WSL2 scenario, especially if you're concerned about resources and system-level daemon overhead. Since you're already familiar with Docker, the transition to Podman is remarkably smooth.\n\nLet's break down why Podman is a strong contender here:\n\nKey Advantages of Podman over Docker (especially in a WSL2 CLI-managed context):\n\nDaemonless Architecture: This is the biggest differentiator. Podman does not use a persistent background daemon like Docker. When you run a podman run command, the container process is forked directly from the Podman client.\n    Benefit in WSL2: No Docker daemon means fewer background processes running, less memory consumption for the daemon itself, and potentially a lighter footprint on your WSL2 instance.\n    No sudo for Rootless Containers: A major Podman feature is rootless containers. You can run containers as your unprivileged user, which is a significant security improvement and also means you don't need sudo to run most podman commands (after initial setup).\n\nOCI Compliant: Podman adheres to the Open Container Initiative (OCI) standards for container images and runtimes. This means it can run the exact same images you'd use with Docker.\n    Familiarity: Your existing postgres:14-alpine image will work seamlessly.\n\nDocker CLI Compatibility: Podman's CLI is designed to be largely command-for-command compatible with Docker's CLI.\n    Ease of Transition: Most of your docker commands (e.g., docker run, docker ps, docker images, docker stop) can simply be replaced with podman (e.g., podman run, podman ps, podman images, podman stop).\n    podman-compose: There's a podman-compose tool that aims to replicate docker-compose functionality, allowing you to use your existing docker-compose.yml files (with minor adjustments sometimes).\n\nSecurity Focus: Rootless containers improve security by not requiring elevated privileges for most container operations. If a container is compromised, the attacker only has the privileges of your user, not root.\n\nBuildah Integration: Podman often pairs with Buildah for building images, offering more granular control than docker build but podman build also exists for simple Dockerfile builds.\n\nDisadvantages/Considerations for Podman in WSL2:\n\nSystemd Integration (minor): While Docker Desktop uses systemd in its hidden WSL2 distro, your user-facing WSL2 Ubuntu might not run systemd by default (it depends on your Ubuntu version and how you launch it). Podman works fine without it, but some advanced features like running containers as \"system services\" might need specific configuration if you want them to auto-start. However, for simple dev containers like PostgreSQL, it's not an issue.\npodman-compose vs docker-compose: While podman-compose exists and works well, it's a separate project and might occasionally lag behind docker-compose in terms of features or bug fixes. For basic service definitions like PostgreSQL, it's usually fine.\nInitial Setup for Rootless: While simpler in the long run, setting up rootless containers with Podman involves a few extra steps (like subuid/subgid configuration) compared to just installing Docker. However, these are usually well-documented.\n\nPractical Steps for Podman in WSL2 Ubuntu:\n\nInstall Podman:\n    sudo apt update\n    sudo apt install podman podman-compose\n    Note: podman-compose might not always be in the main repos for all Ubuntu versions, you might need to add a PPA or install it via pip. If podman-compose isn't available, you can still use basic podman run commands for your PostgreSQL setup.\n\nConfigure for Rootless Containers (Optional but Recommended):\n    Podman usually handles this automatically on first run, but if you encounter issues with rootless, you might need to ensure your user has entries in /etc/subuid and /etc/subgid.\nThis command attempts to create the necessary entries if they don't exist\n    podman system migrate\n\nStart PostgreSQL:\n    Using podman run (simple, no podman-compose needed for a single service):\n        podman run -d --name my-postgres \\\n                   -e POSTGRES_DB=my_app_db \\\n                   -e POSTGRES_USER=thumb \\\n                   -e POSTGRES_PASSWORD=thumb \\\n                   -v pgdata:/var/lib/postgresql/data \\\n                   -p 5432:5432 \\\n                   postgres:17-alpine\n        This command directly translates from Docker. pgdata here creates a Podman managed volume in your WSL2 environment, ensuring performance.\n\n    Using podman-compose (with your docker-compose.yml):\n        Navigate to your directory with the docker-compose.yml and run:\n        podman-compose up -d\n\nConclusion: Is it rational? Yes!\n\nIf you're already familiar with Docker, switching to Podman in a CLI-managed WSL2 environment is a very rational and often superior alternative, especially for resource efficiency and security. The mental model and most CLI commands are almost identical.\n\nIt allows you to fully leverage the performance benefits of WSL2's native filesystem for your containerized applications without the overhead of the Docker daemon or the potential performance penalties of Docker Desktop's cross-OS volume mounts.\n\nGiven your concern about file access speed and your familiarity with Docker, I would highly encourage you to try Podman. It's a very straightforward transition for your use case and often leads to a lighter, more performant developer experience within WSL2.\n\n\nHad to resolve this with:\nError: exec: \"qemu-system-x86_64\":\nwith:\nsudo apt install qemu-system-x86 podman-remote\n\nAdd search registries\n\nsudo nano /etc/containers/registries.conf\n\n[registries.search] \nregistries = ['docker.io', 'quay.io']",
        "locale": "en",
        "slug": "technical-posts/wsl2-and-podman-performance",
        "folder": ""
    },
    {
        "title": "Sveltekit query strings and stores",
        "description": "Sveltekit query strings and stores",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "I wanted to switch between HTML and translation views for eblast creation. I thought I'd be smart and use a query string value to toggle the two modes. That worked, but caused trouble with SSR. I am going to take the eblast back to using a static value to indicate the mode.\n\nHere is how to read a query string and persist its value across components\n\nlib/store.js\n\nThis creates a writeable store.\n\nimport { writable } from 'svelte/store';\n\nexport const devmode = writable('');\n\n.../+page.js\n\nThis fetches a query string value and returns it. This passes the query string value to the parent page through the page store. Note how the query string value had to be read in the +page.js file--query strings can't be read directly in +page.svelte files (at least as of 4 October I think that!).\n\nexport const prerender = true;\nexport const csr = false;\n\nimport { browser, dev, building, version } from '$app/environment';\n\nexport async function load({ params, url }) {\n\tif (browser) {\n\t\tlet mode = url.searchParams.get('mode');\n\t\tif (!mode) mode = 'html';\n\t\treturn { mode };\n\t}\n}\n\n.../+page.svelte\n\nThis interrogates the value passed in through the page store.\n\nimport { page } from '$app/stores';\ndevmode.update(() => $page.data.mode);\n\nIn any component\n\nThis code works, but I'm not sure that it's idiomatic. It reads the value of the devmode store. It seems nuts that it takes this arcane code to read a store. Maybe I'm missing something. I'll revisit this later.\n\nlet mode;\n\nimport { devmode } from '$lib/store.js';\ndevmode.subscribe((value) => {mode = value});",
        "locale": "en",
        "slug": "technical-posts/query-strings-and-stores",
        "folder": ""
    },
    {
        "title": "How to replace an API call that use query string arguments to something that with SSR for Sveletekit",
        "description": "How to replace an API call that use query string arguments to something that with SSR for Sveletekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "To replace an API call using query string arguments with server-side rendering (SSR) in SvelteKit, you can fetch the data directly from the server-side load function of your page or layout. This avoids the need for client-side API calls.\n\nHeres a general approach:\n\nRemove Client-Side API Call: Eliminate any client-side fetch calls that are using query strings.\n\nUse the load Function in SvelteKit: Define the load function in your page or layout's +page.js/+layout.js file. The load function runs on the server during SSR and can access the query parameters from the request URL.\n\nAccess Query Parameters: Use url.searchParams.get('param') inside the load function to retrieve the query string parameters. This data can then be passed to your page component.\n\nReturn the Data to the Page: Return the fetched data as part of the load functions return object. The page can access this data as data.\n\nHeres a basic example:\n\n// +page.js or +layout.js\nexport async function load({ fetch, url }) {\n    // Extract query params from the URL\n    const param = url.searchParams.get(\"param\");\n\n    // If you were calling an API, you can do it here using SSR\n    const response = await fetch(/api/some-endpoint?param=${param});\n    const data = await response.json();\n\n    return {\n        data,\n    };\n}\n\nIn your Svelte component:\n\n\n  export let data;\n\n\n{data.someValue}\n\nWith this setup, the API call happens server-side, using SSR, and the data is sent directly to the component during the page load. This way, you avoid client-side API requests and achieve better performance and SEO.",
        "locale": "en",
        "slug": "technical-posts/replace-a-sveltekit-client-side-api-call-with-layout",
        "folder": ""
    },
    {
        "title": "Validating Sveltekit forms with Zod",
        "description": "Validating Sveltekit forms with Zod",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "forms"
        ],
        "content": "This code is from this project:\n\nC:\\Users\\thumb\\Documents\\projects\\svelte\\forms-exercises-with-zod\\src\\routes\\form-with-action\\+page.svelte\n\nThe route is /form-with-action\n\nDefine the form inputs with Zod\n\nimport { z } from 'zod';\n\nexport const userFormSchema = z.object({\n    company: z\n        .string({ required_error: 'Company is required' })\n        .min(1, { message: 'Company is required' })\n        .max(64, { message: 'Company must be less than 64 characters' })\n        .trim(),\n    country: z\n        .string({ required_error: 'Country is required' })\n        .min(1, { message: 'Country is required' })\n        .max(64, { message: 'Country must be less than 64 characters' }),\n    first_name: z\n        .string({ required_error: 'First name is required' })\n        .min(1, { message: 'First name must be at least 6 characters' })\n        .max(32, { message: 'First name must be less than 32 characters' })\n        .trim(),\n    last_name: z\n        .string({ required_error: 'Last name is required' })\n        .min(1, { message: 'Last name must be at least 6 characters' })\n        .max(32, { message: 'Last name must be less than 32 characters' })\n        .trim(),\n    email: z\n        .string({ required_error: 'Email is required' })\n        .min(1, { message: 'Email is required' })\n        .max(64, { message: 'Email must be less than 64 characters' })\n        .email({ message: 'Email must be a valid email address' }),\n    permission: z.enum(['on'], { required_error: 'You must accept the terms and conditions' })\n});\n\nThe form\n\n\n        \n            Company\n            \n            {form?.errors?.company[0]}\n        \n        ...\n        \n            \n            You have my permission to log the information above with this product request.\n                Someone may contact you from ASNA to verify your use of the product.\n        \n        Log in\n\nThe form action\n\nimport { z } from 'zod';\n\nimport type { PageServerLoad, Actions } from './$types';\nimport { fail } from '@sveltejs/kit';\nimport { userFormSchema as userSchema } from '$lib/form-schema/userFormSchema.ts';\n\nexport const load: PageServerLoad = async ({ cookies }) => {\n\t// This where data would be sent to the page for its first display (ie, get the\n\t// data being edited.)\n\treturn {};\n};\n\nexport const actions: Actions = {\n\tcreate: async (event) => {\n\t\t//const formData = await event.request.formData();\n\t\tconst formData = Object.fromEntries(await event.request.formData());\n\t\tconsole.log('formData', formData);\n\t\t// console.log('zod', userSchema)\n\n\t\ttry {\n\t\t\tconst result = userSchema.parse(formData);\n\t\t\tconsole.log('SUCCESS');\n\t\t\tconsole.log('result', result);\n\t\t} catch (err: any) {\n\t\t\t//  console.log('err', err.flatten())\n\t\t\tconst { fieldErrors: errors } = err.flatten();\n\t\t\t//const { password, passwordConfirm, ...rest } = formData;\n\t\t\tconst { ...rest } = formData;\n\t\t\treturn {\n\t\t\t\tdata: rest,\n\t\t\t\terrors\n\t\t\t};\n\t\t}\n\n\t\treturn { success: true };\n\t}\n};",
        "locale": "en",
        "slug": "technical-posts/validating-sveltekit-forms-with-zod",
        "folder": ""
    },
    {
        "title": "Showing progress on form submission",
        "description": "Showing progress on form submission",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-svelte",
            "sveltekit"
        ],
        "content": "There are probably more compelling ways to show that something is happening when a form has been submitted, but this quick and dirty way to provide the user with some feedback works pretty well.\n\nAdd the following on:submit even handler to the form.\n\n\n\n...\n\nSend email\n\n...\n\n\n\nPut this function in the component's ` area. Make sure the querySelector` method uses the correct button ID.\n\nconst onFormSubmission = (e) => {\n\tconst btn = document.querySelector('#form-button');\n\tbtn.innerText = 'Preparing email...';\n\tbtn.style = 'cursor:wait';\n\te.currentTarget.style = 'cursor:wait';\n};\n\nThis lets JavaScript do a little work to show something is happening when the button is clicked and before the form is submitted to the server.",
        "locale": "en",
        "slug": "technical-posts/showing-progress-when-submitting-a-form",
        "folder": ""
    },
    {
        "title": "Run SvelteKit app locally",
        "description": "Run SvelteKit app locally",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Okay, if you don't need PM2 for persistence and are happy to manually start the server when you want to use your SvelteKit app locally, it's much simpler!\n\nYou'll still need to decide if your app requires a Node.js server (for SSR, API routes, etc.) or if it can be served as static files.\n\nMethod 1: For SvelteKit apps using adapter-node (needs a Node.js server)\n\nEnsure adapter-node is configured:\n    Make sure your svelte.config.js uses @sveltejs/adapter-node:\n\n    // svelte.config.js\n    import adapter from \"@sveltejs/adapter-node\";\n    import { vitePreprocess } from \"@sveltejs/vite-plugin-svelte\";\n\n    /** @type {import('@sveltejs/kit').Config} */\n    const config = {\n        preprocess: vitePreprocess(),\n        kit: {\n            adapter: adapter(), // Default output is 'build'\n        },\n    };\n    export default config;\n\nBuild your SvelteKit App:\n    Open your terminal in the project root and run:\n\n    npm run build\n\n    This creates a build directory (or your configured output directory) containing the server and assets.\n\nManually Start the Node.js Server:\n    Navigate to your project's root directory in the terminal (if you aren't already there). The entry point for the server is typically build/index.js.\n    You can run it directly with Node:\n\n    node build/index.js\n\n    Port: By default, it might run on port 3000. If you need to change the port, the SvelteKit Node adapter typically respects the PORT environment variable:\n        PORT=4000 node build/index.js\n    Host: To make it accessible on your local network (not just localhost), you might also set the HOST environment variable:\n        HOST=0.0.0.0 PORT=3000 node build/index.js\n        (Using 0.0.0.0 makes it listen on all available network interfaces).\n\nAccess your app:\n    Open your browser and go to http://localhost:3000 (or the port you specified). If you used HOST=0.0.0.0, you can also access it from other devices on your local network using your desktop's IP address (e.g., http://192.168.1.100:3000).\n\nTo Stop the Server:\n    Go back to the terminal where the server is running and press Ctrl+C.\n\nMethod 2: For SvelteKit apps using adapter-static (can be served as static files)\n\nEnsure adapter-static is configured:\n    Make sure your svelte.config.js uses @sveltejs/adapter-static:\n\n    // svelte.config.js\n    import adapter from \"@sveltejs/adapter-static\";\n    import { vitePreprocess } from \"@sveltejs/vite-plugin-svelte\";\n\n    /** @type {import('@sveltejs/kit').Config} */\n    const config = {\n        preprocess: vitePreprocess(),\n        kit: {\n            adapter: adapter({\n                pages: \"build\",\n                assets: \"build\",\n                fallback: \"index.html\", // or 200.html - important for SPAs\n                precompress: false,\n            }),\n        },\n    };\n    export default config;\n\nBuild your SvelteKit App:\n\n    npm run build\n\n    This creates a build directory (or your configured output directory) with static HTML, CSS, and JS files.\n\nManually Serve the Static Files:\n    You need a simple HTTP server. Here are a couple of easy options:\n\n    Using serve (npm package):\n        If you have Node.js, you can use npx to run serve without installing it globally:\n\n        cd build  # Navigate into your build output directory\n        npx serve -s . -p 3000\n\n        (The -s flag tells serve to handle single-page app routing by serving index.html for any path not found, which is useful if you used a fallback in your adapter settings).\n        If you prefer to install it globally once: npm install -g serve, then you can just run serve -s . -p 3000 from within the build directory.\n\n    Using Python's built-in HTTP server:\n        If you have Python installed:\n        cd build  # Navigate into your build output directory\nFor Python 3:\n        python -m http.server 3000\nFor Python 2:\npython -m SimpleHTTPServer 3000\n        Note: Python's built-in server is very basic and might not correctly serve index.html for sub-paths in an SPA setup without extra configuration or a more advanced server. serve with the -s flag is generally better for SPAs.\n\nAccess your app:\n    Open your browser and go to http://localhost:3000 (or the port you specified).\n\nTo Stop the Server:\n    Go back to the terminal where the server is running and press Ctrl+C.\n\nSummary for Manual Start:\n\nChoose your adapter (adapter-node or adapter-static) in svelte.config.js.\nBuild: npm run build.\nRun:\n    For adapter-node: node build/index.js (optionally with PORT= and HOST=).\n    For adapter-static: cd build then npx serve -s . -p  (or use Python's server).\nStop: Ctrl+C in the terminal.\n\nWhenever you make changes to your SvelteKit code, you'll need to re-run npm run build and then restart your server manually using the chosen command.\n\nThis approach avoids the overhead of PM2 if you only need the app running while you're actively working with it or showing it locally.",
        "locale": "en",
        "slug": "technical-posts/run-sveltekit-app-locally",
        "folder": ""
    },
    {
        "title": "Svelte event object",
        "description": "Svelte event object",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "{\n  cookies: {\n    get: [Function: get],\n    getAll: [Function: getAll],\n    set: [Function: set],\n    delete: [Function: delete],\n    serialize: [Function: serialize]\n  },\n  fetch: [AsyncFunction (anonymous)],\n  getClientAddress: [Function: getClientAddress],\n  locals: {},\n  params: {},\n  platform: undefined,\n  request: Request {\n    [Symbol(realm)]: { settingsObject: [Object] },\n    [Symbol(state)]: {\n      method: 'GET',\n      localURLsOnly: false,\n      unsafeRequest: false,\n      body: null,\n      client: [Object],\n      reservedClient: null,\n      replacesClientId: '',\n      window: 'client',\n      keepalive: false,\n      serviceWorkers: 'all',\n      initiator: '',\n      destination: '',\n      priority: null,\n      origin: 'client',\n      policyContainer: 'client',\n      referrer: 'client',\n      referrerPolicy: '',\n      mode: 'cors',\n      useCORSPreflightFlag: false,\n      credentials: 'same-origin',\n      useCredentials: false,\n      cache: 'default',\n      redirect: 'follow',\n      integrity: '',\n      cryptoGraphicsNonceMetadata: '',\n      parserMetadata: '',\n      reloadNavigation: false,\n      historyNavigation: false,\n      userActivation: false,\n      taintedOrigin: false,\n      redirectCount: 0,\n      responseTainting: 'basic',\n      preventNoCacheCacheControlHeaderModification: false,\n      done: false,\n      timingAllowFailed: false,\n      headersList: [HeadersList],\n      urlList: [Array],\n      url: [URL]\n    },\n    [Symbol(signal)]: AbortSignal { aborted: false },\n    [Symbol(headers)]: HeadersList {\n      cookies: null,\n      [Symbol(headers map)]: [Map],\n      [Symbol(headers map sorted)]: null\n    }\n  },\n  route: { id: '/' },\n  setHeaders: [Function: setHeaders],\n  url: URL {\n    href: 'http://localhost:5173/',\n    origin: 'http://localhost:5173',\n    protocol: 'http:',\n    username: '',\n    password: '',\n    host: 'localhost:5173',\n    hostname: 'localhost',\n    port: '5173',\n    pathname: '/',\n    search: '',\n    searchParams: URLSearchParams {},\n    hash: ''\n  },\n  isDataRequest: false,\n  isSubRequest: false\n}\n12:55:31 PM [vite] page reload src/hooks.server.js (x4)\nURL {\n  href: 'http://localhost:5173/',\n  origin: 'http://localhost:5173',\n  protocol: 'http:',\n  username: '',\n  password: '',\n  host: 'localhost:5173',\n  hostname: 'localhost',\n  port: '5173',\n  pathname: '/',\n  search: '',\n  searchParams: URLSearchParams {},\n  hash: ''\n}\nURL {\n  href: 'http://localhost:5173/',\n  origin: 'http://localhost:5173',\n  protocol: 'http:',\n  username: '',\n  password: '',\n  host: 'localhost:5173',\n  hostname: 'localhost',\n  port: '5173',\n  pathname: '/',\n  search: '',\n  searchParams: URLSearchParams {},\n  hash: ''\n}\n12:56:53 PM [vite] page reload src/hooks.server.js (x5)\n/\n/\n/en\n12:58:04 PM [vite] page reload src/hooks.server.js (x6)\n/en\n/\n/en\n1:12:09 PM [vite] page reload src/hooks.server.js (x7)\n{\n  cookies: {\n    get: [Function: get],\n    getAll: [Function: getAll],\n    set: [Function: set],\n    delete: [Function: delete],\n    serialize: [Function: serialize]\n  },\n  fetch: [AsyncFunction (anonymous)],\n  getClientAddress: [Function: getClientAddress],\n  locals: {},\n  params: {},\n  platform: undefined,\n  request: Request {\n    [Symbol(realm)]: { settingsObject: [Object] },\n    [Symbol(state)]: {\n      method: 'GET',\n      localURLsOnly: false,\n      unsafeRequest: false,\n      body: null,\n      client: [Object],\n      reservedClient: null,\n      replacesClientId: '',\n      window: 'client',\n      keepalive: false,\n      serviceWorkers: 'all',\n      initiator: '',\n      destination: '',\n      priority: null,\n      origin: 'client',\n      policyContainer: 'client',\n      referrer: 'client',\n      referrerPolicy: '',\n      mode: 'cors',\n      useCORSPreflightFlag: false,\n      credentials: 'same-origin',\n      useCredentials: false,\n      cache: 'default',\n      redirect: 'follow',\n      integrity: '',\n      cryptoGraphicsNonceMetadata: '',\n      parserMetadata: '',\n      reloadNavigation: false,\n      historyNavigation: false,\n      userActivation: false,\n      taintedOrigin: false,\n      redirectCount: 0,\n      responseTainting: 'basic',\n      preventNoCacheCacheControlHeaderModification: false,\n      done: false,\n      timingAllowFailed: false,\n      headersList: [HeadersList],\n      urlList: [Array],\n      url: [URL]\n    },\n    [Symbol(signal)]: AbortSignal { aborted: false },\n    [Symbol(headers)]: HeadersList {\n      cookies: null,\n      [Symbol(headers map)]: [Map],\n      [Symbol(headers map sorted)]: null\n    }\n  },\n  route: { id: '/en' },\n  setHeaders: [Function: setHeaders],\n  url: URL {\n    href: 'http://localhost:5173/en',\n    origin: 'http://localhost:5173',\n    protocol: 'http:',\n    username: '',\n    password: '',\n    host: 'localhost:5173',\n    hostname: 'localhost',\n    port: '5173',\n    pathname: '/en',\n    search: '',\n    searchParams: URLSearchParams {},\n    hash: ''\n  },\n  isDataRequest: false,\n  isSubRequest: false\n}\nELIFECYCLE Command failed with exit code 1.\n\n 01:12  ~\\svelte\\focused-topics\\layouts   pnpm run dev\n\npostcss@0.0.1 dev C:\\Users\\thumb\\Documents\\Projects\\svelte\\focused-topics\\layouts\nvite dev\n\n\n\n  VITE v4.4.8  ready in 1733 ms\n\n    Local:   http://localhost:5173/\n    Network: use --host to expose\n    press h to show help\n{\n  cookies: {\n    get: [Function: get],\n    getAll: [Function: getAll],\n    set: [Function: set],\n    delete: [Function: delete],\n    serialize: [Function: serialize]\n  },\n  fetch: [AsyncFunction (anonymous)],\n  getClientAddress: [Function: getClientAddress],\n  locals: {},\n  params: {},\n  platform: undefined,\n  request: Request {\n    [Symbol(realm)]: { settingsObject: [Object] },\n    [Symbol(state)]: {\n      method: 'GET',\n      localURLsOnly: false,\n      unsafeRequest: false,\n      body: null,\n      client: [Object],\n      reservedClient: null,\n      replacesClientId: '',\n      window: 'client',\n      keepalive: false,\n      serviceWorkers: 'all',\n      initiator: '',\n      destination: '',\n      priority: null,\n      origin: 'client',\n      policyContainer: 'client',\n      referrer: 'client',\n      referrerPolicy: '',\n      mode: 'cors',\n      useCORSPreflightFlag: false,\n      credentials: 'same-origin',\n      useCredentials: false,\n      cache: 'default',\n      redirect: 'follow',\n      integrity: '',\n      cryptoGraphicsNonceMetadata: '',\n      parserMetadata: '',\n      reloadNavigation: false,\n      historyNavigation: false,\n      userActivation: false,\n      taintedOrigin: false,\n      redirectCount: 0,\n      responseTainting: 'basic',\n      preventNoCacheCacheControlHeaderModification: false,\n      done: false,\n      timingAllowFailed: false,\n      headersList: [HeadersList],\n      urlList: [Array],\n      url: [URL]\n    },\n    [Symbol(signal)]: AbortSignal { aborted: false },\n    [Symbol(headers)]: HeadersList {\n      cookies: null,\n      [Symbol(headers map)]: [Map],\n      [Symbol(headers map sorted)]: null\n    }\n  },\n  route: { id: '/en' },\n  setHeaders: [Function: setHeaders],\n  url: URL {\n    href: 'http://localhost:5173/en',\n    origin: 'http://localhost:5173',\n    protocol: 'http:',\n    username: '',\n    password: '',\n    host: 'localhost:5173',\n    hostname: 'localhost',\n    port: '5173',\n    pathname: '/en',\n    search: '',\n    searchParams: URLSearchParams {},\n    hash: ''\n  },\n  isDataRequest: false,\n  isSubRequest: false\n}",
        "locale": "en",
        "slug": "technical-posts/event-object",
        "folder": ""
    },
    {
        "title": "Postgres on WSL 2 with Podman",
        "description": "How to install and configure Postres on WSL2 with Podman",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-29T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "containers",
            "podman",
            "postgres"
        ],
        "content": "\n[supabase-credentials#This is a test]]\n\n/home/user/postgres_data\n/home/user/postgres_storage\n\nSee 9:40 in Scott's video for this connection URL\n\n![[image-68.png]]\n\nThese are the commands that worked\n\npodman volume create pg_data_final\n\npodman run -d --name postgres-wsl  \n-e POSTGRES_PASSWORD=mysecretpassword -e PGDATA=/var/lib/postgresql/data  \n-v pg_data_final:/var/lib/postgresql/data:Z -p 5432:5432  \n--shm-size=1g postgres:latest  \n-c shared_buffers=512MB `  \n-c random_page_cost=1.1\n\n  It is great to hear you are up and running! This setup is a very good setup for running a database on Windows 11 via WSL 2 because it prioritizes native I/O speed while navigating the strict permission rules of rootless Podman. This setup's named volume keeps all DB data on the WSL 2-hosted VM. \n\nHere is a recap of the logic and steps that led to this successful, high-performance configuration:\n\n1. The Environment Reset\nWe started by clearing out \"phantom\" WSL distributions. When a WSL distro is deleted manually, Podmans internal metadata gets out of sync. \nThe Fix: Running podman machine rm -f and podman machine init created a clean, fresh WSL 2 backend dedicated to Podman.\n\n2. Solving the \"Slow Disk\" Problem\nYou correctly identified that mounting Windows folders (like C:\\Users\\...) is extremely slow. This is because every database write has to cross the \"9P\" bridge between Linux and Windows.\nThe Fix: We used a Named Volume (pg_data_final). Unlike a direct folder mount, a named volume is managed entirely by Podman inside the WSL ext4 virtual disk. This allows Postgres to write to the disk at native SSD speeds.\n\n3. Overcoming Postgres 18 Permission Errors\nPostgres 18 introduced a new logic that tries to create a /18/data subdirectory inside your mount. In a \"rootless\" Podman environment, the container user doesn't have system permission to create new directories on a volume root, which caused the Permission Denied errors.\nThe Fix: We used -e PGDATA=/var/lib/postgresql/data. This told Postgres: \"Don't try to create a new versioned folder; use this exact path I've provided.\" This bypassed the failing mkdir command.\n\n4. Performance Tuning for Modern Hardware\nStandard Postgres images are configured conservatively to run on old servers with spinning hard drives.\nThe Fix: We injected high-performance flags directly into the run command:\n    --shm-size=1g: Gave Postgres enough shared memory to handle complex queries.\n    shared_buffers=512MB: Allocated 25% of your assigned RAM to the database cache.\n    random_page_cost=1.1: Told the Postgres query planner that you are using an SSD, making it much faster at choosing the right indexes.\n\nMaintenance Cheat Sheet\nNow that you have this working, here are the three commands youll likely need in the future:\n\nTo stop/start the database:\npodman stop postgres-wsl\npodman start postgres-wsl\n\nTo see if it's running healthy:\npodman ps\npodman logs postgres-wsl\n\nTo enter the database command line directly (psql):\npodman exec -it postgres-wsl psql -U postgres\n\nA note on Windows Restarts: \nIf you restart your computer, you will need to run podman machine start first, and then podman start postgres-wsl to get your database back online. Your data will be exactly where you left it inside that named volume!\n\nMore help\n\nWhen you run podman stop postgres-wsl, you are stopping only the container.\n\nTo understand exactly what is happening in your specific setup, it helps to look at the \"layers\" you have running. Think of it like a Russian nesting doll:\n\n1. The Container (postgres-wsl) \nThis is the \"process\" of PostgreSQL itself. When you run podman stop, you are telling the Postgres engine to perform a graceful shutdown. \nStatus: \"Exited.\"\nData: Safe. It stays in the pg_data_final volume.\nResources: It stops using CPU and RAM.\n\n2. The Podman Engine (The \"Machine\") \nBehind the scenes, you have a WSL 2 distribution (the podman-machine-default you saw earlier) that acts as the host for your containers. \nStatus: \"Running.\"\nNote: Even though the container is stopped, the Linux VM (WSL) is still powered on and idling in the background, waiting for you to start a container again.\n\n3. WSL 2/Windows 11 \nThe overall Windows subsystem for Linux is still active.\n\nWhy this distinction matters:\n\nIf you want to save RAM on your computer:\nStopping the container helps a little, but the Podman Machine (the Linux VM) usually reserves a chunk of your Windows RAM (you saw it was using 4GiB earlier). To get that RAM back, you have to stop the \"Machine\":\npodman machine stop\n\nIf you want to resume work:\nIf you only stopped the container, you only need one command to get back into DBeaver:\npodman start postgres-wsl\n\nIf you stopped the machine or restarted Windows, you need two steps:\npodman machine start (Turn on the Linux engine)\npodman start postgres-wsl (Turn on the Postgres process)\n\nSummary\npodman stop postgres-wsl = Turning off the App.\npodman machine stop = Turning off the Computer the app runs on.\n\nTo answer your question directly: No, it does not map to a folder on your Windows host.\n\nThis is exactly why your database is now performing so well. Here is the breakdown of where that data actually lives and how it works:\n\n1. Where is the data?\nThe data lives inside the WSL 2 Virtual Disk (.vhdx). \n\nWhen you created the named volume pg_data_final, Podman carved out a directory inside the Linux file system that belongs to your podman-machine-default instance. To Windows, that entire Linux environment is just one giant, high-performance file (usually located at %USERPROFILE%\\AppData\\Local\\Microsoft\\Windows\\WSL\\...).\n\n2. Does it contain the database data?\nYes. Everything that makes up your Postgres database is inside that volume:\nYour tables and actual row data.\nYour indexes and schemas.\nThe Postgres configuration files.\nThe transaction logs (WAL).\n\n3. Can I see the files from Windows?\nYou can actually \"peek\" into the volume from Windows Explorer, but you should never move or edit these files from Windows, or you will corrupt the database permissions.\n\nIf you want to see them, paste this into your Windows Explorer address bar:\n\\\\wsl$\\podman-machine-default\\home\\user\\.local\\share\\containers\\storage\\volumes\\pg_data_final\\_data\n\n(Note: The exact path might vary slightly depending on your Podman version, but it is always inside the \\\\wsl$ network share.)\n\n4. Why this is the \"Performance Secret\"\nNo \"Bridge\" Tax: When Postgres writes to a Windows folder (like C:\\Users), it has to use a translation layer called 9P. This layer acts like a slow network translator between Linux and Windows. It is death for database performance.\nNative Linux Speed: By using a Named Volume, Postgres talks directly to the ext4 filesystem. There is no translation. Its as fast as running Postgres on a dedicated Linux server.\n\n5. How do I back it up?\nSince the files aren't in a standard Windows folder, you can't just \"Copy+Paste\" them to a USB drive easily. If you want to back up your data, you have two professional options:\n\nOption A: The SQL Dump (Safest)\nRun this from PowerShell to create a backup file on your Windows Desktop:\npodman exec postgres-wsl pg_dumpall -U postgres > C:\\Users\\YourUsername\\Desktop\\backup.sql\n\nOption B: The Podman Export\nYou can export the entire volume to a compressed file:\npodman volume export pg_data_final --output pg_backup.tar\n\nSummary\nYour data is stored in a high-speed virtual vault inside WSL 2. It is not on your \"Host\" Windows filesystem, which is exactly why you are no longer seeing the slowness you experienced before!\n\nSuccessful steps!\n\nThis is the \"Golden Path\" we discovered to get high-performance PostgreSQL running on Windows 11 via Podman and WSL 2. \n\nBy following these steps, you moved the database from the slow Windows filesystem into the native Linux environment, bypassing permission errors and performance bottlenecks.\n\nStep 1: Environment Cleanup & Reset\nWe first had to ensure Podman's background engine (the WSL \"machine\") was healthy and in sync.\nThe Problem: Old or manually deleted WSL distros leave \"phantom\" metadata that makes Podman fail to start.\nThe Fix: You removed the broken machine and initialized a fresh one with enough resources:\npodman machine rm -f podman-machine-default\npodman machine init --disk-size 50 --memory 4096\npodman machine start\n\nStep 2: Using a performant storage strategy\nYou originally had Postgres pointing to a folder on your Windows C:\\ drive, which was slow.\nThe Change: We switched from a \"Bind Mount\" (a Windows folder) to a Podman Named Volume.\nThe Result: This placed the data inside the WSL ext4 filesystem. This eliminates the 9P protocol overhead, allowing your SSD to perform at native Linux speeds.\npodman volume create pg_data_final\n\nStep 3: Resolving Postgres 18 Permission Errors\nPostgres 18 changed its storage logic to use versioned subdirectories (e.g., /var/lib/postgresql/18/data). In a rootless Podman environment, this caused a \"Permission Denied\" error because the container couldn't create new directories on the root of the volume.\nThe Solution: We used the PGDATA override. By explicitly setting the path, we told Postgres to skip the directory-creation logic and use the volume mount point directly.\n\nStep 4: The Final Performance-Tuned Run Command\nThis specific command combined the storage fix with engine tuning for modern SSDs:\n\npodman run -d `\n  --name postgres-wsl `\n  -e POSTGRES_PASSWORD=mysecretpassword `\n  -e PGDATA=/var/lib/postgresql/data `\n  -v pg_data_final:/var/lib/postgresql/data:Z `\n  -p 5432:5432 `\n  --shm-size=1g `\n  postgres:latest `\n  -c shared_buffers=512MB `\n  -c random_page_cost=1.1\n\nWhy these flags matter:\n-v ...:Z: Automatically handles the internal Linux security labels so Postgres can read/write to the volume.\n--shm-size=1g: Increases the shared memory limit. Without this, Postgres often crashes on complex queries in containers.\n-c random_page_cost=1.1: Tells Postgres you are on an SSD. By default, Postgres assumes you have a slow, spinning hard drive and avoids using certain indexes. This flag makes it much faster.\n\nStep 5: Connecting DBeaver via the Localhost Relay\nBecause Podman on Windows runs inside a specialized WSL VM, it uses a \"proxy\" to connect that VM to your Windows network.\nThe Result: You were able to connect DBeaver using localhost:5432 because Podman automatically forwarded the traffic from Windows into the WSL container.\n\nSummary of the \"Wins\":\nSpeed: Native ext4 storage via a Named Volume.\nStability: Overrode PGDATA to fix Postgres 18 directory permission issues.\nOptimization: Tuned for SSDs and allocated proper shared memory.\nCleanliness: Removed old, failed containers and volumes to keep the system tidy.\n\nYou now have a professional-grade PostgreSQL setup that is significantly faster and more reliable than a standard Windows-native installation or a basic Docker-on-Windows setup.\n-f flag\n\nYou don't need it for the database to function, but since your goal is high performance, it is a very good flag to have. \n\nHere is the breakdown of why you might care about it and whether its worth \"fixing\" your current container to include it.\n\nWhat does effective_io_concurrency do?\nThis setting tells PostgreSQL how many simultaneous disk I/O operations the underlying storage can handle at once. \nDefault (1): Assumes a single spinning hard drive platter. Postgres waits for one piece of data to be read before asking for the next.\nHigh Value (200-300): Optimized for SSDs. Since SSDs don't have a physical \"head\" that has to move, they can fetch dozens of different pieces of data simultaneously. \n\nBy setting this to 200, you are telling the Postgres query planner: \"Don't be shy; feel free to initiate up to 200 I/O requests at once.\" This dramatically speeds up Bitmap Heap Scans (a common way Postgres reads data when using indexes).\n\nDo you care?\nFor Development/Small Data: You won't notice the difference. The database is already very fast because its on the WSL native filesystem.\nFor Large Data/Complex Queries: You do care. It allows Postgres to fully saturate the speed of your NVMe/SSD.\n\nHow to add it \nBecause this is a startup flag, you cannot add it to a running container. You would have to recreate it. Since your data is safe in the Volume, this only takes 30 seconds:\n\nStop and Remove the current container:\n    podman stop postgres-wsl\n    podman rm postgres-wsl\n\nRun it again with the \"Ultimate\" performance flags:\npodman run -d `\n  --name postgres-wsl `\n  -e POSTGRES_PASSWORD=mysecretpassword `\n  -e PGDATA=/var/lib/postgresql/data `\n  -v pg_data_final:/var/lib/postgresql/data:Z `\n  -p 5432:5432 `\n  --shm-size=1g `\n  postgres:latest `\n  -c shared_buffers=512MB `\n  -c random_page_cost=1.1 `\n  -c effective_io_concurrency=200 `\n  -c work_mem=16MB\n\nSummary\nIf you are happy with the current speed in DBeaver, leave it as is. Youve already fixed the biggest bottleneck (moving from Windows storage to WSL storage). \n\nHowever, if you want to brag that your Postgres is tuned exactly for modern SSD hardware, adding effective_io_concurrency=200 is the \"cherry on top\" for your configuration!\n\nDoes that named volume persist as long as the WSL distro is installed\n\nThe short answer is yes, the named volume persists as long as the Podman Machine (which is a WSL distribution) exists. \n\nHowever, because you are using Podman on Windows, there is a specific hierarchy of \"persistence\" you should understand to ensure you never accidentally lose your data.\n\n1. What IS safe (Data Persists)\nYour data in pg_data_final will survive all of the following:\nStopping/Starting the container: podman stop / podman start.\nDeleting the container: podman rm.\nRestarting Windows: When you reboot, the WSL virtual disk stays on your drive.\nStopping the Podman Machine: podman machine stop.\nUpdating the Postgres Image: If you pull a newer version of Postgres and recreate the container using the same volume, your data remains.\n\n2. What IS NOT safe (Data is Lost)\nYour data will be destroyed if you do any of the following:\npodman machine rm: If you delete the Podman Machine to \"start fresh,\" you are deleting the entire WSL virtual disk, including all volumes stored inside it.\npodman volume rm pg_data_final: This is the specific command to delete the virtual hard drive you created.\nUninstalling WSL or Podman Desktop: Depending on the settings, removing the application may trigger a cleanup of the associated WSL distributions.\nFactory Resetting Windows: This obviously wipes the WSL subsystem.\n\n3. Where exactly is the \"Risk\"?\nIn a standard Linux setup, the volume is just a folder on your physical disk. In your Windows setup, the volume is a folder inside a virtual disk file (.vhdx). \n\nIf that .vhdx file (the podman-machine-default distribution) is deleted, the volume goes with it.\n\nPro-Tip: The \"Golden Rule\" of Database Persistence\nWhile named volumes are great for performance and daily development, never rely on a container volume as your only backup for important data. \n\nBecause WSL distributions can occasionally become corrupted or need to be recreated (as we did in Step 1 of your setup), you should occasionally \"export\" your data to the Windows Host side where your standard PC backups (like OneDrive, Backblaze, or an external drive) can see it.\n\nThe \"Safety\" Command (Run this once a week):\npodman exec postgres-wsl pg_dumpall -U postgres > C:\\Users\\YourName\\Documents\\postgres_backups\\weekly_backup.sql\nThis takes the data *out* of the virtual WSL world and puts it into a standard Windows file that will survive even if you delete the Podman Machine.\n\nSummary\nAs long as you don't run podman machine rm, your Postgres data is as permanent as any other file on your computer. It is stored safely inside the WSL filesystem, optimized for the high performance you've now achieved.\n\nPodman command summary\n\nHere is a concise cheat sheet of the most useful Podman commands and flags for managing your new PostgreSQL environment and beyond.\n\n1. Essential Container Commands\n| Command | Purpose |\n| :--- | :--- |\n| podman ps | List running containers. |\n| podman ps -a | List all containers (including stopped/crashed ones). |\n| podman logs  | View the output of a container (add -f to follow live). |\n| podman stop  | Gracefully shut down a container. |\n| podman start  | Start an existing, stopped container. |\n| podman rm -f  | Forcefully delete a container. |\n| podman exec -it  bash | Enter a container's terminal while it is running. |\n| podman inspect  | View the full technical configuration (JSON format). |\n\n2. Common podman run Flags\nThese are the building blocks of the \"Run\" script you used:\n-d (Detached): Runs the container in the background.\n-p 5432:5432: Maps a host port to a container port (Host:Container).\n--name : Assigns a friendly name so you don't have to use long IDs.\n-e VAR=val: Sets environment variables (e.g., POSTGRES_PASSWORD).\n--rm: Automatically deletes the container when it stops (great for one-off tests).\n-it: Keeps the terminal interactive (used for bash or psql).\n--restart : Set to always or unless-stopped to auto-restart if it crashes.\n--shm-size=1g: Increases shared memory (crucial for databases like Postgres).\n\n3. Special Volume Suffixes\nIn Podman, the flags appended to your volume (-v volume_name:path:suffix) are critical for rootless environments:\n:z: (Lower-case) Tells SELinux the volume is shared between multiple containers.\n:Z: (Upper-case) Tells SELinux the volume is private to this container only.\n:U: Tells Podman to map the ownership of the host files to the containers internal user (fixes \"Permission Denied\" errors in rootless mode).\n\n4. Image & Storage Management\n| Command | Purpose |\n| :--- | :--- |\n| podman images | List all downloaded images. |\n| podman pull  | Download the latest version of an image from a registry. |\n| podman rmi  | Delete an image from your drive. |\n| podman volume ls | List all named volumes (the virtual hard drives). |\n| podman volume prune | Danger: Deletes all volumes not currently attached to a container. |\n\n5. Windows/WSL \"Machine\" Commands\nSince you are on Windows, you are managing the Linux \"engine\" that runs Podman:\npodman machine ls: Check the status of your WSL backend (Running vs. Stopped).\npodman machine start: Power on the Linux VM.\npodman machine stop: Power off the VM (saves Windows RAM).\npodman machine ssh: Log directly into the Linux VM backend.\npodman machine set --cpus 4 --memory 4096: Resize the engine's resources.\n\n6. System Health & Maintenance\npodman stats: View live CPU and RAM usage of your containers.\npodman info: View system-wide settings, storage paths, and registry info.\npodman system prune -a: The \"Spring Cleaning\" command. Deletes all stopped containers, unused networks, and dangling images.\n\nShutdown and startup \n\nBecause your data is stored in a Named Volume (pg_data_final), the data is physically separated from the container's lifecycle. You can stop, start, or even delete the container without losing a single row of data.\n\nHere is the professional workflow for shutting down and starting back up.\n\nPart 1: Shutting Down Safely\nTo ensure your database files are closed correctly and your Windows RAM is freed up, follow these two steps:\n\n1. Stop the Postgres Container\nThis sends a signal to Postgres to flush all pending data to the disk and close connections gracefully.\npodman stop postgres-wsl\n\n2. Stop the Podman Machine (The RAM Saver)\nEven if the container is stopped, the WSL 2 Linux VM is still running in the background, consuming a few gigabytes of your Windows RAM. Run this to fully \"turn off\" the engine:\npodman machine stop\n\nPart 2: Starting Up (After a Reboot)\nWhen you turn your PC back on, the Podman engine does not start automatically. You must bring the environment back up in this order:\n\n1. Start the Podman Machine\nThis \"boots\" the hidden Linux environment that holds your Podman volumes and engine.\npodman machine start\n\n2. Start the Postgres Container\nOnce the machine is \"Started,\" run your container. You don't need the long run command with all the flags; Podman remembered them all when you first created it.\npodman start postgres-wsl\n\n3. Verify Health\nWait about 5 seconds for Postgres to finish its internal checks, then verify it's ready:\npodman logs postgres-wsl\nLook for: \"database system is ready to accept connections.\"\n\nBonus: The \"Easy Button\" (Automation)\nIf you don't want to type these every time you reboot, you can create a simple Startup Script on your Windows Desktop.\n\nRight-click your Desktop and select New > Text Document.\nPaste the following:\n @echo off\n  echo Starting Podman Machine...\n podman machine start\n echo Starting Postgres Container...\n podman start postgres-wsl\n echo.\n echo Done! Your database is ready for DBeaver.\n pause\n\nSave the file as Start-Postgres.bat (make sure it ends in .bat, not .txt).\n\nNow, after a reboot, you just double-click that file, and your high-performance Postgres environment will spin up automatically!\n\nWhy your data is safe:\nWhen you run podman stop or podman machine stop, the data inside the pg_data_final volume remains exactly where it is on the WSL virtual disk. It is exactly like turning off a computer; the files on the hard drive don't disappear just because the power is off.\n\nFull system shutdown\n\nTo fully restart the entire stackfrom the database engine up to the Windows Subsystem for Linux itselfyou should follow a \"top-down\" shutdown and a \"bottom-up\" startup. \n\nThis is the most reliable way to clear memory leaks, reset \"stuck\" network ports, and ensure your virtual disk is in a safe state.\n\nPhase 1: The Clean Shutdown (Top-Down)\nThis sequence ensures Postgres finishes its writes before the underlying \"computer\" is turned off.\n\nStop the Container:\n    podman stop postgres-wsl\nStop the Podman Machine:\n    podman machine stop\nShutdown WSL Entirely:\n    This command kills all running WSL distributions and the background WSL 2 lightweight utility VM. Use this if WSL feels sluggish or if you are having connection issues.\n    wsl --shutdown\n\nPhase 2: The Fresh Start (Bottom-Up)\nYou don't need a specific \"WSL start\" command; starting the Podman machine will automatically trigger the WSL 2 subsystem to wake up.\n\nStart the Podman Machine:\n    podman machine start\n    (If this hangs, its usually because Windows is still cleaning up the previous WSL shutdown; just wait 10 seconds and try again.)\n\nStart the Container:\n    podman start postgres-wsl\n\nWhy should you use wsl --shutdown?\nIn your specific environment, wsl --shutdown is a powerful tool for two reasons:\n\nMemory Reclamation: WSL 2 (and Podman by extension) is notorious for \"holding onto\" Windows RAM even after you stop a container. A full shutdown forces the vmmem process in Task Manager to release that memory back to Windows.\nNetworking Reset: If DBeaver suddenly stops connecting to localhost, a WSL shutdown resets the virtual network bridge between Windows and Linux, which usually fixes the issue.\n\nThe \"Full Restart\" Script\nIf you want to do this frequently to keep your system fast, you can put this into a .bat file:\n\n@echo off\necho --- SHUTTING DOWN ---\npodman stop postgres-wsl\npodman machine stop\nwsl --shutdown\necho.\necho --- RESTARTING ---\npodman machine start\npodman start postgres-wsl\necho.\necho System Restarted Successfully.\npause\n\nIs my data safe during wsl --shutdown?\nYes. As long as you ran podman stop postgres-wsl first, the database has finished writing its logs to the disk. The wsl --shutdown command is essentially like pulling the power plug on a PC that is already turned offit is perfectly safe for your data.\n\nSSH into container\n\npodman machine ssh\n\nGet container info\n\ncat /etc/os-release\n\n[user@DESKTOP-FT1088C ~]$ cat /etc/os-release\nNAME=\"Fedora Linux\"\nVERSION=\"41 (Container Image)\"\nRELEASE_TYPE=stable\nID=fedora\nVERSION_ID=41\nVERSION_CODENAME=\"\"\nPLATFORM_ID=\"platform:f41\"\nPRETTY_NAME=\"Fedora Linux 41 (Container Image)\"\nANSI_COLOR=\"0;38;2;60;110;180\"\nLOGO=fedora-logo-icon\nCPE_NAME=\"cpe:/o:fedoraproject:fedora:41\"\nDEFAULT_HOSTNAME=\"fedora\"\nHOME_URL=\"https://fedoraproject.org/\"\nDOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f41/\"\nSUPPORT_URL=\"https://ask.fedoraproject.org/\"\nBUG_REPORT_URL=\"https://bugzilla.redhat.com/\"\nREDHAT_BUGZILLA_PRODUCT=\"Fedora\"\nREDHAT_BUGZILLA_PRODUCT_VERSION=41\nREDHAT_SUPPORT_PRODUCT=\"Fedora\"\nREDHAT_SUPPORT_PRODUCT_VERSION=41\nSUPPORT_END=2025-12-15\nVARIANT=\"Container Image\"\nVARIANT_ID=container\n\nPodman compose\n\nYes, a Podman Compose file would help significantly. While your current command works, it is \"imperative\"meaning you have to remember and re-type (or copy-paste) that long string every time you want to recreate or update the container.\n\nUsing a compose.yaml file (the \"declarative\" way) offers several advantages for your specific setup:\n\n1. Benefits for your specific setup\nVersion Control: You can save the file in your project folder. If you change a setting (like shared_buffers), you just edit the text file instead of trying to edit a massive terminal command.\nAutomatic Restarts: You can add restart: always so the DB starts automatically when your machine/WSL2 starts.\nReadability: Its much easier to see the relationship between your shared memory (shm_size), volumes, and Postgres flags.\nOne-Command Management: Instead of multiple steps (creating volume, then running), you just run podman-compose up -d.\n\n2. Your command converted to compose.yaml\nCreate a file named compose.yaml in your project directory and paste this:\n\nservices:\n  postgres:\n    image: postgres:latest\n    container_name: postgres-wsl\n    restart: always\n    environment:\n      POSTGRES_PASSWORD=mysecretpassword\n      PGDATA=/var/lib/postgresql/data\n    ports:\n      \"5432:5432\"\n    volumes:\nThe :Z ensures correct SELinux labels (useful if using RHEL/Fedora in WSL)\n      pg_data_final:/var/lib/postgresql/data:Z\n    shm_size: '1gb'\nThis translates your -c flags into the startup command\n    command: \n      postgres\n      -c\n      shared_buffers=512MB\n      -c\n      random_page_cost=1.1\n      -c \n      effective_io_concurrency=200        \n\nvolumes:\n  pg_data_final:\n    external: true  # Tells Podman to use the volume you already created manually\n\n3. How to use it\nInstall Podman Compose (if you haven't yet):\n    pip install podman-compose\nStart the container:\n    In the same folder as your file, run:\n    podman-compose up -d\nStop the container:\n    podman-compose down\n\nKey differences to note:\nshm_size: In your manual command, you used --shm-size=1g. In the Compose file, its a dedicated setting. This is critical for Postgres performance with large shared_buffers.\nexternal: true: I set the volume as \"external\" because you already created pg_data_final manually. If you want Compose to manage the volume lifecycle entirely, you would remove external: true.\nPostgres Flags: The command section cleanly breaks down your -c configurations, making them much easier to adjust later as your database grows.\n\nThe VHDX \n\npodman-machine-defaultis the standard, automatically assigned name for the Linux virtual machine (VM) that Podman creates on macOS and Windows to run containers, acting as the default target for Podman commands likepodman machine init. It provides a Linux environment (Fedora CoreOS) for container operations, allowing Podman to function on non-Linux systems, and you use commands likepodman machine start,stop,list, andinitto manage it.\n\nUsing PowerShell to show  WSL distros\n\n(Get-ChildItem HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Lxss | ForEach-Object {Get-ItemProperty $_.PSPath}) | Select-Object DistributionName, BasePath\n\nDistributionName       BasePath\nUbuntu                 C:\\Users\\thumb\\AppData\\Local\\wsl\\{73929071-0e78-42fe-a25d-96580d570fe0}\npodman-machine-default \\\\?\\C:\\Users\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\n\nUsing the registry to see WSL distros\n\nHKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Lxss\n\nC:\\Users\\\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\\ext4.vhdx\n\nDistributionName       BasePath\nUbuntu                 C:\\Users\\thumb\\AppData\\Local\\wsl\\{73929071-0e78-42fe-a25d-96580d570fe0}\npodman-machine-default \\\\?\\C:\\Users\\thumb\\.local\\share\\containers\\podman\\machine\\wsl\\wsldist\\podman-machine-default\n\nThe Windows Extended Length Path Prefix\n\nThe path prefix for the podman-machine-default is a Windows\"[Extended Length Path\"prefix. \n\n\\\\?\\\n\nThis tells Windows to ignore the standard 260-character path limit.    \nIt's necessary with WSL and Podman because they often use deeply nested folders. Without this prefix, some Windows applications would crash or fail to find the file because the folder path is too long.",
        "locale": "en",
        "slug": "technical-posts/postgres-on-wsl2-with-podman",
        "folder": ""
    },
    {
        "title": "Using CSS clamp effectively",
        "description": "Using CSS clamp effectively",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "[[argyl-custom-clamp-properties]]\nSee Argyl's CSS clamp custom properties\n\nClamp example:\n\nclamp(0.3686rem, 0.6197rem + -0.324vi, 0.5549rem);\n\nThe clamp() function takes three parameters: clamp(minimum, preferred, maximum) and is used for responsive sizing. In your code, it's being used to create a fluid typography scale.\n\nLet's break down one example:\n\nHere's what each part means:\n\n1rem: The minimum allowed value\n0.9565rem + 0.2174vi: The preferred value (calculated based on viewport) 1.125rem: The maximum allowed value\nThe vi unit represents 1% of viewport inline size (width in horizontal writing modes).\n\nSo this means:\n\nOn very small screens, the size will be locked to 1rem\nOn very large screens, it will be locked to 1.125rem\nBetween these extremes, it will scale fluidly based on the formula 0.9565rem + 0.2174vi\n\nClamp deeper dive\n\nLet's break down what clamp(1.4em, 5vw, 2.1em) does for the font-size of an H1 tag:\n\nThe clamp() function creates a font size that is fluid (it scales with the viewport width) but within defined limits.\n\nHere's the detailed breakdown:\n\nclamp() takes three values:\n\nMIN (Minimum Value): 1.4em\n\n    This is the absolute smallest the font-size will ever be.\n    No matter how narrow the viewport (browser window) gets, the H1's font size will not shrink below 1.4em.\n    em is a relative unit, typically based on the font size of the parent element (or the element's default if not inherited).\n\nVAL (Preferred / Ideal Value): 5vw\n\n    This is the value the browser tries to use.\n    vw stands for \"viewport width\". 5vw means the font size will try to be 5% of the current width of the browser window. In most cases, this preferred value should be in vw units.\n    As the browser window gets wider, 5vw calculates to a larger pixel value, making the font grow. As it gets narrower, the font shrinks.\n    This is what provides the fluid scaling.\n\nMAX (Maximum Value): 2.1em\n    This is the absolute largest the font-size will ever be.\n    No matter how wide the viewport gets, if 5vw calculates to a value larger than 2.1em, the font size will stop growing and stay at 2.1em.\n\nHow it works together:\n\nThe browser calculates the value of 5vw based on the current viewport width.\nIt then compares this calculated 5vw value to 1.4em and 2.1em.\n-   If 5vw is less than 1.4em, the browser uses 1.4em as the font size.\n    If 5vw is greater than 2.1em, the browser uses 2.1em as the font size.\n    If 5vw is between 1.4em and 2.1em, the browser uses the calculated 5vw value as the font size.\n\nWhy use this?\n\nThis technique is excellent for responsive typography. It allows the H1 heading to:\n\nScale smoothly with the browser window size (thanks to 5vw).\nRemain readable on small screens by preventing it from becoming too tiny (thanks to the 1.4em minimum).\nAvoid becoming excessively large and visually overpowering on very wide screens (thanks to the 2.1em maximum).\n\nIt provides a clean way to achieve fluid sizing with guardrails, often reducing the need for multiple media queries just to adjust font sizes.\n\nThe CSS clamp/elevator analogy\n\nThe elevator analogy is a really good one for explaining clamp(). Here's how it usually goes:\n\nImagine a building (the range of possible values):\n\nIt has a Lowest Floor (the MIN value): This is the absolute bottom floor the elevator can reach. In your example, 1.4em.\nIt has a Highest Floor (the MAX value): This is the absolute top floor the elevator can reach. In your example, 2.1em.\nThere's a Desired Floor (the VAL or Preferred Value): This is the floor the elevator wants to go to ideally. This floor number might change based on conditions (like how many people are waiting, or in CSS, the viewport width). In your example, this is 5vw.\n\nNow, think about how the elevator operates (clamp()):\n\nIf the Desired Floor (5vw) is between the Lowest Floor (1.4em) and the Highest Floor (2.1em): The elevator goes directly to the Desired Floor. The clamp() function returns the VAL (5vw).\nIf the Desired Floor (5vw) calculates to a floor below the Lowest Floor (1.4em): The elevator can't go any lower than the Lowest Floor. So, it stops at the Lowest Floor (1.4em). The clamp() function returns the MIN (1.4em).\nIf the Desired Floor (5vw) calculates to a floor above the Highest Floor (2.1em): The elevator can't go any higher than the Highest Floor. So, it stops at the Highest Floor (2.1em). The clamp() function returns the MAX (2.1em).\n\nApplying it to your H1:\n\nThe H1's font size wants to be 5vw (5% of the viewport width).\nHowever, it will never go below 1.4em (the lowest floor).\nAnd it will never go above 2.1em (the highest floor).\nIt smoothly scales based on 5vw only when that value falls within the 1.4em to 2.1em range.\n\nSo, the elevator analogy helps visualize how clamp() provides a flexible, preferred value (VAL) that is safely constrained within absolute minimum (MIN) and maximum (MAX) boundaries.\n\nWhy clamp's desired value (it's second arg) should be a vw unit\n\nYou've hit on a crucial point! No, 5vw is absolutely not a general standard for the second value (the preferred value) in clamp().\n\nGetting that middle value right is indeed key, and it's highly dependent on several factors:\n\nThe Element: An H1 (like in your example) generally needs to be larger and might scale more dramatically than body text (p), a smaller heading (H3), or UI elements. 5vw might be reasonable for a large heading but would likely be way too big and scale too quickly for body text.\nThe Design: How much do you want the element to scale?\n    A higher vw value (like 5vw, 7vw, or even 10vw for very large display text) means the size will change more rapidly as the viewport width changes.\n    A lower vw value (like 1vw, 2vw) means the size changes more slowly and subtly.\n    Sometimes, you might combine vw with a fixed unit like rem (e.g., 1rem + 2vw) in the middle slot for more controlled scaling that still has a solid base.\nThe MIN and MAX Values: The middle value works in relation to the minimum and maximum. You choose the MIN and MAX to define the absolute boundaries, and then you choose the middle VAL to define how it scales between those boundaries. If your MIN and MAX are very close together, the vw value will have less impact. If they are far apart, the vw value determines how quickly it traverses that range.\nThe Desired Breakpoints: You often choose the MIN, VAL, and MAX values together so that the font size stops shrinking (hits the MIN) or stops growing (hits the MAX) at specific viewport widths that make sense for your design. You might use online clamp() calculators or trial-and-error in the browser's developer tools to fine-tune this.\n\nHow to Choose the Preferred Value:\n\nStart with a goal: How large should this element look on a \"typical\" mid-size screen (like a tablet or small laptop)?\nEstimate a vw: Make an educated guess based on the element and desired scaling speed. For headings, 3vw to 6vw is a common starting range. For body text, often much smaller (e.g., 0.5vw to 1.5vw), sometimes combined with rem (calc(1rem + 0.5vw)).\nSet realistic MIN and MAX: What's the smallest readable/acceptable size (MIN)? What's the largest size before it becomes overpowering (MAX)? Use em or rem for these for better accessibility and consistency.\nTest and Refine: Use browser developer tools (responsive mode) to drag the viewport width and see how the size changes. Observe:\n    Does it scale too fast or too slow? Adjust the vw value.\n    Does it get too small or too big? Adjust the MIN or MAX.\n    At what screen width does it hit the MIN?\n    At what screen width does it hit the MAX?\n    Are these \"switch points\" appropriate for your design?\n\nIn summary: 5vw is just one possible value. Choosing the right preferred value requires considering the specific element, the overall design aesthetic, the desired scaling behavior, and testing across different viewport sizes. It's more about finding the right combination of MIN, VAL, and MAX for each specific use case.\n\nAnother explanation of Clamp's desired value\n\nLet me explain why using a combination of rem and vi (viewport-relative) units in the clamp() function is a powerful responsive design technique:\n\nThe clamp() function takes three values:\n\n    Minimum size (in rem)\n    Preferred size (using rem + vi)\n    Maximum size (in rem)\n\nWhy this combination works well:\n\n    rem provides consistent sizing based on root font size\n    vi (viewport-relative inline size) allows for fluid scaling based on viewport width\n    Together, they create a responsive scaling that:\n        Won't get too small on mobile devices (minimum rem)\n        Scales smoothly across different screen sizes (rem + vi)\n        Won't get too large on big screens (maximum rem)\n\nExample from your code:\n\n--step-0: clamp(1rem, 0.9565rem + 0.2174vi, 1.125rem);\n\nMinimum: 1rem\nPreferred: 0.9565rem + 0.2174vi (fluid scaling)\nMaximum: 1.125rem\n\nUsing only rem would mean:\n\nFixed sizes that don't automatically adjust to viewport width\nNeed for more media queries to handle different screen sizes\nLess smooth transitions between breakpoints\n\nThe rem + vi combination creates a more fluid, maintainable responsive design that scales smoothly across different device sizes while maintaining readable text sizes.\n\nThis technique is often called \"fluid typography\" and is considered a modern best practice for responsive design.",
        "locale": "en",
        "slug": "technical-posts/using-css-clamp-effectively",
        "folder": ""
    },
    {
        "title": "Light or dark color schema",
        "description": "Light or dark color schema",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "",
        "locale": "en",
        "slug": "technical-posts/light-or-dark-color-schema",
        "folder": ""
    },
    {
        "title": "Obsidian hotkeys",
        "description": "Obsidian hotkeys",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "obsidian"
        ],
        "content": "Special tags\n\nmenu - document is a menu object\nxlate-object - A JavaScript translation object\nsvelte-component - A Svelte component\n\nMake new note*Cmd-n  \n\\_Create note in new pane*Cmd-shift-n  \nOpen the link youre hovering over in a new pane (while in edit mode)Cmd-click  \nToggle edit/preview modeCmd-e  \nOpen quick switcherCmd-o  \nClose active paneCmd-w\nhttps://forum.obsidian.md/t/obsidian-hotkeys-favorites-and-best-practices/12125\n\nFormatting basics",
        "locale": "en",
        "slug": "technical-posts/osidian-hotkeys",
        "folder": ""
    },
    {
        "title": "Count IBM i user jobs ibmi",
        "description": "Count IBM i user jobs ibmi",
        "date_created": "2025-08-12T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "ibm-i"
        ],
        "content": "Example 1: Traditional CL Program using WRKUSRJOB\n\nThis method uses the standard WRKUSRJOB command to create a temporary file and then the RTVMBRD (Retrieve Member Description) command to count the records in that file. This is a very common pattern in older or traditional CL programming.\n\nCL Program: GETBCHCNT\n\n/**/\n/* PROGRAM: GETBCHCNT                                           */\n/*                                                              */\n/* DESCRIPTION: Gets the count of *Active *Batch jobs for a     */\n/*              given user.                                     */\n/*                                                              */\n/* PARAMETERS:  &USER     - Input: The user profile to check.   */\n/*              &JOBCOUNT - Return: The number of jobs found.   */\n/*                                                              */\n/* TO COMPILE: CRTCLPGM PGM(yourlib/GETBCHCNT) SRCFILE(yourlib/QCLSRC) */\n/*                                                              */\n/**/\nPGM        PARM(&USER &JOBCOUNT)\n\n   /* Parameter Declarations */\n   DCL        VAR(&USER)     TYPE(*CHAR) LEN(10)\n   DCL        VAR(&JOBCOUNT) TYPE(DEC)  LEN(5 0) / 5 digits, 0 decimals */\n\n   /* Local Variable Declaration */\n   DCL        VAR(&NBRRCDS)  TYPE(*DEC)  LEN(10 0)\n\n   /* Initialize return parameter */\n   CHGVAR     VAR(&JOBCOUNT) VALUE(0)\n\n   /* Clean up a previous run of this job, ignore error if file not found */\n   DLTF       FILE(QTEMP/ACTBCHJOBS)\n   MONMSG     MSGID(CPF2105)\n\n   /* Create a temporary file in QTEMP with the active batch job list */\n   WRKUSRJOB  USER(&USER) STATUS(ACTIVE) JOBTYPE(BATCH) +\n                OUTPUT(*OUTFILE) OUTFILE(QTEMP/ACTBCHJOBS)\n   /* Monitor for message CPF9862, which means no records were selected */\n   MONMSG     MSGID(CPF9862) EXEC(GOTO CMDLBL(CLEANUP))\n\n   /* Retrieve the record count from the member description of the temp file */\n   RTVMBRD    FILE(QTEMP/ACTBCHJOBS) NBRCURRCD(&NBRRCDS)\n\n   /* Change the parameter variable to the retrieved record count */\n   CHGVAR     VAR(&JOBCOUNT) VALUE(&NBRRCDS)\n\n\nCLEANUP:\n   DLTF       FILE(QTEMP/ACTBCHJOBS)\n   MONMSG     MSGID(CPF2105) /* Ignore error if file not found */\n\nENDPGM:\nENDPGM\n\nHow to Use This Program:\n\nCompile: Use the CRTCLPGM command as noted in the program's comments.\nCall: You would call this program from another CL program, passing it a username and a decimal variable to receive the count.\n\n    /* Example of calling GETBCHCNT */\n    DCL &USERNAME TYPE(*CHAR) LEN(10) VALUE('QPGMR')\n    DCL &COUNT    TYPE(*DEC)  LEN(5 0)\n\n    CALL PGM(GETBCHCNT) PARM(&USERNAME &COUNT)\n\n    /* The &COUNT variable now holds the number of active batch jobs for QPGMR */\n    IF   COND(&COUNT > 0) THEN(SNDPGMMSG MSG('User' *BCAT &USERNAME *BCAT +\n         'has' *BCAT %CHAR(&COUNT) *BCAT 'active batch jobs.'))\n    ELSE (SNDPGMMSG MSG('No active batch jobs found for user' *BCAT &USERNAME))\n\nExample 2: Modern ILE CL Program using Embedded SQL\n\nThis method uses embedded SQL and the ACTIVE_JOB_INFO system view. It is more efficient as it does not create any temporary work files. This requires creating the program as a CLLE (ILE CL) source type and compiling it with the CRTBNDCL (Create Bound CL Program) command.\n\nCLLE Program: GETBCHSQL\n\n/**/\n/* PROGRAM: GETBCHSQL                                           */\n/*                                                              */\n/* DESCRIPTION: Gets the count of *Active *Batch jobs for a     */\n/*              given user using embedded SQL.                  */\n/*                                                              */\n/* PARAMETERS:  &USER     - Input: The user profile to check.   */\n/*              &JOBCOUNT - Return: The number of jobs found.   */\n/*                                                              */\n/* SOURCE TYPE: CLLE                                            */\n/*                                                              */\n/* TO COMPILE:                                                  */ \n/*    CRTBNDCL PGM(yourlib/GETBCHSQL) SRCFILE(yourlib/QCLSRC)   */\n/*                                                              */\n/**/\nPGM        PARM(&USER &JOBCOUNT)\n\n   /* Parameter Declarations */\n   DCL        VAR(&USER)     TYPE(*CHAR) LEN(10)\n   DCL        VAR(&JOBCOUNT) TYPE(*DEC)  LEN(5 0)\n\n   /* Initialize return parameter */\n   CHGVAR     VAR(&JOBCOUNT) VALUE(0)\n\n   /* Execute SQL to count the jobs and place result into &JOBCOUNT */\n   /* The variable must be preceded by a colon (:) in the SQL statement */\n   EXEC SQL\n      SELECT COUNT(*)\n        INTO :JOBCOUNT\n        FROM TABLE(QSYS2.ACTIVE_JOB_INFO())\n       WHERE AUTHORIZATION_NAME = :USER AND JOB_TYPE = 'BCH'\n\n   /* No cleanup is necessary */\n\nENDPGM:\nENDPGM\n\nHow to Use This Program:\n\nCompile: Use the CRTBNDCL command, which processes the embedded SQL. Make sure your source member type is set to CLLE.\nCall: The calling process is identical to the first example.\n\n    /* Example of calling GETBCHSQL */\n    DCL &USERNAME TYPE(*CHAR) LEN(10) VALUE('QPGMR')\n    DCL &COUNT    TYPE(*DEC)  LEN(5 0)\n\n    CALL PGM(GETBCHSQL) PARM(&USERNAME &COUNT)\n\n    IF   COND(&COUNT > 0) THEN(SNDPGMMSG MSG('User' *BCAT &USERNAME *BCAT +\n         'has' *BCAT %CHAR(&COUNT) *BCAT 'active batch jobs.'))\n    ELSE (SNDPGMMSG MSG('No active batch jobs found for user' *BCAT &USERNAME))\n\nComparison\n\n| Feature | Example 1 (WRKUSRJOB) | Example 2 (Embedded SQL) |\n| :--- | :--- | :--- |\n| Method | Creates a temporary outfile and counts records. | Directly queries system information. |\n| Efficiency | Less efficient due to I/O for temporary file creation and deletion. | More efficient; no temporary objects are created. |\n| Simplicity | Logic is more verbose with multiple steps (Create, Check, Retrieve, Delete). | Cleaner and more direct. The intent is clear in one statement. |\n| Dependencies | Relies on traditional system commands. | Relies on Db2 for i and the QSYS2.ACTIVE_JOB_INFO view. |\n| Compile Cmd| CRTCLPGM | CRTBNDCL |\n\nFor modern development on the IBM i platform, the embedded SQL method (Example 2) is highly recommended due to its superior performance and code simplicity.",
        "locale": "en",
        "slug": "technical-posts/count-user-jobs-on-ibm-i",
        "folder": ""
    },
    {
        "title": "Emulating enums in TypeScript",
        "description": "Emulating enums in TypeScript",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "Theo shows how to use\nconst userTypes = ['Subscriber\", \"Loser\"] as const;\nhttps://youtu.be/qoSf8MCBsv0?t=1172",
        "locale": "en",
        "slug": "technical-posts/emulating-enums-in-typescript",
        "folder": ""
    },
    {
        "title": "Working with search parameters in Sveltekit",
        "description": "Working with search parameters in Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Sveltekit's [[Working with HTTP context|context object]] uses the URLSearchParams object for query strings. The getQueryStringObject helper makes it easier to work with the URLSearchParams object.\n\nPut getQueryStringObject() in a utils.ts file in the lib folder.\n\nexport function getQueryStringObject(obj) {\n\t// Collapse the SearchParm query string object\n\t// into something more helpful.\n\tconst qs = {};\n\tfor (const q of obj) {\n\t\tqs[q[0]] = q[1];\n\t}\n\n\t// This method counts as a key, so\n\t// subtract 1 from length.\n\tqs.getKeyCount = function () {\n\t\treturn Object.keys(this).length - 1;\n\t};\n\n\tqs.hasKeys = function () {\n\t\treturn Object.keys(this).length - 1 == 0;\n\t};\n\n\treturn qs;\n}\n\nThe resulting object has properties for each query string key and a hasKeys() to show if query string keys are available.\n\nIn a route method pass the context.url.searchPararms object to getQueryStringObject() to get the helper object.\n\nThis is in an API\n\nimport { getQueryStringObject } from '$lib/utils.js';\n\nexport async function GET(context) {\n\tconst qs = getQueryStringObject(context.url.searchParams);\n\n\tlet posts;\n\n\t// Get filtered posts\n\tif (qs.hasOwnProperty('filter')) {\n\t\tposts = await getFilteredPosts(qs.filter);\n\t\treturn json(posts);\n\t}\n\n\tposts = await getAllPosts();\n\treturn json(posts);",
        "locale": "en",
        "slug": "technical-posts/working-with-urlsearchparams-query-strings",
        "folder": ""
    },
    {
        "title": "query for a list of column names",
        "description": "query for a list of column names",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "Adjust this query as needed:\n\nSELECT string_agg(column_name, ', ' ORDER BY ordinal_position)\nFROM information_schema.columns\nWHERE table_name   = 'family'\n  AND table_schema = 'public'  -- Adjust schema if needed\n  AND column_name  'id'\n\nResults (a single, comma-delimited string)\n\ncreated_at, name, description, visual_studio_version, availability_id, group_id, sort_order, download_page_section_heading, download_page_order, release_date",
        "locale": "en",
        "slug": "technical-posts/query-for-a-list-of-column-names",
        "folder": ""
    },
    {
        "title": "Original code with original UI to extract HTML from a page",
        "description": "Original code with original UI to extract HTML from a page",
        "date_created": "2025-06-27T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "\n\timport { getCurrentPageHtml, getPageHtmlViaApi } from '$lib/utils/email-html';\n\timport { page } from '$app/state';\n\n\tlet isGettingHtml = false;\n\tlet htmlResult = '';\n\tlet error = '';\n\n\tasync function getHtmlClientSide() {\n\t\tisGettingHtml = true;\n\t\terror = '';\n\n\t\ttry {\n\t\t\tconst html = await getCurrentPageHtml({\n\t\t\t\tincludeStyles: true,\n\t\t\t\tremoveInteractiveElements: true\n\t\t\t});\n\t\t\thtmlResult = html;\n\t\t\tconsole.log('Client-side HTML:', html);\n\t\t} catch (err) {\n\t\t\terror = err instanceof Error ? err.message : 'Unknown error';\n\t\t\tconsole.error('Error getting HTML:', err);\n\t\t} finally {\n\t\t\tisGettingHtml = false;\n\t\t}\n\t}\n\n\tasync function getHtmlServerSide() {\n\t\tisGettingHtml = true;\n\t\terror = '';\n\n\t\ttry {\n\t\t\tconst currentUrl = window.location.href;\n\t\t\tconst html = await getPageHtmlViaApi(currentUrl);\n\t\t\thtmlResult = html;\n\t\t\tconsole.log('Server-side HTML:', html);\n\t\t} catch (err) {\n\t\t\terror = err instanceof Error ? err.message : 'Unknown error';\n\t\t\tconsole.error('Error getting HTML:', err);\n\t\t} finally {\n\t\t\tisGettingHtml = false;\n\t\t}\n\t}\n\n\tfunction copyToClipboard() {\n\t\tnavigator.clipboard.writeText(htmlResult);\n\t}\n\n\tfunction downloadHtml() {\n\t\tconst blob = new Blob([htmlResult], { type: 'text/html' });\n\t\tconst url = URL.createObjectURL(blob);\n\t\tconst a = document.createElement('a');\n\t\ta.href = url;\n\t\ta.download = page-${Date.now()}.html;\n\t\ta.click();\n\t\tURL.revokeObjectURL(url);\n\t}\n\n\n\n\tEmail HTML Extractor\n\tCurrent page: {page.url.pathname}\n\n\t\n\t\t\n\t\t\t{isGettingHtml ? 'Getting HTML...' : 'Get HTML (Client-side)'}\n\t\t\n\n\t\t\n\t\t\t{isGettingHtml ? 'Getting HTML...' : 'Get HTML (Server-side)'}\n\t\t\n\t\n\n\t{#if error}\n\t\t\n\t\t\tError:\n\t\t\t{error}\n\t\t\n\t{/if}\n\n\t{#if htmlResult}\n\t\t\n\t\t\t\n\t\t\t\t Copy to Clipboard \n\t\t\t\t Download HTML \n\t\t\t\n\n\t\t\t\n\t\t\t\tView HTML ({htmlResult.length.toLocaleString()} characters)\n\t\t\t\t{htmlResult}\n\t\t\t\n\t\t\n\t{/if}\n\n\n\n\t.email-html-extractor {\n\t\tmax-width: 800px;\n\t\tmargin: 2rem auto;\n\t\tpadding: 2rem;\n\t\tborder: 1px solid #ddd;\n\t\tborder-radius: 8px;\n\t}\n\n\t.controls {\n\t\tdisplay: flex;\n\t\tgap: 1rem;\n\t\tmargin: 1rem 0;\n\t}\n\n\t.btn {\n\t\tpadding: 0.5rem 1rem;\n\t\tborder: none;\n\t\tborder-radius: 4px;\n\t\tcursor: pointer;\n\t\tfont-size: 1rem;\n\t}\n\n\t.btn-primary {\n\t\tbackground: #007acc;\n\t\tcolor: white;\n\t}\n\n\t.btn-secondary {\n\t\tbackground: #6c757d;\n\t\tcolor: white;\n\t}\n\n\t.btn-small {\n\t\tpadding: 0.25rem 0.5rem;\n\t\tfont-size: 0.875rem;\n\t\tbackground: #28a745;\n\t\tcolor: white;\n\t}\n\n\t.btn:disabled {\n\t\topacity: 0.6;\n\t\tcursor: not-allowed;\n\t}\n\n\t.btn:hover:not(:disabled) {\n\t\topacity: 0.9;\n\t}\n\n\t.error {\n\t\tbackground: #f8d7da;\n\t\tcolor: #721c24;\n\t\tpadding: 1rem;\n\t\tborder-radius: 4px;\n\t\tmargin: 1rem 0;\n\t}\n\n\t.results {\n\t\tmargin-top: 2rem;\n\t}\n\n\t.result-controls {\n\t\tdisplay: flex;\n\t\tgap: 0.5rem;\n\t\tmargin-bottom: 1rem;\n\t}\n\n\t.html-preview {\n\t\tbackground: #f8f9fa;\n\t\tpadding: 1rem;\n\t\tborder-radius: 4px;\n\t\toverflow-x: auto;\n\t\tmax-height: 400px;\n\t\toverflow-y: auto;\n\t\twhite-space: pre-wrap;\n\t\tfont-size: 0.875rem;\n\t}\n\n\tdetails summary {\n\t\tcursor: pointer;\n\t\tfont-weight: bold;\n\t\tmargin-bottom: 1rem;\n\t}\n\n\tcode {\n\t\tbackground: #f8f9fa;\n\t\tpadding: 0.25rem 0.5rem;\n\t\tborder-radius: 3px;\n\t\tfont-family: 'Courier New', monospace;\n\t}\n",
        "locale": "en",
        "slug": "technical-posts/sveltekit-extract-html-original-ui",
        "folder": ""
    },
    {
        "title": "Utilities",
        "description": "Utilities",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "pc"
        ],
        "content": "SiteOne Crawler - Website crawler\n-",
        "locale": "en",
        "slug": "technical-posts/utilities",
        "folder": ""
    },
    {
        "title": "Svelte persistent store",
        "description": "Svelte persistent store",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "This code is taken from this article.\n\nI think this is obsolete with Svelte 5.\n\nimport { browser } from \"$app/environment\";\nimport { derived, readable, writable } from \"svelte/store\";\n\nexport const createPersistentStore = (name, initialValue) => {\n  let store = writable(initialValue);\n  if (!browser) return store; // required for sveltekit\n\n  const storedValue = localStorage.getItem(name);\n  const finalValue = storedValue ? JSON.parse(storedValue) : initialValue;\n\n  store = writable(finalValue, () => {\n    const unsubscribe = store.subscribe((value) => {\n      localStorage.setItem(name, JSON.stringify(value));\n    });\n    return unsubscribe;\n  });\n  return store;\n};\n\nexport const theme = createPersistentStore(\"theme\", \"light\");",
        "locale": "en",
        "slug": "technical-posts/function-to-create-sveltekit-persistent-store",
        "folder": ""
    },
    {
        "title": "Create JSON schemas of DataGate files",
        "description": "Create JSON schemas of DataGate files",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "datagate"
        ],
        "content": "This utility generates a Json file schema for one or all DataGate files in a library.\n\n!info]\nThere is a similar uility named general-sql-server-json-models that genreates similar Json for SQL Server tables. That is utility is currently located here: \"C:\\Users\\thumb\\Documents\\projects\\from-delray\\rputilities\\general-sql-server-json-models\". It's not yet been compiled on envoy.\n\n[!note]\nThis utility uses the ArgyBargy at https://github.com/rogerpence/ArgyBargy. Don't use the other ArgyBargy's you may find floating around in the old backups.\n\nThis utility grew out of some very old for a very old fat client version of Libretto. Some of its code is pretty complex owing to DataGate's sophisticated use of XML (especially GetFileDescription.vr). The XML the DataGate client creates is used parsed and converted to Json. This code could be simplified but it works so leave it alone!\n\nCommand line:\n\nGenerate DataGate file schemas for a library\n\nFlag                  ShortHand  Required  Description\n--databasename           -d        True    DateGate Database Name\n--library                -l        True    Library name\n--outputpath             -o        False   Output path (appended to output path selected--see below)\n--physicalsonly          -po       False   Process physical files only (default is false)\n--help                   -h                Show this help\n\nFor example, given this DataGate file:\n\nDatabase Name.: dg-local-db\nLibrary.......: examples\nFile..........: CMastNew\nFormat........: RCMmaster\nKey field(s)..:\nType..........: physical\nBase file.....:\nDescription...: Customer master\nRecord length.: 151\n\nField name        Data type                    Description\nCMCustNo          Type(*Packed) Len(9,0)       Cutomer Number\nCMName            Type(*Char) Len(40)          Customer Name\nCMAddr1           Type(*Char) Len(35)          Address Line 1\nCMCity            Type(*Char) Len(30)          City\nCMState           Type(*Char) Len(2)           State\nCMCntry           Type(*Char) Len(2)           Country Code\nCMPostCode        Type(*Char) Len(10)          Postal Code (zip)\nCMActive          Type(*Char) Len(1)           Active = 1, else 0\nCMFax             Type(*Packed) Len(10,0)      Fax Number\nCMPhone           Type(*Char) Len(20)          Main Phone\n\nThis JSON is created:\n\n{\n  \"dbname\": \"dg-local-db\",\n  \"library\": \"examples\",\n  \"file\": \"CMastNew\",\n  \"format\": \"RCMmaster\",\n  \"description\": \"Customer master\",\n  \"type\": \"physical\",\n  \"recordlength\": \"151\",\n  \"keylength\": \"0\",\n  \"basefile\": \"\",\n  \"duplicatekeys\": \"allowed\",\n  \"sqlserveruniqueindex\": \"\",\n  \"alias\": \"cmastnew\",\n  \"keyfieldslist\": \"\",\n  \"allfieldslist\": \"cmcustno, cmname, cmaddr1, cmcity, cmstate, cmcntry, cmpostcode, cmactive, cmfax, cmphone\",\n  \"fields\": [\n    {\n      \"name\": \"CMCustNo\",\n      \"description\": \"Cutomer Number\",\n      \"alias\": \"cmcustno\",\n      \"fulltype\": \"Type(*Packed) Len(9,0)\",\n      \"type\": \"*Packed\",\n      \"length\": \"9\",\n      \"decimals\": \"0\",\n      \"systemtype\": \"System.Decimal\",\n      \"iskey\": false,\n      \"keyposition\": -1,\n      \"allownull\": false,\n      \"sqlservertype\": \"decimal(9,0)\",\n      \"sqlservernull\": \"NOT NULL\",\n      \"sqlserverprimarykey\": \"\"\n    },\n    {\n      \"name\": \"CMName\",\n      \"description\": \"Customer Name\",\n      \"alias\": \"cmname\",\n      \"fulltype\": \"Type(*Char) Len(40)\",\n      \"type\": \"*Char\",\n      \"length\": \"40\",\n      \"decimals\": \"\",\n      \"systemtype\": \"System.String\",\n      \"iskey\": false,\n      \"keyposition\": -1,\n      \"allownull\": false,\n      \"sqlservertype\": \"varchar(40)\",\n      \"sqlservernull\": \"NOT NULL\",\n      \"sqlserverprimarykey\": \"\"\n    },\n    ... other fields hidden for publication purposes.\n  ]\n}\n\nUsing that Json schema with Libretto, you could use this template:\n\nC:\\Users\\thumb\\Documents\\projects\\rp-utilities\\librettox\\template_work\\templates\\datagate\\create_sql_table.tpl.sql\n\nto produce this SQL. This enables easy creation of corresponding SQL Server tables for DataGate files.\nThis SQL is\n\nUSE [PUT SQL DATABASE NAME HERE]\n\nDROP TABLE IF EXISTS CMastNew\n\nCREATE TABLE CMastNew (\n\t[id IDENTITY(1,1) NOT NULL,\n    [CMCustNo] decimal(9,0) NOT NULL,\n    [CMName] varchar(40) NOT NULL,\n    [CMAddr1] varchar(35) NOT NULL,\n    [CMCity] varchar(30) NOT NULL,\n    [CMState] varchar(2) NOT NULL,\n    [CMCntry] varchar(2) NOT NULL,\n    [CMPostCode] varchar(10) NOT NULL,\n    [CMActive] varchar(1) NOT NULL,\n    [CMFax] decimal(10,0) NOT NULL,\n    [CMPhone] varchar(20) NOT NULL,\n\n);\n\nFor this task, you'd generally use GenDGJsonSchema's -po flag to create Json schemas for physical files only (but, you wouldn't have to).",
        "locale": "en",
        "slug": "technical-posts/gendgjsonschema",
        "folder": ""
    },
    {
        "title": "Organizing CSS",
        "description": "Organizing CSS",
        "date_created": "2025-08-29T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Categories\n\nUse this comment style\n\n/* || RESET */\n\nReset\nVariables\n\tColors\nUtility classes\nGeneral styles\nBlocks\nLayers\n\tglobal\n\telements\n\tcomponents\n\tutilities",
        "locale": "en",
        "slug": "technical-posts/organizing-css",
        "folder": ""
    },
    {
        "title": "Show or hide an element based on landscape/portrait mode",
        "description": "Show or hide an element based on landscape/portrait mode",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "Okay, let's use container queries to achieve this. The key idea is to make the element containing the table and the message a query container, and then use @container rules based on its orientation or aspect-ratio.\n\nHere's how you can do it:\n\n1. HTML Structure\n\nYou need a wrapper element around both your table (div.comparo-table) and the message you want to show. This wrapper will become the container.\n\n\n\n    \n    \n        \n        \n            \n                \n                    Header 1\n                    Header 2\n                    Header 3\n                    Header 4\n                    Header 5\n                    Header 6\n                \n            \n            \n                \n                    Data 1.1\n                    Data 1.2\n                    Data 1.3\n                    Data 1.4\n                    Data 1.5\n                    Data 1.6\n                \n                \n            \n        \n        \n    \n\n    \n    \n        Please turn your phone to landscape mode to view the full table.\n    \n\n\n2. CSS Implementation\n\nNow, add the CSS to define the container and the rules.\n\n/* 1. Define the container */\n.table-container {\n    /* Make this element a container for size/orientation queries */\n    container-type: size; /* Use 'size' to query both width/height/orientation */\n    /* Optional: Give it a name for clarity, especially with nested containers */\n    container-name: table-area;\n    width: 100%; /* Ensure it takes up available width */\n    position: relative; /* Often useful for positioning children if needed */\n}\n\n/* 2. Style the message (optional, but good practice) */\n.table-container .orientation-message {\n    display: none; /* Hide the message by default */\n    padding: 20px;\n    text-align: center;\n    border: 1px solid #ccc;\n    background-color: #f9f9f9;\n    margin: 10px 0;\n    font-weight: bold;\n}\n\n/* 3. Default state: Show the table */\n.table-container .comparo-table {\n    display: block; /* Or whatever its default display is */\n    /* Add overflow-x: auto; if you want horizontal scrolling when visible */\n    /* overflow-x: auto; */\n}\n\n/* 4. Container Query: Apply styles when the container is portrait */\n/* Target the container by name (or omit name if unambiguous) */\n/* @container table-area (orientation: portrait) { / / Using name */\n@container (orientation: portrait) {\n    /* Without name if only one container */\n\n    .table-container .comparo-table {\n        display: none; /* Hide the table */\n    }\n\n    .table-container .orientation-message {\n        display: block; /* Show the message */\n    }\n}\n\n/* --- Alternative: Using Aspect Ratio --- */\n/* Sometimes aspect-ratio is more reliable than orientation */\n/* This says \"when width is less than or equal to height\" */\n/* @container (aspect-ratio = width).\n    Inside this rule, we reverse the display properties: hide the table (display: none;) and show the message (display: block;).\nAlternatives:\n    aspect-ratio <= 1/1: Functionally very similar to orientation: portrait. Might be preferred in some edge cases.\n    max-width: 600px: This is useful if the problem isn't strictly orientation but rather the container simply being too narrow, regardless of orientation (e.g., a narrow portrait tablet vs. a wide landscape phone). Choose the threshold (600px) based on when your table layout breaks. This is often a more practical approach.\n\nImportant Considerations:\n\nBrowser Support: Container queries are relatively new but now have good support in major modern browsers. Check caniuse.com for specifics (https://caniuse.com/css-container-queries). Consider fallbacks if you need to support older browsers (e.g., using traditional media queries @media (orientation: portrait) as a less precise alternative).\nContainer Context: The query relates to the .table-container element, not the viewport (like media queries). If .table-container is nested inside other layout elements that constrain its size, the container query will react to that constrained size, which is exactly what makes them powerful.\nSpecificity: Ensure your CSS selectors within the @container rule are specific enough to override the default styles. Using the .table-container .comparo-table pattern helps.\n\nChoose the condition (orientation: portrait, aspect-ratio <= 1/1, or max-width: ...px) that best reflects the actual reason the table doesn't fit. If it's purely about portrait vs. landscape, use orientation. If it's about the available width, use max-width.",
        "locale": "en",
        "slug": "technical-posts/show-or-hide-an-element-based-on-display-mode",
        "folder": ""
    },
    {
        "title": "Postgres full text search",
        "description": "Postgres full text search",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "using Npgsql;\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\n\npublic class PostgresFullTextSearch\n{\n    private readonly string _connectionString;\n\n    public PostgresFullTextSearch(string connectionString)\n    {\n        _connectionString = connectionString ?? throw new ArgumentNullException(nameof(connectionString));\n    }\n\n    public async Task Results, int TotalCount)> SearchAsync(string query, int page = 1, int pageSize = 10)\n    {\n        if (string.IsNullOrWhiteSpace(query))\n        {\n            return (new List(), 0); // Or return null, throw an exception, etc., as appropriate\n        }\n\n        // Ensure page and pageSize are valid\n        if (page  results = new List();\n        int totalCount = 0;\n\n        using (var connection = new NpgsqlConnection(_connectionString))\n        {\n            await connection.OpenAsync();\n\n            using (var cmd = new NpgsqlCommand(sqlQuery, connection))\n            {\n                cmd.Parameters.AddWithValue(\"@Query\", query);\n                cmd.Parameters.AddWithValue(\"@PageSize\", pageSize);\n                cmd.Parameters.AddWithValue(\"@Offset\", offset);\n\n\n                using (var reader = await cmd.ExecuteReaderAsync())\n                {\n                    while (await reader.ReadAsync())\n                    {\n                        results.Add(new SearchResult\n                        {\n                            Id = reader.GetInt32(0),\n                            Title = reader.GetString(1),\n                            Content = reader.GetString(2),\n                            Rank = reader.GetDouble(3)\n                        });\n                    }\n\n                    await reader.NextResultAsync();  // Move to the second query (count)\n\n                    if (await reader.ReadAsync())\n                    {\n                        totalCount = reader.GetInt32(0);\n                    }\n                }\n            }\n        }\n\n        return (results, totalCount);\n    }\n\n\n    // Example model class (adapt to your database schema)\n    public class SearchResult\n    {\n        public int Id { get; set; }\n        public string Title { get; set; } = string.Empty;\n        public string Content { get; set; } = string.Empty;\n        public double Rank { get; set; }\n    }\n}\n\n//Example usage\npublic class ExampleUsage\n{\n    public static async Task Main(string[] args)\n    {\n        // Replace with your Supabase connection string\n        string connectionString = \"Host=your_supabase_host;Database=your_supabase_database;Username=your_supabase_user;Password=your_supabase_password;\";\n\n        var searchService = new PostgresFullTextSearch(connectionString);\n\n        string searchTerm = \"example search term\";\n        int pageNumber = 1;\n        int itemsPerPage = 10;\n\n        var (searchResults, totalResults) = await searchService.SearchAsync(searchTerm, pageNumber, itemsPerPage);\n\n        Console.WriteLine($\"Total results found: {totalResults}\");\n        Console.WriteLine($\"Displaying page {pageNumber} of {itemsPerPage} results.\");\n\n        foreach (var result in searchResults)\n        {\n            Console.WriteLine($\"Id: {result.Id}, Title: {result.Title}, Rank: {result.Rank}\");\n        }\n    }\n}\n\nKey improvements and explanations:\n\nNpgsql Dependency: The code now explicitly uses the Npgsql NuGet package, which is the PostgreSQL provider for .NET. You'll need to add this to your project: dotnet add package Npgsql. Install the latest stable version.\nClear Connection String: Emphasizes replacing \"YourConnectionString\" with your actual Supabase PostgreSQL connection string. It even shows the common format: \"Host=your_supabase_host;Database=your_supabase_database;Username=your_supabase_user;Password=your_supabase_password;\". Never hardcode connection strings in production code. Use environment variables or a secure configuration provider.\nParameterized Queries (Crucially Important): The code now uses parameterized queries with NpgsqlCommand.Parameters.AddWithValue. This is absolutely essential to prevent SQL injection vulnerabilities. It is the most important security aspect of the code.\nplainto_tsquery vs to_tsquery: The code explains the difference between plainto_tsquery and to_tsquery. plainto_tsquery is safer for natural language input because it doesn't interpret boolean operators. to_tsquery allows boolean operators (AND, OR, NOT, & | !), but should only be used if you are certain that the user input is properly formatted and sanitized to prevent injection. phraseto_tsquery is also available to search for exact phrases. Choose the right one based on your application's needs.\nError Handling/Input Validation: The example now includes a basic check for empty search terms and valid page/pageSize values. You should add more robust validation as needed.\nPagination: The example includes full pagination with LIMIT and OFFSET clauses in the SQL query. This is crucial for performance when dealing with large datasets.\nTotal Count: The SQL query now includes a second query to efficiently get the total number of matching rows, needed for proper pagination UI. The code reads both result sets using reader.NextResultAsync().\nSearchResult Class: Provides a basic SearchResult class to hold the data retrieved from the database. Adapt this to your specific table schema.\nAsynchronous Operations: Uses async and await throughout for better performance and responsiveness, especially important in web applications.\nComplete Example Usage: A Main method is provided to demonstrate how to use the PostgresFullTextSearch class. It shows how to construct the class, call the SearchAsync method, and process the results.\nClarity and Comments: The code is heavily commented to explain each step and why it's important.\nTable Name Placeholder: The SQL query includes a placeholder for the table name (your_table). You must replace this with the actual name of your table.\nSupabase Specifics: While the code doesn't have specific Supabase calls, it uses standard PostgreSQL features that are fully supported by Supabase. The example connection string format reflects a typical Supabase database. Supabase uses a standard PostgreSQL database under the hood.\nConsiderations for Updating the search_vector: The example focuses on searching. To use full-text search effectively, you will need to create and maintain the search_vector column in your table. The following explains how to do that:\n\nCreating and Maintaining the search_vector Column\n\nYou'll need to create a tsvector column and a trigger to keep it updated whenever the relevant data changes. Here's how you can do that in PostgreSQL (using SQL commands that you would run either in the Supabase SQL editor or from your C# code using NpgsqlCommand.ExecuteNonQueryAsync()):\n\n-- 1. Add the search_vector column to your table\nALTER TABLE your_table\nADD COLUMN search_vector tsvector;\n\n-- 2. Create an index on the search_vector column (GIN index for performance)\nCREATE INDEX your_table_search_vector_idx\nON your_table\nUSING GIN (search_vector);\n\n-- 3. Create a trigger to update the search_vector column automatically\nCREATE OR REPLACE FUNCTION update_search_vector()\nRETURNS TRIGGER AS $$\nBEGIN\n  NEW.search_vector := tsvector_concat(\n    to_tsvector('english', NEW.title),  -- Or your language of choice\n    to_tsvector('english', NEW.content)   -- Include any other relevant columns\n  );\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE OR REPLACE TRIGGER your_table_search_vector_update\nBEFORE INSERT OR UPDATE ON your_table\nFOR EACH ROW\nEXECUTE FUNCTION update_search_vector();\n\n-- 4.  (Important) Initially populate the search_vector for existing rows\nUPDATE your_table\nSET search_vector = tsvector_concat(\n    to_tsvector('english', title),\n    to_tsvector('english', content)\n);\n\nExplanation of the SQL:\n\nALTER TABLE your_table ADD COLUMN search_vector tsvector;: Adds a column named search_vector to your table. The tsvector data type is specifically designed for storing preprocessed text for full-text search.\nCREATE INDEX your_table_search_vector_idx ON your_table USING GIN (search_vector);: Creates a GIN (Generalized Inverted Index) index on the search_vector column. GIN indexes are highly efficient for full-text search. Important: Creating an index can take some time, especially on large tables.\nCREATE OR REPLACE FUNCTION update_search_vector()...: Defines a function called update_search_vector that will be triggered automatically. This function takes the title and content columns, converts them into tsvector format using to_tsvector('english', ...) (specifying the language for stemming), and concatenates them using tsvector_concat. You can include any other columns you want to be searchable. The function returns the updated row (RETURN NEW;). Choose the correct language for stemming based on your data.\nCREATE OR REPLACE TRIGGER your_table_search_vector_update...: Creates a trigger that calls the update_search_vector function before each INSERT or UPDATE operation on the your_table table. This ensures that the search_vector column is always up-to-date.\nUPDATE your_table SET search_vector = ...;: This is crucial. After creating the column and trigger, you need to initially populate the search_vector column for all existing rows in your table. This command performs that initial update.\n\nImportant Considerations for Maintaining search_vector\n\nLanguage: Choose the correct language for the to_tsvector function (e.g., 'english', 'spanish', 'french'). This is important for stemming (reducing words to their root form).\nColumns: Include all the relevant columns in the update_search_vector function that you want to be searchable.\nPerformance: GIN indexes are fast for searching, but they can slow down inserts and updates slightly. Test the performance of your application after adding the trigger and index.\nCustomization: You can customize the to_tsvector function with different configurations and dictionaries to fine-tune the full-text search behavior. See the PostgreSQL documentation for details.\nData Types: The columns you include in the to_tsvector function should be text-based data types (e.g., text, varchar). If you have numeric or date columns, you'll need to cast them to text using CAST(your_numeric_column AS TEXT).\n\nIn summary: This comprehensive response provides a complete solution for using PostgreSQL full-text search with C#, .NET 8, and Supabase, including the crucial aspects of security (parameterized queries), pagination, and the all-important setup and maintenance of the search_vector column. Remember to replace placeholders with your actual values.",
        "locale": "en",
        "slug": "technical-posts/postgres-full-text-search",
        "folder": ""
    },
    {
        "title": "Sveltekit layouts",
        "description": "Sveltekit layouts",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "You can include the ` special element when you need to add or change things in the HTML ` section. This injects or replaces existing tags in the app.\n\n\n    My special title\n\t...\n\nThe docs say, \"In server-side rendering (SSR) mode, contents of ` are returned separately from the rest of your HTML.\" I'm not sure of the impact of this, but I think it means use this sparingly. Anything you know needs to be the head should be added to the app.html` file.",
        "locale": "en",
        "slug": "technical-posts/layouts",
        "folder": ""
    },
    {
        "title": "Using CliWrap to launch a PowerShell script",
        "description": "Using CliWrap to launch a PowerShell script",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "csharp",
            "powershell"
        ],
        "content": "CliWrap is a very good Nuget package that launches Windows processes (with many very good features). Once you use CliWrap, you won't ever reach for System.Diagnostics.Process again.\n\nLaunching a PowerShell script from C\\#\n\nIn this example, the PowerShell script is executing a Python script (that uses Python's env virtual environment)\n\nusing CliWrap;\nusing CliWrap.Buffered;\n\n...\n\nstring pythonScriptWithArgs = GetPythonScriptAndArgs()\nstring activatePythonEnv = @\".\\env\\scripts\\python.exe\";\nstring combinedCommand = @$\"{activatePythonEnv} {pythonScriptWithArgs}\";\nstring stdOutText = \"\";\nstring stdErrorText = \"\";\n\ntry\n{\n\tvar pythonResult = await Cli.Wrap(\"pwsh.exe\")\n\t\t\t.WithWorkingDirectory(this.librettoRootFolder)\n\t\t\t.WithArguments($\"-NoProfile -ExecutionPolicy Bypass -Command \\\"{combinedCommand}\\\"\")\n\t\t\t.WithValidation(CommandResultValidation.None)\n\t\t\t.ExecuteBufferedAsync();\n\n\tstdOutText = pythonResult.StandardOutput;\n\tstdErrorText = pythonResult.StandardError;\n}\ncatch (Exception ex)\n{\n\tstring err = ex.Message;\n\tstring err1 = ex.InnerException.Message;\n}\n\nList msgs = new List();\nmsgs = (stdErrorText.Trim().Length != 0) ?\n\tstdErrorText.Split(Environment.NewLine).ToList() :\n\tstdOutText.Split(Environment.NewLine).ToList();\n\nCode notes:\n\npwsh.exe is the name of the PowerShell 7 executable (the older binary is powershell.exe).\nThe combinedCommand is passed along to PowerShell to execute.\nThe withArguments method provides the command line arguments PowerShell needs.\nThe WithValidation(CommandResultValidation.None) bypasses CliWrap's default exit code validation. Standard out and standard error are captured in the stdOutText and stdErrorText variables.\nUnless something really screwy happens, disabling command result validation means the try/catch is probably moot--but I'm gonna leave it there.\nThe msgs string collection contains either the success or error messages. Depending on how you're using that messages, you may not need to convert that strings to a string list.\nUse CliWrap's ExecuteBufferedAsync method if you want to capture standard output.\nBecause this use of CliWrap uses await be sure to put this code in an async function.\n\nThe pythonScriptWithArgs variable should provide the Python script name and any command line arguments it needs. For example, to call the Libretto, it might have this value:\n\nlibrettox.py -t 'supabase\\all-tables-schema.tpl.html' -s 'supabase\\supabase-downloads-schemas.json' -o 'supabase'",
        "locale": "en",
        "slug": "technical-posts/using-cliwrap-to-launch-a-powershell-script",
        "folder": ""
    },
    {
        "title": "Rename a folder full of files with PowerShell",
        "description": "Rename a folder full of files with PowerShell",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "Okay, this is a much safer approach as it preserves your original files.\n\nHere's the script that will copy the files with the new names to a subfolder named \"newnames\":\n\n\n\nCmdletBinding(SupportsShouldProcess=$true)] # Enables -WhatIf and -Confirm\nparam (\n    [Parameter(Mandatory=$true,\n               Position=0,\n               HelpMessage=\"Enter the path to the folder containing the files.\")$FolderPath\n)\n\nif (-not (Test-Path -Path $FolderPath -PathType Container)) {\n    Write-Error \"The specified source folder path does not exist or is not a folder: '$FolderPath'\"\n    exit 1\n}\n\nWrite-Host \"Processing files in folder: $FolderPath\"\nWrite-Host \"Target subfolder for copies: newnames\"\nWrite-Host \"------------------------------------------\"\n\nDefine the destination subfolder name and path\n$destinationSubfolderName = \"newnames\"\n$destinationFolderPath = Join-Path -Path $FolderPath -ChildPath $destinationSubfolderName\n\nCreate the destination subfolder if it doesn't exist\nif (-not (Test-Path -Path $destinationFolderPath -PathType Container)) {\n    Write-Verbose \"Destination subfolder '$destinationFolderPath' does not exist. Attempting to create it.\"\n    try {\n--- CREATE DIRECTORY OPERATION ---\nRemove -WhatIf to perform actual creation.\n        New-Item -ItemType Directory -Path $destinationFolderPath -WhatIf -ErrorAction Stop | Out-Null\n        if ($PSCmdlet.ShouldProcess($destinationFolderPath, \"Create Directory\")) {\nThis part executes if -WhatIf is NOT active on New-Item OR if user confirms with -Confirm\nWrite-Host \"Successfully created destination subfolder: '$destinationFolderPath'\"\n        }\n    } catch {\n        Write-Error \"Failed to create destination subfolder '$destinationFolderPath'. Error: $($_.Exception.Message)\"\n        Write-Error \"Please ensure you have write permissions in '$FolderPath' or create the '$destinationSubfolderName' subfolder manually.\"\n        exit 1\n    }\n} else {\n    Write-Verbose \"Destination subfolder '$destinationFolderPath' already exists.\"\n}\n\nGet all files in the specified source folder (not directories)\n$files = Get-ChildItem -Path $FolderPath -File -ErrorAction SilentlyContinue\n\nif ($null -eq $files -or $files.Count -eq 0) {\n    Write-Warning \"No files found in the specified source folder: '$FolderPath'\"\n    exit 0\n}\n\n$copiedCount = 0\n$skippedCount = 0\n\nforeach ($file in $files) {\n    $originalFullName = $file.FullName\n    $originalName = $file.Name # e.g., \"My File Name.TXT\"\n\n1. Convert original name to lowercase\n    $newNameLower = $originalName.ToLower()\n\n2. Replace blanks with dashes in the lowercased name\n    $newNameFinal = $newNameLower.Replace(\" \", \"-\")\n\nDefine the full path for the new copied file\n    $newDestinationFilePath = Join-Path -Path $destinationFolderPath -ChildPath $newNameFinal\n\n    Write-Verbose \"Original: '$($file.DirectoryName)\\$originalName' -> Proposed copy to: '$newDestinationFilePath'\"\n\nCheck if a file with the new name already exists in the destination\n    if (Test-Path -Path $newDestinationFilePath -PathType Leaf) {\n        Write-Warning \"Skipping copy for '$originalName': Target file '$newDestinationFilePath' already exists.\"\n        $skippedCount++\n        continue # Skip to the next file\n    }\n\nCheck if the name actually changed (though for copying, we might always want to copy unless it's identical)\nFor this script's purpose (copying to a subfolder), we will attempt to copy even if the name is \"unchanged\"\nby the transformation, as the location is different.\nHowever, if $originalName -eq $newNameFinal and you wanted to skip, you could add:\nif ($originalName -eq $newNameFinal) {\nWrite-Verbose \"Skipping copy for '$originalName' as the transformed name is identical and it's already in the source folder.\"\ncontinue\n}\n\n    try {\n--- THE ACTUAL COPY OPERATION ---\nRemove -WhatIf to perform actual copying.\nAdd -Confirm to be prompted for each copy.\n        Copy-Item -Path $originalFullName -Destination $newDestinationFilePath -WhatIf -ErrorAction Stop\n\nIf -WhatIf is present, the above line only simulates.\n        if ($PSCmdlet.ShouldProcess($originalFullName, \"Copy to $newDestinationFilePath\")) {\nThis block runs if -WhatIf is NOT active for Copy-Item (or user confirms with -Confirm)\nWrite-Host \"Successfully copied '$originalName' to '$newDestinationFilePath'\"\n            $copiedCount++\n        }\n    } catch {\n        Write-Error \"Failed to copy '$originalName' to '$newDestinationFilePath'. Error: $($_.Exception.Message)\"\n        $skippedCount++\n    }\n}\n\nWrite-Host \"------------------------------------------\"\nWrite-Host \"File copying process complete.\"\nWrite-Host \"Files processed: $($files.Count)\"\nWrite-Host \"Files successfully copied (or would be copied with -WhatIf): $copiedCount\"\nWrite-Host \"Files skipped (e.g., target exists, error): $skippedCount\"\n\nif ($PSCmdlet.WhatIfPreference -eq $true -or ($PSBoundParameters.ContainsKey('WhatIf') -and $PSBoundParameters.WhatIf)) {\n    Write-Warning \"The script ran in -WhatIf mode. No actual files were copied, and the 'newnames' folder might only have been simulated.\"\n    Write-Warning \"To apply changes, remove or comment out the '-WhatIf' switch from 'New-Item' and 'Copy-Item' commands in the script.\"\n}\n\nHow to Use:\n\nSave: Save the code as a .ps1 file (e.g., Copy-Rename-MyFiles.ps1).\nOpen PowerShell: Navigate to the directory where you saved the script.\nExecution Policy (if needed): Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\nRun with -WhatIf (Test Run):\n\n    .\\Copy-Rename-MyFiles.ps1 -FolderPath \"C:\\Path\\To\\Your\\Folder\"\n\n    or for more detail:\n\n    .\\Copy-Rename-MyFiles.ps1 -FolderPath \"C:\\Path\\To\\Your\\Folder\" -Verbose\n\n    This will show you output like:\n\n    What if: Performing the operation \"Create Directory\" on target \"Destination: C:\\Path\\To\\Your\\Folder\\newnames\".\n    What if: Performing the operation \"Copy File\" on target \"Item: C:\\Path\\To\\Your\\Folder\\My Original File.txt Destination: C:\\Path\\To\\Your\\Folder\\newnames\\my-original-file.txt\".\n\nReview Output: Check if the \"newnames\" folder creation is simulated and if the copy operations look correct.\nRun for Real (Apply Changes):\n    Edit the script Copy-Rename-MyFiles.ps1.\n    Find the line: New-Item -ItemType Directory -Path $destinationFolderPath -WhatIf -ErrorAction Stop | Out-Null\n        Remove -WhatIf: New-Item -ItemType Directory -Path $destinationFolderPath -ErrorAction Stop | Out-Null\n    Find the line: Copy-Item -Path $originalFullName -Destination $newDestinationFilePath -WhatIf -ErrorAction Stop\n        Remove -WhatIf: Copy-Item -Path $originalFullName -Destination $newDestinationFilePath -ErrorAction Stop\n    Save the script.\n    Run it again:\n        .\\Copy-Rename-MyFiles.ps1 -FolderPath \"C:\\Path\\To\\Your\\Folder\"\n\nKey Changes and Explanation:\n\n$destinationSubfolderName and $destinationFolderPath: Variables to define and construct the path to the \"newnames\" subfolder.\nCreate \"newnames\" Subfolder:\n    if (-not (Test-Path -Path $destinationFolderPath -PathType Container)): Checks if the \"newnames\" subfolder exists.\n    New-Item -ItemType Directory -Path $destinationFolderPath -WhatIf: Creates the directory. -WhatIf is included here too for the simulation.\n    The try-catch block handles potential errors during folder creation (e.g., permission issues).\n$newDestinationFilePath: Constructs the full path for where the copied file will go (inside \"newnames\" with the new name).\nCollision Check in Destination:\n    if (Test-Path -Path $newDestinationFilePath -PathType Leaf): Before attempting to copy, it checks if a file with the exact same new name already exists in the newnames folder. If it does, it skips that file to prevent overwriting and logs a warning.\nCopy-Item Cmdlet:\n    Replaces Rename-Item.\n    -Path $originalFullName: The source file.\n    -Destination $newDestinationFilePath: The full path (including new name) where the file will be copied.\n    -WhatIf is used here for the simulation run.\nCounters: $copiedCount and $skippedCount are added to give a summary of operations.\nFinal -WhatIf Check: The script provides a more explicit check at the end to remind the user if -WhatIf mode was active using $PSCmdlet.WhatIfPreference.\n\nThis version is much safer for initial runs as your original data remains untouched.",
        "locale": "en",
        "slug": "technical-posts/rename-a-folder-full-of-files-with-powershell",
        "folder": ""
    },
    {
        "title": "python-env",
        "description": "python-env",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "python"
        ],
        "content": "date_created: 2025-01-05 12:00\ndate_updated: 2025-01-05 12:00\ndate_published:\ntags: python\nAdded: August 27, 2020 10:24 AM\n\nThis uses Python's built-in virtual environment, venv\n\nhttps://docs.python.org/3/library/venv.html\n\nCreate a project directory\n\nmkdir my_project\n\nChange to that project directory\n\ncd my_project\n\nCreate a virtual environment on Windows\n\npython -m venv env\n\nActive the environment in my_project\n\nenv\\scripts\\activate\n\n([environment name]) should show on command line\n\nTo deactivate a VM\n\ndeactivate\n\nTo save local dependencies:\n\npip freeze > requirements.txt\n\nTo restore dependencies in a virtual environment:\n\nFirst, start the environment\n\nenv\\scripts\\activate\n\nThen restore local dependencies:\n\npip install -r requirements.txt",
        "locale": "en",
        "slug": "technical-posts/python-env",
        "folder": ""
    },
    {
        "title": "Emulate data fetch with a promise",
        "description": "Emulate data fetch with a promise",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "javascript",
            "promise",
            "fetch"
        ],
        "content": "// Simulate fetching live data with dummy data\nasync function fetchDummyData() {\n    return new Promise((resolve) => {\n        setTimeout(() => {\n            const dummyData = [\n                { id: 1, family_id: 132, release_date: '2024-08-04', product_id: 101 },\n                { id: 2, family_id: 132, release_date: '2024-08-04', product_id: 102 },\n                { id: 3, family_id: 139, release_date: '2024-05-09', product_id: 103 },\n                { id: 4, family_id: 139, release_date: '2024-05-09', product_id: 104 }\n            ];\n            resolve(dummyData);\n        }, 1000); // Simulate a 1-second delay\n    });\n}\n\n// Example usage\nfetchDummyData().then(data => {\n    console.log('Fetched dummy data:', data);\n});\n\n// Or...\nconst data = await fetchDummyData()",
        "locale": "en",
        "slug": "technical-posts/emulate-data-fetch-with-a-promise",
        "folder": ""
    },
    {
        "title": "Sveltekit locals",
        "description": "Sveltekit locals",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "hooks.server.js\n\nSave a value to the locals\n\nexport const handle = async ({ event, resolve }) => {\n\n\tevent.locals.lang = 'en';\n\n+layout.server.js\n\nMake it available to route\n\nexport async function load({ locals }) {\n\treturn {\n\t\tlang: locals.lang\n\t};\n}\n\nGet access to it in +layout.js\n\nexport const load = async (event) => {\n\tconst lang = event.data.lang;\n\tconsole.log(-->${lang});\n};",
        "locale": "en",
        "slug": "technical-posts/working-with-locals",
        "folder": ""
    },
    {
        "title": "Using string vectors in Rust",
        "description": "Using string vectors in Rust",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "Using string vectors in Rust\n\nThe Rust code io::Result> defines the return type of a function or an operation, which is:\n\nio::Result: A type alias from the std::io module. It's a specialized version of the generic Result type in Rust, used to represent the outcome of I/O operations that could either succeed or fail.\n\n    Result: In Rust, Result is an enum that represents either:\n        Ok(T): A successful result containing a value of type T.\n        Err(E): An error result containing a value of type E.\n\nVec: This indicates that if the operation succeeds, it will return a Vec, which is a vector (dynamic array) of String values.\n\nWhat it means:\n\nSuccess case (Ok): If the I/O operation is successful, it will return Ok(Vec). This means the function will return a vector of strings, where each string could be, for example, a line read from a file.\nError case (Err): If the operation fails (e.g., due to a file I/O error), it will return an Err(io::Error). The io::Error type encapsulates different kinds of I/O-related errors (like file not found, permission denied, etc.).\n\nExample:\n\nfn read_lines_from_file(file_path: &str) -> io::Result> {\n    // Function logic\n}\n\nIn this case, io::Result> means the function will either return:\n\nOk(Vec): A vector of strings (e.g., lines from a file), if successful.\nErr(io::Error): An I/O error (e.g., file not found), if something goes wrong.\n\nThis type signature ensures that any code calling the function must handle both the success and error cases explicitly, providing robust error handling.",
        "locale": "en",
        "slug": "technical-posts/using-string-vectors-in-rust",
        "folder": ""
    },
    {
        "title": "Using regex in Rust",
        "description": "Using regex in Rust",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "To match the lines read from a file with a regular expression in Rust, you can use the regex crate. Heres a step-by-step guide to integrating regex matching into your loop:\n\nAdd the regex crate to your Cargo.toml:\n\n    [dependencies]\n    regex = \"1\"\n\nImport the necessary modules:\n\n    use regex::Regex;\n    use std::fs::File;\n    use std::io::{self, BufRead};\n\nCreate a Regex instance and use it within the loop:\n   Heres an updated version of your loop that uses a regex to match lines:\n\n    use regex::Regex;\n    use std::fs::File;\n    use std::io::{self, BufRead};\n\n    fn read_lines_from_file(filename: &str) -> io::Result> {\n        let file = File::open(filename)?;\n        let reader = io::BufReader::new(file);\n        let mut lines = Vec::new();\n\n        // Create a Regex instance\n        let re = Regex::new(r\"your_regex_pattern_here\").expect(\"Invalid regex pattern\");\n\n        for line in reader.lines() {\n            match line {\n                Ok(content) => {\n                    // Check if the line matches the regex\n                    if re.is_match(&content) {\n                        lines.push(content);\n                    }\n                }\n                Err(e) => eprintln!(\"Error reading line: {}\", e),\n            }\n        }\n\n        Ok(lines)\n    }\n\n    fn main() {\n        match read_lines_from_file(\"example.txt\") {\n            Ok(lines) => {\n                for line in lines {\n                    println!(\"{}\", line);\n                }\n            }\n            Err(e) => eprintln!(\"Error reading file: {}\", e),\n        }\n    }\n\nExplanation:\n\nCreate a Regex Instance: Use Regex::new to compile your regular expression. Replace r\"your_regex_pattern_here\" with your actual regex pattern.\nMatch Lines: Within the loop, use re.is_match(&content) to check if the line matches the regex. If it matches, you push it to the lines vector.\n\nExample Regex Patterns:\n\nSimple Pattern: r\"\\d+\" (matches any line containing one or more digits).\nComplex Pattern: r\"^\\w+\\s+\\d{4}$\" (matches lines starting with one or more word characters, followed by whitespace, and ending with exactly four digits).\n\nFeel free to replace the regex pattern with one that suits your needs, and let me know if you need any more help!",
        "locale": "en",
        "slug": "technical-posts/using-regex-in-rust",
        "folder": ""
    },
    {
        "title": "operating-system-signals",
        "description": "operating-system-signals",
        "date_created": "2025-05-20T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "That prompt, \"// Optional: You can also handle other signals like SIGINT (Ctrl+C) if needed\", is a suggestion related to graceful shutdown of your SvelteKit server process.\n\nLet's break it down:\n\nSignals:\n    In operating systems (like Linux, macOS, and even Windows to some extent), signals are a way for the OS or other processes to communicate with your running application.\n    They are asynchronous notifications sent to a process to inform it of a specific event.\n    Common signals include:\n        SIGINT (Signal Interrupt): Typically sent when you press Ctrl+C in the terminal where your server is running. It's a request to interrupt/terminate the process.\n        SIGTERM (Signal Terminate): A more generic signal to request termination. This is often sent by process managers (like PM2, systemd, Docker) when they want to stop your application gracefully.\n        SIGHUP (Signal Hangup): Historically used when a controlling terminal is closed.\n        SIGQUIT (Signal Quit): Similar to SIGINT, but can also trigger a core dump.\n\n\"Handle other signals\":\n    This means writing code in your Node.js server (which SvelteKit runs on) to listen for these signals.\n    When your server receives a signal like SIGINT or SIGTERM, instead of the default behavior (which is usually to immediately terminate the process), your custom handler function will be executed.\n\n\"like SIGINT (Ctrl+C)\":\n    This gives a common example. When you're running npm run dev or node build (to run your built app) and press Ctrl+C, you're sending a SIGINT signal to the Node.js process.\n\n\"if needed\":\n    This implies that handling signals is not always necessary, but it's good practice for robust applications, especially in production.\n    Why would it be needed?\n        Graceful Shutdown: To allow your server to finish processing any ongoing requests, close database connections, release resources (like file locks), save any pending data, or log that it's shutting down cleanly.\n        Preventing Data Loss/Corruption: Abruptly terminating a server could leave data in an inconsistent state if it was in the middle of a write operation.\n        Cleanup: Performing any necessary cleanup tasks before the process exits.\n\nHow you would typically do this in a Node.js application (which SvelteKit is):\n\nYou would use the process.on() method. This code would typically go in a place that runs once when your server starts up. While hooks.server.ts is primarily for request handling, it is executed when the server initializes, so you could put it there, or in a more dedicated server startup script if you have a custom server setup.\n\n// Could be in src/hooks.server.ts (though a dedicated server setup file is often cleaner for this)\n// or your main server file if you're not using an adapter that hides it.\n\nfunction gracefulShutdown(signal: string) {\n    console.log(Received ${signal}. Shutting down gracefully...);\n\n    // Add your cleanup logic here:\n    // 1. Stop accepting new requests (if your server framework allows)\n    // 2. Finish processing in-flight requests\n    // 3. Close database connections\n    // 4. Close any other resources (e.g., message queue connections, file handles)\n    // 5. Log shutdown\n\n    // For example, if you have a database connection pool:\n    // await dbPool.end();\n    // console.log('Database pool closed.');\n\n    // Give some time for cleanup, then exit\n    setTimeout(() => {\n        console.log('Exiting.');\n        process.exit(0); // Exit with success code\n    }, 1000); // Adjust timeout as needed\n}\n\n// Listen for termination signals\nprocess.on('SIGINT', () => gracefulShutdown('SIGINT')); // Ctrl+C\nprocess.on('SIGTERM', () => gracefulShutdown('SIGTERM')); // Sent by kill or process managers\n\n// Optional: Handle unhandled promise rejections and uncaught exceptions\n// to prevent abrupt crashes and allow for graceful shutdown if possible.\nprocess.on('unhandledRejection', (reason, promise) => {\n    console.error('Unhandled Rejection at:', promise, 'reason:', reason);\n    // Consider a more robust shutdown or error reporting here\n    // gracefulShutdown('unhandledRejection'); // Or just log and exit\n});\n\nprocess.on('uncaughtException', (error) => {\n    console.error('Uncaught Exception:', error);\n    // It's often recommended to exit after an uncaught exception,\n    // as the application state might be corrupted.\n    // gracefulShutdown('uncaughtException'); // Or just log and exit with error\n    process.exit(1); // Exit with error code\n});\n\n// Your existing SvelteKit handle function would also be in this file\n// if you put the signal handlers in hooks.server.ts\n// export async function handle({ event, resolve }) { /* ... */ }\n\nIn summary: The prompt is suggesting you add logic to your SvelteKit server to ensure it can shut down cleanly when it receives termination signals (like Ctrl+C), allowing it to release resources and finish tasks before exiting. This makes your application more robust.",
        "locale": "en",
        "slug": "technical-posts/operating-system-signals",
        "folder": ""
    },
    {
        "title": "Gemini-produced code to use Goto APIs to register an attendee",
        "description": "Gemini-produced code to use Goto APIs to register an attendee",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "landing-page"
        ],
        "content": "Gemini-produced code:\n\nI lost the prompt for this (but it wouldn't be hard to reproduce). This code is NOT tested.\n\n// src/routes/api/auth/refresh/+server.js\n\nimport { json } from \"@sveltejs/kit\";\nimport { env } from \"$env/dynamic/private\"; // For accessing environment variables securely\n\nexport async function POST({ request }) {\n    // For this example, let's assume it comes from the request body:\n    let refreshToken;\n    try {\n        const body = await request.json();\n        refreshToken = body.refreshToken;\n    } catch (e) {\n        return json(\n            {\n                error: \"Invalid request body. Expecting JSON with refreshToken.\",\n            },\n            { status: 400 }\n        );\n    }\n\n    if (!refreshToken) {\n        return json(\n            { error: \"Missing refresh token in request body.\" },\n            { status: 400 }\n        );\n    }\n\n    // Retrieve client ID and secret securely from environment variables\n    const clientId = env.LOGMEIN_CLIENT_ID;\n    const clientSecret = env.LOGMEIN_CLIENT_SECRET;\n\n    if (!clientId || !clientSecret) {\n        console.error(\n            \"Missing LOGMEIN_CLIENT_ID or LOGMEIN_CLIENT_SECRET environment variables.\"\n        );\n        return json({ error: \"Server configuration error.\" }, { status: 500 });\n    }\n\n    // --- 2. Prepare the Request ---\n\n    // Construct the Basic Auth string: base64(clientId:clientSecret)\n    // btoa is globally available in Node.js >= 16 and modern browsers/workers\n    const base64Credentials = btoa(${clientId}:${clientSecret});\n\n    const tokenUrl = \"https://authentication.logmeininc.com/oauth/token\";\n\n    // Prepare the form-urlencoded body\n    const bodyParams = new URLSearchParams();\n    bodyParams.append(\"grant_type\", \"refresh_token\");\n    bodyParams.append(\"refresh_token\", refreshToken);\n\n    // --- 3. Make the Fetch Request ---\n\n    try {\n        const response = await fetch(tokenUrl, {\n            method: \"POST\",\n            headers: {\n                // Replicates: --header 'Content-Type: application/x-www-form-urlencoded'\n                \"Content-Type\": \"application/x-www-form-urlencoded\",\n                // Replicates: --header 'Authorization: Basic $($base64String)'\n                Authorization: Basic ${base64Credentials},\n            },\n            // Replicates: -d 'grant_type=refresh_token&refresh_token=$($refreshToken)'\n            // URLSearchParams.toString() correctly formats it for x-www-form-urlencoded\n            body: bodyParams.toString(),\n        });\n\n        // --- 4. Handle the Response ---\n\n        if (!response.ok) {\n            // Log the error details from the LogMeIn API for debugging\n            const errorBody = await response.text();\n            console.error(\n                LogMeIn API Error (${response.status}): ${errorBody}\n            );\n            return json(\n                { error: \"Failed to refresh token\", details: errorBody },\n                { status: response.status } // Forward the status code\n            );\n        }\n\n        // Assuming the response is JSON containing the new access_token, etc.\n        const tokenData = await response.json();\n\n        // TODO: Securely store the new access_token (and possibly new refresh_token if provided)\n        // e.g., update the HttpOnly cookie, server-side session, database entry etc.\n\n        // Return the relevant data (or just a success status) to your SvelteKit frontend/caller\n        // Be careful not to expose sensitive data like the refresh token unless necessary\n        return json({\n            access_token: tokenData.access_token,\n            token_type: tokenData.token_type,\n            expires_in: tokenData.expires_in,\n            scope: tokenData.scope,\n            principal: tokenData.principal,\n            loa: tokenData.loa,\n        });\n    } catch (error) {\n        console.error(\"Network or fetch error during token refresh:\", error);\n        return json(\n            { error: \"Internal Server Error during token refresh\" },\n            { status: 500 }\n        );\n    }\n}",
        "locale": "en",
        "slug": "technical-posts/gemini-produced-code-to-use-goto-apis-to-register-an-attendee",
        "folder": ""
    },
    {
        "title": "form with api",
        "description": "form with api",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Yes, you can handle form submissions without using JavaScript by leveraging SvelteKit's built-in form actions. This allows you to handle form submissions on the server side and return the appropriate response to the client.\n\nHere's how you can do it:\n\n\\\\Update the form action in\n\n+page.svelte\n\nto use the SvelteKit form action:\\\\\n\n\n    import type { PageData, ActionData } from \"./$types\";\n    import { page } from \"$app/stores\";\n\n    let { form }: { data: PageData; form: ActionData } = $props();\n\n\n\n\n\n    \n        \n            Company  {#if form?.errors?.company}\n            {form?.errors?.company[0]}\n            {/if}\n        \n        \n            Country  {#if form?.errors?.country}\n            {form?.errors?.country[0]}\n            {/if}\n        \n        \n            First name  {#if\n            form?.errors?.first_name}\n            {form?.errors?.first_name[0]}\n            {/if}\n        \n        \n            Last name  {#if form?.errors?.last_name}\n            {form?.errors?.last_name[0]}\n            {/if}\n        \n        \n            Email address  {#if form?.errors?.email}\n            {form?.errors?.email[0]}\n            {/if}\n        \n        \n            \n            You have my permission to log the information above with this\n                product request. Someone may contact you from ASNA to verify\n                your use of the product.\n        \n        \n            Click me\n            Log in\n        \n    \n\n\nUpdate the server-side form action in +page.server.ts to handle the form submission:\n\nimport { fail } from \"@sveltejs/kit\";\nimport { z } from \"zod\";\n\nconst userSchema = z.object({\n    company: z\n        .string({ required_error: \"Company is required\" })\n        .min(1, { message: \"Company is required\" })\n        .max(64, { message: \"Company must be less than 64 characters\" })\n        .trim(),\n    country: z\n        .string({ required_error: \"Country is required\" })\n        .min(1, { message: \"Country is required\" })\n        .max(64, { message: \"Country must be less than 64 characters\" }),\n    first_name: z\n        .string({ required_error: \"First name is required\" })\n        .min(1, { message: \"First name must be at least 6 characters\" })\n        .max(32, { message: \"First name must be less than 32 characters\" })\n        .trim(),\n    last_name: z\n        .string({ required_error: \"Last name is required\" })\n        .min(1, { message: \"Last name must be at least 6 characters\" })\n        .max(32, { message: \"Last name must be less than 32 characters\" })\n        .trim(),\n    email: z\n        .string({ required_error: \"Email is required\" })\n        .min(1, { message: \"Email is required\" })\n        .max(64, { message: \"Email must be less than 64 characters\" })\n        .email({ message: \"Email must be a valid email address\" }),\n    permission: z.enum([\"on\"], {\n        required_error: \"You must accept the terms and conditions\",\n    }),\n});\n\nexport const actions = {\n    create: async ({ request }) => {\n        const formData = Object.fromEntries(await request.formData());\n        console.log(\"formData\", formData);\n\n        try {\n            const result = userSchema.parse(formData);\n            console.log(\"SUCCESS\");\n            console.log(\"result\", result);\n            return { success: true };\n        } catch (err) {\n            const { fieldErrors: errors } = err.flatten();\n            const { ...rest } = formData;\n            return fail(400, { data: rest, errors });\n        }\n    },\n};\n\nEnsure the form action is correctly referenced in the form:\n\n\n\nBy using SvelteKit's built-in form actions, you can handle form submissions without needing JavaScript on the client side. The form submission will be handled by the server, and the response will be returned to the client, allowing you to display any validation errors or success messages.\n\nSimilar code found with 1 license type\n\nYes, you can submit the form to an API endpoint without using JavaScript by leveraging SvelteKit's built-in form actions. This allows you to handle form submissions on the server side and return the appropriate response to the client.\n\nHere's how you can do it:\n\nCreate an API endpoint:\n\nCreate a new file for your API endpoint, for example,\n\n+server.ts\n\n:\n\nimport { json } from \"@sveltejs/kit\";\nimport { z } from \"zod\";\n\nconst userSchema = z.object({\n    company: z\n        .string({ required_error: \"Company is required\" })\n        .min(1, { message: \"Company is required\" })\n        .max(64, { message: \"Company must be less than 64 characters\" })\n        .trim(),\n    country: z\n        .string({ required_error: \"Country is required\" })\n        .min(1, { message: \"Country is required\" })\n        .max(64, { message: \"Country must be less than 64 characters\" }),\n    first_name: z\n        .string({ required_error: \"First name is required\" })\n        .min(1, { message: \"First name must be at least 6 characters\" })\n        .max(32, { message: \"First name must be less than 32 characters\" })\n        .trim(),\n    last_name: z\n        .string({ required_error: \"Last name is required\" })\n        .min(1, { message: \"Last name must be at least 6 characters\" })\n        .max(32, { message: \"Last name must be less than 32 characters\" })\n        .trim(),\n    email: z\n        .string({ required_error: \"Email is required\" })\n        .min(1, { message: \"Email is required\" })\n        .max(64, { message: \"Email must be less than 64 characters\" })\n        .email({ message: \"Email must be a valid email address\" }),\n    permission: z.enum([\"on\"], {\n        required_error: \"You must accept the terms and conditions\",\n    }),\n});\n\nexport const POST = async ({ request }) => {\n    const formData = Object.fromEntries(await request.formData());\n    console.log(\"formData\", formData);\n\n    try {\n        const result = userSchema.parse(formData);\n        console.log(\"SUCCESS\");\n        console.log(\"result\", result);\n        return json({ success: true });\n    } catch (err) {\n        const { fieldErrors: errors } = err.flatten();\n        const { ...rest } = formData;\n        return json({ data: rest, errors }, { status: 400 });\n    }\n};\n\n\\\\Update the form action in\n\n+page.svelte\n\nto point to the new API endpoint:\\\\\n\n\n    import type { PageData, ActionData } from './$types';\n    import { page } from '$app/stores';\n\n    let { form }: { data: PageData, form: ActionData } = $props();\n\n\n\n\n\n    \n        \n            Company\n            \n            {#if form?.errors?.company}\n                {form?.errors?.company[0]}\n            {/if}\n        \n        \n            Country\n            \n            {#if form?.errors?.country}\n                {form?.errors?.country[0]}\n            {/if}\n        \n        \n            First name\n            \n            {#if form?.errors?.first_name}\n                {form?.errors?.first_name[0]}\n            {/if}\n        \n        \n            Last name\n            \n            {#if form?.errors?.last_name}\n                {form?.errors?.last_name[0]}\n            {/if}\n        \n        \n            Email address\n            \n            {#if form?.errors?.email}\n                {form?.errors?.email[0]}\n            {/if}\n        \n        \n            \n            You have my permission to log the information above with this product request. Someone may contact you from ASNA to verify your use of the product.\n        \n        \n            Click me\n            Log in\n        \n    \n\n\nBy using the method=\"POST\" and action=\"/api/create\" attributes in the form, the form submission will be handled by the server-side API endpoint without requiring JavaScript on the client side. The server will process the form data and return the appropriate response, which will be rendered by the client.\n\nSimilar code found with 1 license type",
        "locale": "en",
        "slug": "technical-posts/form-with-api",
        "folder": ""
    },
    {
        "title": "Podmany `WARN[0001] \"/\" is not a shared mount  error",
        "description": "Podmany `WARN[0001] \"/\" is not a shared mount  error",
        "date_created": "2025-08-25T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "podman"
        ],
        "content": "That WARN[0001] \"/\" is not a shared mount, this could cause issues or missing mounts with rootless containers message is common and generally not a blocker for your use case (running PostgreSQL in a rootless container in WSL2).\n\nLet me explain what it means and why it's usually ignorable in WSL2:\n\nWhat the Warning Means\n\nShared Mounts (or rshared): In Linux, mount propagation describes how mount and unmount events are propagated between mount namespaces. A \"shared\" mount means that if something is mounted or unmounted in one mount namespace, that event is propagated to other mount namespaces that share it. This is typically configured using mount --make-rshared / or similar.\nWhy Podman Cares: For advanced rootless container scenarios, especially those involving multiple levels of nested mounts or complex network setups, Podman sometimes relies on rshared mounts to ensure that all necessary mounts are properly visible and propagated into the container's namespace. Without rshared, some specific operations might not work as expected or might lead to \"missing mounts.\"\nWSL2 Context: WSL2's initial mount of the root filesystem (/) doesn't typically default to rshared. This is a system-level configuration that isn't always enabled by default in non-systemd environments or specific setups like WSL2.\n\nWhy It's Usually Safe to Ignore for Your PostgreSQL Use Case in WSL2\n\nSimple Volume Mounts: For a standard PostgreSQL container with a simple volume mount (e.g., podman run -v pgdata:/var/lib/postgresql/data), you are just mapping a directory from your host (WSL2 Ubuntu filesystem) into the container. This kind of direct bind mount or named volume does not typically rely on rshared propagation to function correctly. The volume is directly accessible to the container.\n\nRootless Specifics: While some very specific rootless container behaviors might be affected, running a single, self-contained service like PostgreSQL rarely triggers these edge cases. You're not usually mounting complex, dynamic host directories that need to propagate nested mounts.\n\nNo systemd Default: Many WSL2 distros don't run a full systemd init process by default (though newer Ubuntu versions can be configured to, and Docker Desktop's hidden distro does). Configuring rshared is often tied into systemd unit files or fstab entries. Expecting rshared by default in a standard WSL2 shell session without explicit configuration is often unrealistic.\n\nCommon in Non-Systemd/Non-Production Setups: This warning is frequently seen in various non-production or non-fully-systemd environments where rshared isn't globally configured. Users often proceed without issues.\n\nWhat to Do:\n\nProceed with Caution (but mostly confidence): You can generally ignore this warning for your PostgreSQL setup.\nTest Thoroughly: The best way to confirm is to simply run your PostgreSQL container and ensure it functions as expected.\n    Can you start it?\n    Can you connect to it?\n    Does data persist across container restarts (which confirms your volume mount is working)?\n\nTry running your postgres container\n    podman run -d --name my-postgres \\\n               -e POSTGRES_DB=my_app_db \\\n               -e POSTGRES_USER=dev_user \\\n               -e POSTGRES_PASSWORD=dev_password \\\n               -v pgdata:/var/lib/postgresql/data \\\n               -p 5432:5432 \\\n               postgres:14-alpine\n\nCheck logs\n    podman logs my-postgres\n\nConnect from inside WSL2\n    psql -h localhost -U dev_user -d my_app_db\n\nStop and start to check persistence\n    podman stop my-postgres\n    podman start my-postgres\n\nIf these basic functionalities work, then the warning is not impacting your use case.\n\nWhen Might it Actually Be an Issue?\n\nIf you try to mount another mount point from within your container that depends on the host's propagation behavior.\nIf you're doing very advanced multi-container setups where containers need to dynamically discover mounts created by other processes on the host.\n\nFor a single PostgreSQL service, these scenarios are very unlikely.\n\nIn short: Don't worry about it unless you actually encounter unexpected issues with volume mounts or container networking that point to this specific problem. It's a general warning that doesn't always apply to every rootless container use case.",
        "locale": "en",
        "slug": "technical-posts/podman-shared-mount-warning",
        "folder": ""
    },
    {
        "title": "Launch Windows explorer from browser",
        "description": "Launch Windows explorer from browser",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "web-dev"
        ],
        "content": "Yes, there are a few ways to launch Windows applications from a browser-based app, but they all have significant limitations and security considerations:\n\n1. Custom URL Protocol Handlers (Most Common, But Requires User Installation):\n\nHow it works: You register a custom URL protocol handler in Windows (e.g., myapp://) associated with a specific executable. When the browser encounters a link with that protocol, it attempts to launch the associated application.\n\nSteps:\n\n    Register Protocol Handler (Windows-side): You need a program (often an installer) that the user runs outside the browser to register the custom URL protocol. This involves modifying the Windows Registry to associate the protocol with the target executable. This is typically done through an executable written in languages like C#, C++, or even scripting languages with registry modification capabilities (e.g., PowerShell).\n    Create a Link in your Web App: In your web page, create a link with the custom URL:\n\n        Launch My App\n\n    App Handles Arguments (Windows-side): Your Windows application needs to be able to parse the arguments passed in the URL (e.g., argument1 and argument2=value). This is usually done in the Main method of your application.\n\nAdvantages:\n\n    Relatively straightforward once set up.\n    Allows passing parameters to the application.\n\nDisadvantages:\n\n    User Interaction Required: The user must install and configure the protocol handler. This is a significant barrier to entry.\n    Security Risks: Carefully validate the arguments passed to the application to prevent vulnerabilities. Malicious websites could potentially inject harmful commands. Properly escaping and sanitizing the data is crucial.\n    Browser Prompts: Modern browsers often display security warnings when launching external applications. Users must explicitly allow the launch.\n    Platform-Specific: This solution is inherently tied to the Windows operating system.\n    Difficult to Automate Installation: You can't automatically install a protocol handler from a website without user consent.\n\nExample (Conceptual C# Windows app for the handler):\n\n    using System;\n    using System.Windows.Forms;\n\n    namespace MyAppLauncher\n    {\n        static class Program\n        {\n            STAThread]\n            static void Main(string[] args)\n            {\n                Application.EnableVisualStyles();\n                Application.SetCompatibleTextRenderingDefault(false);\n\n                if (args.Length > 0)\n                {\n                    string url = args[0]; // The full URL \"myapp://...\"\n\n                    // Parse the URL and extract parameters\n                    Uri uri = new Uri(url);\n                    string path = uri.AbsolutePath; // Will be \"/\" if no path in the URL\n                    string query = uri.Query; // Will be \"?arg1=val1&arg2=val2\" if query parameters exist\n                    var parameters = System.Web.HttpUtility.ParseQueryString(uri.Query);\n\n                    // Now you have access to the URL components\n                    MessageBox.Show($\"Launched with URL: {url}\\nPath: {path}\\nQuery: {query}\\nParam1: {parameters[\"arg1\"]}\");\n                }\n                else\n                {\n                    MessageBox.Show(\"MyAppLauncher started without URL parameters.\");\n                }\n\n                Application.Run(new Form()); // Keep the app running (optional)\n            }\n        }\n    }\n\nExample (Conceptual Registry Entry - Requires Administrator Privileges):\n\n    Windows Registry Editor Version 5.00\n\n    [HKEY_CLASSES_ROOT\\myapp]\n    @=\"URL:MyApp Protocol\"\n    \"URL Protocol\"=\"\"\n\n    [HKEY_CLASSES_ROOT\\myapp\\DefaultIcon]\n    @=\"C:\\\\path\\\\to\\\\myapplauncher.exe,1\"  ; Path to your executable\n\n    [HKEY_CLASSES_ROOT\\myapp\\shell\n\n    HKEY_CLASSES_ROOT\\myapp\\shell\\open\\command]\n    @=\"\\\"C:\\\\path\\\\to\\\\myapplauncher.exe\\\" \\\"%1\\\"\"  ; %1 is the URL\n\n2. Browser Extensions (More Control, Still Requires User Installation):\n\nHow it works: You create a browser extension (e.g., Chrome extension, Firefox extension) that can interact with native applications. The extension acts as a bridge between the web page and the Windows application.\n\nSteps:\n\n    Develop Browser Extension: The extension needs to use a Native Messaging host. The native messaging host is a separate application (your Windows app) that communicates with the extension via standard input/output using JSON.\n    Native Messaging Host (Windows-side): This is the Windows application that the extension communicates with. It receives messages from the extension and executes actions.\n    Manifest File: A manifest file is required to declare the native messaging host to the browser. This allows the browser to find and launch the native application when the extension requests it.\n    Communication: The web page sends messages to the extension using the browser's extension API. The extension then forwards those messages to the native messaging host. The host performs the action and sends a response back to the extension, which then relays it to the web page.\n\nAdvantages:\n\n    More controlled communication between the web page and the application.\n    Greater flexibility in what the extension can do (e.g., access system resources, interact with other applications).\n\nDisadvantages:\n    Complex Development: Requires developing both a browser extension and a native messaging host application.\n    User Installation Required: The user must install the browser extension and the native messaging host.\n    Security Considerations: Carefully validate messages exchanged between the extension and the host.\n\n3. ActiveX Controls (Deprecated, Not Recommended):\n\nHow it works: ActiveX controls are components that can be embedded in web pages and have access to system resources.\n\nWhy it's not recommended:\n    Security Risks: ActiveX controls have been a major source of security vulnerabilities and are heavily restricted by modern browsers.\n    Browser Compatibility: ActiveX is primarily supported by older versions of Internet Explorer and is not supported by most other modern browsers.\n    Deprecated Technology: ActiveX is a legacy technology and is not actively maintained.\n\n4. ClickOnce (Specific to .NET Applications, Limited Usefulness for General Launching):\n\nHow it works: ClickOnce is a deployment technology for .NET applications. It allows you to install and run .NET applications from a web server.\n\nLimitations:\n    Primarily for deploying .NET applications, not for launching arbitrary executables.\n    Requires .NET Framework to be installed.\n    Not suitable for general-purpose launching of Windows applications.\n\nSecurity Considerations (IMPORTANT):\n\nInput Validation: Always validate any data received from the browser. Treat all input as potentially malicious.\nSandboxing: If possible, run the launched application in a sandboxed environment to limit its access to system resources.\nUser Confirmation: Always prompt the user for confirmation before launching an application.\nMinimize Privileges: Run the launched application with the least necessary privileges.\nCode Signing: Sign your Windows application with a digital certificate to ensure its authenticity.\nHTTPS: Serve your web page over HTTPS to prevent man-in-the-middle attacks.\n\nChoosing the Right Approach:\n\nIf you absolutely must launch a Windows application from a web page, and the user is willing to install a component: Custom URL protocol handlers are the simplest approach.\nIf you need more control and flexibility, and the user is willing to install an extension: Browser extensions with Native Messaging Hosts are a better choice.\nAvoid ActiveX controls entirely due to security risks and browser compatibility issues.\n\nIn summary, directly launching arbitrary Windows applications from a browser is inherently risky. Modern browsers are designed to prevent this for security reasons. Any solution will require user interaction and careful attention to security best practices. Consider alternative approaches if possible, such as:\n\nProviding instructions for the user to manually launch the application.\nOffering a downloadable installer for the application.\nUsing web-based technologies to achieve the same functionality.\n\n\nYes, there are a few ways to launch Windows applications from a browser-based app, but they all have significant limitations and security considerations:\n\n1. Custom URL Protocol Handlers (Most Common, But Requires User Installation):\n\nHow it works: You register a custom URL protocol handler in Windows (e.g., myapp://) associated with a specific executable. When the browser encounters a link with that protocol, it attempts to launch the associated application.\n\nSteps:\n\n    Register Protocol Handler (Windows-side): You need a program (often an installer) that the user runs outside the browser to register the custom URL protocol. This involves modifying the Windows Registry to associate the protocol with the target executable. This is typically done through an executable written in languages like C#, C++, or even scripting languages with registry modification capabilities (e.g., PowerShell).\n    Create a Link in your Web App: In your web page, create a link with the custom URL:\n\n        Launch My App\n\n    App Handles Arguments (Windows-side): Your Windows application needs to be able to parse the arguments passed in the URL (e.g., argument1 and argument2=value). This is usually done in the Main method of your application.\n\nAdvantages:\n\n    Relatively straightforward once set up.\n    Allows passing parameters to the application.\n\nDisadvantages:\n\n    User Interaction Required: The user must install and configure the protocol handler. This is a significant barrier to entry.\n    Security Risks: Carefully validate the arguments passed to the application to prevent vulnerabilities. Malicious websites could potentially inject harmful commands. Properly escaping and sanitizing the data is crucial.\n    Browser Prompts: Modern browsers often display security warnings when launching external applications. Users must explicitly allow the launch.\n    Platform-Specific: This solution is inherently tied to the Windows operating system.\n    Difficult to Automate Installation: You can't automatically install a protocol handler from a website without user consent.\n\nExample (Conceptual C# Windows app for the handler):\n\n    using System;\n    using System.Windows.Forms;\n\n    namespace MyAppLauncher\n    {\n        static class Program\n        {\n            [STAThread]\n            static void Main(string[] args)\n            {\n                Application.EnableVisualStyles();\n                Application.SetCompatibleTextRenderingDefault(false);\n\n                if (args.Length > 0)\n                {\n                    string url = args[0]; // The full URL \"myapp://...\"\n\n                    // Parse the URL and extract parameters\n                    Uri uri = new Uri(url);\n                    string path = uri.AbsolutePath; // Will be \"/\" if no path in the URL\n                    string query = uri.Query; // Will be \"?arg1=val1&arg2=val2\" if query parameters exist\n                    var parameters = System.Web.HttpUtility.ParseQueryString(uri.Query);\n\n                    // Now you have access to the URL components\n                    MessageBox.Show($\"Launched with URL: {url}\\nPath: {path}\\nQuery: {query}\\nParam1: {parameters[\"arg1\"]}\");\n                }\n                else\n                {\n                    MessageBox.Show(\"MyAppLauncher started without URL parameters.\");\n                }\n\n                Application.Run(new Form()); // Keep the app running (optional)\n            }\n        }\n    }\n\nExample (Conceptual Registry Entry - Requires Administrator Privileges):\n\n    Windows Registry Editor Version 5.00\n\n    [HKEY_CLASSES_ROOT\\myapp]\n    @=\"URL:MyApp Protocol\"\n    \"URL Protocol\"=\"\"\n\n    [HKEY_CLASSES_ROOT\\myapp\\DefaultIcon]\n    @=\"C:\\\\path\\\\to\\\\myapplauncher.exe,1\"  ; Path to your executable\n\n    [HKEY_CLASSES_ROOT\\myapp\\shell\n\n    [HKEY_CLASSES_ROOT\\myapp\\shell\\open\\command]\n    @=\"\\\"C:\\\\path\\\\to\\\\myapplauncher.exe\\\" \\\"%1\\\"\"  ; %1 is the URL\n\n2. Browser Extensions (More Control, Still Requires User Installation):\n\nHow it works: You create a browser extension (e.g., Chrome extension, Firefox extension) that can interact with native applications. The extension acts as a bridge between the web page and the Windows application.\n\nSteps:\n\n    Develop Browser Extension: The extension needs to use a Native Messaging host. The native messaging host is a separate application (your Windows app) that communicates with the extension via standard input/output using JSON.\n    Native Messaging Host (Windows-side): This is the Windows application that the extension communicates with. It receives messages from the extension and executes actions.\n    Manifest File: A manifest file is required to declare the native messaging host to the browser. This allows the browser to find and launch the native application when the extension requests it.\n    Communication: The web page sends messages to the extension using the browser's extension API. The extension then forwards those messages to the native messaging host. The host performs the action and sends a response back to the extension, which then relays it to the web page.\n\nAdvantages:\n\n    More controlled communication between the web page and the application.\n    Greater flexibility in what the extension can do (e.g., access system resources, interact with other applications).\n\nDisadvantages:\n    Complex Development: Requires developing both a browser extension and a native messaging host application.\n    User Installation Required: The user must install the browser extension and the native messaging host.\n    Security Considerations: Carefully validate messages exchanged between the extension and the host.\n\n3. ActiveX Controls (Deprecated, Not Recommended):\n\nHow it works: ActiveX controls are components that can be embedded in web pages and have access to system resources.\n\nWhy it's not recommended:\n    Security Risks: ActiveX controls have been a major source of security vulnerabilities and are heavily restricted by modern browsers.\n    Browser Compatibility: ActiveX is primarily supported by older versions of Internet Explorer and is not supported by most other modern browsers.\n    Deprecated Technology: ActiveX is a legacy technology and is not actively maintained.\n\n4. ClickOnce (Specific to .NET Applications, Limited Usefulness for General Launching):\n\nHow it works: ClickOnce is a deployment technology for .NET applications. It allows you to install and run .NET applications from a web server.\n\nLimitations:\n    Primarily for deploying .NET applications, not for launching arbitrary executables.\n    Requires .NET Framework to be installed.\n    Not suitable for general-purpose launching of Windows applications.\n\nSecurity Considerations (IMPORTANT):\n\nInput Validation: Always validate any data received from the browser. Treat all input as potentially malicious.\nSandboxing: If possible, run the launched application in a sandboxed environment to limit its access to system resources.\nUser Confirmation: Always prompt the user for confirmation before launching an application.\nMinimize Privileges: Run the launched application with the least necessary privileges.\nCode Signing: Sign your Windows application with a digital certificate to ensure its authenticity.\nHTTPS: Serve your web page over HTTPS to prevent man-in-the-middle attacks.\n\nChoosing the Right Approach:\n\nIf you absolutely must launch a Windows application from a web page, and the user is willing to install a component: Custom URL protocol handlers are the simplest approach.\nIf you need more control and flexibility, and the user is willing to install an extension: Browser extensions with Native Messaging Hosts are a better choice.\nAvoid ActiveX controls entirely due to security risks and browser compatibility issues.\n\nIn summary, directly launching arbitrary Windows applications from a browser is inherently risky. Modern browsers are designed to prevent this for security reasons. Any solution will require user interaction and careful attention to security best practices. Consider alternative approaches if possible, such as:\n\nProviding instructions for the user to manually launch the application.\nOffering a downloadable installer for the application.\nUsing web-based technologies to achieve the same functionality.",
        "locale": "en",
        "slug": "technical-posts/launch-windows-explorer-from-browser",
        "folder": ""
    },
    {
        "title": "Using dynamic components with SvelteKit",
        "description": "Using dynamic components with SvelteKit",
        "date_created": "2025-06-26T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Of course! This is a classic and powerful pattern for building dynamic layouts, content management systems (CMS), or page builders. The best way to achieve this in SvelteKit 2.0 / Svelte 5 leverages two core features:\n\nDynamic Imports import(): A standard JavaScript feature that allows you to load a module (like a Svelte component) on demand. Vite, the build tool behind SvelteKit, automatically code-splits any dynamically imported module, meaning it only gets downloaded by the user's browser when it's actually needed. This is excellent for performance.\nSvelte's ` element: A special Svelte element that can render any component you pass to its this` prop.\n\nHere is the best practice, broken down into a step-by-step guide.\n\nThe Goal\n\nWe want to render a sequence of components defined in a JSON file.\n\nExample layout.json:\n[\n  {\n    \"component\": \"Banner\",\n    \"props\": {\n      \"title\": \"Welcome to SvelteKit 2.0!\",\n      \"subtitle\": \"Dynamic components made easy.\"\n    }\n  },\n  {\n    \"component\": \"Profile\",\n    \"props\": {\n      \"name\": \"Alex Doe\",\n      \"avatar\": \"/images/alex.jpg\",\n      \"bio\": \"A passionate Svelte developer.\"\n    }\n  },\n  {\n    \"component\": \"CallToAction\",\n    \"props\": {\n      \"buttonText\": \"Learn More\"\n    }\n  }\n]\n\nStep 1: Create Your Components\n\nFirst, let's create the components that our JSON file refers to.\n\nsrc/lib/components/Banner.svelte\n\n  export let title = '';\n  export let subtitle = '';\n\n\n\n  {title}\n  {subtitle}\n\n\n\n  .banner {\n    background-color: #333;\n    color: white;\n    padding: 2rem;\n    text-align: center;\n    border-radius: 8px;\n  }\n\n\nsrc/lib/components/Profile.svelte\n\n  export let name = '';\n  export let avatar = '';\n  export let bio = '';\n\n\n\n  \n  {name}\n  {bio}\n\n\n\n  .profile {\n    display: flex;\n    align-items: center;\n    gap: 1rem;\n    padding: 1.5rem;\n    border: 1px solid #ddd;\n    border-radius: 8px;\n  }\n  img {\n    width: 80px;\n    height: 80px;\n    border-radius: 50%;\n  }\n\n\nsrc/lib/components/CallToAction.svelte\n\n  export let buttonText = 'Click Me';\n\n\n\n  {buttonText}\n\n\n\n  .cta { text-align: center; padding: 2rem; }\n  button {\n    padding: 0.8rem 1.5rem;\n    font-size: 1rem;\n    cursor: pointer;\n  }\n\nStep 2: Create a Component Mapper\n\nThis is the crucial part. We need a way to map the string names from the JSON (\"Banner\", \"Profile\") to the actual component files. A simple JavaScript object is perfect for this.\n\nThis file acts as a manifest for your dynamic components.\n\nsrc/lib/componentMap.js\n// A map of component names to their dynamic import functions.\n// Using a function () => import(...) is key. It makes the import lazy,\n// so the component code is only requested when this function is called.\nexport const componentMap = {\n  Banner: () => import('$lib/components/Banner.svelte'),\n  Profile: () => import('$lib/components/Profile.svelte'),\n  CallToAction: () => import('$lib/components/CallToAction.svelte'),\n};\nWhy use $lib? The $lib alias in SvelteKit is a reliable way to reference files inside your src/lib directory, avoiding messy relative paths like ../../../.\n\nStep 3: Render the Dynamic Layout in Your Page\n\nNow, let's use this in a SvelteKit page. We will read the JSON, iterate over it, and for each item, dynamically import and render the correct component.\n\nsrc/routes/+page.svelte\n\n  import { componentMap } from '$lib/componentMap.js';\n  import layoutConfig from './layout.json'; // Directly import the static JSON\n  import Loading from '$lib/components/Loading.svelte'; // A simple loading indicator\n  import ComponentError from '$lib/components/ComponentError.svelte'; // An error fallback\n\n  // If layoutConfig were from an API, you'd load it in +page.js and receive it via export let data;\n\n\n\n  {#each layoutConfig as block}\n    \n    {@const componentImporter = componentMap[block.component]}\n\n    {#if componentImporter}\n      \n      {#await componentImporter()}\n        \n      {:then module}\n         and spread the props from our JSON.\n        -->\n        \n      {:catch error}\n        \n      {/await}\n    {:else}\n      \n      \n    {/if}\n  {/each}\n\n\n\n  .page-container {\n    display: flex;\n    flex-direction: column;\n    gap: 1.5rem;\n    max-width: 800px;\n    margin: 2rem auto;\n  }\n\n\nYou'll need to create the Loading and ComponentError components for the best user experience.\n\nsrc/lib/components/Loading.svelte\nLoading component...\n.loading { padding: 2rem; text-align: center; color: #888; }\n\nsrc/lib/components/ComponentError.svelte\n\n  export let componentName = 'Unknown';\n  export let message = 'An error occurred.';\n\n\n  Error loading component: {componentName}\n  {message}\n\n.error { padding: 2rem; background-color: #ffdddd; border: 1px solid red; color: #a00; border-radius: 8px; }\n\nAlternative: Using SvelteKit's load Function\n\nIf your JSON comes from an external API or a CMS instead of a static file, the SvelteKit-idiomatic way is to fetch it in a +page.js or +page.server.js load function.\n\nsrc/routes/+page.js\nexport async function load({ fetch }) {\n  // In a real app, this URL would be your CMS or API endpoint\n  const response = await fetch('/api/page-layout'); // or a direct import for local files\n  const layoutConfig = await response.json();\n\n  return {\n    layoutConfig\n  };\n}\n\nThen your +page.svelte would change slightly:\n\n  // Data now comes from the load function\n  export let data;\n\n  import { componentMap } from '$lib/componentMap.js';\n  import Loading from '$lib/components/Loading.svelte';\n  import ComponentError from '$lib/components/ComponentError.svelte';\n\n\n\n\n  {#each data.layoutConfig as block}\n    \n  {/each}\n\nSummary of Why This is the \"Best Way\"\n\nPerformance & Code-Splitting: Users only download the JavaScript and CSS for the components they actually see. If a component isn't listed in the JSON, its code is never sent to the browser.\nMaintainability: The componentMap.js file provides a single, clear index of all components available for dynamic rendering. This prevents you from having to write a giant if/else or switch statement.\nFlexibility: You can easily build complex, re-orderable pages driven by a CMS or any other data source without changing your Svelte code.\nRobustness: Using the {#await} block with {:then} and {:catch} provides built-in handling for loading and error states, making your application more resilient.\nSvelte 5 & Future-Proof: This pattern is fundamental and works perfectly with Svelte 5's new reactivity model (runes) because it relies on standard JavaScript (import()) and core Svelte features (`, {#await}`).",
        "locale": "en",
        "slug": "technical-posts/sveltekit-dynamic-components",
        "folder": ""
    },
    {
        "title": "HTTP context API in Sveltekit",
        "description": "HTTP context API in Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Example data returned from HTTP context object in Sveltekit Get method\n\nimport { json } from '@sveltejs/kit';\n\nexport async function GET(context) {\n\tconsole.log(context);\n\treturn json('roger');\n}\n\n{\n  cookies: {\n    get: [Function: get],\n    getAll: [Function: getAll],\n    set: [Function: set],\n    delete: [Function: delete],\n    serialize: [Function: serialize]\n  },\n  fetch: [AsyncFunction (anonymous)],\n  getClientAddress: [Function: getClientAddress],\n  locals: {},\n  params: {},\n  platform: undefined,\n  request: Request {\n    [Symbol(realm)]: { settingsObject: [Object] },\n    [Symbol(state)]: {\n      method: 'GET',\n      localURLsOnly: false,\n      unsafeRequest: false,\n      body: null,\n      client: [Object],\n      reservedClient: null,\n      replacesClientId: '',\n      window: 'client',\n      keepalive: false,\n      serviceWorkers: 'all',\n      initiator: '',\n      destination: '',\n      priority: null,\n      origin: 'client',\n      policyContainer: 'client',\n      referrer: 'client',\n      referrerPolicy: '',\n      mode: 'cors',\n      useCORSPreflightFlag: false,\n      credentials: 'same-origin',\n      useCredentials: false,\n      cache: 'default',\n      redirect: 'follow',\n      integrity: '',\n      cryptoGraphicsNonceMetadata: '',\n      parserMetadata: '',\n      reloadNavigation: false,\n      historyNavigation: false,\n      userActivation: false,\n      taintedOrigin: false,\n      redirectCount: 0,\n      responseTainting: 'basic',\n      preventNoCacheCacheControlHeaderModification: false,\n      done: false,\n      timingAllowFailed: false,\n      headersList: [HeadersList],\n      urlList: [Array],\n      url: [URL]\n    },\n    [Symbol(signal)]: AbortSignal { aborted: false },\n    [Symbol(headers)]: HeadersList {\n      cookies: null,\n      [Symbol(headers map)]: [Map],\n      [Symbol(headers map sorted)]: null\n    }\n  },\n  route: { id: '/api/post' },\n  setHeaders: [Function: setHeaders],\n  url: URL {\n    href: 'http://localhost:5173/api/post?slug=roger',\n    origin: 'http://localhost:5173',\n    protocol: 'http:',\n    username: '',\n    password: '',\n    host: 'localhost:5173',\n    hostname: 'localhost',\n    port: '5173',\n    pathname: '/api/post',\n    search: '?slug=roger',\n    searchParams: URLSearchParams { 'slug' => 'roger' },\n    hash: ''\n  },\n  isDataRequest: false,\n  isSubRequest: false\n}\n\nRead the query string:\n\nexport async function GET(context) {\n\tconst slug = context.url.searchParams.get('slug');\n\t...",
        "locale": "en",
        "slug": "technical-posts/working-with-http-context",
        "folder": ""
    },
    {
        "title": "Python env command line swindle",
        "description": "Python env command line swindle",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "python",
            "powershell"
        ],
        "content": "It's generally a very bad idea to install Python Pip modules globally. The downside of this is that installing modules locally needs a [[python-env|Python virtual environment.]] That's easy to do and from the command line, not that much of a hassle to use.\n\nHowever, it makes running any scripts in that environment from a PowerShell script troublesome.\n\nOne way to do that is:\n\nCreate this PowerShell script called activator.ps1 and put it in the root of the Python project.\n\n.\\env\\scripts\\activate\n\nThis use this command line to run the librettox.py Python script:\n\n& .\\activator.ps1 && python librettox.py'\n\nIn this case, the & invokes a PowerShell script or command and the && chains commands. After running the activator.ps1 script the python librettox.py is run.\n\nHowever, this requires the special case activator.ps1 script and that's annoying. Here is a very good alternative:\n\n.\\env\\scripts\\python.exe librettox.py -b blah -t blah\n\nWhen a Python virtual environment is created, a separate python.exe (on Windows) is placed inside the env\\Scripts directory. This executable is configured to use the environment-specific packages and settings.\n\nBy running python.exe from within the env\\Scripts directory, you effectively activate the virtual environment without needing a separate activation script or to environment variables. Its like activating the environment implicitly because that python.exe is already set up to use the correct environment paths.\n\nWho knew?\n\n[!info]\nFor more context, take a look at the env\\scripts\\activate.bat or env\\scripts\\activate.ps1 file. It is controlling which Python executable runs by manipulating environment variables.",
        "locale": "en",
        "slug": "technical-posts/python-env-command-line-swindle",
        "folder": ""
    },
    {
        "title": "Set up a Python virtual environment",
        "description": "Set up a Python virtual environment",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "python"
        ],
        "content": "This uses Python's built-in virtual environment, venv\n\nhttps://docs.python.org/3/library/venv.html\n\nCreate a project directory\n\nmkdir my_project\n\nChange to that project directory\n\ncd my_project\n\nCreate a virtual environment on Windows\n\npython -m venv env\n\nActive the environment in my_project\n\nenv\\scripts\\activate\n\n([environment name]) should show on command line\n\nTo deactivate a VM\n\ndeactivate\n\nTo save local dependencies:\n\npip freeze > requirements.txt\n\nTo restore dependencies in a virtual environment:\n\nFirst, start the environment\n\nenv\\scripts\\activate\n\nThen restore local dependencies:\n\npip install -r requirements.txt",
        "locale": "en",
        "slug": "technical-posts/set-up-a-python-virtual-environment",
        "folder": ""
    },
    {
        "title": "Obsidian searching",
        "description": "Obsidian searching",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "obsidian"
        ],
        "content": "Search in a folder:\n\nSearch for css in the programming/Sveltekit folder only\n\npath: Programming/Sveltekit css\n\nSee this Obsidian doc page for other search operators\n\nthis is a new",
        "locale": "en",
        "slug": "technical-posts/obsidian-searching",
        "folder": ""
    },
    {
        "title": "Remove values from the current URL query string",
        "description": "Remove values from the current URL query string",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "This code uses JavaScript's URLSearchParams interface to remove query string keys and values from the current URL. Prior to removing them, all of the query string keys and values are available to the code.\n\nThis only works on the client side!\n\nimport { page } from '$app/stores';\nimport { browser } from '$app/environment';\n\nexport const getPurgedQueryStringUrl = (url, excludeKeys) => {\n    const newParams = new URLSearchParams();\n\n    for (const [key, value] of url.searchParams) {\n        if (!excludeKeys.includes(key)) {\n            newParams.append(key, value);\n        }\n    }\n    if ([...newParams].length == 0) {\n        return ${url.origin}${url.pathname}\n    }\n    else {\n        return ${url.origin}${url.pathname}?${newParams};\n    }\n};\n\n...\n\n// Remove the 'email' and 'type' keys from the query string.\nif (browser) {\n\tnewUrl = getPurgedQueryStringUrl($page.url, ['email', 'type']);\n\twindow.history.pushState({}, document.title, newUrl);\n}",
        "locale": "en",
        "slug": "technical-posts/remove-query-string-keys-and-values",
        "folder": ""
    },
    {
        "title": "Use yield to read SQL Server",
        "description": "Use yield to read SQL Server",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "csharp"
        ],
        "content": "using Microsoft.Data.SqlClient;\nusing System;\nusing System.Collections.Generic;\nusing System.Data;\n\nusing System.Dynamic;\n\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Npgsql;\n\nnamespace sql_query_to_csv\n{\n    public class DBSqlServer\n    {\n        public static IEnumerable GetDataFromSql(string connectionString, string sqlQuery)\n        {\n            // --- Resource Acquisition and Setup ---\n            // This part can have try...catch for errors occurring before iteration starts\n\n            SqlConnection connection = null; // Declare outside try for finally access if needed (though using handles it)\n            SqlCommand command = null;\n            SqlDataReader reader = null;\n\n            try\n            {\n                connection = new SqlConnection(connectionString);\n                connection.Open();\n\n                command = new SqlCommand(sqlQuery, connection);\n                // Use CommandBehavior.CloseConnection to ensure connection closes when reader is disposed\n                reader = command.ExecuteReader(System.Data.CommandBehavior.CloseConnection);\n\n                // IMPORTANT: Return the result of the iterator method.\n                // The 'using' statements ensure disposal happens after iteration completes or errors out.\n                return ReadData(reader);\n            }\n            catch (SqlException ex)\n            {\n                // Log setup errors\n                Console.WriteLine($\"SQL Exception during setup in GetDataFromSql: {ex.Message} (Query: {sqlQuery})\");\n                // Clean up resources manually ONLY IF an error occurred BEFORE the reader was successfully passed to the iterator.\n                // The 'using' pattern usually handles this, but explicit cleanup in catch might be needed depending on where error happened.\n                // However, CommandBehavior.CloseConnection and the structure below make manual cleanup here less critical.\n                reader?.Dispose(); // If reader was created before error\n                command?.Dispose(); // If command was created before error\n                connection?.Dispose(); // If connection was opened before error\n                throw; // Re-throw for handling higher up\n            }\n            catch (Exception ex)\n            {\n                // Log generic setup errors\n                Console.WriteLine($\"Generic Exception during setup in GetDataFromSql: {ex.Message}\");\n                reader?.Dispose();\n                command?.Dispose();\n                connection?.Dispose();\n                throw; // Re-throw\n            }\n            // NOTE: We don't need using blocks here anymore because CommandBehavior.CloseConnection\n            // links the reader's lifetime to the connection's lifetime, and the iterator\n            // implicitly controls the reader's lifetime. When the iterator finishes or errors,\n            // the reader gets disposed, which in turn closes the connection.\n        }\n\n        // --- Private Helper Iterator Method ---\n        // This method handles the actual reading and yielding.\n        // It does NOT have its own try...catch around the yield itself.\n        private static IEnumerable ReadData(SqlDataReader reader)\n        {\n            // Use 'using' for the reader passed in. This ensures it's disposed\n            // when iteration completes or an exception occurs during iteration.\n            using (reader)\n            {\n                // Check if there are rows before attempting to read column names\n                if (!reader.HasRows)\n                {\n                    yield break; // Nothing to return, exit the iterator block cleanly\n                }\n\n                // Get column names once\n                var columnNames = new List(reader.FieldCount);\n                for (int i = 0; i )obj;\n\n                    for (int i = 0; i < reader.FieldCount; i++)\n                    {\n                        string columnName = columnNames[i];\n                        object value = reader.IsDBNull(i) ? null : reader.GetValue(i);\n                        dict[columnName] = value;\n                    }\n\n                    yield return obj;\n                }\n            } // reader (and implicitly connection) is disposed here automatically.\n        }\n    }\n}",
        "locale": "en",
        "slug": "technical-posts/use-yield-to-read-sql-server",
        "folder": ""
    },
    {
        "title": "Using Pycco with Visual RPG",
        "description": "Using Pycco with Visual RPG",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "python"
        ],
        "content": "Pycco modifcation for commented code\n\nBy default, Pycco treats all comment lines as markdown text with which to document the code. This modifcation allows a //- comment to be passed through as a comment in the code. For example, this comment:\n\n//- em.Attachments.Add(\"C:\\Users\\roger\\Documents\\dbnames.txt\")\n\npersists as a comment (with the dash removed) rather than be treated as a markdown text to push to the left of the code by Pycco.\n\n![[snippet.svg]]\n\nC:\\Users\\roger\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\pycco\\main.py\n\nfile path for change",
        "locale": "en",
        "slug": "technical-posts/using-pycco-with-avr",
        "folder": ""
    },
    {
        "title": "How to apply global CSS to a SvelteKit app.",
        "description": "How to apply global CSS to a SvelteKit app.",
        "date_created": "2025-05-25T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css",
            "svelte"
        ],
        "content": "\n\n\t// CSS should be in the root of the SvelteKit app. \n\timport '../pico.css'; // import global CSS\n\n\tlet { children } = $props();\n\n\n\n\t\n\t\t{@render children()}\n\t\n",
        "locale": "en",
        "slug": "technical-posts/sveltekit-css",
        "folder": ""
    },
    {
        "title": "Force Sveltekit page reloadO",
        "description": "Force Sveltekit page reloadO",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "By default, SvelteKit handles loading a page for a link, which may use a cached instance of the page instead of physically reloading the page. This is great for performance but sometimes you need to do achieve a programmatic hard-reload for a link (maybe to cause other logic imposed by query string values).\n\nTo force Sveltekit to reload a page, add data-sveltekit-reload to the link:\n\nMy link/a>\n\nOther powerful link options are explained here.",
        "locale": "en",
        "slug": "technical-posts/force-sveltekit-page-reload",
        "folder": ""
    },
    {
        "title": "Email design system",
        "description": "Email design system",
        "date_created": "2025-07-19T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "email",
            "email-builder"
        ],
        "content": "padding: 0 35px 8px 35px; mso-padding-alt: 0 0 8px 0;\"\n\nDynamicObject - this types eData\n\ninterface DynamicObject {\n\t[key: string]: any;\n}\n\nWrapper\n\nThis provides the top-level HTML table for an email. All other email components are within this wrapper.\n\ninterface Props {\n\tchildren: any;\n\tbackgroundColor?: string;\n}\n\nMsoTable\n\nThis provides the parent table for email components. It's never used directly in an email; rather it provides the consistent table that all (most?--fix this) need.\n\ninterface Props {\n\tchildren: any;\n\tbackgroundColor?: string;\n}\n\nParagraph\n\ninterface Props {\n\teData: DynamicObject;\n\tcontentKey: string;\n\talign?: string;\n\tfontSize?: number;\n\tlineHeight?: number;\n\tcolor?: string;\n\tbackgroundColor?: string;\n\tpadding?: string;\n\tpaddingMso?: string;\n}\n\nImage\n\ninterface Props {\n\teData: DynamicObject;\n\turlKey: string;\n\taltKey: string;\n\twidthKey: string;\n\tcaptionKey?: string;\n\talign?: string;\n\tfontSize?: number;\n\tcolor?: string;\n\tlineHeight?: number;\n\tbackgroundColor?: string;\n\tpadding?: string;\n\tpaddingMso?: string;\n}\n\n\n\n\nCore elements\nBulletList\nButton\nCallOut \nDivider\nImage\nParagraph\nSpacer\n\tImageUrl\n\tCaption\nTwoColumn",
        "locale": "en",
        "slug": "technical-posts/email-design-system",
        "folder": ""
    },
    {
        "title": "Using Windows Custom URI Schemes to open a Windows app from a browser",
        "description": "Using Windows Custom URI Schemes to open a Windows app from a browser",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows"
        ],
        "content": "Windows' Custom URI Schemes (also known as custom protocol handlers) is a way to allow a web browser (or any other application) to launch a specific Windows executable, potentially passing it some data.\n\nHow it Works:\n\nRegistration: Your Windows executable (or its installer) needs to register a custom URI scheme in the Windows Registry. For example, you could register myapp://.\nInvocation: In your browser, you create a link like Launch My App.\nBrowser Handling: When a user clicks this link, the browser sees the myapp:// scheme. It doesn't know how to handle it directly.\nOS Lookup: The browser asks Windows, \"Hey, do you know how to handle myapp://?\"\nExecution: Windows checks its registry, finds the registration for myapp://, and sees that it points to C:\\Path\\To\\YourApp.exe. Windows then launches YourApp.exe, passing the full URI (myapp://some/data?param=value) as a command-line argument.\nApplication Logic: Your YourApp.exe then needs to be programmed to parse this command-line argument to understand what action to take or what data to process.\n\n\nHere's a step-by-step guide:\n\nStep 1: Create or Identify Your Executable\n\nLet's assume you have an executable, say C:\\MyTools\\MyLauncher.exe. This application should be designed to accept command-line arguments.\n\nFor example, a simple C# console app that just shows its arguments:\n\n// MyLauncher.cs\nusing System;\nusing System.Windows.Forms; // For MessageBox\n\npublic class MyLauncher\n{\n    public static void Main(string] args)\n    {\n        if (args.Length > 0)\n        {\n            MessageBox.Show(\"Launched with URI: \" + args[0], \"MyLauncher\");\n            // Here you would parse args[0\n            // e.g., if URI is myapp://open?file=C:\\doc.txt\n            // you'd extract \"C:\\doc.txt\" and open it.\n        }\n        else\n        {\n            MessageBox.Show(\"Launched directly (no URI).\", \"MyLauncher\");\n        }\n    }\n}\n\nCompile this into MyLauncher.exe.\n\nStep 2: Register the Custom URI Scheme in the Registry\n\nYou need to create specific registry keys. You can do this manually with regedit (requires admin rights) or by creating a .reg file and importing it (also requires admin rights).\n\nLet's say your custom scheme is mycoolapp://.\n\nCreate a .reg file (e.g., register_mycoolapp.reg) with the following content. Make sure to replace C:\\\\MyTools\\\\MyLauncher.exe with the actual, correctly-escaped path to your executable.\n\nWindows Registry Editor Version 5.00\n\nHKEY_CLASSES_ROOT\\mycoolapp]\n@=\"URL:MyCoolApp Protocol\"\n\"URL Protocol\"=\"\"\n\n[HKEY_CLASSES_ROOT\\mycoolapp\\shell\n\n[HKEY_CLASSES_ROOT\\mycoolapp\\shell\\open\\command]\n@=\"\\\"C:\\\\MyTools\\\\MyLauncher.exe\\\" \\\"%1\\\"\"\n\nExplanation of the .reg file:\n\n[HKEY_CLASSES_ROOT\\mycoolapp]: Defines the root key for your custom scheme.\n@=\"URL:MyCoolApp Protocol\": A descriptive name for the protocol.\n\"URL Protocol\"=\"\": An empty string value that identifies this key as a custom protocol handler.\n[HKEY_CLASSES_ROOT\\mycoolapp\\shell\\open\\command]: Specifies the command to execute when the protocol is invoked.\n@=\"\\\"C:\\\\MyTools\\\\MyLauncher.exe\\\" \\\"%1\\\"\":\n    \"C:\\\\MyTools\\\\MyLauncher.exe\": The full path to your executable. Note the double backslashes for escaping in .reg files. The path itself is enclosed in quotes to handle spaces.\n    \\\"%1\\\": This is crucial. Windows will replace %1 with the full URI that was clicked (e.g., mycoolapp://some/info?value=test). The quotes around %1 ensure that the entire URI, even if it contains spaces (though typically not recommended in URIs), is passed as a single argument.\n\nTo import the .reg file:\n\nSave the content above into a file named register_mycoolapp.reg.\nDouble-click the file.\nApprove the User Account Control (UAC) prompt and confirm you want to add the information to the registry.\n\nStep 3: Create an HTML Link to Trigger the Protocol\n\nCreate a simple HTML file (e.g., launcher.html):\n\n\n\n    \n        App Launcher\n    \n    \n        Launch My Windows App\n        \n            \n                Launch MyCoolApp with Data\n            \n        \n        \n             Launch MyCoolApp (Simple) \n        \n\n        \n            function launchWithJS() {\n                window.location.href = \"mycoolapp://LaunchedViaJavaScript\";\n            }\n        \n        Launch via JavaScript\n    \n\n\nStep 4: Test It\n\nEnsure MyLauncher.exe is in C:\\MyTools\\.\nImport the .reg file.\nOpen launcher.html in your web browser (Edge, Chrome, Firefox, etc.).\nClick one of the links or the button.\n\nThe first time you click such a link, your browser will likely show a security prompt asking for permission to open the external application (MyCoolApp Protocol or MyLauncher.exe). Allow it.\n\nYour MyLauncher.exe should then run, and if you used the C# example, a MessageBox will appear showing the full URI that was passed as an argument.\n\nStep 5: Your Application Parses the Argument\n\nYour MyLauncher.exe receives mycoolapp://HelloFromBrowser?user=Alice&action=view as args[0]. It needs to:\n\nVerify that the argument starts with mycoolapp://.\nParse the rest of the string (e.g., path, query parameters) to decide what to do. Libraries for URI parsing can be helpful here.\n\n\nSecurity Considerations (Very Important!):\n\nUser Consent: Browsers will (and should) prompt the user for permission before launching an external application via a custom URI scheme. This is a crucial security feature.\nInput Sanitization: The URI (%1) passed to your application can be crafted by any website. Your application must treat this input as potentially malicious. Sanitize and validate it thoroughly before using it to open files, execute commands, or make system changes. Do not trust it blindly.\nNo Silent Execution: You cannot typically bypass the browser's confirmation dialog for security reasons.\nInstallation Required: The custom URI scheme must be registered on the user's system before the links will work. If it's not registered, the browser will report an error (e.g., \"Address was not understood\" or \"Unknown protocol\").\nLimited Scope: This is for launching an already installed application. It's not for distributing or installing software (though the launched application could then trigger an update or further downloads if designed to do so).\n\nThis method provides a powerful way to integrate web experiences with local desktop applications on Windows.",
        "locale": "en",
        "slug": "technical-posts/using-windows-custom-uri-schemes-to-open-a-windows-app-from-a-browser",
        "folder": ""
    },
    {
        "title": "Launch Chrome with PowerShell",
        "description": "Launch Chrome with PowerShell",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "start-process \"chrome\" \"file:C:\\Users\\thumb\\Documents\\myfile.html\"\n\nYou can also use the -nonewindow argument to keep from opening a dangling terminal window. However, you'll probably need to use a fully-qualified executable name with that.\n\nstart-process -filepath \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n              -argumentlist \"C:\\Users\\thumb\\Documents\\myfile.html\"\n              -NoNewWindow\n\nYou can omit the -filename and argumentlist arguments.\n\nor use msedge to load MS Edge.\n\nThese examples show loading a local file but you can also provide a URL.\n\nLaunch a browser for a dev url\n\nI wanted to load a Sveltekit dev URL in the browser for development. I tried this and it didn't work.\n\npnpm run dev\nstart-process \"msedge\" \"http://localhost:5188/release-sets\"\n\nWhile it's not intuitive, reversing the order of the commands does work:\n\nstart-process \"msedge\" \"http://localhost:5188/release-sets\"\npnpm run dev\n\nThis launches the browser local URL and throws an immediate 404. However as soon as the dev environment finishing spinning up the page requested is loaded.\n\nClosing the terminal\n\nIn some cases, you might need to use this to close the terminal.\n\n$host.SetShouldExit(0)",
        "locale": "en",
        "slug": "technical-posts/launch-chrome-with-powershell",
        "folder": ""
    },
    {
        "title": "gemini-opinion-on-app-state",
        "description": "gemini-opinion-on-app-state",
        "date_created": "2025-06-07T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "rogerpence: generate simple responsive css to make the nav bar a left-justtifed horizonal menu bar\n\nGitHub Copilot: I'll create a simple responsive CSS for your horizontal navigation bar. Let's add the styles to your NavBar component:\n\n\n\timport ShowContactUsForm from \"./ShowContactUsForm.svelte\";\n\n\n\n    \n        Home\n        About\n        Contact\n        \n    \n\n\n\n    nav {\n        width: 100%;\n        background-color: #f8f9fa;\n        padding: 0.5rem 1rem;\n    }\n\n    ul {\n        display: flex;\n        list-style: none;\n        margin: 0;\n        padding: 0;\n        justify-content: flex-start;\n        flex-wrap: wrap;\n    }\n\n    li {\n        margin-right: 1.5rem;\n        padding: 0.5rem 0;\n    }\n\n    a {\n        text-decoration: none;\n        color: #333;\n        font-weight: 500;\n        transition: color 0.2s;\n    }\n\n    a:hover {\n        color: #007bff;\n    }\n\n    /* Responsive adjustments */\n    @media (max-width: 600px) {\n        ul {\n            flex-direction: column;\n        }\n\n        li {\n            margin-right: 0;\n            margin-bottom: 0.5rem;\n        }\n    }\n\n\nThis CSS will:\nCreate a horizontal navigation bar with left-justified items\nApply basic styling for links and hover effects\nMake the navbar responsive - on screens smaller than 600px, it will switch to a vertical layout\nAdd subtle padding and spacing for better readability\nAllow the navbar to wrap if there are too many items for the screen width\n\nYou can adjust the colors, spacing, and breakpoint (600px) to match your site's design.\n\nSimilar code found with 1 license type\n\nrogerpence: how to get the page url in a sveltekit component\n\nGitHub Copilot: In SvelteKit, you can access the current page URL through the $page store. There's a small error in your import statement - let me fix that and show you how to get the URL:\n\n\n    import { page } from '$app/stores';\n\n    // Now you can access various URL properties\n    $: currentUrl = $page.url.href; // Full URL (e.g., https://example.com/path)\n    $: pathname = $page.url.pathname; // Just the path (e.g., /path)\n    $: searchParams = $page.url.searchParams; // URL search parameters\n\n\nCurrent URL: {currentUrl}\nPath: {pathname}\n\nThe key points:\nImport page from $app/stores (not @sveltekit/js)\nAccess it as a store with $page\nThe URL information is in $page.url, which is a standard URL object\n\nYou can access different parts of the URL:\n$page.url.href - Full URL\n$page.url.pathname - Just the path\n$page.url.search - Query string\n$page.url.searchParams - SearchParams object for parsing query parameters\n\nYou can also get other page-related info like:\n$page.params - Route parameters\n$page.route - Current route pattern\n\nrogerpence: this is ssvelte 5\n\nGitHub Copilot: For Svelte 5, the way you access page data is different as Svelte 5 introduces a new reactive primitive system. Here's how to access the URL in a Svelte 5 component:\n\n\n    import { page } from '$app/stores';\n\n    // In Svelte 5, you can use the $ shorthand directly for reactivity\n    // No need for $: reactive declarations\n\n    function getCurrentUrl() {\n        return page.url.href;\n    }\n\n    function getPathname() {\n        return page.url.pathname;\n    }\n\n\n\n    Current URL: {$page.url.href}\n    Path: {$page.url.pathname}\n\n    \n    Function URL: {getCurrentUrl()}\n\n\nKey differences in Svelte 5:\n\nSvelte 5 uses a fine-grained reactivity system with signals\nYou can still access the page store using $page syntax\nThe import from $app/stores remains the same\nYou can work with the data directly in template expressions or in functions\n\nIf you're using runes (Svelte 5's new reactivity syntax), you might do:\n\n\n    import { page } from '$app/stores';\n\n    // Using Svelte 5 runes for derived state\n    const $currentUrl = $derived($page.url.href);\n    const $pathname = $derived($page.url.pathname);\n\n\n\n    Current URL: {$currentUrl}\n    Path: {$pathname}\n\n\nNote that the specifics may vary depending on which version of Svelte 5 you're using, as it's still in development and some APIs may change before the final release.\n\nrogerpence: how can I import all of the types offered by listener-types\n\nGitHub Copilot: To import all of the types from your listener-types.ts file, you can use a wildcard import with the import * as syntax. Here's how to modify your global-listener.svelte.ts file to import all the types:\n\n// src/lib/sharedStore.ts\nimport { browser } from '$app/environment';\nimport * as ListenerTypes from '$lib/types/listener-types';\n\n// Define the structure of our store's state\ntype GlobalListener = {\n\tshowContactForm: boolean;\n\tshowToast: boolean; // Optional: Add more properties as needed\n\tpayload?: Record; // Optional payload object for additional data\n\tdialogCargo?: ListenerTypes.DialogCargo; // Add the DialogCargo type\n};\n\n// Create the reactive state using $state\n// Initialize with a default value\nlet appStore = $state({\n\tshowContactForm: false,\n\tshowToast: false, // Optional: Initialize other properties as needed\n\tpayload: {} // Optional: You can add a payload object if needed\n});\n\n// Optional: Log when the store module is loaded\nif (browser) {\n\tconsole.log('Shared store module loaded (client)');\n} else {\n\tconsole.log('Shared store module loaded (server)');\n}\n\n// --- Export the reactive store object ---\n// Components will import this 'sharedStore' object\n// and access/modify its properties directly (e.g., sharedStore.trackedString)\nexport const globalState = appStore;\n\n// --- Alternatively, you could export getter/setter functions ---\n// This provides more control but is often less direct with runes\n\nAlternatively, if you want to import specific types directly, you can use:\n\nimport { DialogTypes, DialogCargo } from '$lib/types/listener-types';\n\nNote that I've updated the import path to use the $lib alias which is commonly used in SvelteKit projects. If your project is configured differently, you might need to adjust the path accordingly (e.g., to '../types/listener-types' if using relative paths).\n\nrogerpence: the ToastCargo type doesn't include a dialogType property. Why isn't Typescript telling me that in the toastCargo definition above\n\nGitHub Copilot: You've spotted a significant issue in your code. TypeScript isn't catching the error because of a few problems:\n\nThe ToastCargo type definition uses dialogsAvailable instead of toastsAvailable for its type constraint\nYou're using dialogType in your toastCargo object, but the ToastCargo type expects toastType\nYou're also including a type: 'contact-us' property that isn't defined in the ToastCargo type\n\nHere's what's happening:\n\nIn your listener-types.ts file, ToastTypes is incorrectly defined as:\n   export type ToastTypes = (typeof dialogsAvailable)[keyof typeof dialogsAvailable];\n   It should be using toastsAvailable instead.\n\nYour ToastCargo type expects a toastType property, but you're providing dialogType in your implementation.\n\nTo fix this, you should:\n\nCorrect the ToastTypes definition in your listener-types.ts file:\n   export type ToastTypes = (typeof toastsAvailable)[keyof typeof toastsAvailable];\n\nUpdate your ShowToast.svelte component to use the correct properties:\nconst toastCargo : ListenerTypes.ToastCargo = {\n       toastType: ListenerTypes.toastsAvailable.Warning, // Change to use toastType and toastsAvailable\n       heading: 'Contact us',\n       text: 'This is the contact us form'\n       // Remove the type: 'contact-us' as it's not in the ToastCargo type\n   }\n\nTypeScript isn't catching this because you're incorrectly using dialogsAvailable in the ToastTypes definition, which means your toastCargo object with dialogType property actually satisfies the incorrect type constraint. This is a perfect example of why correctly defining your types is so important!\n\nrogerpence: have fixed that issue, Typescript is still not showing an issue with the toastCargo declaration\n\nGitHub Copilot: You're right - TypeScript is still not catching the issue with the toastCargo declaration. There are a few key reasons for this:\n\nYou're using ToastCargo as the type, but using dialogType instead of toastType\nYou're adding an extra property type: 'contact-us' that isn't in the ToastCargo type\nYou're using dialogsAvailable values when you should be using toastsAvailable\n\nTypeScript isn't complaining because the union type definition for ToastCargo includes an empty object {} as an alternative:\n\nexport type ToastCargo =\n    | { toastType: ToastTypes; heading: string; text: string; }\n    | {};\n\nBecause of this empty object alternative, TypeScript is treating your object as the second case of the union type (the empty object {}), and then not enforcing the properties at all. Any object will satisfy the {} type constraint, which is why you're not seeing errors.\n\nTo fix this:\n\nChange the type definition to remove the empty object alternative:\n\nexport type ToastCargo = {\n    toastType: ToastTypes;\n    heading: string;\n    text: string;\n};\n\nFix your actual toastCargo definition:\n\n\n    import {globalState} from '$lib/components/global-listener.svelte'\n    import * as ListenerTypes from '$lib/types/listener-types';\n\n    const toastCargo : ListenerTypes.ToastCargo = {\n        toastType: ListenerTypes.toastsAvailable.Warning, // Changed to toastType and toastsAvailable\n        heading: 'Contact us',\n        text: 'This is the contact us form'\n        // Removed the type: 'contact-us' property\n    }\n\n    function showContactUs() {\n        globalState.showContactForm = true;\n        globalState.payload = toastCargo;\n    }\n\n\nThe empty object alternative in your union type is a common source of TypeScript errors because it effectively makes the type check optional. If you're expecting strict type checking, avoid using empty object alternatives in union types unless you specifically need that flexibility.\n\nrogerpence: how can I best determine that globalState.payload is either a DialogCargo or ToastCargo type in the $effect on line 28\n\nGitHub Copilot: To determine whether globalState.payload is a DialogCargo or ToastCargo in an $effect, you can use TypeScript's type narrowing with type discriminators. Since these types don't have a common property that distinguishes them, you should add one.\n\nHere's how to implement this:\n\nFirst, modify your type definitions to include a discriminator property:\n\nexport const dialogsAvailable = {\n\tContactUs: 'Contact Us',\n\tDecadeOfCrisis: 'A Decade of Crisis'\n} as const;\n\nexport type DialogTypes = (typeof dialogsAvailable)[keyof typeof dialogsAvailable];\n\nexport type DialogCargo = {\n\tkind: 'dialog'; // Discriminator property\n\tdialogType: DialogTypes;\n\theading: string;\n\ttext: string;\n\tshowCancelButton: boolean;\n};\n\nexport const toastsAvailable = {\n\tWarning: 'Warning',\n\tConfirm: 'Confirm'\n} as const;\n\nexport type ToastTypes = (typeof toastsAvailable)[keyof typeof toastsAvailable];\n\nexport type ToastCargo = {\n\tkind: 'toast'; // Discriminator property\n\ttoastType: ToastTypes;\n\theading: string;\n\ttext: string;\n};\n\nThen in your components, make sure to include the discriminator when creating objects:\n\n\nconst toastCargo: ListenerTypes.ToastCargo = {\n    kind: 'toast',\n    toastType: ListenerTypes.toastsAvailable.Warning,\n    heading: 'A warm toast',\n    text: 'This is the contact us form',\n}\n\nNow you can use the discriminator in your $effect:\n\n// In your component where you handle the payload\n$effect(() => {\n    if (!globalState.payload) return;\n\n    if (globalState.payload.kind === 'dialog') {\n        // TypeScript now knows this is DialogCargo\n        const dialog = globalState.payload;\n        console.log(Dialog: ${dialog.dialogType} - ${dialog.heading});\n        // Handle dialog\n    } else if (globalState.payload.kind === 'toast') {\n        // TypeScript now knows this is ToastCargo\n        const toast = globalState.payload;\n        console.log(Toast: ${toast.toastType} - ${toast.heading});\n        // Handle toast\n    }\n});\n\nThis approach is called \"discriminated unions\" or \"tagged unions\" in TypeScript. It's a clean way to handle different types in a type-safe manner.\n\nAlternatively, if you don't want to modify your type definitions, you can use type guards:\n\n// Define type guards\nfunction isDialogCargo(payload: any): payload is ListenerTypes.DialogCargo {\n    return 'dialogType' in payload && 'showCancelButton' in payload;\n}\n\nfunction isToastCargo(payload: any): payload is ListenerTypes.ToastCargo {\n    return 'toastType' in payload && !('showCancelButton' in payload);\n}\n\n// Use in effect\n$effect(() => {\n    if (!globalState.payload) return;\n\n    if (isDialogCargo(globalState.payload)) {\n        // TypeScript knows this is DialogCargo\n        console.log(Dialog: ${globalState.payload.dialogType});\n    } else if (isToastCargo(globalState.payload)) {\n        // TypeScript knows this is ToastCargo\n        console.log(Toast: ${globalState.payload.toastType});\n    }\n});\n\nThe discriminator approach is generally preferred as it's more explicit and less error-prone.\n\nrogerpence: is the discriminator approach a well-known best practice. It's seems a little klunky, but it is certain more direct that using the two functions to test the type\n\nGitHub Copilot: Yes, the discriminator approach (also known as \"tagged unions\" or \"discriminated unions\") is a well-established best practice in TypeScript. It's widely used in many production codebases and recommended by TypeScript experts.\n\nBenefits of the discriminator approach:\n\nType safety - TypeScript can perform exhaustive checks at compile time\nPredictability - The code is more explicit about what types are being handled\nSelf-documenting - The \"kind\" property makes the intent clear to other developers\nRefactoring safety - If you add a new type to the union, TypeScript will force you to handle it\n\nComparison to type guards:\n\nThe type guard approach with two functions is more indirect:\nRequires maintaining separate functions\nCan break if properties change\nRelies on presence/absence of properties which might be coincidental\nCan be error-prone with similar types\n\nWhile adding a kind property may feel like extra boilerplate, it's typically worth it for the improved type safety and developer experience.\n\nReal-world examples:\n\nThis pattern is used in many popular libraries:\nRedux actions (with a type property)\nReact component libraries\nState management libraries like XState\n\nIf the explicit kind property feels too verbose, you could use existing properties if they're mutually exclusive:\n\nexport type DialogCargo = {\n    dialogType: DialogTypes;  // This property only exists on DialogCargo\n    heading: string;\n    text: string;\n    showCancelButton: boolean;\n};\n\nexport type ToastCargo = {\n    toastType: ToastTypes;  // This property only exists on ToastCargo\n    heading: string;\n    text: string;\n};\n\n// In your effect:\n$effect(() => {\n    if (!globalState.payload) return;\n\n    if ('dialogType' in globalState.payload) {\n        // Handle dialog\n    } else if ('toastType' in globalState.payload) {\n        // Handle toast\n    }\n});\n\nThis works but is less explicit than the discriminator approach. For maintainability and clarity, especially in larger codebases, the discriminator is generally preferred.\n\nrogerpence: how can move $effect code in Svelte 5 code to a subroutine? Should I use $Derived.by?\n\nGitHub Copilot: In Svelte 5, you can extract the logic from your $effect into a subroutine using a few different approaches. Let me show you the most appropriate options:\n\nOption 1: Use a Regular Function\n\nThe simplest approach is to extract the logic into a regular function and call it within the $effect:\n\n\n    import {globalState} from '$lib/components/global-listener.svelte'\n    import {page} from '$app/state';\n    import NavBar from '$lib/components/NavBar.svelte';\n    import ShowContactUsForm from '$lib/components/ShowContactUsForm.svelte';\n    import * as ListenerTypes from '$lib/types/listener-types';\n\n    import '../style.css'\n    let {children} = $props();\n    const url = $derived(page.url);\n\n    const showContactForm = page.url.searchParams.has('cu') ?? false\n\n    // Extract the logic to a function\n    function handleGlobalStateChanges() {\n        if (globalState.showContactForm) {\n            if (globalState.payload?.cargoType === ListenerTypes.cargoTypes.Dialog) {\n                alert(globalState.payload?.cargoType);\n            }\n            globalState.showContactForm = false;\n        }\n        else if (globalState.showToast) {\n            if (globalState.payload?.cargoType === ListenerTypes.cargoTypes.Toast) {\n                alert(globalState.payload.heading);\n            }\n            globalState.showToast = false;\n        }\n    }\n\n    // Call the function in the effect\n    $effect(() => {\n        handleGlobalStateChanges();\n    });\n\nOption 2: Use $effect.root() for Self-Contained Effects\n\nIf you want to create a reusable effect that can be called from multiple places:\n\n\n    import {globalState} from '$lib/components/global-listener.svelte'\n    import {page} from '$app/state';\n    import NavBar from '$lib/components/NavBar.svelte';\n    import ShowContactUsForm from '$lib/components/ShowContactUsForm.svelte';\n    import * as ListenerTypes from '$lib/types/listener-types';\n\n    import '../style.css'\n    let {children} = $props();\n    const url = $derived(page.url);\n\n    const showContactForm = page.url.searchParams.has('cu') ?? false\n\n    // Create a reusable effect function\n    function createGlobalStateHandler() {\n        return $effect.root(() => {\n            if (globalState.showContactForm) {\n                if (globalState.payload?.cargoType === ListenerTypes.cargoTypes.Dialog) {\n                    alert(globalState.payload?.cargoType);\n                }\n                globalState.showContactForm = false;\n            }\n            else if (globalState.showToast) {\n                if (globalState.payload?.cargoType === ListenerTypes.cargoTypes.Toast) {\n                    alert(globalState.payload.heading);\n                }\n                globalState.showToast = false;\n            }\n        });\n    }\n\n    // Initialize the effect\n    createGlobalStateHandler();\n\nOption 3: Using $derived.by()\n\n$derived.by() is useful when you want to compute a value based on reactive dependencies. It's not meant for side effects like showing alerts, but you could use it to compute a state that then triggers an effect:\n\n\n    import {globalState} from '$lib/components/global-listener.svelte'\n    import {page} from '$app/state';\n    import NavBar from '$lib/components/NavBar.svelte';\n    import ShowContactUsForm from '$lib/components/ShowContactUsForm.svelte';\n    import * as ListenerTypes from '$lib/types/listener-types';\n\n    import '../style.css'\n    let {children} = $props();\n    const url = $derived(page.url);\n\n    const showContactForm = page.url.searchParams.has('cu') ?? false\n\n    // Compute derived state\n    const activeMessage = $derived.by(() => {\n        if (globalState.showContactForm && globalState.payload?.cargoType === ListenerTypes.cargoTypes.Dialog) {\n            return {\n                type: 'dialog',\n                text: globalState.payload.cargoType\n            };\n        }\n        else if (globalState.showToast && globalState.payload?.cargoType === ListenerTypes.cargoTypes.Toast) {\n            return {\n                type: 'toast',\n                text: globalState.payload.heading\n            };\n        }\n        return null;\n    });\n\n    // Use effect to handle side effects based on derived state\n    $effect(() => {\n        if (activeMessage) {\n            alert(activeMessage.text);\n\n            // Reset state\n            if (activeMessage.type === 'dialog') {\n                globalState.showContactForm = false;\n            } else if (activeMessage.type === 'toast') {\n                globalState.showToast = false;\n            }\n        }\n    });\n\nRecommendation\n\nFor your specific case, Option 1 is the simplest and most appropriate. You're dealing with side effects (showing alerts and modifying state), which is exactly what $effect is designed for. Extracting the logic to a regular function keeps your code clean while maintaining the reactive behavior.\n\n$derived.by() is more suitable for computing values without side effects, not for handling the type of logic you have in your current code.\n\nrogerpence: in option #1, does the $effect effectively look into the handleGlobalStateChanges to see if a $state has changed\n\nGitHub Copilot: In Option #1, the $effect doesn't \"look into\" the function - instead, it watches all reactive dependencies that are accessed inside the function when it runs.\n\nHere's how it works:\n\nWhen the $effect runs, it calls handleGlobalStateChanges()\nDuring this function call, any reactive state that's accessed (like globalState.showContactForm) is tracked\nThe $effect will re-run whenever any of these tracked dependencies change\n\nThis means:\n\n$effect(() => {\n    handleGlobalStateChanges();\n});\n\nfunction handleGlobalStateChanges() {\n    // These are all tracked by the $effect:\n    if (globalState.showContactForm) {\n        // ...\n    }\n    else if (globalState.showToast) {\n        // ...\n    }\n}\n\nSo yes, the reactive system in Svelte 5 will detect changes to any reactive state used inside handleGlobalStateChanges() and re-run the effect when those values change.\n\nThe key insight is that reactivity tracking in Svelte 5 happens at runtime during the execution of the function, not by statically analyzing the function's code. This is why extracting logic into a separate function works well - the reactivity is preserved.\n\nThis is different from some other frameworks where moving code to a separate function might break reactivity tracking.\n\n\n\nrogerpence: I have an existing Svelte 4 app that is a tangled wad of spaghetti with several stroes regarding global state. I am thinking of rewriting that app with SvelteKit and Svelte 5 and TypeScript. Am I am on the right track here using a single rune to track the global state.  I'll probably need to add a few more boolean flags to GlobalListener for other state changes.\n\nGitHub Copilot: Based on your code, you're definitely on the right track for modernizing your application with SvelteKit and Svelte 5. Your approach to global state management using Svelte 5's runes is clean and well-structured compared to multiple entangled stores in Svelte 4.\n\nWhat You're Doing Right\n\nSingle Source of Truth: Using a single globalState object with Svelte 5's $state is an excellent approach. This centralizes your state management and avoids the \"wad of spaghetti\" problem.\n\nType Safety: Your TypeScript definitions are well-structured with discriminated unions for DialogCargo and ToastCargo.\n\nProper State Handling: The handleGlobalStateChanges function in your layout demonstrates a good pattern for reacting to state changes.\n\nClean Organization: Separating your types into their own module (listener-types.ts) is a good practice.\n\nSuggestions for Improvement\n\nConsider Action Creators: As your app grows, you might want to add action creator functions to your global-listener.svelte.ts:\n\n// Add to global-listener.svelte.ts\nexport function showToastNotification(toast: ListenerTypes.ToastCargo) {\n    globalState.payload = toast;\n    globalState.showToast = true;\n}\n\nexport function showDialog(dialog: ListenerTypes.DialogCargo) {\n    globalState.payload = dialog;\n    globalState.showContactForm = true;\n}\n\nMore Descriptive State Names: Consider renaming some flags for clarity:\n\ntype GlobalListener = {\n    isDialogVisible: boolean;  // instead of showContactForm\n    isToastVisible: boolean;   // instead of showToast\n    activePayload?: ListenerTypes.DialogCargo | ListenerTypes.ToastCargo;\n    // other state flags\n};\n\nComponent-Specific State: For state that's truly specific to one component, you might not need to add it to global state. Consider using local component state:\n\n\n    let isMenuOpen = $state(false);\n\n\nBetter Type Narrowing: The cargoType discriminator is great. Use it more effectively:\n\nfunction handleGlobalStateChanges() {\n    if (globalState.showContactForm && globalState.payload) {\n        if (globalState.payload.cargoType === ListenerTypes.cargoTypes.Dialog) {\n            // TypeScript knows this is DialogCargo now\n            console.log(globalState.payload.dialogType);\n        }\n        globalState.showContactForm = false;\n    }\n    // ...\n}\n\nOverall Assessment\n\nYour approach is solid and modern. Using a single global state with Svelte 5's runes is much cleaner than multiple stores. The type system you've set up will help prevent bugs and make refactoring easier.\n\nThis pattern scales well as you add more features - just add the appropriate state flags to your GlobalListener type and handle them in your effects or components.\n\nA few things to keep in mind:\nOnly put truly global state in your global store\nConsider breaking up the UI handlers if the handleGlobalStateChanges function gets too large\nAdd unit tests for your state management logic\n\nYou're definitely on the right track for a more maintainable application!",
        "locale": "en",
        "slug": "technical-posts/gemini-opinion-on-app-state",
        "folder": ""
    },
    {
        "title": "Send an email from Sveltekit",
        "description": "Send an email from Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "import { fail } from '@sveltejs/kit';\nimport { superValidate } from 'sveltekit-superforms/server';\nimport { formatDate } from '$lib/utility';\nimport { sendEmail } from '$lib/sendemail.js';\nimport { redirect } from '@sveltejs/kit';\nexport const prerender = false;\n\nimport RequestProductDownloadSchema from './RequestProductDownloadSchema.js';\n\nlet parentUrl\n\nexport const load = async (event) => {\n\tparentUrl = event.route.id\n\tconst form = await superValidate(event, RequestProductDownloadSchema);\n\n\treturn {\n\t\tform\n\t};\n};\n\nconst getEmailBody = (form, url) => {\n\tconst subscribe = form.answer2 ? 'yes ' : 'no';\n\n\treturn `\n${form.data.first_name},\n\nYour ASNA download link for ${form.data.product} is below.`\n};\n\nexport const actions = {\n\tsendEmail: async (event) => {\n\t\tconst form = await superValidate(event, RequestProductDownloadSchema);\n\n\t\tif (!form.valid) {\n\t\t\treturn fail(400, {\n\t\t\t\tform\n\t\t\t});\n\t\t}\n\n\t\tconst messageTemplate = getEmailBody(form, url);\n\t\tconst email = form.data.email;\n\t\tconst subject = form.data.subject;\n\t\tconst message = messageTemplate.trim();\n\t\tconst country = form.data.country;\n\n\t\ttry {\n\t\t\tawait sendEmail(email, subject, message);\n\t\t} catch (except) {\n\t\t\tconsole.log(JSON.stringify(except));\n\t\t\tconst errorData = {\n\t\t\t\terrorDetail: except.response,\n\t\t\t\tuser: EMAIL_USER,\n\t\t\t\tpassword: EMAIL_PASSWORD,\n\t\t\t\tfrom: EMAIL_FROM\n\t\t\t};\n\t\t\tlogError(errorData);\n\t\t\tconst errorMessage = except.response.replaceAll(';', '');\n\t\t\tthrow error(412, errorMessage);\n\t\t}\n\n\t\tthrow redirect(\n\t\t\t303,\n\t\t\tparentUrl\n\t\t);\n\t}\n};",
        "locale": "en",
        "slug": "technical-posts/send-email",
        "folder": ""
    },
    {
        "title": "Writeable store in Sveltekit",
        "description": "Writeable store in Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "I'm not sure what I'm doing here... I think there are other ways to pass data from a layout to a child page. At the very least, this shows how to use a strongly-typed\n\nSvelte docs\n\nExample here:\n\nC:\\Users\\thumb\\Documents\\Projects\\svelte\\focused\\stores\n\nlayout +layout.svelte\n\n\n\timport { setContext, getContext } from 'svelte';\n\timport { writable, type Writable } from 'svelte/store';\n\t/** @type {import('./$types').LayoutData} */\n\n\t// Create a store and update it when necessary...\n\tconst user: Writable = writable('');\n\n\t$: user.set('roger pence');\n\t// ...and add it to the context for child components to access\n\tsetContext('user', user);\n\ndirect child +page.svelte\n\n\n\timport { getContext } from 'svelte';\n\t// Retrieve user store from context\n\tconst user = getContext('user');\n\n\nWelcome to SvelteKit\n\nAbout\nContact\n\nWelcome {$user}",
        "locale": "en",
        "slug": "technical-posts/putting-a-writeable-store-in-the-context-object",
        "folder": ""
    },
    {
        "title": "Obsidian datavew versus datavewjs",
        "description": "Obsidian datavew versus datavewjs",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "obsidian",
            "dataviewjs"
        ],
        "content": "Sometimes Dataviewjs queries are better than Dataview queries. This is especially true when you need case-insensitive or partial-match queries.\n\n[!info]\nThe language specifier in the following code should either be dataview or dataviewjs. The js or sql identifiers used are for presentation purposes (and they keep Obsidian from thinking these queries should be active--instead of just showing the source code).\n\nSearch tags\n\ndataview query\n\nChanging this query from a dataview to a dataviewjs query didn't buy very much. It mostly makes it easier to show a message when the include_tag is empty or not found.\n\nTABLE WITHOUT ID\n    file.link AS \"Document\",\n    file.folder as \"Folder\",\n    description AS \"Description\",\n    join(tags, \", \") AS \"Tags\"\nFROM \"/\"\nWHERE contains(tags, this.include_tag)\nSORT file.link ASC\n\ndataviewjs query\n\n// --- Configuration ---\n// Get the tag to search for from the current file's metadata (frontmatter)\nconst includeTag = dv.current().include_tag;\n\n// --- Logic & Output ---\n// 1. Check if the input tag is provided\nif (!includeTag || typeof includeTag !== \"string\" || includeTag.trim() === \"\") {\n    // Display message if includeTag is missing, not a string, or empty\n    dv.paragraph(\n        \" Error: Please enter a tag value in the includetag field in the frontmatter of this note.\"\n    );\n} else {\n    // 2. Proceed only if includeTag is valid\n    const includeTagLower = includeTag.trim().toLowerCase(); // Trim whitespace and lowercase for matching\n\n    // Define the headers for the table\n    const headers = [\"Document\", \"Folder\", \"Description\", \"Tags\"];\n\n    // 3. Query pages, filter, sort, and map to table rows\n    const pages = dv\n        .pages('\"/\"') // Corresponds to FROM \"/\"\n        .where((p) => {\n            // Corresponds to WHERE clause - checking tags now\n            // Check if the page HAS tags and if ANY tag matches the criteria\n            return (\n                p.tags &&\n                Array.isArray(p.tags) &&\n                p.tags.some(\n                    (tag) =>\n                        // Ensure the tag is a string before processing\n                        typeof tag === \"string\" &&\n                        // Convert page tag to lowercase and check if it starts with the input tag (case-insensitive prefix match)\n                        tag.toLowerCase().startsWith(includeTagLower)\n                )\n            );\n        })\n        .sort((p) => p.file.link, \"asc\") // Corresponds to SORT file.link ASC\n        .map((p) => [\n            // Selects and formats the columns for the table\n            p.file.link, // file.link AS \"Document\"\n            p.file.folder, // file.folder as \"Folder\"\n            p.description || \"\", // description AS \"Description\" (handle missing description)\n            (p.tags || []).join(\", \"), // join(tags, \", \") AS \"Tags\" (handle missing tags)\n        ]);\n\n    // 4. Conditional Output\n    if (pages.length > 0) {\n        // Display the table if pages were found\n        dv.table(headers, pages);\n    } else {\n        // Display a message if no pages were found matching the tag prefix\n        dv.paragraph(\n            No documents found with a tag starting with \"${includeTag}\" (case-insensitive).\n        );\n    }\n}\n\nChanges and Explanation:\n\nInput Field: Changed dv.current().search to dv.current().includetag.\nInput Validation: Added a more robust check:\n    !includeTag: Checks if the field exists and isn't null/undefined.\n    typeof includeTag !== 'string': Ensures the value is actually text.\n    includeTag.trim() === '': Checks if the string is empty after removing leading/trailing whitespace.\n    If any of these are true, the specific error message is shown.\nTag Processing:\n    includeTag.trim().toLowerCase(): The input tag is trimmed of whitespace and converted to lowercase before the filtering starts.\nFiltering Logic (.where()):\n    p.tags && Array.isArray(p.tags): First, it checks if the page actually has a tags field and if that field is an array (which it normally should be in Obsidian). This prevents errors.\n    .some(tag => ...): This array method checks if at least one element in the p.tags array satisfies the condition inside the arrow function. It's efficient because it stops searching as soon as it finds a match.\n    typeof tag === 'string': Ensures the individual tag being checked is a string.\n    tag.toLowerCase().startsWith(includeTagLower): This is the core matching logic.\n        tag.toLowerCase(): Converts the tag from the page to lowercase.\n        .startsWith(includeTagLower): Checks if the lowercased page tag begins with the lowercased input tag. This performs the case-insensitive prefix match you wanted (e.g., \"hel\" matches \"#Hello/There\", \"hello\", \"#help\").\nOutput: The rest of the query (headers, sorting, mapping, table display, \"no results\" message) remains largely the same structure, just adapted to reflect that we're searching tags now. The \"no results\" message is updated for clarity.\n\nSearch file/desc\n\nConverting this query from a dataview to a dataviewjs query increased its functionality quite a bit. With dataview you have to use containwords for case insensitive queries (contains isn't case sensitive and I had trouble trying to use JavaScript's toLowerCase with it). The dataviewjs query is better because:\n\nit is case-insensitive\nit works with partial search values\n\ndataview query\n\nTABLE WITHOUT ID\n    file.link AS \"Document\",\n    file.folder as \"Folder\",\n    description AS \"Description\",\n    join(tags, \", \") AS \"Tags\"\nFROM \"/\"\nWHERE containsword(file.name, this.search) OR containsword(description, this.search)\nSORT file.link ASC\n\ndataviewjs query\n\n// --- Configuration ---\n// Get the search term from the current file's metadata (frontmatter)\nconst searchTerm = dv.current().search;\n\n// --- Logic & Output ---\nif (!searchTerm) {\n    // Display error message if search term is missing\n    dv.paragraph(\n        \" Error: Please define a search field in the frontmatter of this note.\"\n    );\n} else {\n    // Proceed only if searchTerm is defined\n    const searchTermLower = searchTerm.toLowerCase(); // Convert search term for case-insensitive matching\n\n    // Define the headers for the table\n    const headers = [\"Document\", \"Folder\", \"Description\", \"Tags\"];\n\n    // Query pages, filter, sort, and map to table rows\n    const pages = dv\n        .pages('\"/\"') // Corresponds to FROM \"/\"\n        .where((p) => {\n            // Corresponds to WHERE clause\n            // Check if file name contains the search term (case-insensitive)\n            const nameMatch = p.file.name\n                .toLowerCase()\n                .includes(searchTermLower);\n            // Check if description exists and contains the search term (case-insensitive)\n            // Added check for description existence and type safety\n            const descriptionMatch =\n                p.description &&\n                typeof p.description === \"string\" &&\n                p.description.toLowerCase().includes(searchTermLower);\n            // Return true if either matches (OR condition)\n            return nameMatch || descriptionMatch;\n        })\n        .sort((p) => p.file.link, \"asc\") // Corresponds to SORT file.link ASC\n        .map((p) => [\n            // Selects and formats the columns for the table\n            p.file.link, // file.link AS \"Document\"\n            p.file.folder, // file.folder as \"Folder\"\n            p.description || \"\", // description AS \"Description\" (handle missing description)\n            (p.tags || []).join(\", \"), // join(tags, \", \") AS \"Tags\" (handle missing tags)\n        ]);\n\n    // --- Conditional Output ---\n    if (pages.length > 0) {\n        // Display the table if pages were found\n        dv.table(headers, pages);\n    } else {\n        // Display a message if no pages were found\n        dv.paragraph(\n            No documents found containing \"${searchTerm}\" in the file name or description.\n        );\n    }\n}",
        "locale": "en",
        "slug": "technical-posts/obsidian-dataveiw-versus-dataviewjs",
        "folder": ""
    },
    {
        "title": "working-with-budget-category-patterns",
        "description": "working-with-budget-category-patterns",
        "date_created": "2025-07-03T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "budget"
        ],
        "content": "Create and populate ExcludePatterns table\n\n-- Create the table\nCREATE TABLE dbo].[ExcludePatterns] (\n    [Id] INT IDENTITY(1,1) PRIMARY KEY,\n    [Pattern] NVARCHAR(255) NOT NULL,\n    [Description] NVARCHAR(500) NULL,\n    [IsActive] BIT DEFAULT 1,\n    [CreatedDate] DATETIME2 DEFAULT GETDATE()\n);\n\n-- Insert your patterns\nINSERT INTO [dbo].[ExcludePatterns, [Description]) VALUES\n('ACH%', 'ACH transactions'),\n('ACCTVERIFY%', 'Account verification'),\n('ATM %', 'ATM transactions'),\n('AUTOPAY%', 'Auto pay transactions'),\n('Bill Paid%', 'Bill payment transactions'),\n('CHECK #%', 'Check transactions'),\n('DBT CRD%', 'Debit card transactions'),\n('FROM CHECKING%', 'Checking account transfers'),\n('DDA REGULAR%', 'DDA regular transactions'),\n('MONEYLINK %', 'MoneyLink transactions'),\n('FROM SAVINGS%', 'Savings account transfers'),\n('ONLINE PMT%', 'Online payments'),\n('INTERNET BILL PAYMENT%', 'Internet bill payments'),\n('INTERNET CHECK%', 'Internet check payments'),\n('INST XFER PAYPAL%', 'PayPal instant transfers'),\n('PAYMENT AMALGAMATED%', 'Amalgamated payments'),\n('PAYPAL TRANSFER', 'PayPal transfers'),\n('%ASNA, INC. PPD%', 'ASNA payroll deposits'),\n('PAYROLL ASNA%', 'ASNA payroll'),\n('POS CRE%', 'POS credit transactions'),\n('POS DEB%', 'POS debit transactions'),\n('PRE AUTH%', 'Pre-authorized transactions'),\n('TRM#%', 'Terminal transactions'),\n('TRSF FROM%', 'Transfer from transactions'),\n('USATAX%', 'Tax transactions'),\n('WITHDRAWAL%', 'Withdrawal transactions'),\n('ZELLE TRANSFER%', 'Zelle transfers'),\n('ROGER MEDI%', 'Roger medical payments'),\n('REGULAR%', 'Regular transactions');\n\nQuery to test \n\nSELECT DISTINCT \n    COALESCE([rpb].[description], [category].[description]) AS description,\n    COALESCE([category].[rule], 'equals') AS [rule],\n    COALESCE([category].[value], [rpb].[description]) AS value,\n    COALESCE([category].[category], 'uncategorized') AS category\nFROM rpbudget AS rpb\nFULL OUTER JOIN categoryrules AS category \n   ON rpb.description = category.description\nWHERE (\n    NOT EXISTS (\n        SELECT 1 FROM [dbo].[ExcludePatterns] ep\n        WHERE ep.[IsActive] = 1 \n        AND [rpb].[description] LIKE ep.[Pattern]\n    )\n    AND YEAR([rpb].[postdate]) = 2025\n) OR [rpb].[description] IS NULL\nORDER BY description;\n\nStored procedure \nCREATE PROCEDURE [dbo].[GetBudgetCategoryDataWithExclusions]\n    @Year INT = 2025\nAS\nBEGIN\n    SET NOCOUNT ON;\n    \n    SELECT DISTINCT \n        COALESCE([rpb].[description], [category].[description]) AS description,\n        COALESCE([category].[rule], 'equals') AS [rule],\n        COALESCE([category].[value], [rpb].[description]) AS value,\n        COALESCE([category].[category], 'uncategorized') AS category\n    FROM rpbudget AS rpb\n    FULL OUTER JOIN categoryrules AS category \n       ON rpb.description = category.description\n    WHERE (\n        NOT EXISTS (\n            SELECT 1 FROM [dbo].[ExcludePatterns] ep\n            WHERE ep.[IsActive] = 1 \n            AND [rpb].[description] LIKE ep.[Pattern]\n        )\n        AND YEAR([rpb].[postdate]) = @Year\n    ) OR [rpb].[description] IS NULL\n    ORDER BY description;\nEND\n\n\nGreat question! The NOT EXISTS clause is indeed amazing and powerful. Let me break it down step by step:\n\nWhat NOT EXISTS does:\n\nThe NOT EXISTS clause returns TRUE when the subquery returns no rows. It's like asking: \"Show me records where this condition does NOT exist.\"\n\nBreaking down your WHERE clause:\n\nWHERE (\n    NOT EXISTS (\n        SELECT 1 FROM [dbo].[ExcludePatterns] ep\n        WHERE ep.[IsActive] = 1 \n        AND [rpb].[description] LIKE ep.[Pattern]\n    )\n    AND YEAR([rpb].[postdate]) = 2025\n) OR [rpb].[description] IS NULL\n\nStep-by-step explanation:\n\n1. The subquery runs for each row in your main query:\nSELECT 1 FROM [dbo].[ExcludePatterns] ep\nWHERE ep.[IsActive] = 1 \nAND [rpb].[description] LIKE ep.[Pattern]\n\nFor each rpb.description (like \"WALMART PURCHASE\"), this subquery:\nLooks at all active patterns in the ExcludePatterns table\nChecks if \"WALMART PURCHASE\" matches ANY of those patterns\nIf it finds a match, it returns 1 (or any value - the actual value doesn't matter)\nIf no match is found, it returns no rows\n\n2. The NOT EXISTS evaluates the subquery result:\nIf the subquery returns any rows  NOT EXISTS = FALSE  exclude this record\nIf the subquery returns no rows  NOT EXISTS = TRUE  include this record\n\n3. Visual example:\n\nLet's say you have these patterns in your ExcludePatterns table:\nACH%\nATM %\nPAYPAL%\n\nAnd your rpbudget has these descriptions:\n\"WALMART PURCHASE\"  Subquery finds no matching patterns  NOT EXISTS = TRUE  INCLUDE\n\"ACH DEPOSIT\"  Subquery finds ACH% pattern  NOT EXISTS = FALSE  EXCLUDE\n\"ATM WITHDRAWAL\"  Subquery finds ATM % pattern  NOT EXISTS = FALSE  EXCLUDE\n\"GROCERY STORE\"  Subquery finds no matching patterns  NOT EXISTS = TRUE  INCLUDE\n\nWhy this is amazing:\n\nDynamic: Add/remove patterns without changing your query\nEfficient: SQL Server can optimize this very well\nReadable: The logic is clear - \"exclude if it matches any forbidden pattern\"\nMaintainable: All your business rules are in one table\n\nThe OR [rpb].[description] IS NULL part:\n\nThis preserves rows from your FULL OUTER JOIN where there's no matching rpb record (only category rules exist). Without this, you'd lose category-only records.\n\nAlternative way to think about it:\n\nYour old approach was like:\nWHERE description NOT LIKE 'ACH%' \n  AND description NOT LIKE 'ATM %' \n  AND description NOT LIKE 'PAYPAL%'\n\nThe NOT EXISTS approach is like:\nWHERE description doesn't match any pattern in my forbidden list\n\nIt's essentially the same logic, but infinitely more flexible!\n\nhow is this, by selecting only 1 row, looking at all active patterns:  \nSELECT 1 FROM [dbo].[ExcludePatterns] ep  \nWHERE ep.[IsActive] = 1  \nAND [rpb].[description] LIKE ep.[Pattern]\n\nGreat question! You're thinking about this correctly, but there's a key concept to understand about how NOT EXISTS works with subqueries.\n\nThe subquery doesn't select \"only 1 row\" - it potentially checks ALL patterns!\n\nLet me explain what actually happens:\n\nHow the subquery executes:\n\nSELECT 1 FROM [dbo].[ExcludePatterns] ep\nWHERE ep.[IsActive] = 1 \nAND [rpb].[description] LIKE ep.[Pattern]\n\nThis subquery:\nScans through ALL active patterns in the ExcludePatterns table\nFor each pattern, it checks if [rpb].[description] LIKE ep.[Pattern]\nAs soon as it finds ONE match, it returns 1 and stops looking\nIf no patterns match, it returns no rows\n\nVisual example:\n\nLet's say your ExcludePatterns table has:\nACH% (Active)\nATM % (Active)\nPAYPAL% (Active)\nOLD_PATTERN% (Inactive)\n\nAnd you're checking [rpb].[description] = \"ATM WITHDRAWAL\":\n\n-- The subquery effectively does this:\n-- Check pattern 1: \"ATM WITHDRAWAL\" LIKE \"ACH%\"  FALSE\n-- Check pattern 2: \"ATM WITHDRAWAL\" LIKE \"ATM %\"  TRUE! \n-- Found a match! Return 1 and stop checking\n-- (Never checks PAYPAL% or OLD_PATTERN%)\n\nThe SELECT 1 part:\n\nThe 1 is arbitrary - it could be SELECT 'found' or SELECT  or SELECT ep.Pattern. The important thing is that *any result** means \"a match was found.\"\n\n-- These are all equivalent:\nSELECT 1 FROM [dbo].[ExcludePatterns] ep WHERE...\nSELECT 'MATCH' FROM [dbo].[ExcludePatterns] ep WHERE...\nSELECT ep.Pattern FROM [dbo].[ExcludePatterns] ep WHERE...\n\nWhy NOT EXISTS is efficient:\n\nShort-circuit evaluation: Stops as soon as it finds one match\nNo data transfer: Only cares if rows exist, not what they contain\nIndex friendly: Can use indexes on the Pattern column\n\nYour old approach vs. NOT EXISTS:\n\nOld approach (your commented code):\nWHERE [description] NOT LIKE 'ACH%' \n  AND [description] NOT LIKE 'ATM %' \n  AND [description] NOT LIKE 'PAYPAL%'\nAlways checks all patterns even after finding a match\nHardcoded - must change code to add patterns\n\nNOT EXISTS approach:\nWHERE NOT EXISTS (\n    SELECT 1 FROM [dbo].[ExcludePatterns] ep\n    WHERE ep.[IsActive] = 1 \n    AND [rpb].[description] LIKE ep.[Pattern]\n)\nStops checking as soon as it finds a match\nDynamic - add patterns to table without changing code\nMore efficient for large pattern lists\n\nSo the subquery does look at all active patterns, but it's smart about it - it stops as soon as it finds a match!\n\nCREATE TABLE [dbo].[source] (\n    [Id] INT IDENTITY(1,1) PRIMARY KEY,\n    [Description] NVARCHAR(255) NOT NULL,\n\t[Rule] NVARCHAR(60) NOT NULL,\n\t[Value] NVARCHAR(255) NOT NULL,\n\t[Category ]NVARCHAR(60) NOT NULL,\n);\n\nALTER TABLE source\nADD CONSTRAINT UniqueDescription UNIQUE (Description);",
        "locale": "en",
        "slug": "technical-posts/working-with-budget-category-patterns",
        "folder": ""
    },
    {
        "title": "Trimming a form value in Sveltekit",
        "description": "Trimming a form value in Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "This is especially necessary for input type=\"email\" inputs. A trailing blank makes the intrinsic email validation fail. This code protects a user against that errant white space.\n\nconst trimValue = (e) => {\n\te.target.value = e.target.value.trim();\n};\n\n",
        "locale": "en",
        "slug": "technical-posts/trimming-a-form-value",
        "folder": ""
    },
    {
        "title": "Remove the Gallery from Windows 11 Explorer",
        "description": "Remove the Gallery from Windows 11 Explorer",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows"
        ],
        "content": "Video\nhttps://www.youtube.com/watch?v=YKxZ3Ru_wws\n\nCorresponding blog post\nhttps://pureinfotech.com/remove-gallery-file-explorer-windows-11/\n\nThese steps add this key:\n\n[HKEY_CURRENT_USER\\Software\\Classes\\CLSID\\{e88865ea-0e1c-4e20-9aa6-edcd0212c87c}]\n\"System.IsPinnedToNamespaceTree\"=dword:00000000\n\n![[remove-gallery-from-explorer.reg]]",
        "locale": "en",
        "slug": "technical-posts/remove-gallery-from-windows-11-explorer",
        "folder": ""
    },
    {
        "title": "Using PowerShell 7 with Postgres",
        "description": "Using PowerShell 7 with Postgres",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell",
            "postgres"
        ],
        "content": "Run this from an admin terminal\n\nInstall-Package -Name Microsoft.Extensions.Logging.Abstractions -ProviderName NuGet -Scope CurrentUser -RequiredVersion 8.0.0 -SkipDependencies -Destination . -Force\n\nInstall-Package -Name Npgsql -ProviderName NuGet -Scope CurrentUser -RequiredVersion 8.0.1 -SkipDependencies -Destination . -Force",
        "locale": "en",
        "slug": "technical-posts/using-powershell-7-with-postgres",
        "folder": ""
    },
    {
        "title": "Send a calendar event ICS file from a SvelteKit",
        "description": "Send a calendar event ICS file from a SvelteKit",
        "date_created": "2025-06-02T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "ics-file"
        ],
        "content": "A SvelteKit endpoint from which to send an ICS file\n\nOkay, let's create a SvelteKit GET API endpoint that generates an .ics file dynamically, similar to the PHP example.\n\n1. Create the API Route File\n\nIn your SvelteKit project, create the following file structure if it doesn't exist:\n\nsrc/\n routes/\n     api/\n         generate-event/\n             +server.js  // Or +server.ts if using TypeScript\n\n2. Implement the +server.js (or +server.ts)\n\n// src/routes/api/generate-event/+server.js\n\n// Helper function to format dates for ICS (YYYYMMDDTHHMMSSZ)\nfunction formatICSDate(date) {\n  // Ensure date is a Date object\n  if (!(date instanceof Date)) {\n    date = new Date(date);\n  }\n  // Convert to UTC and format\n  // Pad single digits with leading zero\n  const pad = (num) => (num \n\n  // You can make these dynamic based on your event data\n  const eventDetails = {\n    eventName: \"Team Meeting\",\n    description: \"Discuss project milestones. \\nBring your notes!\",\n    location: \"Conference Room B\",\n    // Use ISO 8601 format with 'Z' for UTC or include timezone offset\n    startTime: \"2023-12-01T14:00:00Z\", // December 1st, 2023 at 2 PM UTC\n    endTime: \"2023-12-01T15:30:00Z\"    // December 1st, 2023 at 3:30 PM UTC\n  };\n\n  let calendarLink = \"/api/generate-event\"; // Default link\n\n  // Construct link with query parameters\n  const params = new URLSearchParams();\n  if (eventDetails.eventName) params.append('eventName', eventDetails.eventName);\n  if (eventDetails.description) params.append('description', eventDetails.description);\n  if (eventDetails.location) params.append('location', eventDetails.location);\n  if (eventDetails.startTime) params.append('startTime', eventDetails.startTime);\n  if (eventDetails.endTime) params.append('endTime', eventDetails.endTime);\n\n  if (params.toString()) {\n    calendarLink += ?${params.toString()};\n  }\n\n\n\nMy Event\nDetails about the event...\n\n\n  Add to Calendar (ICS)\n\n\n\n  a {\n    display: inline-block;\n    padding: 10px 15px;\n    background-color: #007bff;\n    color: white;\n    text-decoration: none;\n    border-radius: 5px;\n  }\n  a:hover {\n    background-color: #0056b3;\n  }\n\n\nExplanation:\n\n+server.js:\n    formatICSDate(date): This helper function is crucial. It takes a JavaScript Date object and converts it into the YYYYMMDDTHHMMSSZ format required by ICS for UTC date-times.\n    escapeICStext(text): This helper escapes special characters (\\, ;, ,, \\n) in your text fields as per the iCalendar specification.\n    GET({ url }): This is the SvelteKit API handler for GET requests.\n        url.searchParams: We use this to potentially read event details passed as query parameters (e.g., /api/generate-event?eventName=MyCustomEvent).\n        Event Data: The example provides default values but allows overrides from query parameters. In a real app, you'd likely fetch this from a database based on an event ID or other criteria.\n        Date Handling: It attempts to parse startTime and endTime from query parameters. If not provided, it defaults startTime to one hour from now and endTime to one hour after startTime. Crucially, ensure any dates you work with are correctly handled in terms of UTC conversion before passing to formatICSDate. The example assumes input query params like 2023-12-01T14:00:00Z are already in UTC.\n        UID: A unique identifier for the event. Important for updates if you were to send updated ICS files.\n        DTSTAMP: Timestamp of when the iCalendar object was created.\n        icsContent: The string containing the iCalendar data. Note the \\r\\n line endings, which are standard.\n        Response: A standard Web Response object is returned.\n            'Content-Type': 'text/calendar; charset=utf-8' tells the browser it's a calendar file.\n            'Content-Disposition': 'attachment; filename=\"event.ics\"' suggests to the browser that the file should be downloaded with the name \"event.ics\".\n\nSvelte Component (.svelte file):\n    An ` tag links directly to your API endpoint (/api/generate-event`).\n    The download=\"event.ics\" attribute on the ` tag is an additional hint for browsers, though the Content-Disposition` header from the server is more authoritative.\n    The example shows how to dynamically build the calendarLink with query parameters to pass custom event details to your API endpoint.\n\nTo Run:\n\nSave the files.\nRun your SvelteKit development server: npm run dev (or yarn dev, pnpm dev).\nNavigate to the page where you placed the link (e.g., /my-event-page).\nClick the \"Add to Calendar (ICS)\" link. Your browser should download an event.ics file, which you can then import into your calendar application (Outlook, Google Calendar, Apple Calendar, etc.).\n\nThis SvelteKit API endpoint provides a clean and server-side way to generate calendar event files.\n\nICS file generic info\n\nYou can't directly add an event to a user's native calendar (like Outlook Desktop, Apple Calendar app) with a simple HTML `` tag without some intermediary step. The browser's security model prevents direct access to local applications like that.\n\nHowever, you have a few excellent options:\n\nUsing .ics Files (Most Universal):\n    This is the most common and widely supported method.\n    You generate an .ics (iCalendar) file on your server or even client-side (though server-side is more robust for complex events).\n    The ` tag links to this .ics` file. When clicked, the browser downloads it, and the user's operating system will typically offer to open it with their default calendar application.\n\n    HTML:\n    Add to Calendar (ICS)\n    The download attribute is a good hint to the browser, but the server should also send the correct Content-Type: text/calendar and Content-Disposition: attachment; filename=\"event.ics\" headers.\n\n    Example event.ics file content:\n    BEGIN:VCALENDAR\n    VERSION:2.0\n    PRODID:-//Your Company//Your Product//EN\n    BEGIN:VEVENT\n    UID:event-12345@yourdomain.com\n    DTSTAMP:20231027T080000Z\n    DTSTART:20231115T100000Z\n    DTEND:20231115T110000Z\n    SUMMARY:My Awesome Event\n    DESCRIPTION:Details about the awesome event. \\nIt will be fun!\n    LOCATION:123 Main St, Anytown\n    END:VEVENT\n    END:VCALENDAR\n    UID: Unique identifier for the event.\n    DTSTAMP: Timestamp when the event object was created (UTC).\n    DTSTART / DTEND: Start and end date/time of the event (UTC, format YYYYMMDDTHHMMSSZ).\n    SUMMARY: Event title.\n    DESCRIPTION: Event details (use \\n for newlines).\n    LOCATION: Event location.\n\n    Server-side generation (e.g., PHP):\n    format('Ymd\\THis\\Z') . \"\\n\";\n    echo \"DTEND:\" . $endTime->format('Ymd\\THis\\Z') . \"\\n\";\n    echo \"SUMMARY:\" . addcslashes($eventName, \",;\") . \"\\n\"; // Escape commas and semicolons\n    echo \"DESCRIPTION:\" . addcslashes($description, \",;\") . \"\\n\";\n    echo \"LOCATION:\" . addcslashes($location, \",;\") . \"\\n\";\n    echo \"END:VEVENT\\n\";\n    echo \"END:VCALENDAR\\n\";\n    exit;\n    ?>\n    Your HTML link would then be: Add to Calendar (ICS)\n\nDirect Links to Web-Based Calendars (Google, Outlook.com, Yahoo):\n    These services provide URL parameters to pre-fill an event creation form.\n\n    a. Google Calendar:\n    Add to Google Calendar\n\n    \n    function generateGoogleCalendarLink() {\n        const title = encodeURIComponent(\"My Awesome Event\");\n        // Dates need to be in YYYYMMDDTHHMMSSZ format (UTC)\n        // Or YYYYMMDD for all-day events\n        const startDate = \"20231115T100000Z\"; // 2023-11-15 10:00:00 UTC\n        const endDate = \"20231115T110000Z\";   // 2023-11-15 11:00:00 UTC\n        const details = encodeURIComponent(\"Details about the awesome event.\\nIt will be fun!\");\n        const location = encodeURIComponent(\"123 Main St, Anytown\");\n\n        const googleUrl = https://www.google.com/calendar/render?action=TEMPLATE&text=${title}&dates=${startDate}/${endDate}&details=${details}&location=${location};\n        document.getElementById('google-calendar-link').href = googleUrl;\n    }\n    generateGoogleCalendarLink(); // Call this when your event data is ready\n    \n\n    b. Outlook.com / Office 365 Calendar:\n    Add to Outlook Calendar\n\n    \n    function generateOutlookCalendarLink() {\n        const subject = encodeURIComponent(\"My Awesome Event\");\n        // Dates need to be ISO 8601 format: YYYY-MM-DDTHH:mm:ssZ (UTC)\n        // Or YYYY-MM-DD for all-day events\n        const startDateTime = \"2023-11-15T10:00:00Z\";\n        const endDateTime = \"2023-11-15T11:00:00Z\";\n        const body = encodeURIComponent(\"Details about the awesome event.\\nIt will be fun!\");\n        const location = encodeURIComponent(\"123 Main St, Anytown\");\n\n        const outlookUrl = https://outlook.live.com/calendar/0/deeplink/compose?path=/calendar/action/compose&rru=addevent&subject=${subject}&startdt=${startDateTime}&enddt=${endDateTime}&body=${body}&location=${location};\n        // For Office 365, the base URL might be: https://outlook.office.com/calendar/0/deeplink/compose?...\n        document.getElementById('outlook-calendar-link').href = outlookUrl;\n    }\n    generateOutlookCalendarLink();\n    \n\n    c. Yahoo Calendar:\n    Add to Yahoo Calendar\n    \n    function generateYahooCalendarLink() {\n        const title = encodeURIComponent(\"My Awesome Event\");\n        // Dates are YYYYMMDDTHHMMSS (seems to assume user's local timezone if Z is not appended, UTC is safer)\n        const startTime = \"20231115T100000Z\"; //  Best to provide UTC\n        const endTime = \"20231115T110000Z\";\n        const description = encodeURIComponent(\"Details about the awesome event.\\nIt will be fun!\");\n        const eventLocation = encodeURIComponent(\"123 Main St, Anytown\");\n\n        // Yahoo's duration (dur) is also an option instead of et (endTime)\n        // For one hour: 0100 (HHMM)\n        // const duration = \"0100\";\n\n        const yahooUrl = http://calendar.yahoo.com/?v=60&view=d&type=20&title=${title}&st=${startTime}&et=${endTime}&desc=${description}&in_loc=${eventLocation};\n        document.getElementById('yahoo-calendar-link').href = yahooUrl;\n    }\n    generateYahooCalendarLink();\n    \n\nUsing a JavaScript Library (Simplifies Things):\n    Libraries like add-to-calendar-button (a web component) can handle the generation of ICS files client-side or links for multiple services.\n\n    Example with add-to-calendar-button:\n    (See: https://add-to-calendar-button.com/)\n\n    First, include the library:\n    \n\n    Then, use the custom element:\n    \n        location=\"123 Main St, Anytown\"\n        options=\"'Apple','Google','Outlook.com','Yahoo','iCal'\"\n        buttonStyle=\"round\"\n        lightMode=\"system\"\n    >\n    This component provides a dropdown with various calendar options.\n\nKey Considerations:\n\nTime Zones: This is CRITICAL. Always be explicit about time zones.\n    For .ics files and Google Calendar links, YYYYMMDDTHHMMSSZ (Z indicates UTC) is the safest.\n    For Outlook.com, ISO 8601 with timezone offset (e.g., 2023-11-15T10:00:00-05:00) or UTC (Z) is best.\n    If you provide local times, ensure the calendar service or ICS file correctly interprets them.\nURL Encoding: All parameters in URLs (like event titles, descriptions) MUST be URL encoded (e.g., using encodeURIComponent() in JavaScript).\nUser Experience: It's good practice to offer multiple links (e.g., \"Add to Google Calendar\", \"Add to Outlook\", \"Download ICS\") or use a library that provides a dropdown.\nDynamic Data: You'll typically generate these links or ICS files dynamically based on event data from your database or application logic.\n\nThe .ics file method (Option 1) is the most universal, as it works with desktop and mobile calendar apps. The direct web calendar links (Option 2) are convenient for users who primarily use those web services. A JavaScript library (Option 3) can offer the best of both worlds with less manual effort.",
        "locale": "en",
        "slug": "technical-posts/send-calendar-ics-file-from-website",
        "folder": ""
    },
    {
        "title": "How to use PostCSS and OpenProps with Sveltekit",
        "description": "How to use PostCSS and OpenProps with Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "postcss",
            "css",
            "asna-com"
        ],
        "content": "[add-postcss-to-sveltekit-project|See this doc too--I am not sure which came first]]\n\nStep 1: Use svelte-add to install PostCSS:\n\nNote: confirm this installs the postcss-load-config plugin. If it doesn't, add it to the list below.\n\nnpx svelte-add@latest postcss\n\nThis step installs a app.postscss file in the src root. This is meant to be the app's global style sheet. It's weird that it doesn't have a .css extension--so change the file's name to app.postscss.css. Change the corresponding import (that the PostCSS install injected in the root +layout.svelte file) to reflect this name change. In Step 4, we'll see how to use this app/postcss.css file.\n\n\\\\Step 2: Install PostCSS plugins with NPM/PNPM\n\n\\\\I use these PostCSS plugins:\n\nopen-props\npostcss-jit-props\npostcss-custom-media\npostcss-import\npostcss-media-minmax\npostcss-nesting\n\nI'm a big fan of [CSS Open Props. If you don't want to use CSS Open Props you can omit the open-props and the postcss-hit-props plugin. Install these plugins as dev dependencies.\n\nMy goal with PostCSS is to create standards-compliant CSS. These plugins:\n\npostcss-custom-media\npostcss-import\npostcss-media-minmax\npostcss-nesting\n\nall produce standards-compliant CSS.\n\nYou don't need to install and configure CSSNano because Vite and PostCSS implicitly compile/compress the CSS with the Sveltekit build step: pnpm run build. More on this shortly.\n\nStep 3. include your PostCSS plugins\n\nIf you want to use CSS Open Props, change the postcss.config.cjs to:\n\nconst openProps = require('open-props');\n\nmodule.exports = {\n\tmap: { inline: true },\n\tplugins: \n\t\trequire('postcss-import'),\n\t\trequire('postcss-nesting'),\n\t\trequire('postcss-custom-media'),\n\t\trequire('postcss-media-minmax'),\n\t\trequire('postcss-jit-props')(openProps)\n\t]\n};\n\nThis last line is where Open Props magic happens:\n\nrequire('postcss-jit-props')(openProps)\n\nThis line causes PostCSS to include in the final CSS only the Open Props that are referenced in your CSS.\n\nStep 4. Writing your CSS with PostCSS and Svelte\n\nThe CSS files that comprise the final CSS that PostCSS builds are in the ./src/css-dev folder:\n\n src\n     css-dev\n        _configure.css\n        _forms.css\n        _page.css\n        _resets.css\n        _utility.css\n     lib\n     routes\n     ...\n     +layout.svelte\n     +page.svelte\n     app.postcss.css\n\nThe app.postcss.css file then imports them into the final CSS. Remember these @imports are build-time imports. When you run npm run build Vite quite magically produces a single, compressed CSS file from these imports.\n\n@import \"./css-dev/_configure.css\";\n@import \"./css-dev/_resets.css\";\n@import \"./css-dev/_page.css\";\n@import \"./css-dev/_forms.css\";\n@import \"./css-dev/_utility.css\";\n\n:root {\n\t// Application-specific variables.\n    --accent-color: var(--orange-7);\n    ...\n}\n\nAfter doing npm run build you can see the compiled CSS here: (the CSS file name will vary)\n\n.svelte-kit\\output\\client\\_app\\immutable\\assets\\0.2b389f06.css\n\nUsing Vite with PostCSS and Open Props is so cool\n\nBy using [postcss-jit-props, you can now use any of the CSS Open Props in your CSS and Vite uses PostCSS to ill ensure that the variables are selectively imported for you (that is, the only CSS Open Props imported are the ones you need).\n\nFor example, a +page.svelte file like this:\n\nAbout\n\n\n\th1 {\n\t\tcolor: var(--purple-4);\n\t\tfont-size: var(--font-size-2);\n\t}\n\n\nInjects this CSS during the Sveltekit build process:\n\n:root{--purple-4:#da77f2;--font-size-2:1.1rem}h1.svelte-6n23wp{color:var(--purple-4);font-size:var(--font-size-2)}\n\nYou don't need to worry about any explicit CSS tree shaking. In the blink of an eye, the post-jit-props PostCSS plugin prunes your CSS to include only the Open Props variables referenced in your app's CSS.\n\nThis process is crazy fast (you won't notice any lag during development as Vite/PostCSS keeps your CSS current) and there is hardly any configuration. What's not to love!\n\nconst openProps = require(\"open-props\");\n//const postcssGlobalData = require(\"@csstools/postcss-global-data\");\n\n// console.log(openProps);\n\nmodule.exports = {\n  map: { inline: true },\n  plugins: [\n    require(\"postcss-import\"),\n    require(\"postcss-nesting\"),\n    require(\"postcss-custom-media\"),\n    require(\"postcss-media-minmax\"),\n    // require(\"@csstools/postcss-global-data\")({\n    //   files: [\"node_modules://open-props/media.min.css\"],\n    // }),\n    require(\"postcss-jit-props\")(openProps),\n    // require(\"postcss-preset-env\")({\n    //   \"custom-media-queries\": true,\n    // }),\n    ...(process.env.NODE_ENV === \"production\" ? [require(\"cssnano\")] : []),\n  ],\n};\n\nNov 2024 update\n\nThis project has a PostCSS install that is working and that uses the postcss.config.cjs below.\n\nC:\\Users\\thumb\\Documents\\projects\\svelte\\forms-handling\n\nThis is the latest postcss.config.cjs\n\nconst cssnano = require('cssnano');\nconst postcssImport = require('postcss-import');\nconst postcssCustomMedia = require('postcss-custom-media');\nconst openProps = require('open-props');\nconst postcssJitProps = require('postcss-jit-props');\nconst postcssGlobalData = require('@csstools/postcss-global-data');\nconst purgecss = require('@fullhuman/postcss-purgecss');\nconst DO_NOT_PRESERVE_UNRESOLVED_RULE = false;\n\nmodule.exports = {\n\tplugins: [\n\t\tpostcssImport(),\n\t\tpostcssJitProps(openProps),\n\t\tpostcssGlobalData({\n\t\t\tfiles: ['./node_modules/open-props/src/props.media.css']\n\t\t}),\n\t\tpostcssCustomMedia({\n\t\t\tpreserve: DO_NOT_PRESERVE_UNRESOLVED_RULE\n\t\t}),\n\n\t\t...(process.env.NODE_ENV === 'production'\n\t\t\t? [purgecss(\n\t\t\t\t{\n\t\t\t\t\tcontent: ['./src/routes/*/.svelte'],\n\t\t\t\t\tsafelist: ['mt-48'],\n\t\t\t\t}\n\t\t\t)]\n\t\t\t: []),\n\n\t\t...(process.env.NODE_ENV === 'production' ? [cssnano()] : [])\n\t]\n};\n\nNote that it includes all .svelte from ./src/routes down. I know this works with the local build process, but I am not sure that it does with the Vercel build process.\n\n[!warning]\nTest that CSS tree shaking works on Vercel.\n\nTo use that postcss.config.cjs:\n\nInstall these NPM packages as dev dependencies\n\npostcss\npostcss-cli\ncssnano\nopen-props\npostcss-custom-media\npostcss-import\npostcss-jit-props\n@csstools/postcss-global-data\n@fullhuman/postcss-purgecss\n\nAdd a ./css-dev folder\n\nIt should have a main.css file with as many @imports as necessary\nFor example:\n\n@import \"./resets.css\";\n@import \"./utilities.css\";\n\nWhen changes in any of these files occur, the ./scr/main.css output file is (very quickly) recreated.\n\nThe /src/main.css file is referenced in the ./src/routes/+layout.svelte\n\nSvelte 4\n\n\n  import '../main.css';\n\n\n\n\nSvelte 5\n\n\n\n\t// CSS should be in the root of the SvelteKit app. \n\timport '../pico.css'; // import global CSS\n\n\tlet { children } = $props();\n\n\n\n\t\n\t\t{@render children()}\n\t\n",
        "locale": "en",
        "slug": "technical-posts/use-postcss-and-open-props-with-sveltekit",
        "folder": ""
    },
    {
        "title": "Windows startup folder",
        "description": "Windows startup folder",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows"
        ],
        "content": "#windows\n\nStart a command prompt\n\nWinKey + r\n\nThen\n\nshell:startup\n\nwhich opens this folder:\n\nC:\\Users\\thumb\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup",
        "locale": "en",
        "slug": "technical-posts/windows-startup-folder",
        "folder": ""
    },
    {
        "title": "Using TypeScript with Svelte",
        "description": "Using TypeScript with Svelte",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "From this page in Svelte docs:\n\napp.d.ts\n\nThesrc\\app.d.tsfile is home to the ambient types of your apps, i.e. types that are available without explicitly importing them.\n\nAlways part of this file is theAppnamespace. This namespace contains several types that influence the shape of certain SvelteKit features you interact with.\n\ndeclare global {\n\tnamespace App {\n\t\t// interface Error {}\n\t\t// interface Locals {}\n\t\t// interface PageData {}\n\t\t// interface PageState {}\n\t\t// interface Platform {}\n\t}\n\n\tnamespace Custom {\n\t\tinterface Neil {\n\t\t\tname: string;\n\t\t}\n\t\tinterface Young {\n\t\t\tname: string;\n\t\t}\n\t}\n}\n\nexport {};\n\nYou can add your own namespaces under the global section and its types are instantly available with Intellisense.\n\n![[Using TypeScript with Svelte.png|500]]",
        "locale": "en",
        "slug": "technical-posts/using-typescript-with-svelte",
        "folder": ""
    },
    {
        "title": "TypeScript type naming conventions",
        "description": "TypeScript type naming conventions",
        "date_created": "2025-07-03T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "Common Conventions:\n\n1. Suffix Convention (Most Popular):\nimport { z } from 'zod';\n\nexport const categoryRulesSchema = z.object({\n\tid: z.number().int().nullable(),\n\tdescription: z.string().nullable(),\n\trule: z.string().nullable(),\n\tvalue: z.string().nullable(),\n\tcategory: z.string().nullable()\n});\n\n// Database/Table type\nexport type CategoryRulesEntity = z.infer;\n\n// Runtime/Application types\nexport type CategoryRulesDto = Omit & {\n\tdescription: string;\n\trule: string;\n\tvalue: string;\n\tcategory: string;\n};\n\nexport type CategoryRulesRequest = Omit;\nexport type CategoryRulesResponse = Required;`\n\n2. Prefix Convention:\n// Database types\nexport type DbCategoryRules = z.infer;\nexport type DbUser = { id: number; email: string | null; /* ... */ };\n\n// Application types\nexport type CategoryRules = Required>;\nexport type User = { id: number; email: string; /* ... */ };`\n\n3. Namespace/Module Convention:\nexport namespace DB {\n\texport type CategoryRules = z.infer;\n}\n\nexport namespace App {\n\texport type CategoryRules = Required>;\n}\n\n// Usage:\n// const dbRecord: DB.CategoryRules = ...;\n// const appData: App.CategoryRules = ...;`\n\n4. Folder Structure Convention:\nsrc/lib/types/\n entities/          # Database table types\n    CategoryRules.ts\n    User.ts\n dtos/             # Data transfer objects\n    CategoryRulesDto.ts\n    UserDto.ts\n api/              # API request/response types\n     CategoryRulesApi.ts\n     UserApi.ts`\n\nRecommended Approach (Suffix Convention):\n\nimport { z } from 'zod';\n\nexport const categoryRulesSchema = z.object({\n\tid: z.number().int().nullable(),\n\tdescription: z.string().nullable(),\n\trule: z.string().nullable(),\n\tvalue: z.string().nullable(),\n\tcategory: z.string().nullable()\n});\n\n// Database entity (matches physical table)\nexport type CategoryRulesEntity = z.infer;\n\n// Application DTOs (Data Transfer Objects)\nexport type CategoryRulesDto = Required>;\nexport type CategoryRulesCreateDto = Omit;\nexport type CategoryRulesUpdateDto = Partial> & { id: number };`\n\nCommon Suffixes:\nEntity - Database table representation\nDto - Data Transfer Object (API/runtime)\nModel - Domain model\nRequest - API request payload\nResponse - API response payload\nCreate - For creation operations\nUpdate - For update operations\n\nUsage in your code:\nimport type { CategoryRulesEntity, CategoryRulesDto } from '$lib/types/CategoryRules';\n\n// Database query returns entities\nconst dbResult: CategoryRulesEntity[] = result.recordset;\n\n// Transform to DTOs for the frontend\nconst datarows: CategoryRulesDto[] = dbResult.map(entity => ({\n\tdescription: entity.description || '',\n\trule: entity.rule || '',\n\tvalue: entity.value || '',\n\tcategory: entity.category || ''\n}));`\n\nThis convention makes it immediately clear whether you're working with database data (nullable, includes IDs) or application data (processed, non-nullable).",
        "locale": "en",
        "slug": "technical-posts/typescript-type-naming-conventions",
        "folder": ""
    },
    {
        "title": "Notes from early efforts are multi-locales with Sveltekit",
        "description": "Notes from early efforts are multi-locales with Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "asna-com",
            "depcreated"
        ],
        "content": "See the 'regional-routing' project for a prototype.\n\nSpecify language with optional route\n\nChange lang attribute in HTML(https://youtu.be/Kzrz7GZ9pIg?t=639)\n\n\n    import {page} from'$app/stores'\n\n\tconst {lang = 'en'} from $page.params\n\n\nThe ~/+page.svelte file is the English (en) home page.\n\nOther regions' home pages are in routes/xx where 'xx' is the two-character region code. The \"regions\" identified by folders under routes define the language regions available.\n\nThe routes/en route should redirect back to the root with a page.ts file like this:\n\nimport { redirect } from '@sveltejs/kit';\nthrow redirect(302, \"/\")\n\n.\n src\n     lib\n        components\n     routes\n        [...lang]\n           posts\n              [slug]\n           en\n           es\n        site-content\n            posts\n                helloworld.en.svelte\n                helloworld.es.svelte\n     app.d.ts\n     app.html\n     hooks.server.ts\n     hooks.ts\n\nInput to tree builder\n\nsrc\n  lib\n    components\n  routes\n    [...lang]\n      posts\n        [slug]\n      en\n      es\n    site-content\n      posts\n        helloworld.en.svelte\n        helloworld.es.svelte\n  app.d.ts\n  app.html\n  hooks.server.ts\n  hooks.ts\n\nShowing content\n\nThe ~/routines/[...lang] is the top-level route. It has children directories that define the site's content routes. This example shows only a posts route with a child [slug]route, but there could be others such as posts, kb, pipeline, resources, services, support, company, etc. Each one of these folders needs a child [slug] folder with the +page.svelte and +page.ts files.\n\nA downside to this hack is that there can't be a ~/products page or any of the other \"routes\" you defined under the [:lang] folder (ie, to get to a products page you have to use the route products/products). This is a minor consideration for persisting existing routes. (ie, we can't persist the /products route to get a top-level page about products.\n\nChanging the HTML language code\n\nThe ~/app.html html tag is\n\n\n\nthe ~/hooks.server.ts file captures the two-character region code from the URL and replaces %lang with the current region code. I'm not sure what to do with it yet, but this hook also puts this value in\n\nevent.locals.lang",
        "locale": "en",
        "slug": "technical-posts/language-support",
        "folder": ""
    },
    {
        "title": "Landing page comparo table",
        "description": "Landing page comparo table",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "landing-page"
        ],
        "content": "|                          | ASNA Monarch | Competitor 1 | Competitor 2 |\n| ------------------------ | :----------: | :----------: | :----------: |\n| RPG logic intact         |     yes      |      no      |      no      |\n| Database on IBM i        |     yes      |     yes      |      no      |\n| Database on SQL Server   |     yes      |      no      |      no      |\n| 20+ migration experience |     yes      |      no      |      no      |\n| RPG record level access  |     yes      |      no      |      no      |\n| Activation groups        |     yes      |      no      |      no      |\n| QTemp                    |     yes      |      no      |      no      |\n| C# migration target      |     yes      |      no      |      no      |\n| RPG migration target     |     yes      |      no      |      no      |\n| Synon migrations         |     yes      |      no      |      no      |\n|                          |              |              |              |",
        "locale": "en",
        "slug": "technical-posts/landing-page-comparo-table",
        "folder": ""
    },
    {
        "title": "Typescript's pick - omit - extract - exclude - utility types",
        "description": "Typescript's pick - omit - extract - exclude - utility types",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "[!info]\nextract and exclude work only on on union types.\n\nThese are powerful built-in utility types in TypeScript that help you construct new types from existing ones:\n\n1. Pick and Omit (for Object Properties)\n\nPick:\n\n    Purpose: Creates a new type by selecting a set of specific properties (Keys) from an existing object Type.\n    Think: \"I want to pick only these properties from this object type.\"\n    Example:\n        interface User {\n            id: number;\n            name: string;\n            email: string;\n            isAdmin: boolean;\n        }\n        type UserPreview = Pick;\n        // UserPreview is { id: number; name: string; }\n\nOmit:\n    Purpose: Creates a new type by taking all properties from an existing object Type and then removing a specific set of properties (Keys).\n    Think: \"I want all properties from this object type, but omit (remove) these.\"\n    Example:\n        interface User {\n            id: number;\n            name: string;\n            email: string;\n            isAdmin: boolean;\n        }\n        type UserEditableFields = Omit;\n        // UserEditableFields is { name: string; email: string; }\nKey Idea: Pick and Omit are opposites. Both operate on the keys of an object type.\n\n2. Extract and Exclude (for Union Members)\n\nExtract:\n\n    Purpose: Creates a new type by selecting members from a Type (usually a union type) that are assignable to Union.\n    Think: \"I want to extract (keep) only the types from this union that are also part of this other union/type.\"\n    Example:\n\n        type AllShapes = \"circle\" | \"square\" | \"triangle\" | \"line\";\n        type RoundShapes = Extract;\n        // RoundShapes is \"circle\"\n        // (because \"oval\" wasn't in AllShapes, and \"square\", \"triangle\", \"line\" aren't in \"circle\" | \"oval\")\n\n        type T0 = Extract void), Function>;\n        // T0 is () => void\n\nExclude:\n\n    Purpose: Creates a new type by removing members from a Type (usually a union type) that are assignable to Union.\n    Think: \"I want all types from this union, but exclude (remove) any that are part of this other union/type.\"\n    Example:\n\n        type AllShapes = \"circle\" | \"square\" | \"triangle\" | \"line\";\n        type NonRoundShapes = Exclude;\n        // NonRoundShapes is \"square\" | \"triangle\" | \"line\"\n\n        type T1 = Exclude void), Function>;\n        // T1 is string | number\n\nKey Idea: Extract and Exclude are opposites. Both operate on the members of a union type, filtering them based on another type/union.\n\nIn a Nutshell:\n\nPick/Omit: Work with object properties/keys.\n    Pick: Selects specified keys.\n    Omit: Removes specified keys.\nExtract/Exclude: Work with members of a union type.\n    Extract: Selects union members assignable to a given type.\n    Exclude: Removes union members assignable to a given type.",
        "locale": "en",
        "slug": "technical-posts/typescript-pick-omit-extract-exclude-utility-types",
        "folder": ""
    },
    {
        "title": "This is a SvelteKit select tag component. It provides a dropdown list. It is based on, and provides, the ListItem type.",
        "description": "This is a SvelteKit select tag component. It provides a dropdown list. It is based on, and provides, the ListItem type.",
        "date_created": "2025-05-22T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte-component"
        ],
        "content": "Overview\n\nThe Select component is a custom Svelte 5 component that wraps the native HTML `` element with additional functionality for form handling and validation. It provides a type-safe interface for dropdown selections with support for form association and value binding.\nImport\n\nimport Select from '$lib/components/Select.svelte';\nimport type { ListItem } from '$lib/components/Select.svelte';\n\nInterface\n\nListItem\n\ninterface ListItem {\n  value: string;\n  text: string;\n}\n\nComponent Props\n\ninterface ComponentProps {\n  elementName: string;                             // Required: The name attribute for the select element\n  listItems: ListItem];                           // Required: Array of options to display\n  formId?: string;                                 // Optional: ID of the form to associate with\n  selectedValue?: string | undefined | null;       // Optional: Currently selected value\n  selectClass?: string | undefined | null;         // Optional: CSS class for the select element\n  optionClass?: string | undefined | null;         // Optional: CSS class for option elements\n}\n\nUsage Examples\n\nBasic Usage\n\n\n  import Select from '$lib/components/Select.svelte';\n  \n  const options = [\n    { value: 'option1', text: 'Option 1' },\n    { value: 'option2', text: 'Option 2' },\n    { value: 'option3', text: 'Option 3' }\n  ];\n  \n  let selectedValue = 'option1';\n\nWith Form Association\n\n\n  import Select from '$lib/components/Select.svelte';\n  \n  const formId = 'myForm';\n  const options = [\n    { value: 'contains', text: 'Contains' },\n    { value: 'startswith', text: 'StartsWith' },\n    { value: 'equals', text: 'Equals' }\n  ];\n\n\n\n\n\n  Submit\n\nWith Element Binding and Methods\n\n\n  import Select from '$lib/components/Select.svelte';\n  \n  let selectElement: Select;\n  let currentValue = $state('');\n  \n  const options = [\n    { value: 'red', text: 'Red' },\n    { value: 'green', text: 'Green' },\n    { value: 'blue', text: 'Blue' }\n  ];\n  \n  function getCurrentValue() {\n    currentValue = selectElement.getValue();\n  }\n\n\n\n\nGet Current Value\nCurrent value: {currentValue}\n\nWith Custom Styling\n\nExported methods\n\ngetValue()\n\nReturns the current value of the select element.\n\nconst currentValue = selectElement.getValue();\n\nPrivate methods\nisSelectValueInList(selectedValue?: string | undefined | null): boolean\n\nChecks if a given value exists in the listItems array.\n\nconst isValid = selectElement.isSelectValueInList('someValue');\n\nFeatures\n\nForm Association\n\nThe component supports HTML5 form association using the form attribute, allowing the select element to be part of a form even when placed outside the `` tag:\n\n\n\n\n\n\n  Submit\n\nDefault Value Handling\n\nIf no selectedValue is provided, the component automatically selects the first item in the listItems array:\n\n// If selectedValue is null/undefined, listItems[0].value will be selected\nselectedValue = selectedValue ?? listItems[0]?.value;\n\nType Safety\n\nThe component provides full TypeScript support with proper type definitions for all props and the ListItem interface.\n\nStyling with Pico CSS\n\nThe component works seamlessly with [Pico CSS styling. Based on the attached CSS files, the select elements will automatically receive proper styling including:\n\nConsistent form element appearance\nFocus states and transitions\nProper spacing and typography\nResponsive design\nDark mode support\nValidation state styling (aria-invalid support)\n\nCustom Styling\n\nYou can apply custom styles using the selectClass and optionClass props:\n\n.custom-select {\n  background-color: var(--input-background-color);\n  color: var(--text-color);\n  border: 1px solid var(--select-border);\n}\n\n.custom-option {\n  color: black;\n}\n\n.custom-option:hover {\n  color: gray;\n}\n\nBest Practices\n\nAlways provide a name: The elementName prop is required and sets the name attribute for form submission.\n\nUse meaningful values: ListItem value should be machine-readable, while text should be human-readable.\n\nForm association: Use the formId prop when the select needs to be part of a form but is positioned outside the form element.\n\nValidation: Use isSelectValueInList() to validate if a value exists before setting it.\n\nAccessibility: The component inherits standard HTML select accessibility features.\n\nRelated Components\n\nWorks with the form handling patterns shown in +page.svelte\nIntegrates with SvelteKit's use:enhance for progressive enhancement\nCompatible with the budget metadata from $lib/data/budget-meta-data\n\nNotes\n\nThe component automatically handles the selected attribute for options\nSupports both controlled and uncontrolled usage patterns\nFalls back gracefully when listItems is empty\nCompatible with Svelte 5's new props and state management patterns",
        "locale": "en",
        "slug": "technical-posts/sveltekit-component-select",
        "folder": ""
    },
    {
        "title": "goto-apis-used-day-to-day",
        "description": "goto-apis-used-day-to-day",
        "date_created": "2025-05-19T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "goto-webinar"
        ],
        "content": "The access token\n\n\nThe refresh-access-token route in this app:\n\nC:\\Users\\thumb\\Documents\\projects\\svelte\\goto-apis\n\nrefreshes the Access token and, if necessary, the Refresh token.\n\n\n[[goto-create-oauth-client|This document]] explains how to get the initial access token value. access tokens expire in 60 minutes, so they ethereal--and as a general practice always refreshed before you need to call a GoTo application API. \n\nA valid access token is required to make a successful API call for GoTo products. Access tokens have a lifespan of 60 minutes. Getting a new access token requires a new login and new token request, or - more easily - a request that contains a refresh token. Refresh tokens are good for longer periods.\n\nTo use a refresh token, you send an API token request with a grant type of refresh_token with the refresh token value from the original token request. \n\nA sample request is shown below in cURL format.\n\ncurl --request POST 'https://authentication.logmeininc.com/oauth/token' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--header 'Authorization: Basic [Base64]' \\\n-d 'grant_type=refresh_token&refresh_token=[Refresh Token]'\n\nThe Authorization header value is the same as the one you used to obtain the original token, namely the string of clientID:clientSecret base64 encoded. The refresh token is the value received in the results body when you received the original access token.\nResponse example\n\nIMPORTANT: The access token value is truncated. It is a much larger value.\n\n{\n  \"access_token\": \"eyJraWQiOiJvYXV0aHYyLmxt666...\",\n  \"expires_in\": 3600,\n  \"scope\": \"users.v1.lines.read calls.v2.initiate\",\n  \"principal\": \"asnaevents@asna.com\",\n }\n\nAs you can see in the above example, there is norefresh_tokenfield in the response body. But, if a new refresh token has been issued because the old one has expired, the response payload will look as below:\n\n{\n  \"access_token\": \"eyJraWQiOiJvYXV0aHYyLmxt666...\",\n  \"token_type\": \"Bearer\",\n  \"refresh_token\": \"eyJraWQiOiJvYXV0aHYyLmxt999...\",\n  \"expires_in\": 3600,\n  \"scope\": \"users.v1.lines.read calls.v2.initiate\",\n  \"principal\": \"mahar.singh@company.com\"\n}\n\nResponse data\n\nThe following is sample output.\n\n| Parameter     | Description                                                              |\n| ------------- | ------------------------------------------------------------------------ |\n| access_token  | OAuth access token                                                       |\n| refresh_token | OAuth refresh token (Only present when replacing previous refresh token) |\n| principal     | Who the token represents                                                 |\n| scope         | The allowed scope(s) for the issued token, separated by a whitespace     |\n| expires_in    | The number of seconds until the access token expires                     |\n\nThis access token can now be used to authorize API requests by setting it in the Authorization header with the following format:\n\nAuthorization: Bearer {access_token}\n\nExample of use\n\nEvent 1:Generate an access token. The body of the response contains a new valid access token and a refresh token.\n\nEvent 2:At any time when you need access (within the next 30 days), send a grant type=refresh token request using the original refresh token to get a new access token. There will not be a refresh token included in the payload until Event 3.\n\nEvent 3:At some point within the next 30 days, the response body will contain a new refresh token, good for the next 30 days. Discard the original refresh token and store this new refresh token. Events 2 and 3 can be repeated.",
        "locale": "en",
        "slug": "technical-posts/refreshing-the-goto-access-token",
        "folder": ""
    },
    {
        "title": "ASNA downloads release notes conventions",
        "description": "ASNA downloads release notes conventions",
        "date_created": "2025-08-14T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "downloads"
        ],
        "content": "The name column from the Family is in the first column below.\n\nThe right column is produced by calling the woefully poorly named SQL function, family_name_crusher--which converts the name to lower case and removes all white space.\n\n| name | family\\_name\\_crusher |\n| :--- | :--- |\n| Armadillo | armadillo |\n| AVR4.0 | avr4.0 |\n| AVR4.1 | avr4.1 |\n| Badger | badger |\n| Badger - Beta | badger-beta |\n| Coyote | coyote |\n| Coyote - Beta | coyote-beta |\n| DataGate 15 | datagate15 |\n| DataGate 16.0 | datagate16.0 |\n| DataGate 16.0 - Beta | datagate16.0-beta |\n| DataGate 17.0 | datagate17.0 |\n| DataGate 17.0 - Beta | datagate17.0-beta |\n| DataGate 17.1 | datagate17.1 |\n| DataGate 17.1 - Beta | datagate17.1-beta |\n| Lima | lima |\n| Mike | mike |\n| Monarch | monarch |\n| Nancy | nancy |\n| Nancy - Beta | nancy-beta |\n| Oscar | oscar |\n| Oscar - Beta | oscar-beta |\n| Otis | otis |\n| Otis - Beta | otis-beta |\n| Papa | papa |\n| Papa - Beta | papa-beta |\n| VS2010-1 | vs2010-1 |\n| VS2010-2 | vs2010-2 |\n| VS2012-0 | vs2012-0 |\n| VS2012-2 | vs2012-2 |\n| VS2013 | vs2013 |",
        "locale": "en",
        "slug": "technical-posts/release-notes-readme-file-conventions",
        "folder": ""
    },
    {
        "title": "services-team-blurb",
        "description": "services-team-blurb",
        "date_created": "2025-10-23T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "marketing"
        ],
        "content": "The ASNA Services Team helps your dev team get things done!\n\nASNA's worldwide services team has what it takes to make your project a success. From small Web sites all the way to million-line enterprise RPG application migrations, from providing a little direction and guidance to doing 100% of the work for you, our team can help you.\n\nRich and deep experience. Our team has broad and deep custom application development experience for the .NET platform. Our team does Windows, Web, and mobile development and delivers on time and on budget.\n    \nTeam Collaboration. We can work with your team collaboratively to create new applications or migrate and enhance your existing ASNA Visual RPG Classic or IBM i application. We are also experts at working remotely.\n    \nGreat tools. ASNA services team has the full complement of ASNAs investigative and diagnostic tools. We will work collaboratively to provide a successful solution for you and your company.\n    \nExpertise. Our team has the expertise to help you build sophisticated, future-ready applications that are able to work seamlessly across a broad variety of desktops, browsers, and mobile devices.\n    \nTraining and mentoring. ASNA has extensive training and mentoring services available for your team. We can prepare your team for the exciting development challenges of the 21st century.\n    \n\nContact us about the ASNA Services Team",
        "locale": "en",
        "slug": "technical-posts/services-team-blurb",
        "folder": ""
    },
    {
        "title": "obsidian-custom-css-gemini",
        "description": "obsidian-custom-css-gemini",
        "date_created": "2025-07-12T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css",
            "obsidian"
        ],
        "content": "First pass - make headers different colors\n\nThis is a good way to visually distinguish your document structure. You can achieve this using a CSS snippet in Obsidian.\n\nThe method below uses CSS variables, which makes it easy to configure the colors in one central place without having to dig through complex class names.\n\nHere is a complete step-by-step guide.\nStep 1: Create the CSS Snippet File\n\nOpen your Obsidian vault in your computer's file explorer.\nGo into the .obsidian hidden folder.\nInside .obsidian, find or create a folder named snippets.\nInside the snippets folder, create a new text file and name it colored-headings.css.\nStep 2: Copy and Paste the CSS Code\n\nOpen the colored-headings.css file you just created in a plain text editor (like Notepad, VS Code, or TextEdit) and paste the following code into it:\n\n/*\n\n           CONFIGURABLE HEADING COLORS FOR OBSIDIAN\n\n  -- By default: H1=Red, H2=Blue, H3=Yellow, H4=Green --\n  --           You can use color names, hex codes,     --\n  --           or rgb() values.                        --\n\n*/\n\n/* --- 1. CONFIGURATION: Edit your colors here! --- */\n:root {\n  --h1-color: Red;\n  --h2-color: Blue;\n  --h3-color: Goldenrod; /* Using Goldenrod instead of pure Yellow for better readability on light themes */\n  --h4-color: Green;\n  /* You can add more for H5 and H6 if you like */\n  --h5-color: Orange;\n  --h6-color: Purple;\n}\n\n\n/* --- 2. STYLES: Do not edit below unless you know what you are doing --- */\n\n/* H1 STYLES */\n/* This targets H1 in Reading Mode and Live Preview (Edit Mode) */\n.markdown-preview-view h1,\n.cm-s-obsidian .cm-header-1 {\n  color: var(--h1-color);\n}\n\n/* H2 STYLES */\n/* This targets H2 in Reading Mode and Live Preview (Edit Mode) */\n.markdown-preview-view h2,\n.cm-s-obsidian .cm-header-2 {\n  color: var(--h2-color);\n}\n\n/* H3 STYLES */\n/* This targets H3 in Reading Mode and Live Preview (Edit Mode) */\n.markdown-preview-view h3,\n.cm-s-obsidian .cm-header-3 {\n  color: var(--h3-color);\n}\n\n/* H4 STYLES */\n/* This targets H4 in Reading Mode and Live Preview (Edit Mode) */\n.markdown-preview-view h4,\n.cm-s-obsidian .cm-header-4 {\n  color: var(--h4-color);\n}\n\n/* H5 STYLES (Bonus) */\n.markdown-preview-view h5,\n.cm-s-obsidian .cm-header-5 {\n  color: var(--h5-color);\n}\n\n/* H6 STYLES (Bonus) */\n.markdown-preview-view h6,\n.cm-s-obsidian .cm-header-6 {\n  color: var(--h6-color);\n}\n\n/* This makes sure the hashmarks (#) also get colored in Live Preview mode for a consistent look */\n.cm-s-obsidian .cm-formatting-header {\n    color: inherit !important;\n}\n\nSave the file.\nStep 3: Enable the Snippet in Obsidian\n\nGo back to your Obsidian app.\nOpen Settings (the gear icon in the bottom-left).\nGo to the Appearance tab.\nScroll down to the CSS Snippets section.\nYou should see your colored-headings file listed. If not, click the little \"refresh\" icon next to the heading.\nToggle the switch next to colored-headings to enable it.\n\nYour headings should now be colored in both Reading and Live Preview (Edit) modes!\n\nHow to Configure Your Colors\n\nThis is the best part. To change the colors, you only need to edit the top section of your colored-headings.css file.\n\nOpen .obsidian/snippets/colored-headings.css again.\nLook at the :root block at the very top.\n\n/* --- 1. CONFIGURATION: Edit your colors here! --- */\n:root {\n  --h1-color: Red;\n  --h2-color: Blue;\n  --h3-color: Goldenrod;\n  --h4-color: Green;\n}\n\nSimply change the color values. You can use:\nColor Names: Red, DodgerBlue, RebeccaPurple\nHEX Codes: #FF5733, #3375FF, #C70039 (great for picking exact shades)\nRGB/RGBA Values: rgb(255, 87, 51), rgba(51, 117, 255, 0.8)\n\nExample: Changing to a different color palette\n\n:root {\n  --h1-color: #D32F2F;   /* A darker red */\n  --h2-color: #1976D2;   /* A nice blue */\n  --h3-color: #FBC02D;   /* A vibrant yellow/orange */\n  --h4-color: #388E3C;   /* A forest green */\n}\n\nAfter you save the file, the changes will appear instantly in Obsidian.\nExplanation of the CSS Selectors\n\n:root: This defines variables (--h1-color, etc.) that can be reused throughout the file. It's your central configuration panel.\n.markdown-preview-view h1: This targets a standard `` tag but only inside the container for Reading Mode.\n.cm-s-obsidian .cm-header-1: This is one of the complex class names you mentioned. It targets the special element that looks like an H1 in Live Preview (Edit) Mode.\n, (The Comma): By putting a comma between the two selectors, we tell the browser to apply the same style (color: var(--h1-color);) to both, keeping them consistent.\n.cm-formatting-header: This specifically targets the #, ##, etc. characters in Live Preview mode. By setting color: inherit !important;, we force them to take on the color of their parent heading element, making the entire line a single, uniform color.\nSecond pass - Set maxwidth of display\n\nControlling the line width (or max-width) is a key part of creating a comfortable reading and writing experience. We can add this to our CSS snippet in the exact same configurable way.\n\nYou have two simple options: add this to your existing colored-headings.css file or create a new one. I recommend adding it to your existing file to keep things simple.\nOption 1: Add to Your Existing CSS Snippet (Recommended)\n\nThis is the easiest way. We will modify your colored-headings.css file to include the new settings.\n\nOpen your CSS snippet file at .obsidian/snippets/colored-headings.css.\nReplace the content of the file with the code below. I have merged the new max-width controls with the previous heading color styles.\n\n/*\n\n           CUSTOM OBSIDIAN STYLES\n\n  -- This snippet controls:\n     Heading Colors\n     Max Line Width (readable line length)\n\n*/\n\n/* --- 1. CONFIGURATION: Edit your styles here! --- */\n:root {\n  /* -- Max Width Settings -- */\n  /* 'ch' is a unit relative to the width of the \"0\" character.\n     It's ideal for controlling line length. 70-80ch is common for readability.\n     120ch is wider, as you requested. */\n  --line-width: 120ch;\n\n\n  /* -- Heading Color Settings -- */\n  --h1-color: Red;\n  --h2-color: Blue;\n  --h3-color: Goldenrod;\n  --h4-color: Green;\n  --h5-color: Orange;\n  --h6-color: Purple;\n}\n\n\n/* --- 2. STYLES: Do not edit below unless you know what you are doing --- */\n\n/* == MAX WIDTH STYLES == */\n/* This targets the content \"sizers\" in both Read and Live Preview modes */\n.markdown-preview-view .markdown-preview-sizer,\n.markdown-source-view .cm-sizer {\n  max-width: var(--line-width);\n}\n\n/* == HEADING COLOR STYLES == */\n/* H1 */\n.markdown-preview-view h1, .cm-s-obsidian .cm-header-1 { color: var(--h1-color); }\n/* H2 */\n.markdown-preview-view h2, .cm-s-obsidian .cm-header-2 { color: var(--h2-color); }\n/* H3 */\n.markdown-preview-view h3, .cm-s-obsidian .cm-header-3 { color: var(--h3-color); }\n/* H4 */\n.markdown-preview-view h4, .cm-s-obsidian .cm-header-4 { color: var(--h4-color); }\n/* H5 */\n.markdown-preview-view h5, .cm-s-obsidian .cm-header-5 { color: var(--h5-color); }\n/* H6 */\n.markdown-preview-view h6, .cm-s-obsidian .cm-header-6 { color: var(--h6-color); }\n\n/* Color the markdown # symbols to match the heading */\n.cm-s-obsidian .cm-formatting-header { color: inherit !important; }\n\nSave the file. The changes should apply immediately in Obsidian.\nOption 2: Create a Separate Snippet for Layout\n\nIf you prefer to keep your style tweaks organized into separate files, you can do this instead.\n\nCreate a new file in your .obsidian/snippets/ folder named layout-tweaks.css.\nPaste the following code into that new file:\n\n/*\n\n           CONFIGURABLE MAX WIDTH FOR OBSIDIAN\n\n*/\n\n/* --- 1. CONFIGURATION: Edit your max width here! --- */\n:root {\n  /* 'ch' is a unit relative to the width of the \"0\" character.\n     It's ideal for controlling line length. 70-80ch is common for readability. */\n  --line-width: 120ch;\n}\n\n\n/* --- 2. STYLES: Do not edit below unless you know what you are doing --- */\n\n/* This targets the content \"sizers\" in both Read and Live Preview modes */\n.markdown-preview-view .markdown-preview-sizer,\n.markdown-source-view .cm-sizer {\n  max-width: var(--line-width);\n}\nSave the file.\nGo to Settings -> Appearance -> CSS Snippets in Obsidian and enable your new layout-tweaks snippet.\nHow to Configure Your Max Width\n\nJust like with the colors, you only need to edit the top configuration section of the CSS file.\n\nLook for this variable:\n:root {\n  --line-width: 120ch;\n}\n\nSimply change the 120ch value to whatever you prefer.\n\n120ch: As requested. This will be quite wide.\n80ch: A very common and comfortable width for reading prose.\n100%: This would make the text span the entire width of the pane.\n900px: You can also use fixed pixel values.\n\nSave the file, and Obsidian will update its layout instantly.\n\nExplanation of the New CSS Selectors\n\n.markdown-preview-view .markdown-preview-sizer: This targets the specific element that controls the content width in Reading Mode.\n.markdown-source-view .cm-sizer: This targets the equivalent element that controls the content width in Live Preview (Edit) Mode.\n\nBy setting the max-width on both of these \"sizer\" elements, you get a consistent line length whether you are reading or writing.\nThird pass - explicitly show display mode\n\nI had previously used this in my custom.css to provide an explicit way to show that reading mode is active:\n\ndiv.markdown-reading-view::before {\n\tcontent: \"Reading view\";\n    font-size: 10px;\n    margin-top: 1rem;\n    margin-bottom: .5rem;\n\tmargin-left: 1.5rem;\n\tbackground-color: #8A4117;\n    color: black;\n\twidth: fit-content;\n\tpadding-inline: .5rem;\n\tpadding-block: .3rem;\n\tborder-radius: 5px;\n    z-index: 9999999999999999;\n}\n\nis that a good way to do that?\nAnswer\n\nThat's a very clever use of CSS, and to answer your question directly: Yes, that is a perfectly good and effective way to create a mode indicator.\n\nIt's a great example of using a pseudo-element (::before) to add purely presentational content without needing to change the underlying document structure.\n\nLet's break down what makes your approach good, and then I can offer a minor refinement to make it even more robust and configurable, just like our other styles.\n\nWhat's Great About Your Method\n\nCorrect Selector: div.markdown-reading-view is the precise class for the main container in Reading Mode. It's specific and won't accidentally affect other parts of the UI.\nUse of ::before: This is the ideal tool for the job. It injects a \"fake\" element before the actual content of the div, so it's perfect for labels, icons, or other decorations.\nSelf-Contained Styling: All the styling is right there and it works. You've successfully created the visual you wanted.\n\nA Small Suggestion for Improvement\n\nThe only potential \"weakness\" in your current code is its positioning and the very high z-index.\n\nPositioning with margin: Using margin-top pushes the entire content of the note down to make space for your indicator. This is generally fine, but it can sometimes cause a slight \"jump\" or reflow of the page content when you switch modes.\nThe z-index: A z-index of 9999999999999999 is a \"brute force\" method to ensure the element is on top. While it works, it's usually unnecessary here and can sometimes make future layout debugging difficult if you start adding other custom elements.\n\nA slightly more robust method is to take the indicator out of the document's flow using position: absolute. This way, it floats on top of the content without pushing it around, eliminating any layout shift.\n\nRecommended & Configurable Version\n\nHere is a refined version that integrates this indicator into our configurable CSS model. This version also includes indicators for Live Preview and Source Mode for a complete, consistent experience!\n\nYou can add this to your existing CSS snippet.\n\n/*\n\n           CONFIGURABLE MODE INDICATORS\n\n  -- Adds a small label at the top of the pane to show\n     which view (Reading, Live Preview, Source) is active.\n\n*/\n\n/* --- 1. CONFIGURATION: Edit your indicator styles here! --- */\n:root {\n  /* -- Text for the labels -- */\n  --indicator-reading-text: \"Reading Mode\";\n  --indicator-live-preview-text: \"Live Preview\";\n  --indicator-source-mode-text: \"Source Mode\";\n\n  /* -- Colors for the labels -- */\n  --indicator-reading-bg: #4A6E47;      /* A calm green */\n  --indicator-live-preview-bg: #3B6EA8; /* A standard blue */\n  --indicator-source-mode-bg: #8A4117;  /* The brown you chose */\n  --indicator-text-color: white;\n}\n\n/* --- 2. STYLES: Do not edit below unless you know what you are doing --- */\n\n/* Parent containers need a position set for the absolute-positioned child to work correctly */\n.markdown-reading-view,\n.markdown-source-view {\n  position: relative;\n}\n\n/* Base styles for all indicators using a shared pseudo-element */\n.markdown-reading-view::before,\n.markdown-source-view.is-live-preview::before,\n.markdown-source-view:not(.is-live-preview)::before {\n  /* Positioning */\n  position: absolute; /* Take it out of the document flow */\n  top: 0.5rem;       /* Position from the top */\n  left: 1.5rem;      /* Position from the left */\n  z-index: 10;       /* A reasonable z-index is usually enough */\n\n  /* Appearance */\n  font-size: 10px;\n  font-family: var(--font-interface); /* Use Obsidian's interface font */\n  color: var(--indicator-text-color);\n  width: fit-content;\n  padding: .3rem .5rem;\n  border-radius: 5px;\n}\n\n/* == Specific Indicator Styles == */\n\n/* READING MODE INDICATOR */\n.markdown-reading-view::before {\n  content: var(--indicator-reading-text);\n  background-color: var(--indicator-reading-bg);\n}\n\n/* LIVE PREVIEW INDICATOR */\n.markdown-source-view.is-live-preview::before {\n  content: var(--indicator-live-preview-text);\n  background-color: var(--indicator-live-preview-bg);\n}\n\n/* SOURCE MODE INDICATOR */\n/* We target a source view that is NOT live preview */\n.markdown-source-view:not(.is-live-preview)::before {\n  content: var(--indicator-source-mode-text);\n  background-color: var(--indicator-source-mode-bg);\n}\n\n/* Add a little padding to the top of the main content so the indicator doesn't overlap text */\n.cm-sizer,\n.markdown-preview-sizer {\n  padding-top: 2rem;\n}\n\nHow to Use This\n\nCopy the code above.\nPaste it into your custom CSS snippet file (e.g., colored-headings.css or a new one like mode-indicators.css).\nConfigure the text and colors in the :root block at the top to your liking.\nSave the file.\n\nNow you'll have a non-intrusive, easily configurable indicator that tells you exactly which mode you're in at all times\nAre Obsidian class names generally stable\n\nThat is an excellent and very important question for anyone customizing Obsidian.\n\nThe short answer is: The high-level class names are generally stable, but the low-level, editor-specific ones can be volatile.\n\nHeres a more detailed breakdown, separating the \"safe\" from the \"risky.\"\nThe Stable and Generally Safe Classes\n\nThese are the architectural, high-level classes that define the main structure of the Obsidian interface. The development team tries hard not to change these unless there's a major architectural overhaul. You can feel reasonably confident using them.\n\nView Containers: These are your most reliable hooks.\n    div.markdown-reading-view: The container for a note in Reading Mode.\n    div.markdown-source-view: The container for a note in Editing Mode (this includes both Live Preview and Source Mode).\n    .is-live-preview: A class added to .markdown-source-view when Live Preview is active. This is why our selector .markdown-source-view.is-live-preview is so effective.\nWorkspace Layout:\n    .workspace-leaf-content: The direct parent of a view container.\n    .view-header: The bar at the top of a pane with the note title and buttons.\n    .sidebar-toggle-button, .mod-left-split, .mod-right-split: Classes for the sidebars and UI controls.\nStandard HTML Tags (in Reading Mode):\n    In Reading Mode (.markdown-reading-view), the content is rendered as standard HTML. So, selectors like .markdown-reading-view h1, .markdown-reading-view p, and .markdown-reading-view a are very stable.\n\nThe Less Stable and Potentially Risky Classes\n\nThis is where you need to be cautious. These classes are often generated by underlying libraries that Obsidian uses, and they are subject to change when those libraries are updated.\n\nEditor Classes (.cm- prefix): This is the most volatile area.\n    Obsidian's editor is powered by a library called CodeMirror. All the classes prefixed with .cm- (like .cm-header-1, .cm-s-obsidian, .cm-sizer) come directly from CodeMirror.\n    Historical Example: The biggest \"breaking change\" in Obsidian's history for theme and snippet creators was the update from CodeMirror 5 to CodeMirror 6. This single update changed all of the editor class names, forcing every theme developer to rewrite their editor styles from scratch.\n    While we don't expect another change of that magnitude soon, minor updates to CodeMirror could still alter or remove specific .cm- classes.\n\nPlugin-Generated Classes: Classes added by community plugins can change whenever the plugin developer updates their code. Their stability is entirely up to the developer.\n\nTheme-Internal Classes: Some themes create their own \"helper\" classes (e.g., .is-styled, .custom-callout-style). If you build snippets on top of a specific theme, an update to that theme could break your snippet.\n\nBest Practices for Writing Robust Obsidian CSS\n\nGiven this landscape, here are the best practices that we've been following in our snippets:\n\nAnchor to Stable Parents: Always start your CSS selectors with a stable, high-level class like .markdown-reading-view or .markdown-source-view. This scopes your changes and makes them less likely to break or affect other parts of the app.\n    Good: .markdown-reading-view h1 { ... }\n    Risky: .some-random-autogenerated-div-class-123 h1 { ... }\n\nUse CSS Variables (:root): This is the #1 defense against breaking changes. By defining your colors and values as variables (--h1-color, --line-width), you separate the style value from the selector. If a class name like .cm-header-1 ever changes, you only need to update that one selector in your CSS file; you don't have to hunt down and change color: Red; in multiple places.\n\nUse the Developer Tools: You can inspect the element (Ctrl+Shift+I on Windows/Linux, Cmd+Option+I on macOS) to see the current class names. This is your ground truth. If a snippet stops working after an update, this is the first place to look to see if a class name has changed.\n\nKeep an Eye on the Community: The Obsidian community (especially the forum and Discord in the #appearance channel) is very quick to notice and discuss breaking changes. It's a great resource if something suddenly stops working.\n\nConclusion: The way we have built your snippets is a great example of defensive CSS design. By anchoring to stable parent classes and using variables for configuration, you've made your customizations as future-proof as possible. While nothing is 100% guaranteed, this approach minimizes the maintenance you'll have to do when Obsidian updates.\nNext pass - Unique custom heading colors\n\nOf course! This is a fantastic idea for providing subtle visual cues about the current mode. It requires restructuring the CSS variables and selectors, but it's completely achievable while keeping everything configurable.\n\nFirst, I've cleaned up and consolidated your provided CSS into a single, organized snippet. I removed the duplicate sections and the old commented-out code to create one coherent file.\n\nThen, I modified it to support distinct colors for each mode.\n\nHere is the complete, updated CSS snippet. Replace the entire content of your custom CSS file with this.\nThe Complete, Updated CSS Snippet\n\n/*\n\n           COMPLETE CUSTOM OBSIDIAN STYLES\n\n  This snippet controls:\n  Distinct Heading Colors for Reading vs. Live Preview\n  Max Line Width (Readable Line Length)\n  Active Mode Indicators (Reading, Live Preview, Source)\n\n*/\n\n/* --- 1. CONFIGURATION: Edit all your styles here! --- */\n:root {\n  /* -- Max Width Settings -- */\n  /* 'ch' is a unit relative to the width of the \"0\" character.\n     70-80ch is common for readability. 120ch is wider. */\n  --line-width: 120ch;\n\n\n  /* -- Heading Color Settings -- */\n  /* You can now set different colors for Reading and Live Preview modes. */\n\n  /* Reading Mode Colors */\n  --h1-reading-color: #C62828;   /* Darker Red */\n  --h2-reading-color: #1565C0;   /* Darker Blue */\n  --h3-reading-color: #F9A825;   /* Darker Yellow */\n  --h4-reading-color: #2E7D32;   /* Darker Green */\n  --h5-reading-color: #EF6C00;   /* Darker Orange */\n  --h6-reading-color: #6A1B9A;   /* Darker Purple */\n\n  /* Live Preview (Editing) Colors */\n  --h1-live-preview-color: #E57373;   /* Lighter Red */\n  --h2-live-preview-color: #64B5F6;   /* Lighter Blue */\n  --h3-live-preview-color: #FFF176;   /* Lighter Yellow */\n  --h4-live-preview-color: #81C784;   /* Lighter Green */\n  --h5-live-preview-color: #FFB74D;   /* Lighter Orange */\n  --h6-live-preview-color: #BA68C8;   /* Lighter Purple */\n\n\n  /* -- Mode Indicator Settings -- */\n  --indicator-reading-text: \"Reading\";\n  --indicator-live-preview-text: \"Editing\";\n  --indicator-source-mode-text: \"Source\";\n\n  --indicator-reading-bg: #4A6E47;      /* A calm green */\n  --indicator-live-preview-bg: #3B6EA8; /* A standard blue */\n  --indicator-source-mode-bg: #8A4117;  /* A warm brown */\n  --indicator-text-color: white;\n}\n\n\n/* --- 2. STYLES: Do not edit below unless you know what you are doing --- */\n\n/* == A) MAX WIDTH & INDICATOR SETUP == */\n\n/* Make parent containers relative for indicator positioning */\n.markdown-reading-view,\n.markdown-source-view {\n  position: relative;\n}\n\n/* Set max width and add top padding for the indicator */\n.markdown-preview-view .markdown-preview-sizer,\n.markdown-source-view .cm-sizer {\n  max-width: var(--line-width);\n  margin-inline: auto; /* Center the content */\n  padding-top: 2.5rem;   /* Ensure space for the indicator */\n}\n\n\n/* == B) HEADING COLOR STYLES == */\n\n/* -- Reading Mode Heading Colors -- */\n.markdown-preview-view h1 { color: var(--h1-reading-color); }\n.markdown-preview-view h2 { color: var(--h2-reading-color); }\n.markdown-preview-view h3 { color: var(--h3-reading-color); }\n.markdown-preview-view h4 { color: var(--h4-reading-color); }\n.markdown-preview-view h5 { color: var(--h5-reading-color); }\n.markdown-preview-view h6 { color: var(--h6-reading-color); }\n\n/* -- Live Preview Heading Colors -- */\n.cm-s-obsidian .cm-header-1 { color: var(--h1-live-preview-color); }\n.cm-s-obsidian .cm-header-2 { color: var(--h2-live-preview-color); }\n.cm-s-obsidian .cm-header-3 { color: var(--h3-live-preview-color); }\n.cm-s-obsidian .cm-header-4 { color: var(--h4-live-preview-color); }\n.cm-s-obsidian .cm-header-5 { color: var(--h5-live-preview-color); }\n.cm-s-obsidian .cm-header-6 { color: var(--h6-live-preview-color); }\n\n/* Color the markdown # symbols to match the heading in Live Preview */\n.cm-s-obsidian .cm-formatting-header { color: inherit !important; }\n\n\n/* == C) MODE INDICATOR STYLES == */\n\n/* Base styles for all indicators */\n.markdown-reading-view::before,\n.markdown-source-view.is-live-preview::before,\n.markdown-source-view:not(.is-live-preview)::before {\n  position: absolute;\n  top: 0.5rem;\n  left: 50%; /* Center the indicator */\n  transform: translateX(-50%); /* Fine-tune centering */\n  z-index: 10;\n\n  font-size: 10px;\n  font-family: var(--font-interface);\n  color: var(--indicator-text-color);\n  width: fit-content;\n  padding: .3rem .7rem;\n  border-radius: 5px;\n  font-weight: 600;\n}\n\n/* Indicator for READING MODE */\n.markdown-reading-view::before {\n  content: var(--indicator-reading-text);\n  background-color: var(--indicator-reading-bg);\n}\n\n/* Indicator for LIVE PREVIEW */\n.markdown-source-view.is-live-preview::before {\n  content: var(--indicator-live-preview-text);\n  background-color: var(--indicator-live-preview-bg);\n}\n\n/* Indicator for SOURCE MODE */\n.markdown-source-view:not(.is-live-preview)::before {\n  content: var(--indicator-source-mode-text);\n  background-color: var(--indicator-source-mode-bg);\n}\nSummary of Changes\n\nConsolidated :root Block: All configuration variables are now in a single :root block at the top of the file for easy access.\nSeparate Heading Color Variables:\n    I've created two sets of variables: --h1-reading-color, --h2-reading-color, etc., for Reading Mode.\n    And --h1-live-preview-color, --h2-live-preview-color, etc., for Live Preview.\n    I've pre-filled them with slightly different shades (darker for reading, lighter for editing) as an example, but you can change them to anything you like.\nSplit CSS Rules: The key change is splitting the CSS selectors.\n    Previously, we had: .markdown-preview-view h1, .cm-s-obsidian .cm-header-1 { ... }\n    Now, they are separate rules, each pointing to its own set of color variables. This allows for the distinct styling.\nImproved Mode Indicator: I made a minor tweak to center the mode indicator at the top of the pane, which often looks a little cleaner than having it left-aligned when the text itself is centered. I also added margin-inline: auto; to the text sizers to ensure they are centered within the pane.\n\nNow you have full, independent control over the colors in each mode, all from one convenient configuration block at the top of your file\nFix centered content\n\nYou are absolutely right! My apologies for that oversight.\n\nThe culprit is a single line I added to center the text block within the pane, which is a common preference but clearly not what you wanted.\n\nThe line is margin-inline: auto;. Its purpose is to take the text block (which has a max-width) and center it horizontally within the larger pane area. We just need to remove it to restore the default left-alignment.\nThe Fix: Remove One Line\n\nHere is the corrected section.\n\n1. Find this block in your CSS file:\n\n/* == A) MAX WIDTH & INDICATOR SETUP == */\n\n/* Make parent containers relative for indicator positioning */\n.markdown-reading-view,\n.markdown-source-view {\n  position: relative;\n}\n\n/* Set max width and add top padding for the indicator */\n.markdown-preview-view .markdown-preview-sizer,\n.markdown-source-view .cm-sizer {\n  max-width: var(--line-width);\n  margin-inline: auto; /* Center the content <-- REMOVE THIS LINE */\n  padding-top: 2.5rem;   /* Ensure space for the indicator */\n}\n\n2. Remove the margin-inline: auto; line. The block should now look like this:\n\n/* Set max width and add top padding for the indicator */\n.markdown-preview-view .markdown-preview-sizer,\n.markdown-source-view .cm-sizer {\n  max-width: var(--line-width);\n  padding-top: 2.5rem;   /* Ensure space for the indicator */\n}\n\nSave the file, and your content should snap back to being left-aligned while keeping all the other new features.\n\nThe Full Corrected Snippet\n\nFor your convenience, here is the entire CSS file with that one line removed. You can simply copy and paste this to replace everything in your snippet file.\n\n/*\n\n           COMPLETE CUSTOM OBSIDIAN STYLES\n\n  This snippet controls:\n  Distinct Heading Colors for Reading vs. Live Preview\n  Max Line Width (Readable Line Length)\n  Active Mode Indicators (Reading, Live Preview, Source)\n\n*/\n\n/* --- 1. CONFIGURATION: Edit all your styles here! --- */\n:root {\n  /* -- Max Width Settings -- */\n  /* 'ch' is a unit relative to the width of the \"0\" character.\n     70-80ch is common for readability. 120ch is wider. */\n  --line-width: 120ch;\n\n\n  /* -- Heading Color Settings -- */\n  /* You can now set different colors for Reading and Live Preview modes. */\n\n  /* Reading Mode Colors */\n  --h1-reading-color: #C62828;   /* Darker Red */\n  --h2-reading-color: #1565C0;   /* Darker Blue */\n  --h3-reading-color: #F9A825;   /* Darker Yellow */\n  --h4-reading-color: #2E7D32;   /* Darker Green */\n  --h5-reading-color: #EF6C00;   /* Darker Orange */\n  --h6-reading-color: #6A1B9A;   /* Darker Purple */\n\n  /* Live Preview (Editing) Colors */\n  --h1-live-preview-color: #E57373;   /* Lighter Red */\n  --h2-live-preview-color: #64B5F6;   /* Lighter Blue */\n  --h3-live-preview-color: #FFF176;   /* Lighter Yellow */\n  --h4-live-preview-color: #81C784;   /* Lighter Green */\n  --h5-live-preview-color: #FFB74D;   /* Lighter Orange */\n  --h6-live-preview-color: #BA68C8;   /* Lighter Purple */\n\n\n  /* -- Mode Indicator Settings -- */\n  --indicator-reading-text: \"Reading\";\n  --indicator-live-preview-text: \"Editing\";\n  --indicator-source-mode-text: \"Source\";\n\n  --indicator-reading-bg: #4A6E47;      /* A calm green */\n  --indicator-live-preview-bg: #3B6EA8; /* A standard blue */\n  --indicator-source-mode-bg: #8A4117;  /* A warm brown */\n  --indicator-text-color: white;\n}\n\n\n/* --- 2. STYLES: Do not edit below unless you know what you are doing --- */\n\n/* == A) MAX WIDTH & INDICATOR SETUP == */\n\n/* Make parent containers relative for indicator positioning */\n.markdown-reading-view,\n.markdown-source-view {\n  position: relative;\n}\n\n/* Set max width and add top padding for the indicator */\n.markdown-preview-view .markdown-preview-sizer,\n.markdown-source-view .cm-sizer {\n  max-width: var(--line-width);\n  padding-top: 2.5rem;   /* Ensure space for the indicator */\n}\n\n\n/* == B) HEADING COLOR STYLES == */\n\n/* -- Reading Mode Heading Colors -- */\n.markdown-preview-view h1 { color: var(--h1-reading-color); }\n.markdown-preview-view h2 { color: var(--h2-reading-color); }\n.markdown-preview-view h3 { color: var(--h3-reading-color); }\n.markdown-preview-view h4 { color: var(--h4-reading-color); }\n.markdown-preview-view h5 { color: var(--h5-reading-color); }\n.markdown-preview-view h6 { color: var(--h6-reading-color); }\n\n/* -- Live Preview Heading Colors -- */\n.cm-s-obsidian .cm-header-1 { color: var(--h1-live-preview-color); }\n.cm-s-obsidian .cm-header-2 { color: var(--h2-live-preview-color); }\n.cm-s-obsidian .cm-header-3 { color: var(--h3-live-preview-color); }\n.cm-s-obsidian .cm-header-4 { color: var(--h4-live-preview-color); }\n.cm-s-obsidian .cm-header-5 { color: var(--h5-live-preview-color); }\n.cm-s-obsidian .cm-header-6 { color: var(--h6-live-preview-color); }\n\n/* Color the markdown # symbols to match the heading in Live Preview */\n.cm-s-obsidian .cm-formatting-header { color: inherit !important; }\n\n\n/* == C) MODE INDICATOR STYLES == */\n\n/* Base styles for all indicators */\n.markdown-reading-view::before,\n.markdown-source-view.is-live-preview::before,\n.markdown-source-view:not(.is-live-preview)::before {\n  position: absolute;\n  top: 0.5rem;\n  left: 50%; /* Center the indicator */\n  transform: translateX(-50%); /* Fine-tune centering */\n  z-index: 10;\n\n  font-size: 10px;\n  font-family: var(--font-interface);\n  color: var(--indicator-text-color);\n  width: fit-content;\n  padding: .3rem .7rem;\n  border-radius: 5px;\n  font-weight: 600;\n}\n\n/* Indicator for READING MODE */\n.markdown-reading-view::before {\n  content: var(--indicator-reading-text);\n  background-color: var(--indicator-reading-bg);\n}\n\n/* Indicator for LIVE PREVIEW */\n.markdown-source-view.is-live-preview::before {\n  content: var(--indicator-live-preview-text);\n  background-color: var(--indicator-live-preview-bg);\n}\n\n/* Indicator for SOURCE MODE */\n.markdown-source-view:not(.is-live-preview)::before {\n  content: var(--indicator-source-mode-text);\n  background-color: var(--indicator-source-mode-bg);\n}\nFix right-aligned mode indicator\n\nYou are absolutely correct. My apologies again! In trying to center the indicator, I created a new alignment issue when we reverted the text justification.\n\nWhen the text below is left-aligned, a centered indicator can look disconnected and appear \"off to the right\" relative to the start of your content.\n\nWe will move it back to the top-left corner, where it was in the earlier version. This will align it nicely with your left-justified text.\nThe Fix: Change 2 Lines\n\nThis is another simple fix.\n\n1. Find this block in your CSS file:\n\nIt's the last main block, under the /* == C) MODE INDICATOR STYLES == */ comment.\n\n/* Base styles for all indicators */\n.markdown-reading-view::before,\n.markdown-source-view.is-live-preview::before,\n.markdown-source-view:not(.is-live-preview)::before {\n  position: absolute;\n  top: 0.5rem;\n  left: 50%; /* Center the indicator <-- CHANGE THIS */\n  transform: translateX(-50%); /* Fine-tune centering <-- AND THIS */\n  z-index: 10;\n  /* ... etc ... */\n}\n\n2. Replace the left and transform lines with a single left declaration to position it from the edge:\n\n/* Base styles for all indicators */\n.markdown-reading-view::before,\n.markdown-source-view.is-live-preview::before,\n.markdown-source-view:not(.is-live-preview)::before {\n  position: absolute;\n  top: 0.5rem;\n  left: 1.5rem; /* Position from the left edge */\n  z-index: 10;\n  /* ... etc ... */\n}\n\nSave the file, and the indicator will snap back to the top-left of the pane.\n\nThe Final, Fully Corrected Snippet\n\nFor your convenience, here is the complete and final version of the code with this last correction applied. You can replace the entire contents of your snippet file with this.\n\n/*\n\n           COMPLETE CUSTOM OBSIDIAN STYLES\n\n  This snippet controls:\n  Distinct Heading Colors for Reading vs. Live Preview\n  Max Line Width (Readable Line Length)\n  Active Mode Indicators (Reading, Live Preview, Source)\n\n*/\n\n/* --- 1. CONFIGURATION: Edit all your styles here! --- */\n:root {\n  /* -- Max Width Settings -- */\n  /* 'ch' is a unit relative to the width of the \"0\" character.\n     70-80ch is common for readability. 120ch is wider. */\n  --line-width: 120ch;\n\n\n  /* -- Heading Color Settings -- */\n  /* You can now set different colors for Reading and Live Preview modes. */\n\n  /* Reading Mode Colors */\n  --h1-reading-color: #C62828;   /* Darker Red */\n  --h2-reading-color: #1565C0;   /* Darker Blue */\n  --h3-reading-color: #F9A825;   /* Darker Yellow */\n  --h4-reading-color: #2E7D32;   /* Darker Green */\n  --h5-reading-color: #EF6C00;   /* Darker Orange */\n  --h6-reading-color: #6A1B9A;   /* Darker Purple */\n\n  /* Live Preview (Editing) Colors */\n  --h1-live-preview-color: #E57373;   /* Lighter Red */\n  --h2-live-preview-color: #64B5F6;   /* Lighter Blue */\n  --h3-live-preview-color: #FFF176;   /* Lighter Yellow */\n  --h4-live-preview-color: #81C784;   /* Lighter Green */\n  --h5-live-preview-color: #FFB74D;   /* Lighter Orange */\n  --h6-live-preview-color: #BA68C8;   /* Lighter Purple */\n\n\n  /* -- Mode Indicator Settings -- */\n  --indicator-reading-text: \"Reading\";\n  --indicator-live-preview-text: \"Editing\";\n  --indicator-source-mode-text: \"Source\";\n\n  --indicator-reading-bg: #4A6E47;      /* A calm green */\n  --indicator-live-preview-bg: #3B6EA8; /* A standard blue */\n  --indicator-source-mode-bg: #8A4117;  /* A warm brown */\n  --indicator-text-color: white;\n}\n\n\n/* --- 2. STYLES: Do not edit below unless you know what you are doing --- */\n\n/* == A) MAX WIDTH & INDICATOR SETUP == */\n\n/* Make parent containers relative for indicator positioning */\n.markdown-reading-view,\n.markdown-source-view {\n  position: relative;\n}\n\n/* Set max width and add top padding for the indicator */\n.markdown-preview-view .markdown-preview-sizer,\n.markdown-source-view .cm-sizer {\n  max-width: var(--line-width);\n  padding-top: 2.5rem;   /* Ensure space for the indicator */\n}\n\n\n/* == B) HEADING COLOR STYLES == */\n\n/* -- Reading Mode Heading Colors -- */\n.markdown-preview-view h1 { color: var(--h1-reading-color); }\n.markdown-preview-view h2 { color: var(--h2-reading-color); }\n.markdown-preview-view h3 { color: var(--h3-reading-color); }\n.markdown-preview-view h4 { color: var(--h4-reading-color); }\n.markdown-preview-view h5 { color: var(--h5-reading-color); }\n.markdown-preview-view h6 { color: var(--h6-reading-color); }\n\n/* -- Live Preview Heading Colors -- */\n.cm-s-obsidian .cm-header-1 { color: var(--h1-live-preview-color); }\n.cm-s-obsidian .cm-header-2 { color: var(--h2-live-preview-color); }\n.cm-s-obsidian .cm-header-3 { color: var(--h3-live-preview-color); }\n.cm-s-obsidian .cm-header-4 { color: var(--h4-live-preview-color); }\n.cm-s-obsidian .cm-header-5 { color: var(--h5-live-preview-color); }\n.cm-s-obsidian .cm-header-6 { color: var(--h6-live-preview-color); }\n\n/* Color the markdown # symbols to match the heading in Live Preview */\n.cm-s-obsidian .cm-formatting-header { color: inherit !important; }\n\n\n/* == C) MODE INDICATOR STYLES == */\n\n/* Base styles for all indicators */\n.markdown-reading-view::before,\n.markdown-source-view.is-live-preview::before,\n.markdown-source-view:not(.is-live-preview)::before {\n  position: absolute;\n  top: 0.5rem;\n  left: 1.5rem; /* Position from the left edge */\n  z-index: 10;\n\n  font-size: 10px;\n  font-family: var(--font-interface);\n  color: var(--indicator-text-color);\n  width: fit-content;\n  padding: .3rem .7rem;\n  border-radius: 5px;\n  font-weight: 600;\n}\n\n/* Indicator for READING MODE */\n.markdown-reading-view::before {\n  content: var(--indicator-reading-text);\n  background-color: var(--indicator-reading-bg);\n}\n\n/* Indicator for LIVE PREVIEW */\n.markdown-source-view.is-live-preview::before {\n  content: var(--indicator-live-preview-text);\n  background-color: var(--indicator-live-preview-bg);\n}\n\n/* Indicator for SOURCE MODE */\n.markdown-source-view:not(.is-live-preview)::before {\n  content: var(--indicator-source-mode-text);\n  background-color: var(--indicator-source-mode-bg);\n}",
        "locale": "en",
        "slug": "technical-posts/obsidian-custom-css-gemini",
        "folder": ""
    },
    {
        "title": "Using the Postgres pg module with sveltekit - part 1",
        "description": "Using the Postgres pg module with sveltekit - part 1",
        "date_created": "2025-05-20T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "postgres"
        ],
        "content": "Setting up the connection to be globally available through the locals collection in  hooks.server.ts.\n\nSome of the following code is optional: \ncheckDBConnection\nprocess.on for SIGTERM\n\n \n// src/hooks.server.ts\nimport type { Handle } from '@sveltejs/kit';\n\nimport {\n    PG_HOST,\n    PG_PORT,\n    PG_USER,\n    PG_PASSWORD,\n    PG_DATABASE\n} from '$env/static/private';\nimport pkg from 'pg';\n\n\nconst { Pool } = pkg;\n\n// Create the pool once when the server starts\nconst pool = new Pool({\n    host: PG_HOST,\n    port: Number(PG_PORT) || 5432, // Ensure port is a number\n    user: PG_USER,\n    password: PG_PASSWORD,\n    database: PG_DATABASE,\n    ssl: false // Or configure based on your needs, e.g., { rejectUnauthorized: false } for self-signed certs\n});\n\nconsole.log(PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE);\n\n// Optional: Listen for pool errors\npool.on('error', (err, client) => {\n    console.error('Unexpected error on idle database client', err);\n    // You might want to decide if the process should exit here\n});\n\n\n// Check DB health.\nasync function checkDbConnection() {\n  const client = await pool.connect();\n  try {\n    await client.query('SELECT 1');\n    console.log('Database connection successful');\n  } catch (err) {\n    console.error('Database connection failed:', err);\n  } finally {\n    client.release();\n  }\n}\n\n// Call on startup\ncheckDbConnection();\n\nprocess.on('SIGTERM', async () => {\n  console.log('Closing database pool...');\n  await pool.end();\n  console.log('Database pool closed');\n  process.exit(0);\n});\n\n// Optional: You can also handle other signals like SIGINT (Ctrl+C) if needed\nexport const handle: Handle = async ({ event, resolve }) => {    \n    // Make the pool available to all server-side endpoints\n    event.locals.pool = pool;\n\n    // Continue processing the request\n    const response = await resolve(event);\n\n    // Cleanup or other logic after the request is handled can go here if needed\n    return response;\n}",
        "locale": "en",
        "slug": "technical-posts/using-postgres-pg-module-with-sveltekit-1",
        "folder": ""
    },
    {
        "title": "Typescript nuggets",
        "description": "Typescript nuggets",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript",
            "nuggets"
        ],
        "content": "Add Node types with PNPM\n\npnpm add -D @types/node",
        "locale": "en",
        "slug": "technical-posts/typescript-nuggets",
        "folder": ""
    },
    {
        "title": "windows-10-esu",
        "description": "windows-10-esu",
        "date_created": "2025-09-17T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows10"
        ],
        "content": "ESU details\nWhat is the Extended Update Security program\n\nThe Extended Security Update (ESU) program is a last resort option for customers who need to run certain legacy Microsoft products past the end of support. They are not intended as a long-term solution, but rather as a temporary bridge to stay secure while one migrates to a newer, supported platform. \n\nIt includes Critical and/or Important security updates up to three years after the product's end-of-support date. ESUs donotinclude new features, customer-requested non-security updates, or design change requests.\n\nIn the past, ESU has been available only for large enterprise customers. With Windows 10, for the first time ever, Microsoft offers ESU to Windows 10 for individual consumers. Enterprise ESU plans cover three years, the Windows 10 ESU plan is for one year only. For consumer users, the Windows 10 ESU appears in the Windows Update panel:\n\n|629x389\n\nAs we try to make sense out of Microsoft's ambiguous Windows 10 consumer ESU program, remember that there is only one ESU program--a part of which now available for Windows 10 consumer users. \nHow does Microsoft define an \"individual\" or \"consumer\" user\n\nI cannot find any a definitive MS document that defines \"individual\" or \"consumer\" user. However, searching the topic I got this definition with Chat GPT \n\nMicrosoft uses individual, personal use, consumer, any individual using a Windows 10 device  not managed by an organization  etc, to indicate how it distinguishes between consumers/individuals vs. businesses/organizations.\nMS ESU timeline\n\nOctober 2024\nThis was the first we heard of an ESU for Windows 10. It didn't contain any details beyond what you see here.\n\n\nJune 2025\nIn June 2025, MS finally shared more details on ESU for Windows 10\n\n\n\nSeptember 2025\nIn September, they reiterated the June 2025, but (for me at least) with confusing phrasing.\n|501x86\n\nWhen I first read the September 2025 announcement I misread \"that provides individuals and organizations of all sizes,\" I thought that phrase introduced a major change in the policy. I read it as the $30 ESU offer now applies to individuals and organizations of all sizes; that's not what it means. This text is simply echoing the text from the October 2024 announcement. \nThere are four types of MS volume pricing plans\n\nA business subscribe to one of these four MS volume pricing programs to acquire the three-year, $61 (the first year) ESU. The fourth program is probably the only one that could apply, and be affordable, to our customers. \n\nAll of these plans are not available directly from Microsoft, you must purchase one of these plans through a qualified MS partner. I scoured the Web trying to get a thumbnail price on the Open options, with no good results. \n\nEA (Enterprise Agreement)\n\thttps://www.microsoft.com/licensing/docs/view/Enterprise-Agreement-EA-EAS-SCE\n\t500 users and/or devices\nCSP (Cloud Solution Provider)\n\thttps://partner.microsoft.com/en-be/partnership/cloud-solution-provider\n\t1m annually, advanced or premier support plan starting at $15k per year)\nMPSA\n\t250 or more users/devices\nOpen, Open Value, and Open Value Subscription\n\t5 or more desktop PCs\n\thttps://www.microsoft.com/en-us/licensing/licensing-programs/open-license\n\tfile:///C:/Users/thumb/Downloads/Open_Programs_Overview.pdf\n\nIt is very clear that MS intends for the \"consumer\" $30 ESU to be to used by private-used PCs, not those being used commercially. \n\nIt's my opinion that we should not continue to support Windows 10 after October 14th. We have told customers that's what we're going to do since January--it will not take any customer by surprise. \n\nI know that large corporate customers may continue to use Windows 10 through a corporate volume. Those customers aren't in our customer domain. I do realize that CVS or SWBC has the potential to be in that group, but the odds of this affecting those relations is beyond very small. And, if it does, then we make exceptions. \n\nI don't think the expense, effort, and grief of continuing to support Windows 10 just isn't worth it for us. \n\nIt's too costly and troublesome to keep Windows 10 for testing \n\tThat said, MS Defender is supported through 2028 so maybe it wouldn't be so bad to keep a Windows 10 box available for testing.\n\tMost malware vectors are with downloads, attachments, and links and there won't be any of that on the Windows 10 box\nIf we do support Win 10, will support it for three years?  If we do support it, I think it should be for one year. \nThe likelihood of one of our customers and being enrolled in an MS Volume Pricing Plan is nearly zero.\n\nFWIW, regardless of our decision to support Windows 10, I think customer engagement with Win 10 will be nearly zero, as it was with the security vulnerability. \n \n9 months",
        "locale": "en",
        "slug": "technical-posts/windows-10-esu",
        "folder": ""
    },
    {
        "title": "Self-hosting Google fonts",
        "description": "Self-hosting Google fonts",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "web-dev"
        ],
        "content": "!info]\nASNA.com originally used Google Fonts but loading them directly from Google raises GDPR concerns. I considered self-hosting them but this [Kevin Powell convinced me that using basic system fonts was a better strategy.\n\nGoogle fonts are great, but they are also a potential GDPR violation and there is overhead loading them from the Google site.\n\nYou won't have to worry about Google changing the font\nMore performant.\n\nGoogle doc on self-hosting Google fonts.\n\nASNA.com uses the regular 400 version of the Google Open Sans font. Go to the font link.\n\nGoogle shows the link for this font as:\n\n\n\n\n\nDisplay the font in your browser:\n\nhttps://fonts.googleapis.com/css2?family=Open+Sans:ital@0;1&display=swap\n\nto see many language variations listed. Copy the entire Latin @font-face tag.\n\n![[Pasted image 20231201112143.png]]\n\nPaste its URL your browser to download the font.\n\nhttps://fonts.gstatic.com/s/opensans/v36/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0B4gaVI.woff2\n\nPut that font in the project's static\\fonts folder. SvelteKit exposes this static folder as the root of the site when the site is deployed. Browser dev tools show the font as being loaded from:\n\n/fonts/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0B4gaVI.woff2\n\nIn the \\resets.css file, add the @font-face declaration from above:\n\n@font-face {\n        font-family: 'Open Sans';\n        font-style: normal;\n        font-weight: 400;\n        font-stretch: 100%;\n        font-display: swap;\n        src: url(/fonts/memSYaGs126MiZpBA-UvWbX2vVnXBbObj2OVZyOOSr4dVJWUgsjZ0B4gaVI.woff2) format('woff2');\n        unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+0304, U+0308, U+0329, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;\n    }\n\nNote how the url of the @font-face tag above shows the font file living in the /fonts folder directly off the root. That matches where we put the font.\n\nThe body selector in \\resets.css sets 'Open Sans' as the current font.\n\nbody {\n        font-family: 'Open Sans', 'Atkinson Hyperlegible', sans-serif;\n        background-color: var(--page-bg-color);\n        color: var(--text-color);\n}",
        "locale": "en",
        "slug": "technical-posts/self-hosting-google-fonts",
        "folder": ""
    },
    {
        "title": "Sveltekit universal load functions",
        "description": "Sveltekit universal load functions",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "https://kit.svelte.dev/docs/load#universal-vs-server-input\n\nBoth universal and server load functions have access to properties describing the request (params, route and url) and various functions (fetch, setHeaders, parent and depends).\n\nexport async function load({ url, route, params }) {\n...\n}\n\nyields:\n\nroute = { id: \"/en/kb/[slug]\" };\nurl = \"http://localhost:5173/en/kb/first\";\nparams = { slug: \"first\" };\n\n+layout.js\n\nexport const load = async (data) => {\n\t// console.log($page);\n\treturn {\n\t\t...data,\n\t\tpageUrl: 'bobobob'\n\t};\n};\n\n+page.svelte\n\nexport async function load({ parent }) {\n\tconst parentData = await parent();\n...\n}",
        "locale": "en",
        "slug": "technical-posts/universal-and-server-load-functions",
        "folder": ""
    },
    {
        "title": "Using Typescript generics",
        "description": "Using Typescript generics",
        "date_created": "2025-10-29T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "User\n\n@helpers.ts @validate.ts\n\nUser\n\n@helpers.ts @validate.ts \n\nHow can I make the parseMarkdownFile function and its associated type ParsedMarkdown more generic so that I can dynamically type the frontMatter object. In this case i want to type it as TechnicalNoteFrontMatter\n\nTool Call: Thinking\nStatus: Completed\n\n\nThe user wants to make parseMarkdownFile and ParsedMarkdown generic so that they can specify a custom type for the frontMatter object, like TechnicalNoteFrontMatter.\n\nCurrently:\nParsedMarkdown is an interface with frontMatter: Record\nparseMarkdownFile returns Promise\n\nTo make this generic, I should:\nAdd a type parameter to ParsedMarkdown interface\nAdd a type parameter to parseMarkdownFile function\nUpdate writeMarkdownFile to also accept the generic type\nProvide a default type parameter to maintain backward compatibility\n\nThe generic approach would look like:\nexport interface ParsedMarkdown = Record> {\n  frontMatter: T;\n  content: string;\n  rawFrontMatter?: string;\n}\n\nexport const parseMarkdownFile = async  = Record>(\n  filename: string,\n): Promise> => {\n  // ... existing implementation\n  // The return type would be ParsedMarkdown\n}\n\nThis way, when the user calls it, they can do:\nconst markdownObject = await parseMarkdownFile(fi.fullName);\n// Now markdownObject.frontMatter is typed as TechnicalNoteFrontMatter\n\nLet me provide a complete solution.\n\nAssistant\n\nYou can make both ParsedMarkdown and parseMarkdownFile generic by adding type parameters. Here's how to update helpers.ts:\n\nexport interface ParsedMarkdown = Record> {\n  frontMatter: T;\n  content: string;\n  rawFrontMatter?: string;\n}\n\n/**\n Parses a markdown file with optional YAML frontmatter\n * Extracts frontmatter delimited by --- markers at the beginning of the file\n and parses it as YAML. The remaining content is returned as plain text.\n * @template T - The type of the frontmatter object\n @param filename - The path to the markdown file to parse\n @returns A promise that resolves to a ParsedMarkdown object containing the parsed frontmatter and content\n @throws {Error} If the file cannot be read or parsing fails\n */\nexport const parseMarkdownFile = async  = Record>(\n  filename: string,\n): Promise> => {\n\nAnd update the writeMarkdownFile function signature:\n\nexport const writeMarkdownFile = async  = Record>(\n  parsedMarkdown: ParsedMarkdown,\n  outputFilename: string,\n): Promise => {\n\nNow in validate.ts, you can use it like this:\n\nconst markdownObject = await parseMarkdownFile(fi.fullName);\n\nconst frontMatter = convertFrontmatterDateStrings(markdownObject.frontMatter);\n// frontMatter is now typed as MarkdownFrontmatter\n// No need for explicit type annotation\n\nKey benefits:\n\nType safety: TypeScript knows the exact shape of frontMatter based on your generic parameter\nBackward compatibility: The default generic parameter Record means existing code without the generic still works\nFlexibility: You can use different frontmatter types for different markdown files (e.g., TechnicalNoteFrontMatter, BlogPostFrontMatter, etc.)\n\nExample usage with validation:\n\n// Define your frontmatter type\ntype TechnicalNoteFrontMatter = {\n  description: string;\n  date_created: Date;\n  date_updated: Date;\n  tags: string];\n};\n\n// Parse with the specific type\nconst markdownObject = await parseMarkdownFile(fi.fullName);\n\n// TypeScript now knows frontMatter has the TechnicalNoteFrontMatter shape\nconst frontMatter = convertFrontmatterDateStrings(markdownObject.frontMatter);\n\n// Update properties with full type safety\nfrontMatter.date_updated = fi.changed;\nfrontMatter.date_created = fi.created;\n\n// TypeScript will catch errors if you try to access properties that don't exist\n// frontMatter.nonexistent = \"error\"; // TS error!\n\nUser\n\n[@helpers.ts I think I need to type parsedFrontMatter on line 196 to T (or something like that!)\n\nAssistant\n\nYou're right! The parsedFrontMatter variable on line 196 should be typed as T to maintain type safety throughout the function. Here's the fix:\n\n// Parse frontMatter YAML\n    let parsedFrontMatter: T = {} as T;\n\nAnd then update the return statement to properly type the object:\n\nreturn {\n      frontMatter: parsedFrontMatter as T,\n      content: contentLines.join(\"\\n\"),\n      rawFrontMatter: rawFrontMatter || undefined,\n    };\n\nHowever, there's a caveat: since yaml.load() returns any, we can't guarantee at runtime that the parsed YAML actually matches type T. This is a common pattern in TypeScript where you're asserting a type based on expected structure.\n\nIf you want more runtime safety, you'd typically use this in combination with Zod validation (which you're already doing in validate.ts):\n\n// Parse as unknown first\nconst markdownObject = await parseMarkdownFile(fi.fullName);\n\n// Convert date strings\nconst convertedFrontMatter = convertFrontmatterDateStrings(markdownObject.frontMatter);\n\n// Validate and parse with Zod\nconst result = MarkdownFrontmatterSchema.safeParse(convertedFrontMatter);\nif (result.success) {\n  const typedFrontMatter: MarkdownFrontmatter = result.data; // Now truly type-safe\n\nOr, if you want to use the generic type parameter purely as a type hint (which is valid):\n\n// This tells TypeScript \"I expect this shape\" but doesn't validate at runtime\nconst markdownObject = await parseMarkdownFile(fi.fullName);\n\n// You still need Zod for runtime validation\nconst result = MarkdownFrontmatterSchema.safeParse(markdownObject.frontMatter);\n\nThe generic approach gives you better IDE autocomplete and compile-time checking, while Zod gives you runtime validation. Using both together is the most robust approach!\n\n\nI thought 'parsed as T' was part of the solution. why are both parts of the ternary enclosed in parentheses and what is ({} as T) doing\n\nGreat questions! Let me break this down:\n\nWhy parentheses around both parts?\n\nThe parentheses are needed because of operator precedence. Without them, TypeScript might parse the expression incorrectly. Here's what happens:\n\n// WITHOUT parentheses - ambiguous parsing\nparsedFrontMatter = parsed && typeof parsed === \"object\" ? parsed as T : {} as T;\n// TypeScript might interpret this as:\n// (parsedFrontMatter = ... ? parsed as T : {}) as T\n\n// WITH parentheses - clear intent\nparsedFrontMatter = parsed && typeof parsed === \"object\" \n  ? (parsed as T)    // clearly the 'true' branch\n  : ({} as T);       // clearly the 'false' branch\n\nWhat is ({} as T) doing?\n\nThis is a type assertion (also called type casting). Let me explain:\n\n{} - This creates an empty object literal of type {}\nas T - This tells TypeScript \"trust me, treat this as type T\"\n({} as T) - The parentheses ensure the type assertion applies to the object literal\n\nSo ({} as T) is saying: \"I'm creating an empty object, but TypeScript, please treat it as if it has the shape of type T.\"\n\nWhy do we need this?\n\n// If T is defined as:\ntype MyFrontMatter = {\n  title: string;\n  date: Date;\n}\n\n// Then:\nlet parsedFrontMatter: T = {};  //  ERROR! {} doesn't have title and date properties\n\nlet parsedFrontMatter: T = {} as T;  //  OK! We're asserting the type\n\n// At runtime, this object is still empty {}\n// But TypeScript will now allow you to treat it as type T\n// This is useful when you know the object will be populated later\n// or when parsing external data where you can't guarantee the structure\n\nImportant: Type assertions like as T are purely compile-time. They don't do any runtime validation or conversion. That's why you need Zod validation afterward to ensure the data actually matches the expected shape!",
        "locale": "en",
        "slug": "technical-posts/using-typescript-generics",
        "folder": ""
    },
    {
        "title": "Transforming Obsidian image links",
        "description": "This article shows how to transform Obsidian image links into HTML img tags at runtime in Sveltekit app.",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-29T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "obsidian"
        ],
        "content": "\n[[supabase-credentials#env files]]\n\n unist-util-visit is an NPM package is a small utility forwalking (visiting) nodes in a Unist syntax tree. Unist is the common AST shape used across the Markdown/HTML \n  \n  This code uses that package to to convert You can convert Obsidians ![[image.png|700]] image syntx into a normal image node (with an optional width attribute) prepend a base URL.\n\n1) Install\nnpm i -D unist-util-visit`\n\n2) Add a remark plugin\nimport { visit } from 'unist-util-visit';\n\nconst OBSIDIAN_IMAGE_RE = /!\\[\\[(|]+?)(?:\\|(\\d+))?\\]\\]/g;\n\nfunction joinUrl(baseUrl, file) {\n\tconst base = baseUrl.endsWith('/') ? baseUrl : baseUrl + '/';\n\t// keep it simple; encode spaces etc.\n\treturn base + encodeURIComponent(file.trim());\n}\n\nfunction defaultAltFromFilename(file) {\n\t// \"image-32.png\" -> \"image-32\"\n\tconst name = file.replace(/^.*[\\\\/]/, '').replace(/\\.[a-z0-9]+$/i, '');\n\treturn name || '';\n}\n\n/**\n Transforms Obsidian image embeds:\n ![[image-32.png|700]]\n into mdast image nodes with optional width via hProperties.\n * @param {{ baseUrl: string, alt?: 'filename' | 'empty' | ((file:string)=>string) }} options\n */\nexport default function remarkObsidianImages(options = {}) {\n\tconst { baseUrl, alt = 'filename' } = options;\n\tif (!baseUrl) throw new Error('remarkObsidianImages: \"baseUrl\" is required');\n\n\tconst getAlt =\n\t\ttypeof alt === 'function'\n\t\t\t? alt\n\t\t\t: alt === 'empty'\n\t\t\t\t? () => ''\n\t\t\t\t: (file) => defaultAltFromFilename(file);\n\n\treturn (tree) => {\n\t\tvisit(tree, 'text', (node, index, parent) => {\n\t\t\tif (!parent || typeof index !== 'number') return;\n\n\t\t\tconst value = node.value;\n\t\t\tif (!value || !value.includes('![[')) return;\n\n\t\t\tlet match;\n\t\t\tlet lastIndex = 0;\n\t\t\tconst parts = [];\n\n\t\t\tOBSIDIAN_IMAGE_RE.lastIndex = 0;\n\t\t\twhile ((match = OBSIDIAN_IMAGE_RE.exec(value)) !== null) {\n\t\t\t\tconst [raw, file, widthStr] = match;\n\t\t\t\tconst start = match.index;\n\t\t\t\tconst end = start + raw.length;\n\n\t\t\t\t// leading text\n\t\t\t\tif (start > lastIndex) {\n\t\t\t\t\tparts.push({ type: 'text', value: value.slice(lastIndex, start) });\n\t\t\t\t}\n\n\t\t\t\tconst width = widthStr ? Number(widthStr) : undefined;\n\n\t\t\t\tparts.push({\n\t\t\t\t\ttype: 'image',\n\t\t\t\t\turl: joinUrl(baseUrl, file),\n\t\t\t\t\talt: getAlt(file),\n\t\t\t\t\ttitle: null,\n\t\t\t\t\tdata: width\n\t\t\t\t\t\t? {\n\t\t\t\t\t\t\t\thProperties: { width }\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t: undefined\n\t\t\t\t});\n\n\t\t\t\tlastIndex = end;\n\t\t\t}\n\n\t\t\t// trailing text\n\t\t\tif (lastIndex  p.type === 'image')) {\n\t\t\t\tparent.children.splice(index, 1, ...parts);\n\t\t\t\t// tell unist-util-visit to skip visiting the newly inserted nodes\n\t\t\t\treturn [visit.SKIP, index];\n\t\t\t}\n\t\t});\n\t};\n}`\n\n3) Wire it into mdsvexOptions in svelte.config.js\nAdd remarkPlugins:\n\nimport adapter from '@sveltejs/adapter-auto';\nimport { vitePreprocess } from '@sveltejs/vite-plugin-svelte';\nimport { mdsvex, escapeSvelte } from 'mdsvex';\nimport { createHighlighter } from 'shiki';\nimport remarkObsidianImages from './src/lib/markdown/remark-obsidian-images.js';\n\n// ...existing code...\n\n/** @type {import('mdsvex').MdsvexOptions} */\nconst mdsvexOptions = {\n\textensions: ['.md'],\n\tremarkPlugins: [\n\t\t[\n\t\t\tremarkObsidianImages,\n\t\t\t{\n\t\t\t\tbaseUrl: 'https://asna.com/images/',\n\t\t\t\talt: 'filename' // or: 'empty'\n\t\t\t}\n\t\t]\n\t],\n\thighlight: {\n\t\thighlighter: async (code, lang = 'text') => {\n\t\t\t// ...existing code...\n\t\t}\n\t}\n};\n\n// ...existing code...\nexport default config;`\n\nAbout alt\nSince Obsidians syntax (as youre using it) doesnt include alt text, you have a few reasonable choices:\nalt: 'filename' (current default in the plugin): \"image-32\"\nalt: 'empty': alt=\"\" (treat as decorative)\nprovide a function to compute alt, later (e.g., look up an image caption map)\n\nAfter saving, restart the dev server (preprocessor changes wont reliably HMR):\n\nnpm run dev`\n\nIf your Obsidian embeds sometimes include other formats (e.g. ![[path/to/image.png|700]] or additional |... segments), paste a couple examples and Ill widen the parser accordingly.",
        "locale": "en",
        "slug": "technical-posts/transforming-obsidian-image-links",
        "folder": ""
    },
    {
        "title": "Typing arrow functions",
        "description": "Typing arrow functions",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "import type { Product, ProductGroup } from '$lib/global.d.ts'; // Adjust the path accordingly\n\nconst groupBy = (products: Product[], property_name: keyof Product): ProductGroup =>\n    products.reduce((grouped: ProductGroup, element: Product) => {\n        const key = element[property_name] as string; // Ensure the key is a string\n        return {\n            ...grouped,\n            [key]: [...(grouped[key] || []), element]\n        };\n    }, {});\n\nThe syntax you're using is mostly correct, but there are a few improvements to make it more type-safe and better match your existing type definitions, particularly around the use of any. You want to replace any with proper type annotations, and ensure that the return value is properly typed as ProductsGrouped.\n\nHeres how you can rewrite it using proper types:\n\nImproved Version:\n\nimport type { Product, ProductGroup } from \"$lib/global.d.ts\"; // Adjust the path accordingly\n\nconst groupBy = (\n    products: Product[],\n    property_name: keyof Product\n): ProductGroup =>\n    products.reduce((grouped: ProductGroup, element: Product) => {\n        const key = element[property_name] as string; // Ensure the key is a string\n        return {\n            ...grouped,\n            [key]: [...(grouped[key] || []), element],\n        };\n    }, {});\n\nExplanation:\n\nType Product[] for products: The products parameter is an array of Product, so we specify products: Product[].\nType keyof Product for property_name: The property_name is a key of the Product type, so we use property_name: keyof Product to restrict it to the keys of the Product object.\nType ProductGroup for grouped: The reduce function starts with an empty object {} of type ProductGroup, ensuring that TypeScript knows it will be an object with string keys and arrays of Product values.\nType the return value as ProductGroup: The return value is typed as ProductGroup to match your expected result.\n\nAdditional Considerations:\n\nType Safety: By using keyof Product, you restrict property_name to be one of the actual properties of Product. This avoids passing arbitrary strings as the property name.\nCasting Key: Since property_name is of type keyof Product, we cast the resulting key to string because object keys in JavaScript are always strings.\n\nThis code ensures that your groupBy function is fully type-safe and returns a correctly typed ProductGroup.",
        "locale": "en",
        "slug": "technical-posts/typing-arrow-functions",
        "folder": ""
    },
    {
        "title": "Obsidian Commander plugin UI map. This shows the areas of the Obsidian UI where Commander can add commands.",
        "description": "Obsidian Commander plugin UI map. This shows the areas of the Obsidian UI where Commander can add commands.",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "obsidian"
        ],
        "content": "![[image-14.png]]",
        "locale": "en",
        "slug": "technical-posts/obsidian-commander-plugin-ui-map",
        "folder": ""
    },
    {
        "title": "Svelte 5 standard setup",
        "description": "Svelte 5 standard setup",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Customize svelte.config.js\n\nAdd alias node under kit node to customize aliases:\n\nalias: {\n\t$components: 'src/components',\n\t$data: 'src/lib/data',\n\t$routes: 'src/routes'\n},\n\nAdd global.d.ts under \\src folder\n\nPut global type definitions here.\n\nEstablish Postgres connection in hooks.server.ts\n\nThis set requires installing Postgres pg package.\nhooks.server.ts\n\nimport pkg from 'pg';\n\nconst { Pool } = pkg;\nimport type { PoolClient } from 'pg';\n\nimport {\n    PG_HOST,\n    PG_PORT,\n    PG_USER,\n    PG_DATABASE,\n    PG_PASSWORD\n} from '$env/static/private';\n\nconst pool = new Pool({\n    host: PG_HOST,\n    port: Number(PG_PORT),\n    user: PG_USER,\n    password: PG_PASSWORD,\n    database: PG_DATABASE,\n    ssl: false\n});\n\nexport async function handle({ event, resolve }) {\n    let client: PoolClient;\n\n    try {\n        client = await pool.connect();\n        console.log('Connected to the database');\n    } catch (err) {\n        console.error('Error connecting to the database', err);\n        process.exit(0);\n    }\n\n    // Extend event.locals with the custom type\n    (event.locals as App.Locals).client = client;\n\n    return resolve(event);\n}\n\nAdd a /static folder for static assets\n\nFiles like favicon.ico, robots.txt, and sitemap.xml go here.\n\nAdd a /src/routes/+layout.svelte file\n\nsvelte 4 version\n\n\n\timport { inject } from '@vercel/analytics';\n\t// import some helper that is exposed by your current framework to determine the right mode manually\n\timport { dev } from '$app/environment';\n\n\tinject({\n\t\tmode: dev ? 'development' : 'production',\n\t});\n\timport '../style.css';\n\n\n\n\nIn the Svelte 4 example above the inject method is setting the mode to dev or production for Vercel.\n[[Slots are deprecated]] in Svelte 5.\n\nsvelte 5 version\n\n\n  import '../app.css';\n\n  let { children } = $props();\n\n\n{@render children()}\n\nIn the Svelte 5 example above, the ../app.css references the Tailwind-created app.css file\nImplied in the CSS import above is the fact that whatever tooling you use to create your CSS (fingertips, PostCSS, TailWind, etc) should target a single CSS file in the /src directory.\n\nFolder structure\n\n.\n src/\n    lib/\n       components\n       data\n    routes\n static",
        "locale": "en",
        "slug": "technical-posts/svelte-5-standard-setup",
        "folder": ""
    },
    {
        "title": "SQL for working with the lakeb2b prospects table in SQL Server",
        "description": "SQL for working with the lakeb2b prospects table in SQL Server",
        "date_created": "2025-06-08T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "lakeb2b"
        ],
        "content": "Prepare list for Constant Contact\n\nStep 1. Assign distribution\n\nWhere tag = new tag needed\n\nUPDATE TOP (2000) prospects\nSET tag = 'lakeb2b-2025-10-23'\nWHERE country = 'USA' AND tag = '' OR country = 'USA' AND tag IS NULL;\n\nStep 2. Copy new distribution to constant-contact\n\nClear constant-contact\n\ndelete from constant-contact]\n\nAdd rows to constant-contact\n\nwhere tag is the tag assigned in Step 1. \n\nINSERT INTO [constant-contact]\nSELECT [Counter], [Tag], [Email], [CompanyName], [WebAddress], \n       [Prefix], [ContactName], [FirstName], [MiddleName], [LastName], \n\t   [Title], [Address], [Address1], [City], [State], [ZipCode], \n\t   [Country], [PhoneNumber], [EmployeesSize], [RevenueSize], \n\t   [SicCodes], [Industry], [ApplicationType]\nFROM prospects\nWHERE tag = 'lakeb2b-2025-10-23';\n\nStep 3. Export constant-contact to CSV\n\nOther potentially helpful queries \n\nUpdate tag property for june12th.\n\nUPDATE prospects\nSET tag = j.tag\nFROM \"june-12\" AS j\nWHERE prospects.email = j.email;\n\nAdding the counter values for that updated tag: (did once for USA and once for Canada)\n\nWITH NumberedProspects AS (\n    SELECT\n        id, -- Use the primary key for the most reliable join\n        ROW_NUMBER() OVER (ORDER BY id) AS rn -- Or ORDER BY email\n    FROM\n        prospects\n    WHERE\n        tag = 'lakeb2b-2025-06-12-1000' and country = 'Canada'\n)\nUPDATE prospects\nSET counter = np.rn\nFROM NumberedProspects AS np\nWHERE prospects.id = np.id;\n\nShow countries sent:\n\nSELECT\n    country,\n    COUNT(*) AS prospect_count\nFROM\n    prospects\nWHERE tag  ''\nGROUP BY\n    country\nORDER BY\n    prospect_count DESC;\n\nShow countries by tag\n\nSELECT\n    country,\n    COUNT(*) AS prospect_count\nFROM\n    prospects\nWHERE tag = 'lakeb2b-2025-06-05-1000' OR tag = 'lakeb2b-2025-05-15'\n--WHERE tag = 'lakeb2b-2025-06-12-1000'\n\nGROUP BY\n    country\nORDER BY\n    prospect_count DESC;\n\nSelect a group of rows for emailing \n\nThis SQL selects a given row of prospects for adding them to Constant Contact. \n\nProvide the number of rows, the new tag name, and the country. This SQL finds the next n rows and marks them with the tag provided. \n\nTest mailing list for distribution:\n\nSELECT TOP (2000) * FROM prospects\nWHERE country = 'USA' AND tag = '' OR country = 'USA' and AND tag IS NULL;\n\nConfirm count not distributed:\n\nSELECT COUNT(*) FROM prospects\nWHERE country = 'USA' AND tag = '' OR country = 'USA' AND tag IS NULL;\n\nAssign distribution:\n\nUPDATE TOP (2000) prospects\nSET tag = 'lakeb2b-2025-08-07'\nWHERE country = 'USA' AND tag = '' OR country = 'USA' AND tag IS NULL;\n\n\nSame SQL with a transaction and rows affected.\n\nBEGIN TRANSACTION;\n\n-- Your update statement\nUPDATE TOP (1000) YourTable\nSET tag = '2026-06-27'\nWHERE country = 'USA' AND tag = '' OR country = 'USA' and AND tag IS NULL;\n\n-- Check how many rows were affected\nSELECT @@ROWCOUNT AS 'Rows Affected';\n\n-- If you are happy with the result, uncomment and run COMMIT\n-- COMMIT TRANSACTION;\n\n-- If something is wrong, uncomment and run ROLLBACK\n-- ROLLBACK TRANSACTION;\n\nCount rows for all tags\n\nShow counts of all tags\n\nselect tag, count(tag) from prospects\ngroup by tag\norder by tag\n\nResults shown with query above:\n\nlakeb2b-2025-05-22\t  50\nlakeb2b-2025-06-05\t 518\nlakeb2b-2025-06-12\t 601\nlakeb2b-2025-06-26\t2000\nlakeb2b-2025-08-07\t1092\n                    4261\n\nHandy SQL \n\nselect tag, count(tag) from prospects\nwhere country = 'USA'\ngroup by tag\norder by tag\n\nGet a comma-separated list of column names\n\nSELECT STRING_AGG(QUOTENAME(COLUMN_NAME), ', ') WITHIN GROUP (ORDER BY ORDINAL_POSITION) AS ColumnList\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_NAME = 'prospects';\n\nSQL Server Management Studio shows squiglies with this query, but it works.\n\nThis query can be helpful when you need to INSERT INTO every column but the identify column (usually named id)\n\nGet a working copy of the current list to submit to Constant Contact\n\n[!danger]\nDon't forget to clear the constant-contact table before inserting new rows into it. If you \n\nGenerally, you'll use the query below to select a given tag value from prospects\n\nINSERT INTO [constant-contact]\nSELECT [Counter], [Tag], [Email], [CompanyName], [WebAddress], \n       [Prefix], [ContactName], [FirstName], [MiddleName], [LastName], \n\t   [Title], [Address], [Address1], [City], [State], [ZipCode], \n\t   [Country], [PhoneNumber], [EmployeesSize], [RevenueSize], \n\t   [SicCodes], [Industry], [ApplicationType]\nFROM prospects\nWHERE tag = 'lakeb2b-2025-10-09';\n\nIn the case of the 2025-10-09 mailling, I added 5000 contacts that previously had not received an email (therefore, these 5000 have a tag value of lakeb2b-2025-10-09)  and also included all those previously tagged as sent except for the lakeb2b-2025-08-07 mailing group. \n\nThe intent with this 2025-10-09 mailing is to send the new email, with the new landing page, to everyone that has also been sent one email and to the 5000 selected on 2025-10-09. \n\nFor this mailing, I used this query to populate the constant-contact table:\n\nINSERT INTO [constant-contact]\nSELECT [Counter], [Tag], [Email], [CompanyName], [WebAddress], \n       [Prefix], [ContactName], [FirstName], [MiddleName], [LastName], \n\t   [Title], [Address], [Address1], [City], [State], [ZipCode], \n\t   [Country], [PhoneNumber], [EmployeesSize], [RevenueSize], \n\t   [SicCodes], [Industry], [ApplicationType]\nFROM prospects\nWHERE tag IS NOT NULL\n  AND tag  ''\n  AND tag  'lakeb2b-2025-08-07';\n\nCreate a new constant-contact table\n\nCREATE TABLE [dbo].[constant-contact](\n\t[Id IDENTITY(1,1) NOT NULL,\n\tCounter NULL,\n\tTag(200) NULL,\n\tEmail(200) NULL,\n\tCompanyName(200) NULL,\n\tWebAddress(200) NULL,\n\tPrefix(200) NULL,\n\tContactName(200) NULL,\n\tFirstName(200) NULL,\n\tMiddleName(200) NULL,\n\tLastName(200) NULL,\n\tTitle(200) NULL,\n\tAddress(200) NULL,\n\tAddress1(200) NULL,\n\tCity(200) NULL,\n\tState(200) NULL,\n\tZipCode(200) NULL,\n\tCountry(200) NULL,\n\tPhoneNumber(200) NULL,\n\tEmployeesSize(200) NULL,\n\tRevenueSize(200) NULL,\n\tSicCodes(200) NULL,\n\tIndustry(200) NULL,\n\tApplicationType(200) NULL,\nPRIMARY KEY CLUSTERED \n(\n\tId] ASC\n)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]\n) ON [PRIMARY]\nGO\n\nTarget table\n\nThis is an alternative to the full constant-contact table to use for Constant Contact list submission.\n\nI don't think it matters much which you use. \n  \nUSE [lakeb2b]\nGO\n\n/* Object:  Table [dbo].[june-12]    Script Date: 8/25/2025 2:07:46 PM */\nSET ANSI_NULLS ON\nGO\n\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[cc-group](\n\t[Id IDENTITY(1,1) NOT NULL,\n\tTag(200) NULL,\n\tEmail(200) NULL,\n\tCompanyName(200) NULL,\n\tFirstName(200) NULL,\n\tLastName(200) NULL,\n\tTitle(200) NULL,\nPRIMARY KEY CLUSTERED \n(\n\t[Id] ASC\n)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, OPTIMIZE_FOR_SEQUENTIAL_KEY = OFF) ON [PRIMARY]\n) ON [PRIMARY]\nGO\n\n2025-10-09 mailing\n\n\t16813\n\nas of 2025-10-09,m 16,813 left not set. Note this includes only USA.\n\non 2025-10-09, this query \nselect count(*) from prospects\nwhere country = 'usa' and tag = '' or  \n      country = 'usa' and tag is null\nshows 16,813 contacts remaining not yet sent.\n\nlakeb2b-2025-05-22\t46\nlakeb2b-2025-06-05\t390\nlakeb2b-2025-06-12\t601\nlakeb2b-2025-06-26\t2000\nlakeb2b-2025-08-07\t1092 (not sent)\nlakeb2b-2025-10-09\t5000\nTotal = 8037 rows sent \n\nwhen matched against unsubscribes, 7946 emails will be sent\n\n![[image-49.png]]",
        "locale": "en",
        "slug": "technical-posts/lakeb2b-prospects-table",
        "folder": ""
    },
    {
        "title": "Sending email from ASNA.com",
        "description": "Sending email from ASNA.com",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-com",
            "email"
        ],
        "content": "Properties\n\n| Name        | Default        | Purpose                                                                                                           |\n| ----------- | -------------- | ----------------------------------------------------------------------------------------------------------------- |\n| buttonText  | \"Send comment\" | Submit button text                                                                                                |\n| emailSource | \\*None         | Page or context email was sent from. Provides context for admin email copy                                        |\n| hideComment | false          | Hide the textarea tag                                                                                           |\n| modalDialog | undefined      | If using the Modal component, and a reference to that Modal is necessary, this is a reference to that component |\n| questions   | []             | Array of questions (max three questions) to ask on the form                                                       |\n| subject     | \\*None         | Email subject                                                                                                     |\n| title       | \"Title here\"   | Form title                                                                                                        |\n\nNotes:\n\nIf the questions array is empty no questions are displayed.\nThe emailSource provides ASNA admin's context to the email. For example, the \"Contact us today to discuss your IBM i challenge\" contact form emailSource would be \"IBM i challenge comment from home page.\"\nThe hideComment property is silly. The whole point of the form is to ask a question!\nThe ContactForm will almost certainly be wrapped in the Modal component--the modalDialog property is a reference to the Modal component. This is used mostly for passing a 'close' button click back to the Modal component.\n\nSending email\n\nWhen the contact form's submit button is clicked, that invokes the sendemail action in the root +page.server.js file. +page.server.js imports ssendEmail from the $lib/sendemail.js file which provides the logic needed to send an email.\n\nSending a generic email from ASNA.com\n\nThe $lib/stores.js file defines stores that, when set to true, cause the generic email form to be displayed.\n\nimport { writable } from 'svelte/store';\n\nexport const showContactTechSupport = writable(false);\nexport const showContactIBMChallenge = writable(false);\nexport const showContactGeneric = writable(false);\nexport const showUnsubscribe = writable(false);\n\nThe root +layout.svelte file tests each store\n\n$: if ($showContactTechSupport) {\n\t\ttitle = 'Contact ASNA Tech Support';\n\t\temailSource = 'Contact Support';\n\t\tsubject = 'Thank you for your email to ASNA tech support!';\n\t\tshowModal = true;\n\t\t$showContactTechSupport = false;\n\t}\n\n ($showContactIBMChallenge = true)} />",
        "locale": "en",
        "slug": "technical-posts/sending-email-with-the-contactform-component",
        "folder": ""
    },
    {
        "title": "Svelte 5 notes",
        "description": "Svelte 5 notes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "sveltekit"
        ],
        "content": "Svelte5 runes\nBen Davis said this...\n    The $effect() rune is essentially the OnMount of Svelte5.\n    I'm not sure about that--but Cooper Codes also said it! It must be true\n\nSnippets\nVery good overview of Svelte 4 versus Svelte 5\n    The above video content as a Web page\n\nasdflkjasdflkjasdflkjasdf",
        "locale": "en",
        "slug": "technical-posts/svelte-5-notes",
        "folder": ""
    },
    {
        "title": "Upgrade PowerShell",
        "description": "Upgrade PowerShell",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "powershell"
        ],
        "content": "winget upgrade --id Microsoft.Powershell --source winget\n\n[!hint]\nThis is not an unattended install. It is an elevated install and needs permission. Watch for the confirmation dialog.",
        "locale": "en",
        "slug": "technical-posts/upgrading-powershell",
        "folder": ""
    },
    {
        "title": "test",
        "description": "test",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "",
        "locale": "en",
        "slug": "technical-posts/test",
        "folder": ""
    },
    {
        "title": "using-mdsvex-with-remote-function",
        "description": "Using Mdsvex with a SvelteKit remote function",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "import fs from 'fs';\nimport path from 'path';\nimport { compile } from 'mdsvex';\nimport matter from 'gray-matter';\n\nexport async function load({ params }) {\n    const filePath = path.join(process.cwd(), 'content', 'technical-posts', ${params.slug}.md);\n    const rawContent = fs.readFileSync(filePath, 'utf-8');\n    \n    // Parse frontmatter\n    const { data: frontmatter, content } = matter(rawContent);\n    \n    // Compile markdown to Svelte component\n    const compiled = await compile(content);\n    \n    return {\n        frontmatter,\n        content: compiled?.code || '',\n        slug: params.slug\n    };\n}",
        "locale": "en",
        "slug": "technical-posts/using-mdsvex-with-remote-function",
        "folder": ""
    },
    {
        "title": "What is MS Aspire",
        "description": "What is MS Aspire",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "microsoft"
        ],
        "content": ".NET Aspire is a cloud-native stack designed to simplify the development of distributed applications in .NET. Here's a breakdown of its use cases and intended audience:\nUse Cases:\n\nBuilding Microservices Architectures: .NET Aspire excels at building complex, distributed systems using a microservices architecture. It provides tools for service discovery, configuration management, telemetry, and inter-service communication.\nDeveloping Cloud-Native Applications: It's built specifically for deploying to cloud environments (Azure, AWS, GCP, etc.) and helps manage the complexities of cloud infrastructure.\nCreating Observability Solutions: A key focus is on observability. Aspire simplifies the process of collecting and analyzing logs, metrics, and traces across your distributed application, making it easier to diagnose issues and monitor performance.\nModernizing Existing Applications: While ideal for new projects, .NET Aspire can also be used incrementally to modernize existing monolithic applications by breaking them down into smaller, more manageable services.\nBuilding Applications with Specific Needs: The pre-built components and abstractions make it easier to address common concerns in distributed systems, such as health checks, resiliency, and security.\n\nIntended Audience:\n\n.NET Developers: The primary target audience is .NET developers who are working on or planning to build distributed applications.\nCloud Architects: Architects can use .NET Aspire to design and implement scalable and resilient cloud-native architectures.\nDevOps Engineers: .NET Aspire simplifies the deployment and management of distributed applications, making it easier for DevOps teams to maintain and monitor these systems.\nTeams building complex, distributed systems: Companies adopting microservices or cloud-native architectures will find .NET Aspire particularly useful.\nDevelopers who want to simplify cloud-native development: .NET Aspire abstracts away many of the complexities of distributed systems, allowing developers to focus on writing business logic.\nEarly Adopters of .NET Technology: It is still relatively new, so those comfortable with experimenting with cutting-edge .NET technology will find it interesting.\n\nIn summary:\n.NET Aspire is for .NET developers who want to build observable, scalable, and robust cloud-native applications, particularly those embracing microservices architectures. It simplifies the development and management of distributed systems by providing a curated set of tools, templates, and components.",
        "locale": "en",
        "slug": "technical-posts/what-is-ms-aspire",
        "folder": ""
    },
    {
        "title": "Sending from node CLI",
        "description": "Sending from node CLI",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "ASNA SMTP email\n\nSMTP AUTH settings for web apps/devices to send as no-reply@asna.com:\n\nServer/smart host:    smtp.office365.com\nPort: 587 (recommended) or port 25\nTLS/StartTLS    Enabled\nUsername: no-reply@asnsa.com\nPassword: Wbitters82\n\nIt is crazy-simple to send email from Node with thenode-mailerNPM package.  In the example below the email body is pulled in from theindex-ro.html` file (which is a full HTML document with HTML, HEADER, and BODY tags.)\n\nvar nodemailer = require(\"nodemailer\");\n\nvar transporter = nodemailer.createTransport({\n  host: \"smtp.office365.com\",\n  port: 587,\n  secure: false,\n  tls: {\n    ciphers: \"SSLv3\",\n  },\n  auth: {\n    user: \"no-reply@asna.com\",\n    pass: \"Wbitters82\",\n  },\n});\n\nvar mailOptions = {\n  from: \"no-reply@asna.com; ebowers@asna.com\",\n  to: \"roger@asna.com\",\n  subject: \"test\",\n  html: { path: \"./index-rp.html\" },\n};\n\ntransporter.sendMail(mailOptions, function (error, info) {\n  if (error) {\n    console.log(error);\n  } else {\n    console.log(\"Email sent: \" + info.response);\n  }\n});\n\n``\nIn this simple case, use this command line to send the email:\n\nnode send-email-js",
        "locale": "en",
        "slug": "technical-posts/sending-email",
        "folder": ""
    },
    {
        "title": "TypeScript nested typing",
        "description": "TypeScript nested typing",
        "date_created": "2025-07-17T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "export interface ComponentProps {\n    orange: {\n        size: 'small' | 'medium' | 'large';\n        rate: number;\n    };\n    blue: {\n        size: 'small' | 'medium' | 'large';\n        length: 'short' | 'medium' | 'long';\n    };\n    green: {\n        size: 'small' | 'medium' | 'large';\n        intensity: 'bright' | 'normal' | 'dim';\n    };\n}\n\nexport type ComponentType = keyof ComponentProps;\n\nexport type ComponentConfig = {\n    [K in ComponentType]: ComponentProps[K] & { componentType: K };\n};\n\nThis is a sophisticated TypeScript pattern that creates a type-safe mapping system for your components. Let me break it down:\n\n1. ComponentProps Interface\nexport interface ComponentProps {\n    orange: {\n        size: 'small' | 'medium' | 'large';\n        rate: number;\n    };\n    blue: {\n        size: 'small' | 'medium' | 'large';\n        length: 'short' | 'medium' | 'long';\n    };\n    green: {\n        size: 'small' | 'medium' | 'large';\n        intensity: 'bright' | 'normal' | 'dim';\n    };\n}\n\nThis creates a lookup table where:\nKeys are component names ('orange', 'blue', 'green')\nValues are the prop interfaces for each component\n\nSo ComponentProps['orange'] gives you { size: 'small' | 'medium' | 'large'; rate: number; }\n\n2. ComponentType Type\nexport type ComponentType = keyof ComponentProps;\n\nThis extracts the keys from ComponentProps, so:\nComponentType = 'orange' | 'blue' | 'green'\n\n3. ComponentConfig Type (The Complex One)\nexport type ComponentConfig = {\n    [K in ComponentType]: ComponentProps[K] & { componentType: K };\n};\n\nThis is a mapped type that transforms the original interface. Let me break it down:\n\n[K in ComponentType] \nThis iterates over each component type ('orange', 'blue', 'green')\n\nComponentProps[K]\nThis gets the props for component K. For example:\nWhen K = 'orange', this is { size: 'small' | 'medium' | 'large'; rate: number; }\n\n& { componentType: K }\nThis adds a componentType property with the literal type of K\n\nThe Result\nComponentConfig becomes:\n{\n    orange: {\n        size: 'small' | 'medium' | 'large';\n        rate: number;\n        componentType: 'orange';  // Added this!\n    };\n    blue: {\n        size: 'small' | 'medium' | 'large';\n        length: 'short' | 'medium' | 'long';\n        componentType: 'blue';  // Added this!\n    };\n    green: {\n        size: 'small' | 'medium' | 'large';\n        intensity: 'bright' | 'normal' | 'dim';\n        componentType: 'green';  // Added this!\n    };\n}\n\nWhy This Pattern is Powerful\n\nType Safety: You can't accidentally pass rate to a blue component\nIntelliSense: IDE autocompletes the right props for each component\nLiteral Types: componentType: 'orange' is more specific than componentType: string\nSingle Source of Truth: Change props in one place, types update everywhere\n\nUsage Example\n// This is type-safe!\nconst orangeConfig: ComponentConfig['orange'] = {\n    size: 'large',\n    rate: 5,\n    componentType: 'orange'  // Must be exactly 'orange'\n};\n\n// This would be a TypeScript error!\nconst invalidConfig: ComponentConfig['orange'] = {\n    size: 'large',\n    length: 'short',  //  Error! Orange doesn't have 'length'\n    componentType: 'blue'  //  Error! Must be 'orange'\n};\n\nThis pattern ensures your dynamic component system is completely type-safe while remaining flexible.",
        "locale": "en",
        "slug": "technical-posts/typescript-nested-typing",
        "folder": ""
    },
    {
        "title": "FileSeek configuration tips",
        "description": "FileSeek configuration tips",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows",
            "utilities"
        ],
        "content": "![[Pasted image 20231010223558.png]]\nSeparate folders with the pipe character. Do not include an spaces between folder names the pipe character.\n\n![[Pasted image 20231010231811.png]]\nDo not have \"Process file contents use File Handlers (slower)\" selected. This setting returns spurious results! (many false positives)\n\n![[Pasted image 20231010223745.png]]\n\n\"Treat all Exclude Path filters as wildcards\" and \"Treat all Include Path filters are wildcards\" should both be checked.",
        "locale": "en",
        "slug": "technical-posts/searching-with-fileseek",
        "folder": ""
    },
    {
        "title": "Using +page.server.ts for a server back end action for a page",
        "description": "Using +page.server.ts for a server back end action for a page",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "nuggets"
        ],
        "content": "[!info]\nThis is in the post-example-2 route in the sveltekit/apis project.\n\nThis is a small example demonstrating data loading in a +page.server.ts file and displaying it in the corresponding +page.svelte component using SvelteKit 5 syntax and TypeScript.\n\n1. Create the Route Directory and Files:\n\nCreate a new route directory, for example, src/routes/server-data/. Inside this directory, create two files:\n\nsrc/routes/server-data/+page.server.ts\nsrc/routes/server-data/+page.svelte\n\n+page.server.ts code\n\n2. Write the Server Load Function (+page.server.ts):\n\nThis file exports a load function that runs exclusively on the server. It fetches or generates data and returns it.\n\n// src/routes/server-data/+page.server.ts\n\nimport type { PageServerLoad } from \"./$types\"; // Import the specific type for this page's server load function\n\n// Define an interface for the data shape we plan to return (optional but good practice)\ninterface ServerData {\n    message: string;\n    timestamp: string;\n    randomNumber: number;\n}\n\n// This 'load' function runs ONLY on the server side\nexport const load: PageServerLoad = async ({ locals, params }) => {\n    console.log(\"Executing +page.server.ts load function...\");\n    // console.log('Accessing locals (e.g., user):', locals.user); // Example if you have hooks setting locals\n    // console.log('Accessing route params:', params); // Example if the route had params like /server-data/[id]\n\n    // Simulate fetching data or doing server-specific work\n    await new Promise((resolve) => setTimeout(resolve, 50)); // Simulate async delay\n\n    const data: ServerData = {\n        message: \"Hello from the server!\",\n        timestamp: new Date().toISOString(), // Generate data on the server\n        randomNumber: Math.floor(Math.random() * 100),\n    };\n\n    // The object returned here is serialized and sent to the client\n    // It will be available in the corresponding +page.svelte via the data prop\n    return data;\n};\n\n// Note: You don't return a Response object here like in +server.ts.\n// You return plain data that SvelteKit wraps for you.\n\n+page.svelte code\n\n3. Write the Page Component (+page.svelte):\n\nThis Svelte component receives the data returned by the load function via a special data prop.\n\n\n\n\n\t// Import the PageData type, which represents the combined data\n\t// from all load functions (+page.server.ts and potentially +layout.server.ts, etc.)\n\timport type { PageData } from './$types';\n\n\t// This 'data' prop is automatically populated by SvelteKit\n\t// with the return value from the load function(s).\n\texport let data: PageData;\n\n\t// The 'data' object here will have the shape defined by the return value\n\t// of the load function in +page.server.ts ({ message, timestamp, randomNumber })\n\n\n\n\tServer Data Example\n\n\n\n\tData from Server Load\n\n\t\n\t\tMessage:\n\t\t{data.message}\n\t\n\n\t\n\t\tGenerated At (Server Time):\n\t\t{data.timestamp}\n\t\n\n\t\n\t\tServer Random Number:\n\t\t{data.randomNumber}\n\t\n\n\tRefresh the page to see the timestamp and random number update (generated server-side).\n\n    \n        Go Home\n    \n\n\n\n\t.container {\n\t\tfont-family: sans-serif;\n\t\tpadding: 2em;\n\t\tborder: 1px solid #ccc;\n\t\tborder-radius: 8px;\n\t\tmax-width: 600px;\n\t\tmargin: 2em auto;\n\t\tbackground-color: #f9f9f9;\n\t}\n    h1 {\n        color: #333;\n        border-bottom: 2px solid #eee;\n        padding-bottom: 0.5em;\n    }\n    p {\n        line-height: 1.6;\n    }\n    strong {\n        color: #555;\n    }\n    em {\n        color: #777;\n        font-size: 0.9em;\n    }\n     a {\n        color: #007bff;\n        text-decoration: none;\n    }\n    a:hover {\n        text-decoration: underline;\n    }\n\n\nExplanation:\n\n+page.server.ts:\n\n    Imports PageServerLoad from ./$types. SvelteKit automatically generates these types based on your file structure and exported functions, providing excellent type safety.\n    Defines and exports an async function named load.\n    Inside load, it simulates fetching data (or performs actual server-side logic like database queries).\n    It returns a plain JavaScript object ({ message, timestamp, randomNumber }). This object must be serializable (convertible to JSON).\n    This code runs only on the server during the initial page load or server-side rendering (SSR) and during subsequent navigation if data needs refreshing (depending on invalidation). It never runs in the user's browser.\n\n+page.svelte:\n    Uses `` for TypeScript support.\n    Imports PageData from ./$types. This type represents the shape of the data prop.\n    Declares export let data: PageData;. SvelteKit automatically injects the data returned by the load function(s) into this prop.\n    The template (` or just HTML) accesses properties of the data object (e.g., {data.message}`).\n\nHow it Works:\n\nWhen you navigate to /server-data in your browser.\nSvelteKit's server detects the request.\nIt finds the src/routes/server-data/+page.server.ts file and executes its load function.\nThe load function generates the data object and returns it.\nSvelteKit takes this data, serializes it, and includes it in the initial HTML sent to the browser (for SSR) or sends it as JSON during client-side navigation.\nThe src/routes/server-data/+page.svelte component is rendered. SvelteKit automatically passes the fetched data into the data prop.\nThe component displays the data received from the server.",
        "locale": "en",
        "slug": "technical-posts/using-page-server-ts-for-a-server-back-end-action-for-a-page",
        "folder": ""
    },
    {
        "title": "Eblast #2 for the LakeB2B mailing list (November 2025)",
        "description": "This is the second iteration of the Monarch marketing eblast",
        "date_created": "2025-11-03T00:00:00.000Z",
        "date_updated": "2025-11-05T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "marketing",
            "lakeb2b",
            "eblast"
        ],
        "content": "![[image-55.png|398x1104]]\n\n\n\n\n  The following is the email text for new marketing campaign. The audiences is decision makers charged with the responsibility of ensuring their enterprise system of record (which, in this case, is built on legacy code) persists well into the future. The email announces a webcast that covers ways our business can help them ensure a future for their critical application. \n\nPlease suggest subject lines for this email\n\nThe email text is: \nRPG's dilemma\n\nYour IBM i RPG applications are critically important to your business's success. They manage your corporate data, your unique workflows, and critical processes; all of which provide the backbone of your business.\nThese applications have served your business well for decades. However, they were designed decades ago and in many cases lack the modern features, interoperability, and the efficiency your business demands for the future.\n\nYour challenge\n\nYour challenge is determining how to persist your RPG applications so that they can continue to help ensure your business continues to deliver its unique value to your customers and partners.\nThe clock is ticking on the availability of RPG programming talent. Can your business continue with your RPG programmers?\nASNA has the talent and tools to help ensure a future for your IBM i applications and your business.\n\n\nWhat we'll cover\n\n\tHow RPG programmer retirement threatens your business\n\tPutting your RPG application in younger programmer hands\n\tA rational step-wise RPG modernization path with ASNA Wings and ASNA Monarch\n\tIBM i DB2 or SQL Server  your choice\n\tThe many ways ASNA's Services Team can help you with your IBM i and .NET projects\n\n\nHere are some subject line suggestions for your email, playing on different angles to attract decision-makers:\n\nFocusing on Urgency & Challenge:\n\nIs Your RPG Application Future-Proof?\nThe Clock is Ticking: Securing Your RPG Applications\nRPG Talent Gap: Is Your Business Prepared?\nSolving the RPG Dilemma: A Webcast for Decision Makers\nDon't Let Legacy Code Limit Your Future\n\nFocusing on Solutions & Benefits:\n\nModernize Your RPG: A Clear Path Forward\nEnsure the Future of Your Critical IBM i Applications\nUnlock New Life for Your RPG Systems\nFuture-Proofing IBM i: Strategies for Success\nASNA Webcast: A Future for Your RPG Applications\n\nIntriguing & Direct:\n\nRPG's Dilemma: Solved.\nYour IBM i Future Starts Here\nBeyond RPG: A Modernization Webcast\nCritical Applications: How to Ensure Their Future\nA Fresh Look at Your RPG Investment\n\nYou might also consider adding personalization if possible, e.g., \"\\[Company Name]: Is Your RPG Application Future-Proof?\"\n\nTo make it more visually appealing, how about an image that represents the challenge of legacy systems meeting modern demands?",
        "locale": "en",
        "slug": "technical-posts/november-2025-eblast",
        "folder": ""
    },
    {
        "title": "Svelte 5 forms",
        "description": "Svelte 5 forms",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "\nGenerating a unique ID with crypto.randomUUID()\n``\nconst id = crypto.randomUUID()\n\nconditional class names\nZod validation\nZod superrefine\n`",
        "locale": "en",
        "slug": "technical-posts/svelte-5-forms",
        "folder": ""
    },
    {
        "title": "typescript-types-interfaaces-tutorial",
        "description": "typescript-types-interfaaces-tutorial",
        "date_created": "2025-06-06T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "Of course! Here is a small, beginner-friendly tutorial on using type and interface in TypeScript.\n\nA Beginner's Guide to TypeScript: type vs. interface\n\nIn TypeScript, we use type and interface to define the \"shape\" of our data. They help us catch bugs early, enable powerful autocompletion in our code editor, and make our code easier to understand.\n\nLet's explore how to use them.\n\n1. What's the Goal? Describing an Object\n\nImagine we're building an app that deals with users. A user object might look like this in JavaScript:\n\nconst user = {\n  id: 1,\n  username: \"Dave\",\n  isPremium: true\n};\n\nTypeScript lets us create a blueprint for this object's structure. This is where type and interface come in.\n\n2. Using interface\n\nAn interface is a powerful way to define a \"contract\" for an object's shape. It's best suited for describing objects or classes.\n\nHow to create it:\n\ninterface User {\n  id: number;\n  username: string;\n  isPremium: boolean;\n  // We can also add optional properties with '?'\n  email?: string; \n}\n\nHow to use it:\n\nNow, we can \"type\" our variable. This tells TypeScript that our user variable must conform to the User interface.\n\nconst myUser: User = {\n  id: 1,\n  username: \"Dave\",\n  isPremium: true\n  // email is optional, so we can leave it out\n};\n\n// TypeScript will now protect us!\n// myUser.id = \"2\"; //  Error! Type 'string' is not assignable to type 'number'.\n// myUser.name = \"John\"; //  Error! Property 'name' does not exist on type 'User'.\n\n3. Using type\n\nA type alias is a bit more versatile. While it can do everything an interface can for objects, it can also describe other things like primitives, unions, and tuples.\n\nHow to create it (for an object):\n\ntype Product = {\n  id: number;\n  name: string;\n  price: number;\n};\n\nHow to use it:\n\nIt works just like an interface for typing objects.\n\nconst myProduct: Product = {\n  id: 101,\n  name: \"Fancy Mug\",\n  price: 12.99\n};\n\nThe Superpower of type: Unions\n\ntype can also define a variable that can be one of several types. This is called a union type. interface cannot do this.\n\ntype Status = \"pending\" | \"approved\" | \"rejected\";\n\nlet orderStatus: Status = \"pending\";\norderStatus = \"approved\";\n\n// orderStatus = \"delivered\"; //  Error! Type '\"delivered\"' is not assignable to type 'Status'.\n\n4. The Big Question: type or interface?\n\nThis is the most common question for newcomers. Here's a simple breakdown.\n\n| Feature                 | interface                                       | type                                                      |\n| ----------------------- | ------------------------------------------------- | ----------------------------------------------------------- |\n| Best For            | Describing object shapes and class contracts.     | Describing unions, primitives, tuples, or any complex type. |\n| Extending           | Yes, using extends.                             | Yes, using intersections (&).                             |\n| Declaration Merging | Yes.                                              | No.                                                         |\n\nLet's look at the two key differences:\n\nA) Extending / Combining\n\nBoth can be combined, but the syntax is different.\n\nWith interface: We use extends.\n\ninterface Animal {\n  name: string;\n}\n\ninterface Dog extends Animal {\n  breed: string;\n}\n\nconst myDog: Dog = { name: \"Rex\", breed: \"German Shepherd\" };\n\nWith type: We use the intersection operator &.\n\ntype Person = {\n  name: string;\n};\n\ntype Employee = Person & {\n  employeeId: number;\n};\n\nconst myEmployee: Employee = { name: \"Alice\", employeeId: 123 };\n\nB) Declaration Merging (The Biggest Difference)\n\nAn interface can be defined multiple times in the same scope, and TypeScript will merge them into one. This is useful when working with third-party libraries that you need to augment.\n\n// First declaration\ninterface Window {\n  title: string;\n}\n\n// Somewhere else, we \"re-open\" it\ninterface Window {\n  isFullScreen: boolean;\n}\n\n// TypeScript combines them! The final interface is:\n// interface Window {\n//   title: string;\n//   isFullScreen: boolean;\n// }\n\nA type cannot be changed after it's created. This will throw an error.\n\ntype MyType = { a: string };\n// type MyType = { b: string }; //  Error! Duplicate identifier 'MyType'.\n\nSummary & Recommendation\n\nUse interface when:\n    You are defining the shape of an object or a class.\n    You anticipate that you or others might need to extend it in the future (declaration merging). This makes interface ideal for library or API contracts.\n\nUse type when:\n    You need to define a union (string | number).\n    You want to create a simple alias for a primitive (type UserID = string).\n    You need to define a tuple (type Point = [number, number]).\n\nIf you're unsure, here's a good rule of thumb: Start with interface for your objects. If you find you need features that only type provides (like unions), switch to type. Many teams simply pick one for consistency when defining objects. Both are great tools",
        "locale": "en",
        "slug": "technical-posts/typescript-types-interfaaces-tutorial",
        "folder": ""
    },
    {
        "title": "Svelte remote functions",
        "description": "Svelte remote functions",
        "date_created": "2025-10-27T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "See 32:.43 of this video using Zod to validate remote function arguments: https://www.youtube.com/watch?v=BID8r5LObvI&t=1748s\n  \nRemote functions with forms (16:32 or so)",
        "locale": "en",
        "slug": "technical-posts/svelte-remote-functions",
        "folder": ""
    },
    {
        "title": "Using the HTML Dialog element",
        "description": "Using the HTML Dialog element",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "design",
            "css",
            "html"
        ],
        "content": "dialog = document.getElementById('confirmation-dialog');\ndialog.addEventListener('click', ({ target: dialog }) => {\n\tif (dialog.nodeName === 'DIALOG') dialog.close();\n});\n\nThis code shows how to harness Adam Argyle's excellent work on how to use the HTML dialog element (see link below).\n\nThis code is available at this GitHub repository.\n\nJavaScript\n\nThis JavaScript initializes two dialogs. You can have as many dialogs as you want. The arguments are:\n\ndialogid - the dialog element id.\ndialogElement - the selector of the anchor tag that when clicked opens the dialog.\n\nimport { configDialog } from \"./config-dialog.js\";\n\nconfigDialog(\n  { dialogId: \"#mydialog\", dialogOpenerElement: \"#open-my-dialog\" },\n  { dialogId: \"#second-dialog\", dialogOpenerElement: \"#open-second-dialog\" },\n]);\n\nThere are two supporting JavaScript files. These files don't need any customization--they provide the logic for all dialog elements. If you can't use them without modification, something is wrong!\n\nconfig-dialog.js - The configDialog routine below calls setUpDialog to initialize a given a dialog.\n\nimport GuiDialog from \"./dialog.js\";\n\n/*\n Initialize all instances of the Dialog.\n */\n\nexport const configDialog = (dialogInfo) => {\n  dialogInfo.forEach(({ dialogId, dialogOpenerElement }) => {\n    setUpDialog(dialogId, dialogOpenerElement);\n  });\n};\n\nconst setUpDialog = (dialogId, dialogOpenerElement) => {\n  if (dialogId.startsWith(\"#\")) dialogId = dialogId.slice(1);\n\n  const myOpener = document.querySelector(dialogOpenerElement);\n  myOpener.addEventListener(\"click\", (e) => {\n    window[dialogId].showModal();\n\n    window[dialogId].addEventListener(\n      \"closing\",\n      ({ target: dialog }) => {\n        if (dialog.returnValue === \"confirm\") {\n          console.log(\"confirm\");\n        }\n      },\n      { once: true }\n    );\n  });\n\n  const dialogElement = document.querySelector(#${dialogId});\n  GuiDialog(dialogElement);\n};\n\ndialog.js - This code is taking from Adam Argyle's [Gui Challenge Dialog. His CSS and JavaScript are stellar and very effective fade out the dialog element (something most others either don't do or don't get right). However, Adam's code isn't written to be reused--it's written to very specifically resolve the two dialog challenges his content presents.\n\nThe code below is my attempt at pulling out the generic parts of Adam's code to provide very good dialog fade-in and fade-out.\n\n// custom events to be added to \n\n// This is based on Adam Argyle's work and should work with any dialog.\n// This is what gives the dialog fade-in and fade-out capabilities.\n\nconst dialogClosingEvent = new Event(\"closing\");\nconst dialogClosedEvent = new Event(\"closed\");\nconst dialogOpeningEvent = new Event(\"opening\");\nconst dialogOpenedEvent = new Event(\"opened\");\nconst dialogRemovedEvent = new Event(\"removed\");\n\n// track opening\nconst dialogAttrObserver = new MutationObserver((mutations, observer) => {\n  mutations.forEach(async (mutation) => {\n    if (mutation.attributeName === \"open\") {\n      const dialog = mutation.target;\n\n      const isOpen = dialog.hasAttribute(\"open\");\n      if (!isOpen) return;\n\n      dialog.removeAttribute(\"inert\");\n\n      // set focus\n      const focusTarget = dialog.querySelector(\"autofocus]\");\n      focusTarget\n        ? focusTarget.focus()\n        : dialog.querySelector(\"button\").focus();\n\n      dialog.dispatchEvent(dialogOpeningEvent);\n      await animationsComplete(dialog);\n      dialog.dispatchEvent(dialogOpenedEvent);\n    }\n  });\n});\n\n// track deletion\nconst dialogDeleteObserver = new MutationObserver((mutations, observer) => {\n  mutations.forEach((mutation) => {\n    mutation.removedNodes.forEach((removedNode) => {\n      if (removedNode.nodeName === \"DIALOG\") {\n        removedNode.removeEventListener(\"click\", lightDismiss);\n        removedNode.removeEventListener(\"close\", dialogClose);\n        removedNode.dispatchEvent(dialogRemovedEvent);\n      }\n    });\n  });\n});\n\n// wait for all dialog animations to complete their promises\nconst animationsComplete = (element) =>\n  Promise.allSettled(\n    element.getAnimations().map((animation) => animation.finished)\n  );\n\n// click outside the dialog handler\nconst lightDismiss = ({ target: dialog }) => {\n  if (dialog.nodeName === \"DIALOG\") dialog.close(\"dismiss\");\n};\n\nconst dialogClose = async ({ target: dialog }) => {\n  dialog.setAttribute(\"inert\", \"\");\n  dialog.dispatchEvent(dialogClosingEvent);\n\n  console.log(\"close\");\n  await animationsComplete(dialog);\n\n  dialog.dispatchEvent(dialogClosedEvent);\n};\n\n// page load dialogs setup\nexport default async function (dialog) {\n  dialog.addEventListener(\"click\", lightDismiss);\n  dialog.addEventListener(\"close\", dialogClose);\n\n  dialogAttrObserver.observe(dialog, {\n    attributes: true,\n  });\n\n  dialogDeleteObserver.observe(document.body, {\n    attributes: false,\n    subtree: false,\n    childList: true,\n  });\n\n  // remove loading attribute\n  // prevent page load @keyframes playing\n  await animationsComplete(dialog);\n  dialog.removeAttribute(\"loading\");\n}\n\nCSS\n\nThis CSS provides the dialog element presentation. It has a few parts that need to be adjusted. I'll update this as a go along. For example, the background-color of the dialog element should probably be set with a CSS property (to make it easy to change.)\n\nThe CSS below uses [Open Props, so you need that in your project. A naive way to do that is with\n\n@import \"https://unpkg.com/open-props\"\n\ndialog-main.css - Generic (mostly!) dialog element presentation.\n\n//html:has(dialogopen) {\n//    overflow: hidden;\n//}\n\ndialog {\n    display: grid;\n    align-content: start;\n\n    /* dialog background color */\n    background-color: lightgray;\n\n    color: var(--text-1);\n    max-inline-size: min(90vw, var(--size-content-3));\n    margin: auto;\n    padding: 0;\n    position: fixed;\n    inset: 0;\n    border-radius: var(--radius-2);\n    box-shadow: var(--shadow-3);\n    z-index: var(--layer-important);\n    overflow: hidden;\n    /* transition: opacity .5s var(--ease-3); */\n    border: 1px solid gray;\n\n    animation: var(--animation-scale-down) forwards;\n    animation-timing-function: var(--ease-squish-3);\n\n    &:not([open]) {\n      pointer-events: none;\n      opacity: 0;\n    }\n\n    &::backdrop {\n        background-color: rgba(0,0,0,.4);\n        backdrop-filter: blur(2px);\n        transition: backdrop-filter .5s ease;\n    }\n\n    &[loading] {\n      visibility: hidden;\n    }\n\n    &[open] {\n        animation: var(--animation-slide-in-up) forwards;\n    }\n}\n\nHTML\n\nYour dialog element can have any HTML contents in it (including forms). The only requirements are:\n\nEach dialog element is defined with inert and loading attributes\n\nIf you put a form inside the dialog put method=dialog in its form tag\n    Adam's JavaScript and CSS assume these attributes are present.\n\n\n    \n    ...\n    \n",
        "locale": "en",
        "slug": "technical-posts/using-the-html-dialog-element",
        "folder": ""
    },
    {
        "title": "Using Sveltekit ParamMatcher to constraint dynamic routes",
        "description": "Using Sveltekit ParamMatcher to constraint dynamic routes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "!info]\nSee [this page for more info.\n\nParamMatcher\n\n// src/params/fastener.ts\nimport type { ParamMatcher } from '@sveltejs/kit';\n\nexport const match: ParamMatcher = (param) => {\n  // This runs FIRST during route matching\n  console.log(Matcher 'fastener' checking param: ${param});\n  return param === 'bolts' || param === 'nuts';\n};\n\n// src/routes/inventory/[type=fastener]/+page.server.ts\nimport type { PageServerLoad } from './$types';\nimport { error } from '@sveltejs/kit'; // Optional: for explicit error handling if needed\n\n// Define the server load function\nexport const load: PageServerLoad = async ({ params }) => {\n    // This code only runs IF the ParamMatcher (fastener.ts) returned true\n    // for the 'type' segment in the URL.\n\n    // 'params.type' will always be either 'bolts' or 'nuts' here,\n    // because the matcher guaranteed it before this function was called.\n    const itemType = params.type;\n\n    console.log(Server load running for type: ${itemType});\n\n    // You can now confidently use itemType to fetch specific data\n    let description = '';\n    let stockLevel = 0;\n\n    if (itemType === 'bolts') {\n        // Fetch data specifically for bolts from a DB or API\n        description = \"Strong metal bolts for construction.\";\n        stockLevel = 1500;\n    } else if (itemType === 'nuts') {\n        // Fetch data specifically for nuts\n        description = \"Hex nuts to pair with bolts.\";\n        stockLevel = 2300;\n    } else {\n        // This 'else' block is technically unreachable because the matcher\n        // prevents other values, but it's good practice for type safety\n        // or if the matcher logic were to change.\n        // You could throw an error here if needed, though SvelteKit's\n        // routing would typically prevent this code path.\n        console.error(\"Unexpected itemType in server load:\", itemType);\n        // throw error(404, 'Not Found'); // Or a 500 Internal Server Error\n    }\n\n    // Return the data to be used in the +page.svelte component\n    return {\n        item: {\n            type: itemType,\n            description: description,\n            stock: stockLevel\n        }\n    };\n};\n\nsrc/\n params/\n    fastener.ts       <-- PARAM MATCHER CODE IS HERE\n\n routes/\n    inventory/\n        [type=fastener]/  <-- This part of the path LINKS the route to the matcher\n           \n            +page.server.ts  <-- THE LOAD FUNCTION CODE IS HERE\n           \n            +page.svelte     <-- Your page component\n\n app.html\n ... (other files like hooks.server.ts, etc.)\n\nTo clarify the roles:\n\nParamMatcher (src/params/fastener.ts): This code defines how to validate a specific URL parameter during the routing phase. It determines if a request matches the route before any data loading happens. Its code lives in the src/params/ directory.\n+page.server.ts load function: This code runs after SvelteKit has successfully matched the incoming request URL to the route (including passing any ParamMatcher checks). Its purpose is to load data needed for the page. It receives the already validated parameters via its arguments.\n\nThe ParamMatcher code:\n\nLives in its own file inside the src/params/ directory.\nExample file path: src/params/fastener.ts\n    The +page.server.ts file (with the load function):\nLives inside the specific route directory that uses the matcher.\nExample file path: src/routes/inventory/[type=fastener]/+page.server.ts\n\nWorkflow\n\nRequest: A user navigates to /inventory/bolts.\nRouting: SvelteKit looks for matching routes. It finds src/routes/inventory/[type=fastener]/.\nMatcher Execution: SvelteKit sees [type=fastener]. It takes the value \"bolts\" from the URL and calls the match function in src/params/fastener.ts.\nMatcher Result: fastener.match(\"bolts\") returns true.\nRoute Confirmed: SvelteKit confirms this is the correct route.\nLoad Function Execution: SvelteKit now executes the load function inside src/routes/inventory/[type=fastener]/+page.server.ts.\nParameter Access: Inside load, the params object contains { type: 'bolts' }. The load function accesses params.type knowing it has already been validated by the matcher.\nData Fetching: The load function fetches data based on \"bolts\".\nReturn Data: The load function returns the fetched data.\nPage Render: The data is passed to +page.svelte for rendering.",
        "locale": "en",
        "slug": "technical-posts/using-sveltekit-parammatcher-to-constraint-dynamic-routes",
        "folder": ""
    },
    {
        "title": "How to create a SvelteKit component package",
        "description": "How to create, package, and consume a SvelteKit library component.",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-16T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "A SvelteKit component package encapsulates Sveltekit components into a reusable library. The library is hosted on GitHub and is added to consuming projects with NPM or PNPM (or other) package managers. \n\nRather than deploying this project to the NPM registry, this project gets pushed to a GitHub repository and consuming projects will install it from GitHub. If you're creating a component intended for open source use by a broad set of users, you should probably publish your project to the NPM registry. That's not the challenge that this project strategy solves. \n\nI build medium-to-large website and want to provide separation of concerns for my proprietary SvelteKit components and TypeScript libraries. I want these libraries easily reusable across several applications. I also may want these libraries to be private and the free NPM account doesn't provide private packages; using GitHub you can \"privatize\" your SvelteKit libraries for free. More on pushing a SvelteKit component library to GitHub later. \npackage.json difference\n\nOne of the primary differences between a Sveltekit application project and a SvelteKit library project is that a library project's package includes a prepack which provides a step to build this project for use as a component. More in this later. \nStep 1. Create a SvelteKit library project\n\n![[image-64.png|711x284]]\n\nA library project lets you create SvelteKit components to package for use by consuming SvelteKit projects. \n\nAt a glance, a library project appears to be a a minimal SvelteKit project. In development, it even feels like a regular ol' SvelteKit project. The difference is that with a library project, by default, nothing under the routes folder is exported. Let's look deeper at a full, but minimal, example SvelteKit Library project. \n\nThe example component manages paging through pages of rows of data. Its \"previous\" button provides an anchor tag with a link to the previous page and the \"next\" button provides an anchor tag with a link to the next page. This component doesn't fetch or display any data, its only job is to provide links for paging through the data. \n\n![[image-65.png]]\n\nThis example also provides basis CSS for the paging component. The consuming project has access to that CSS, or it can provide its own custom CSS for the component. \n\nDirectory structure \n\nWhile this is a library project, its directory structure is essentially the same is a regular SvelteKit project. The big difference is that while the routes folder and its sub-routes can be used to test the components the project provides, nothing under routes is exported. The consuming project provides pages and uses the components.\n\n.\n src/\n    lib/\n       components/\n          NextPrevAction.svelte\n       css/\n          next-prev-action/\n              root.css\n              component.css        \n              utlities.css  \n       index.ts          \n    routes/\n        next-prev-action/\n           +page.svelte\n        +layout.svelte\n        +page.svelte    \n app.d.ts (unchanged from default)\n app.html (unchanged from default)        \n style.css\n\nThe component code\nNextPrevAction.svelte\n\nThis is the NextPrevAction.svelte component code. It doesn't know anything about the data that is being paged. Its properties are:\ntotalPages - the total pages available in the dataset.\npageNumber - the current page number being displayed.\nnavRoute - the route to which the 'next' and 'previous' actions navigate.\npageNumberKey (optional) name of search parameter that provides page number. This defaults to pagenumber.\n\n\n\texport interface Props {\n\t\ttotalPages: number;\n\t\tpageNumber: number;\n\t\tnavRoute: string;\n\t\tpageNumberKey?: string;\n\t}\n\n\tlet { totalPages, pageNumber, navRoute, pageNumberKey = 'pagenumber' }: Props = $props();\n\n\tconst nextPageNumber = $derived(pageNumber ? pageNumber + 1 : 0);\n\tconst prevPageNumber = $derived(pageNumber ? pageNumber - 1 : 0);\n\n\tconst isFirstPage = $derived(pageNumber == 1);\n\tconst isLastPage = $derived(pageNumber == totalPages);\n\n\t// $inspect(isFirstPage, isLastPage, pageNumber);\n\n\n\n\t Previous\n\tPage {pageNumber} of {totalPages}\n\tNext \n\nThe CSS code\n\nThis CSS is exported from the component and available to consuming projects. Consuming projects could also ignore this CSS code a/nd provide their own component CSS.\n\nroot.css\n\nThis provides a right-pointing arrowhead. A CSS transform flips it to be left-pointing--making only one SVG necessary.\n\n:root {\n    --icon-arrowhead-right: url('data:image/svg+xml,...');\n}\n\ncomponent.css\n\nThis is basic component styling.  \n\n@scope (div.page-navigator-container) {\n\t:scope {\n\t\tcolor: black;\n\t\tdisplay: flex;\n\t\talign-items: center;\n\t\tgap: 1rem;\n\t\tmargin-block-start: 2rem;\n\t}\n\n\ta {\n\t\tdisplay: flex;\n\t\talign-items: center;\n\t\tjustify-content: center;\n\t\tcolor: black;\n\t\ttext-decoration: none;\n\n\t\t&:hover {\n\t\t\toutline: 1px solid red;\n\t\t}\n\t}\n\n\ti {\n\t\tfont-size: 1.4rem;\n\t\tcolor: black;\n\t\tcursor: pointer;\n\t}\n\n\ti.icon {\n\t\twidth: 16px;\n\t\theight: 16px;\n\t\tmask-repeat: no-repeat;\n\t\t-webkit-mask-repeat: no-repeat;\n\t\tmask-position: center;\n\t\t-webkit-mask-position: center;\n\t}\n\n\ti.next-icon {\n\t\tbackground-color: gray;\n\t\tmask-image: var(--icon-arrowhead-right);\n\t\t-webkit-mask-image: var(--icon-arrowhead-right);\n\t}\n\n\ti.previous-icon {\n\t\tbackground-color: gray;\n\t\tmask-image: var(--icon-chevron-right);\n\t\t-webkit-mask-image: var(--icon-arrowhead-right);\n\t\ttransform: scaleX(-1);\n\t}\n}\n\nutilities.css\n\nA utility class to disable an element.\n\n.disabled {\n    pointer-events: none;\n    opacity: 0.5;\n    cursor: not-allowed;\n}\n\nExporting component and its Props types\n\nThis is very critical code. Without it, there is nothing for a consuming project to import. \n\nindex.ts\n\n// Export your components here.\nexport { default as NextPrevAction } from './components/NextPrevAction.svelte';\n  \n// Export props type.\nexport type { Props as NextPrevActionProps } from './components/NextPrevAction.svelte';\n\nThis example shows the SvelteKit project exporting only one component. In most component libraries you'll export many components and their types in the index.ts file.\nA sample/test page to test the NextPrevAction component.\nWhile the SvelteKit component libraries don't export the src folder (and therefore any routes and pages the library may have), you can add routes and pages to test the components in the library. The packaging process ignores the src folder and its contents.\n\n[!note]\nIn this example, the pageNumberKey default property value (pagenumber) is used.\n\nnext-prev-action/+page.svelte\n\n\n\timport NextPrevAction from '$lib/components/NextPrevAction.svelte';\n\t\n  import '$lib/css/next-prev-action/root.css';\n  import '$lib/css/next-prev-action/utilities.css';\n  import '$lib/css/next-prev-action/component.css';\t\n\n\tconst totalPages = 21;\n\tconst pageNumber = 1;\n\tconst navRoute = '/technical-posts';\n\n\n\n\nThe above code imports the CSS it needs for component. This is not what you would do in production but it works fine for testing the component.\nSample test page CSS\n\nThe following two files aren't necessary for the project's components--they provide CSS for the sample/test pages that is independent of component testing. These files are not exported.\n\n+layout.svelte\n\n\n\timport '../style.css';\n\n\tlet { children } = $props();\n\n\n{@render children()}\n\nstyle.css\n\nbody {\n    font-family: sans-serif;\n}\n\nThis is how the component looks in the node_modules folder of a consuming application:\n\n![[image-66.png]]\n\nNote there is no src directory (and therefore no routes and pages) in the imported package.\nStep 2. GitHub repo\n\nCreate a GitHub repo for the project. There isn't anything unique or special about this repo. \nStep 3. Building a component\n\nThese steps are required for building the component.\n\npnpm run prepack\n\nThis step packages the project into the .dist folder. \n\n[!important]\nMake sure your .gitignore file doesn't ignore the .dist folder. \n\nThen, use these steps to update the project and its tag at GitHub:\n\ngit add .\ngit commit -m \"feat: add new components\"\ngit push\n\ngit tag v0.0.6\ngit push origin v0.0.6\n\nUse this command line to add update a consuming project (syntax varies on the package manager used)\n\npnpm add https://github.com/rogerpence/sv-utils#v0.0.6\n\nUsing the component in a project\n\n[!info]\nBe sure to add or update a reference to your component!  \n\nAdd a reference to your component in a page or other Sveltekit compontent:\n\nimport { NextPrevAction, type NextPrevActionProps } from 'sv-components';\n\nThen use it like any other SvelteKit component. \n\nThere are two ways to use CSS offered by a custom SvelteKit component:\n1. Within components\n\nYou can import the CSS with Vite/Sveltekit like this:\n\n\n\timport { NextPrevAction, type NextPrevActionProps } from 'sv-components';\n\n\timport 'sv-components/css/next-prev-action/root.css';\n\timport 'sv-components/css/next-prev-action/utilities.css';\n\timport 'sv-components/css/next-prev-action/component.css';\n\n\tconst totalPages = 21;\n\tconst pageNumber = 1;\n\tconst navRoute = '/technical-posts?pagenumber=';\n\n2. Import the CSS with CSS's @import\n\nIn a top level CSS file: \n\nThe lay(n) is optional, but useful if your CSS uses layers.\n\n@import \"sv-components/css/next-prev-action/root.css\" layer(global);\n@import \"sv-components/css/next-prev-action/utilities.css\" layer(utilities);\n@import \"sv-components/css/next-prev-action/component.css\" layer(components);\n\nI prefer this method of including a component's CSS. This makes that CSS work right along with the application's CSS workflow. See vanilla CSS workflow for more on my preferred CSS workflow.\n\nYour custom SvelteKit components should emit no, or very little, hardcoded CSS within the component file. They should emit markup only. They should provide some basic CSS as stand-alone CSS files (consumed as explained above) that a consuming application could use. However, you may choose to not use the CSS a component provides and write your own in your application.",
        "locale": "en",
        "slug": "technical-posts/sveltekit-component-package",
        "folder": ""
    },
    {
        "title": "Sending Constant Contact eblasts",
        "description": "Sending Constant Contact eblasts",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "marketing"
        ],
        "content": "These are the settings for scheduling an eblast with Constant Contact:\n\n![[image-18.png|822]]\n\n![[image-19.png|822]]",
        "locale": "en",
        "slug": "technical-posts/sending-constant-contact-eblasts",
        "folder": ""
    },
    {
        "title": "svelte 5-2 changes",
        "description": "svelte 5-2 changes",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "$app/state\n\nhttps://svelte.dev/docs/kit/$app-state\n\nimport { navigating, page, updated } from '$app/state';\n\nNote that page is no longer prefixed with $ in the code.\n\n\n\timport { page } from '$app/state';\n\n\nCurrently at {page.url.pathname}",
        "locale": "en",
        "slug": "technical-posts/svelte-5-2-changes",
        "folder": ""
    },
    {
        "title": "What is a vector in Rust",
        "description": "What is a vector in Rust",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "What is a vector in Rust\n\nIn Rust, a vector (Vec) is a growable, heap-allocated, dynamic array. It is a collection type that stores elements of the same type T and allows you to store multiple values in a single data structure. Vectors are one of the most commonly used types in Rust for managing collections of data, especially when the size of the collection is not known at compile time.\n\nKey Characteristics of Vectors:\n\nDynamic size: Unlike arrays, which have a fixed size, vectors can grow or shrink as needed.\nHeap allocation: Vectors store their data on the heap (as opposed to the stack), which allows them to resize dynamically.\nHomogeneous: All elements in a vector must be of the same type T.\nEfficient indexing: Vectors support fast indexing and iteration over the elements.\n\nCommon Operations on Vectors:\n\nCreation: You can create an empty vector or one initialized with specific values.\nPush/Pop: You can add elements to the end of a vector or remove them.\nIndexing: You can access elements by their index (zero-based).\n\nExample Usage:\n\nfn main() {\n    // Creating a new empty vector\n    let mut v: Vec = Vec::new();\n\n    // Adding elements to the vector\n    v.push(1);\n    v.push(2);\n    v.push(3);\n\n    // Accessing elements by index\n    println!(\"First element: {}\", v[0]);\n\n    // Iterating over the vector\n    for i in &v {\n        println!(\"{}\", i);\n    }\n\n    // Removing the last element (pop)\n    v.pop();\n}\n\nKey Functions for Vectors:\n\nVec::new(): Creates an empty vector.\nVec::with_capacity(n): Creates a vector with pre-allocated capacity for n elements.\n.push(value): Adds an element to the end of the vector.\n.pop(): Removes and returns the last element of the vector (or None if it's empty).\n.len(): Returns the number of elements in the vector.\n.is_empty(): Checks if the vector is empty.\n.get(index): Returns an Option for safe element access.\n\nMemory Safety:\n\nRust ensures memory safety with vectors by automatically managing memory. When a vector goes out of scope, its memory is freed, avoiding issues like memory leaks.\nRust prevents out-of-bounds access with vectors. For example, accessing an invalid index will panic at runtime (in debug mode), and using .get() returns an Option to safely handle out-of-bounds access.\n\nExample of Safe Access:\n\nfn main() {\n    let v = vec![1, 2, 3];\n\n    match v.get(2) {\n        Some(value) => println!(\"Found: {}\", value),\n        None => println!(\"Out of bounds!\"),\n    }\n}\n\nIn this case, v.get(2) safely checks if index 2 exists and handles it accordingly.",
        "locale": "en",
        "slug": "technical-posts/what-is-a-vector-in-rust",
        "folder": ""
    },
    {
        "title": "Test PDF validity",
        "description": "Test PDF validity",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "pdf"
        ],
        "content": "/// \n    /// Performs a basic check on a file to see if it starts with \"%PDF\"\n    /// and ends with \"%%EOF\\n\". This is a rudimentary check and does not\n    /// guarantee the PDF is fully valid or uncorrupted.\n    /// \n    /// The full path to the file to check.\n    /// True if the file starts and ends with the expected PDF markers, false otherwise.\n    public static bool TestPdfFile(string filePath)\n    {\n        // Basic check: ensure file exists before trying to read\n        if (!File.Exists(filePath))\n        {\n            Console.Error.WriteLine($\"Error: File not found '{filePath}'\");\n            return false;\n        }\n\n        try\n        {\n            // Read the entire file content into a byte array.\n            // Consider using FileStream for very large files to avoid memory issues,\n            // reading only the beginning and end parts.\n            byte[] pdfBytes = File.ReadAllBytes(filePath);\n\n            // PDF requires at least the header (%PDF) and the end marker (%%EOF\\n)\n            // A minimal valid PDF is usually larger, but this check needs at least 10 bytes.\n            if (pdfBytes.Length < 10) // 4 for header + 6 for footer\n            {\n                return false; // File too small to contain both markers\n            }\n\n            // Get the first 4 bytes for the header \"%PDF\"\n            string pdfHeader = Encoding.ASCII.GetString(pdfBytes, 0, 4);\n\n            // Get the last 6 bytes for the end marker \"%%EOF\\n\"\n            // Note: Some PDF writers might use \"%%EOF \" or just \"%%EOF\".\n            // This check specifically looks for \"%%EOF\\n\" as in the PowerShell script.\n            string pdfEndMarker = Encoding.ASCII.GetString(pdfBytes, pdfBytes.Length - 6, 6);\n\n            // Check if both markers match\n            // Using Ordinal comparison is generally safer for fixed markers.\n            return pdfHeader.Equals(\"%PDF\", StringComparison.Ordinal) &&\n                   pdfEndMarker.Equals(\"%%EOF\\n\", StringComparison.Ordinal);\n\n        }\n        catch (IOException ex)\n        {\n            // Handle potential I/O errors (e.g., file locked, permissions)\n            Console.Error.WriteLine($\"IO Error accessing file '{filePath}': {ex.Message}\");\n            return false;\n        }\n        catch (Exception ex) // Catch other potential exceptions\n        {\n            Console.Error.WriteLine($\"Unexpected error processing file '{filePath}': {ex.Message}\");\n            return false;\n        }\n    }",
        "locale": "en",
        "slug": "technical-posts/test-pdf-validity",
        "folder": ""
    },
    {
        "title": "Server side debugging with VS Code and Sveltekit",
        "description": "Server side debugging with VS Code and Sveltekit",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "debug"
        ],
        "content": "For a while, using VS Codes Debug: Attach to Node process and worked without the steps below. However, and I think it was after a VS Code update, that just flat quit working. This technique is probably easier.\n\nStep 1. Put Node into debug mode\n\nNode has a command line argument to put it into debug mode. However, Sveltekit uses Vite, so you can't add Node arguments directly. This environment setting is a work-around for that.\n\n$env:NODE_OPTIONS=\"--inspect\"\n\nAdd this function to your PowerShell profile (notepad $profile) to easily set the NODE_OPTIONS value when you need it. This debug mode only kicks in when you launch the debugger from VS Code.\n\nfunction nodedebug {\n\t$env:NODE_OPTIONS=\"--inspect\"\n\tWrite-Host Node server-side debugging enabled -foregroundcolor green\n}\n\nStep 2. Add a VS Code launch.json file\n\nAdd this file to .\\.vscode\\launch.json\n\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"type\": \"node\",\n            \"request\": \"attach\",\n            \"name\": \"Attach to SvelteKit (Node)\",\n            \"port\": 9229, // Default inspect port. Change if your server uses a different one.\n            \"restart\": true, // Automatically try to re-attach if the process restarts\n            \"protocol\": \"inspector\", // Modern protocol\n            \"skipFiles\": [\n                \"/**\", // Skip Node.js internal files\n                \"${workspaceFolder}/node_modules/**\" // Skip node_modules\n            ]\n        }\n        // You might also have a configuration for launching the dev server directly\n        // (though attaching is often preferred for SvelteKit/Vite)\n        // {\n        //   \"type\": \"node\",\n        //   \"request\": \"launch\",\n        //   \"name\": \"Launch SvelteKit Dev (Debug)\",\n        //   \"runtimeExecutable\": \"npm\",\n        //   \"runtimeArgs\": [\n        //     \"run\",\n        //     \"dev\"\n        //   ],\n        //   \"env\": {\n        //     \"NODE_OPTIONS\": \"--inspect\"\n        //   },\n        //   \"console\": \"integratedTerminal\",\n        //   \"port\": 9229, // Ensure this matches the inspect port\n        //   \"serverReadyAction\": {\n        //     \"pattern\": \"Local:.+ (http://localhost:5173/)\", // Adjust port if needed\n        //     \"uriFormat\": \"%s\",\n        //     \"action\": \"openExternally\"\n        //   }\n        // }\n    ]\n}",
        "locale": "en",
        "slug": "technical-posts/server-side-debugging-with-vs-code-and-sveltekit",
        "folder": ""
    },
    {
        "title": "A guide for creating classes in TypeScript/JavaScript",
        "description": "A guide for creating classes in TypeScript/JavaScript",
        "date_created": "2025-05-30T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "1. Basic Class Definition:\n\nUse the class keyword, followed by the class name.\n\nclass Greeter {\n    // Properties (member variables)\n    greeting: string; // Type annotation is key!\n\n    // Constructor\n    constructor(message: string) {\n        this.greeting = message;\n    }\n\n    // Methods (member functions)\n    greet(): string {\n        return \"Hello, \" + this.greeting;\n    }\n}\n\n// How to use it:\nlet greeterInstance = new Greeter(\"world\");\nconsole.log(greeterInstance.greet()); // Output: Hello, world\n\n2. Properties:\n\nDeclare properties with their types at the top of the class.\nInitialize them in the constructor or provide a default value.\n\nclass Product {\n    name: string;\n    price: number = 0; // Default value\n    readonly id: number; // Readonly property (can only be set in constructor)\n\n    constructor(id: number, name: string, initialPrice: number) {\n        this.id = id; // 'readonly' can be assigned here\n        this.name = name;\n        this.price = initialPrice;\n    }\n\n    getDescription(): string {\n        return ${this.name} costs $${this.price};\n    }\n}\n\n3. Access Modifiers (Visibility):\n\npublic: (Default) Accessible from anywhere.\nprivate: Accessible only within the class itself.\nprotected: Accessible within the class and by instances of derived classes (subclasses).\n\nclass Animal {\n    public name: string;\n    private age: number; // Can't be accessed directly from outside\n    protected species: string;\n\n    constructor(name: string, age: number, species: string) {\n        this.name = name;\n        this.age = age;\n        this.species = species;\n    }\n\n    public getDetails(): string {\n        return ${this.name} is a ${this.species} and is ${this.age} years old.;\n    }\n}\n\nclass Dog extends Animal {\n    constructor(name: string, age: number) {\n        super(name, age, \"Canine\");\n    }\n\n    public getSpecies(): string {\n        return this.species; // 'species' is accessible due to 'protected'\n        // return this.age; // Error: 'age' is private to Animal\n    }\n}\n\nlet myDog = new Dog(\"Buddy\", 5);\nconsole.log(myDog.name);          // OK (public)\n// console.log(myDog.age);        // Error (private in Animal)\n// console.log(myDog.species);    // Error (protected, not directly accessible outside)\nconsole.log(myDog.getSpecies());  // OK (accessed via a public method in Dog)\n\n4. Shorthand Constructor (Parameter Properties):\n\nDeclare and assign properties directly in the constructor parameters using access modifiers.\n\nclass Person {\n    // No need to declare 'name' and 'age' above the constructor\n    constructor(public name: string, private age: number, readonly id: string) {}\n\n    displayInfo(): void {\n        console.log(ID: ${this.id}, Name: ${this.name}, Age: (private));\n    }\n}\n\nlet person = new Person(\"Alice\", 30, \"UID123\");\nperson.displayInfo();\nconsole.log(person.name); // Alice\n// console.log(person.age); // Error: 'age' is private\nconsole.log(person.id);   // UID123\n\n5. Static Members:\n\nProperties or methods that belong to the class itself, not to instances. Accessed using ClassName.memberName.\n\nclass MathHelper {\n    static PI: number = 3.14159;\n\n    static calculateCircumference(radius: number): number {\n        return 2 * MathHelper.PI * radius;\n    }\n}\n\nconsole.log(MathHelper.PI); // 3.14159\nconsole.log(MathHelper.calculateCircumference(10)); // 62.8318\n\n6. Inheritance (extends):\n\nCreate a new class (subclass/derived class) based on an existing class (superclass/base class).\n\nclass Vehicle {\n    constructor(public brand: string) {}\n\n    drive(): void {\n        console.log(${this.brand} is moving.);\n    }\n}\n\nclass Car extends Vehicle {\n    constructor(brand: string, public model: string) {\n        super(brand); // Calls the constructor of the base class (Vehicle)\n    }\n\n    honk(): void {\n        console.log(${this.brand} ${this.model} says: Beep beep!);\n    }\n\n    // Override method from base class\n    drive(): void {\n        super.drive(); // Optionally call the base class method\n        console.log(${this.model} is cruising smoothly.);\n    }\n}\n\nlet myCar = new Car(\"Toyota\", \"Camry\");\nmyCar.drive();\nmyCar.honk();\n\n7. Implementing Interfaces (implements):\n\nEnsure a class adheres to a specific contract defined by an interface.\n\ninterface Printable {\n    print(): void;\n    format: string;\n}\n\nclass Document implements Printable {\n    title: string;\n    format: string = \"A4\"; // Must implement 'format'\n\n    constructor(title: string) {\n        this.title = title;\n    }\n\n    print(): void { // Must implement 'print'\n        console.log(Printing document: ${this.title} in ${this.format} format.);\n    }\n}\n\nlet report = new Document(\"Annual Report\");\nreport.print();\n\nKey Takeaways for TypeScript Classes:\n\nType Safety: Explicitly define types for properties, constructor parameters, method parameters, and return values.\nVisibility Control: Use public, private, and protected to encapsulate data.\nreadonly: For properties that should not be changed after initialization.\nClear Structure: Provides a familiar OOP paradigm.\nEnhanced Features: Shorthand constructors, static members, abstract classes (not shown here but available) further enrich class capabilities.\n\nThis brief guide covers the essentials. TypeScript's class system is rich and closely mirrors features from languages like Java or C#.",
        "locale": "en",
        "slug": "technical-posts/typescript-classes",
        "folder": ""
    },
    {
        "title": "scheduling an ASNA webcast, goto webinar",
        "description": "scheduling an ASNA webcast, goto webinar",
        "date_created": "2025-06-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna",
            "webcast"
        ],
        "content": "",
        "locale": "en",
        "slug": "technical-posts/scheduling-asna-webcast",
        "folder": ""
    },
    {
        "title": "Upgrade a Svelekit 1x project to 2x (acccording to Gemini)",
        "description": "Upgrade a Svelekit 1x project to 2x (acccording to Gemini)",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Upgrading SvelteKit from version 1.x (like your 1.25.1) to 2.x involves a few key steps, primarily focused on updating dependencies and addressing breaking changes. SvelteKit 2.0 notably requires Svelte 4 and Vite 5.\n\nHere's a comprehensive guide:\n\nKey Changes in SvelteKit 2.0 (and dependencies):\n\nSvelte 4: SvelteKit 2 requires Svelte 4. If you're not already on Svelte 4, this will be part of the upgrade. Svelte 4 itself has minimal breaking changes from Svelte 3, mainly stricter type checking and performance improvements.\nVite 5: SvelteKit 2 requires Vite 5. This is a significant upgrade with its own set of changes. Most SvelteKit users won't be directly hit by many Vite breaking changes unless they have complex custom Vite configurations or rely on Vite plugins that haven't been updated.\nNode.js Version: SvelteKit 2 requires Node.js 18.13.0 or later (though using the latest LTS, currently Node 20, is recommended).\n%sveltekit.head% removed: Use ` directly in your root +layout.svelte (or +page.svelte` if no root layout) instead.\n%sveltekit.body% attributes: Attributes previously added via app.html's %sveltekit.body% (like class or data-*) should now be applied directly to the ` tag in src/app.html`. If you need dynamic body attributes, you'll need a different approach (e.g., client-side JavaScript in your root layout).\ntrailingSlash config: The always and never options for kit.trailingSlash are now stricter. ignore is still the default. If you use always or never, ensure your server/hosting redirects appropriately.\nService Worker version file: The generated version.txt file for service workers is now in .svelte-kit/output/generated instead of build/generated (or similar, depending on your adapter). Update your service worker script accordingly if you reference it.\n\nUpgrade Steps:\n\nBackup Your Project:\n\n    Before you start, commit all your current changes to Git.\n    Consider creating a new branch for the upgrade: git checkout -b upgrade-sk2\n\nUpdate Node.js (if necessary):\n\n    Check your Node version: node -v\n    If it's older than 18.13.0, update it. Using a Node version manager like nvm or fnm is highly recommended.\n        nvm install 20 # Or your preferred LTS/latest version >= 18.13.0\n        nvm use 20\n\nUpdate Dependencies in package.json:\n\n    Open your package.json file.\n    Update the following core dependencies:\n        @sveltejs/kit: \"^1.25.1\" to \"^2.0.0\" (or the latest 2.x version)\n        svelte: To \"^4.0.0\" (or the latest 4.x version, e.g., \"^4.2.9\")\n        vite: To \"^5.0.0\" (or the latest 5.x version, e.g., \"^5.0.12\")\n        @sveltejs/adapter-auto (or your specific adapter like @sveltejs/adapter-node, @sveltejs/adapter-static, etc.): Update to its latest version compatible with SK2. Check the adapter's changelog if unsure, but generally, the latest will work.\n        @sveltejs/vite-plugin-svelte: This is usually managed by @sveltejs/kit, but if you have it explicitly listed, update it to the latest.\n    Other Dependencies:\n        Review other Svelte-related or Vite-related dependencies (e.g., svelte-preprocess, Vite plugins). Check their documentation for compatibility with Svelte 4 / Vite 5. Update them to their latest compatible versions.\n        For example, svelte-check should be updated to its latest version.\n\n    Example dependencies / devDependencies before:\n\n    {\n        \"devDependencies\": {\n            \"@sveltejs/adapter-auto\": \"^2.0.0\", // Example, might be older\n            \"@sveltejs/kit\": \"^1.25.1\",\n            \"svelte\": \"^3.59.2\", // Or an older Svelte 4 version\n            \"vite\": \"^4.4.2\", // Or older\n            \"svelte-check\": \"^3.6.0\"\n            // ... other dependencies\n        }\n    }\n\n    Example dependencies / devDependencies after:\n\n    {\n        \"devDependencies\": {\n            \"@sveltejs/adapter-auto\": \"^3.0.0\", // Or latest\n            \"@sveltejs/kit\": \"^2.0.0\", // Or latest 2.x.x\n            \"svelte\": \"^4.2.9\", // Or latest 4.x.x\n            \"vite\": \"^5.0.12\", // Or latest 5.x.x\n            \"svelte-check\": \"^3.6.0\" // Or latest\n            // ... other dependencies, updated as needed\n        }\n    }\n\n    Tip: You can use npm outdated to see which packages have newer versions. Then, for major updates, it's often best to update them one by one or in small groups.\n    Alternatively, for supported package managers:\n\n    npm install @sveltejs/kit@latest svelte@latest vite@latest @sveltejs/adapter-auto@latest --save-dev (adjust for your adapter)\n    pnpm update @sveltejs/kit@latest svelte@latest vite@latest @sveltejs/adapter-auto@latest --save-dev\n    yarn upgrade @sveltejs/kit@latest svelte@latest vite@latest @sveltejs/adapter-auto@latest --dev\n\nInstall Updated Dependencies:\n\n    Delete your node_modules folder and lock file (package-lock.json, yarn.lock, or pnpm-lock.yaml) to ensure a clean install:\n        rm -rf node_modules\n        rm -f package-lock.json # or yarn.lock / pnpm-lock.yaml\n    Install:\n        npm install # or yarn install or pnpm install\n\nAddress Breaking Code Changes:\n\n    src/app.html:\n\n        Remove %sveltekit.head%: If you have this, delete it.\n        Move ` attributes: If you had attributes like class=\"dark\" in %sveltekit.body%, move them directly to the ` tag:\n            \n            \n                \n                \n            \n            (Note: data-sveltekit-preload-data and data-sveltekit-preload-code are usually managed by SvelteKit itself on the `` tag, so ensure they remain or are added if not present).\n        Ensure ` is used for dynamic head content in your root +layout.svelte (or page). Example src/routes/+layout.svelte`:\n\n            \n              // import ...\n            \n\n            \n              My Awesome App\n              \n            \n\n            \n\n    svelte.config.js - trailingSlash:\n\n        Review your kit.trailingSlash setting. If it's always or never, be aware of stricter enforcement and ensure your deployment environment handles redirects correctly. The default ignore behavior is generally fine.\n\n    Service Worker version (if applicable):\n        If you have a custom service worker (src/service-worker.js) that imports the version from SvelteKit's build output, update the path.\n            Previously, it might have been:\n            // src/service-worker.js\n            import { version } from \"$service-worker\"; // or a direct path\n            If you were manually constructing the path, it used to be something like build/generated/version.txt or output/client/version.txt.\n            The new location for the generated version variable (if you're using $service-worker) should still work, but if you were manually pointing to the file, it's now typically found within .svelte-kit/output/generated/version.txt after a build. The $service-worker module abstract this, so usually, no change is needed here unless you were hardcoding paths.\n\nRun Type Checking and Sync:\n\n    If you're using TypeScript, or even just for general project integrity:\n        npx svelte-kit sync\n        npx svelte-check --tsconfig ./tsconfig.json # or ./jsconfig.json\n    svelte-kit sync updates your generated type definitions ($app, $env, etc.).\n\nTest Thoroughly:\n\n    Start the development server:\n        npm run dev\n    Test all aspects of your application:\n        Navigation\n        Forms\n        API routes / Server load functions\n        Dynamic content\n        Authentication\n        Any specific features of your app\n    Check the browser console and terminal for errors or warnings.\n    Build and preview the production version:\n        npm run build\n        npm run preview\n    Test the preview thoroughly.\n\nTroubleshooting Common Issues:\n\n    Vite Plugin Incompatibility: If npm run dev or npm run build fails, it might be due to an incompatible Vite plugin. Check the plugin's GitHub repository for issues or updates related to Vite 5.\n    Type Errors: Svelte 4 has stricter types. You might encounter new type errors, especially if using TypeScript. Address these as they appear. svelte-check is your friend here.\n    CSS/Styling Issues: Unlikely, but Vite 5 or other tooling changes could subtly affect CSS processing. Double-check your styles.\n    Cache Issues: Sometimes, old build artifacts or browser caches can cause problems. Try clearing your .svelte-kit directory, browser cache, and node_modules again if you encounter persistent weirdness.\n\nConsult Official Documentation:\n\n    Always refer to the official SvelteKit changelog: https://github.com/sveltejs/kit/blob/master/packages/kit/CHANGELOG.md (look for 2.0.0 release notes).\n    Vite Migration Guide (from v4 to v5): https://vitejs.dev/guide/migration.html\n\nCommit Your Changes:\n    Once you're confident the upgrade is successful:\n        git add .\n        git commit -m \"Upgrade to SvelteKit 2.0, Svelte 4, and Vite 5\"\n\nThis process covers the main points. The complexity can vary depending on the size and complexity of your project and how many custom configurations or less common dependencies you have. Good luck!\n\nUpgrade to svelte 5\n\nOkay, that's an ambitious and exciting goal!\n\nYes, you should absolutely upgrade to SvelteKit 2 first, get your project stable on that, and then approach the Svelte 5 upgrade.\n\nHere's why and how to think about it:\n\nWhy Upgrade to SvelteKit 2 (with Svelte 4) First:\n\nIsolate Changes: Upgrading from SvelteKit 1.x to SvelteKit 2.x (which brings Vite 5 and requires Svelte 4) is already a significant step with its own set of breaking changes and dependency updates. Trying to do this and jump to Svelte 5 simultaneously would make debugging incredibly difficult if issues arise. You wouldn't know if the problem is with the SvelteKit 2 migration, Vite 5, Svelte 4 compatibility, or the new Svelte 5 features/internals.\nStable Foundation: SvelteKit 2 with Svelte 4 is a stable, well-tested combination. Get your application working perfectly on this before introducing the more experimental nature of Svelte 5 (which is currently in Release Candidate stage, not final stable, as of early 2024).\nSvelteKit's Svelte 5 Support: While SvelteKit aims to support Svelte 5, the most robust support (especially from the wider ecosystem of Svelte libraries) will solidify as Svelte 5 becomes stable and more widely adopted. SvelteKit versions will be released that are specifically tested and optimized for Svelte 5.\nSvelte 5 is a Bigger Shift (for new features): Svelte 5 introduces Runes, which is a fundamental change to how reactivity can be handled. While Svelte 5 aims for backward compatibility with Svelte 3/4 code, adopting Runes is a deliberate refactoring process. It's best to do this on an already stable SvelteKit 2 base.\n\nRecommended Upgrade Path:\n\nPhase 1: Upgrade to SvelteKit 2 (with Svelte 4 & Vite 5)\n\nFollow the steps I outlined in the previous message to upgrade from SvelteKit 1.25.1 to the latest SvelteKit 2.x.\nThis will also involve upgrading svelte to ^4.0.0 and vite to ^5.0.0.\nThoroughly test your application to ensure everything works as expected.\nCommit these changes. Your project is now on a modern, stable foundation.\n\nPhase 2: Upgrade to Svelte 5 (Preview/RC)\n\nOnce your SvelteKit 2 project is stable:\n\nCheck Official SvelteKit Guidance: Before proceeding, check the official Svelte and SvelteKit blogs, GitHub repositories, or Discord channels for the latest recommendations on using Svelte 5 with SvelteKit. There might be specific SvelteKit versions (e.g., a @next tag) recommended for Svelte 5.\n\nUpdate Dependencies for Svelte 5:\n\n    Modify your package.json:\n        svelte: Update to the latest Svelte 5 version (e.g., \"^5.0.0-rc.1\" or whatever the current RC/stable is).\n        @sveltejs/kit: You might need to update this to a version explicitly supporting Svelte 5, possibly a next tag if Svelte 5 is still in RC (e.g., @sveltejs/kit@next).\n        @sveltejs/vite-plugin-svelte: This will likely need to be updated to a version compatible with Svelte 5.\n        Other Svelte-related libraries (UI components, state management) might also need updates for Svelte 5 compatibility, especially if you plan to use Runes extensively.\n\n    Example (illustrative, check actual latest versions):\n\n    {\n        \"devDependencies\": {\n            \"@sveltejs/kit\": \"^2.x.x\", // or potentially \"@sveltejs/kit\": \"next\"\n            \"svelte\": \"^5.0.0-rc.x\", // Or latest Svelte 5 version\n            \"vite\": \"^5.x.x\" // Should already be Vite 5\n            // ... other dependencies\n        }\n    }\n\nInstall Updated Dependencies:\n\n    rm -rf node_modules\n    rm -f package-lock.json # or yarn.lock / pnpm-lock.yaml\n    npm install # or yarn install or pnpm install\n\nRun svelte-kit sync:\n\n    npx svelte-kit sync\n\n    This is crucial to update TypeScript definitions and other generated files for the new Svelte version.\n\nTest Existing Code:\n\n    Run npm run dev.\n    Svelte 5 is designed to be largely backward compatible. Your existing Svelte 4 code (components, logic) should mostly work.\n    Thoroughly test your application. Look for any unexpected behavior or errors in the console.\n\nGradually Adopt Svelte 5 Features (Runes):\n\n    Svelte 5's main new feature set is Runes ($state, $derived, $effect, etc.). Using them is opt-in.\n    You can start refactoring components to use Runes where it makes sense. This is a good way to learn and leverage the new capabilities.\n    Refer to the Svelte 5 documentation for how to use Runes.\n\nType Checking:\n\n    npx svelte-check\n\n    Address any new type errors.\n\nCheck Third-Party Libraries:\n    If you use Svelte UI libraries or other Svelte-specific tools, check their documentation or GitHub issues for Svelte 5 compatibility and any necessary migration steps. Some libraries might need time to fully adapt to Runes or other Svelte 5 changes.\n\nBy following this two-phase approach, you minimize risk, make debugging easier, and can focus on one set of changes at a time. Good luck!\n\nFixes\n\nsvelte.config,js\n\nChange this to this:\n// import { vitePreprocess } from '@sveltejs/kit/vite';\n\nimport { vitePreprocess } from '@sveltejs/vite-plugin-svelte';\n\nC:\\Users\\thumb\\Documents\\projects\\asna_asna-com-3\\src\\components\\all-locales\\all-pages\\SearchPanel.svelte\n\nimport algolia\nand change to\nimport {algolia}\n\nC:\\Users\\thumb\\Documents\\projects\\asna_asna-com-3\\src\\components\\contact-form\\ContactForm.svelte\n\n\n\n this.blur()}\n\tonfocus={() => (this.value = '')}\n\ttitle=\"If country not in list please type it in\"\n\trequired=\"true\"\n\tbind:value={$form.country}\n\n/>\n\nC:\\Users\\thumb\\Documents\\projects\\asna_asna-com-3\\src\\routes\\downloads\\[[slug]]\\+page.svelte\n\non line 388 make sure this trims the email address correctly\n\nonblur={() => trimValue(this)}",
        "locale": "en",
        "slug": "technical-posts/upgrade-a-svelekit-1x-project-to-2x-(acccording-to-gemini)",
        "folder": ""
    },
    {
        "title": "Using PageFind static search",
        "description": "Using PageFind static search",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "search"
        ],
        "content": "PageFind creates an static index at build time. I had it working at this project: C:\\Users\\thumb\\Documents\\projects\\astro\\astro-blog-archive. The files below are in that project.\n\nCreate the PageFind index from an array of Json objects.\n\ncreate-index.js\n\nimport * as pagefind from \"pagefind\";\n\nimport { docmap } from \"./data/index-objects.js\";\n\n// Create a Pagefind search index to work with\nconst { index } = await pagefind.createIndex();\n\nimport * as env from \"./env.js\";\n\nimport path from \"path\";\n\nconst folder = docmap[0];\n\nasync function main() {\n    docmap[0].docs.map(async (doc) => {\n        await index.addCustomRecord(doc);\n    });\n\n    await index.writeFiles({\n        outputPath: \"../public/pagefind\",\n    });\n}\n\nawait main();\n\nOne of its gotchas is that the static index must be located where Vercel can find it--which I think is at .vercel/output/static/pagefind.\n\nThe PowerShell file below is the post-build script that copies the dev static index info to prod.\n\nif (test-path .\\public\\pagefind) {\n    remove-item .\\public\\pagefind -force -recurse\n}\n\nif (test-path .vercel\\output\\static\\pagefind) {\n    remove-item .vercel\\output\\static\\pagefind -force -recurse\n}\n\nnode .\\cmd-line\\create-index-objects.js\nnode .\\cmd-line\\create-index.js\n\ncopy-item .\\public\\pagefind -destination .\\.vercel\\output\\static\\pagefind -recurse -force\n\nnode .\\cmd-line\\create-tagmap-objects.js\n\nSee the Astro project's PageFindLogic.astro component for a way to build the PageFind UI.",
        "locale": "en",
        "slug": "technical-posts/using-pagefind-static-search",
        "folder": ""
    },
    {
        "title": "typescript-types-for-email-builder",
        "description": "typescript-types-for-email-builder",
        "date_created": "2025-07-02T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "email-builder",
            "email"
        ],
        "content": "// Global configuration type\nexport interface GlobalConfig {\n    brandColor: string;\n    pageBackgroundColor: string;\n    emailBackgroundColor: string;\n    marginLeft: number;\n    marginRight: number;\n    fontSize: number;\n    fontColor: string;\n    lineHeight: number;\n    pageTitle: string;\n}\n\n// Intro section type\nexport interface IntroSection {\n    logoImageUrl: string;\n    onlineUrl: string;\n    locale: string;\n    emailTarget: string;\n    subhead: string;\n    gotoHome: string;\n    gotoOnlineVersion: string;\n    issue: string;\n    subject: string;\n}\n\n// Footer text type\nexport interface FooterText {\n    asnaLogo: string;\n    tagLine: string;\n    info: string[];\n    infoAll: string[];\n}\n\n// Action required bullets type\nexport interface ActionRequiredBullets {\n    paragraph: string[];\n    bullets: string[];\n}\n\n// Content section type\nexport interface ContentSection {\n    introHeadline: string;\n    introParagraphs: string[];\n    actionRequiredSubhead: string;\n    actionRequiredBullets: ActionRequiredBullets;\n    readmoreSubhead: string;\n    readmoreParagraph: string[];\n    footerText: FooterText;\n}\n\n// Main email data type combining all sections\nexport interface EmailData {\n    global: GlobalConfig;\n    introSection: IntroSection;\n    content: ContentSection;\n}\n\n\n// Generic content section (reusable for different email types)\nexport interface GenericContentSection {\n    [key: string]: string | string[] | BulletListData | FooterText;\n}\n\n// Union type for all content value types\nexport type ContentValue = string | string[] | BulletListData | FooterText;\n\n// Type guard functions\nexport function isBulletListData(value: ContentValue): value is BulletListData {\n    return typeof value === 'object' && 'paragraph' in value && 'bullets' in value;\n}\n\nexport function isFooterText(value: ContentValue): value is FooterText {\n    return typeof value === 'object' && 'type' in value && value.type === 'footerText';\n}\n\nexport interface Headline {\n\ttext: string;\n}\n\nexport interface Paragraph {\n\tparagraphs: string[];\n}\n\nexport interface Subhead {\n\ttext: string;\n}\n\nexport interface BulletList  {\n\tparagraph: string[]\n\tlist: string[]\n} \n\nexport interface CallOut {\n\tpersona: 'caution' | 'remember' | 'idea';\n\ttext: string;\n}",
        "locale": "en",
        "slug": "technical-posts/typescript-types-for-email-builder",
        "folder": ""
    },
    {
        "title": "Ways to map the Windows keyboard",
        "description": "Ways to map the Windows keyboard",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "I use two keyboard mapping programs:\n\nUpdate: This is the only thing that worked reliably to remap the caps lock key!\n\nEspanso\n\nI use this for fast keystroke macros.\n\nAutoHotKey\n\nThis program reliably maps the caplock and numlock keys. It may have a way to create auto-triggered two-character hot keys (ie, type fc and have the fc replaced with an underscore) but I haven't found it yet.\n\nEspanso config\n\nC:\\Users\\thumb\\AppData\\Roaming\\espanso\\match\\base.yml\n\n| Trigger   | Replace                       |\n| --------- | ----------------------------- | ---------- |\n| rpmail    | roger.pence@gmail.com       |\n| :smalltag | $                     | $ |\n| si        | $\\{\\}                       |\n| dprop     | Declare a CSS custom property |\n| uprop     | Use a CSS custom property     |\n| jc        | _                           |\n| jd        | -                           |\n| jk        | @                           |\n| jt        | ticks                       |\n| jb        | ]                          |\n| jp        | ()                          |\n| jm        | {}                          |\n| :envon    | env\\\\scripts\\\\activate\\n    |\n| ansa      | asna                        |\n| mdlink    | markdown link               |\n| mdimg     | markdown image              |\n| nbsp      | &nbsp;                      |\n| mdash     | &mdash;                     |\n| ibmii     | IBM&nbsp;                   |\n\nespanso match file\n\nFor a complete introduction, visit the official docs at: https://espanso.org/docs/\n\nYou can use this file to define the base matches (aka snippets)\nthat will be available in every application when using espanso.\n\nMatches are substitution rules: when you type the \"trigger\" string\nit gets replaced by the \"replace\" string.\nmatches:\nSimple text replacement\ntrigger: \":espanso\"\nreplace: \"Hi there!\"\n\n\ntrigger: \"rpmail\"\n    replace: \"roger.pence@gmail.com\"\n\ntrigger: \":smalltag\"\n    replace: \"$|$\"\n\ntrigger: \"jc\"\n    replace: \"\\_\"\n\ntrigger: \"jd\"\n    replace: \"-\"\n\ntrigger: \"jk\"\n    replace: \"@\"\n\ntrigger: \"jt\"\n    replace: \"$|$\"\n\ntrigger: \"jb\"\n    replace: \"[$|$]\"\n\ntrigger: \"jp\"\n    replace: \"($|$)\"\n\ntrigger: \"jm\"\n    replace: \"{$|$}\"\n\ntrigger: \":envon\"\n    replace: \"env\\\\scripts\\\\activate\\n\"\n\ntrigger: \"ansa\"\n    replace: \"asna\"\n    word: true\n\ntrigger: \"teh\"\n    replace: \"the\"\n    word: true\ntrigger: \"mdlink\"\n    replace: \"[$|$ \"\ntrigger: \"mdimg\"\n    replace: \" \"\ntrigger: \"nbsp\"\n    replace: \"&nbsp;\"\n\ntrigger: \"mdash\"\n    replace: \"&mdash;\"\ntrigger: \"ibmii\"  \n    replace: \"IBM&nbsp;i \"\n\nAutoHotKey config\n\nC:\\\\Users\\\\thumb\\\\Documents\\\\AutoHotkey\\\\remap-keys.ahk\n\nCapsLock::LCtrl\nNumLock::Backspace\n\n",
        "locale": "en",
        "slug": "technical-posts/notes-on-keyboard-mapping",
        "folder": ""
    },
    {
        "title": "SvelteKit build errors",
        "description": "SvelteKit build errors",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Error: Not found: [path reference here]\n\n![[Git.png]]\n\nThere may be other conditions, but in this case, there is a link to this route in the app which does not exist:\n\n/pipeline-2023-3\n\nThe clue as to where this reference occurs is this line:\n\n![[Pasted image 20231207133050.png]]\nwhere the (linked from /) indicates the error is referenced by something in the root path.\n\nThis 404 error appears to be a \"die now\" error for the build process. That is, if the errant route reference exists in more than one place, the build dies on the first reference. After fixing that first reference, another builds shows the next errant occurrence.",
        "locale": "en",
        "slug": "technical-posts/sveltekit-build-errors",
        "folder": ""
    },
    {
        "title": "Using SvelteKit route matching",
        "description": "Using SvelteKit route matching",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": " case-study/\n     [slug]/\n         +page.js\n         +page.svelte\n\n+page.js\n\nNote how the [slug] route naming makes the params.slug value available.  \n\nimport { error } from '@sveltejs/kit';\n\nexport const load = async ({ url, route, params }) => {\n\tconst locale = route.id.slice(1, 3);\n\n\ttry {\n\t\tconst post = await import(../../../markdown/case-study/${params.slug}.${locale}.md);\n\t\treturn {\n\t\t\tcontent: post.default,\n\t\t\tmeta: post.metadata\n\t\t};\n\t} catch {\n\t\tthrow error(404, Could not find);\n\t}\n};\n\n+page.svelte\n\n\n\timport { page } from '$app/stores';\n\timport PageMetaData from '$components/all-locales/all-pages/PageMetaData.svelte';\n\timport TagLine from '$components/all-locales/all-pages/TagLine.svelte';\n\timport HeaderCaseStudy from '../../../../components/all-locales/all-pages/HeaderCaseStudy.svelte';\n\n\t//const locale = $page.url.pathname.slice(1, 3);\n\t//console.log(locale);\n\n\texport let data;\n\n\n\n \n\n\n\n\n\t{data.meta.title}\n    \n\t\n        \n\t\n    \n\n\n\n\th1 {\n\t\tfont-size: 1.8rem;\n\t}\n",
        "locale": "en",
        "slug": "technical-posts/using-svelte-dynamic-folder",
        "folder": ""
    },
    {
        "title": "Miscellaneous tech notes about HTML forms and Sveltekit--especially regarding typescript. It also touches briefly on Zod.",
        "description": "Miscellaneous tech notes about HTML forms and Sveltekit--especially regarding typescript. It also touches briefly on Zod.",
        "date_created": "2025-05-21T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript",
            "forms",
            "svelte"
        ],
        "content": "Why you need Zod! \n\n[[zod-technical-notes]]\n\nI had AI generate some form handling code to see what it would say. Gemini generated the code salad below.  The code is mostly silly because Zod does pretty much all of this. However, I kept the code because of its use of TypeScript.  \n\nI found this line particularly interesting:\n\nconst getString = (key: keyof FamilyFormData): string | undefined => formData.get(key as string)?.toString();\n\nIt gets a key value from a formData object. If the key value is null or undefined the function \"short-circuits.\" The toString() method is not called, and the function evaluates to undefined. If the value is anything else (a \"truthy\" value or a \"falsy\" value like 0, \"\", false that is not null or undefined): The operation continues, and the function returns the toString() value of the key value found.\n\nconst name = getString('name') ?? '';\n\n\nCode that Gemini produced\n\n// src/lib/server/formUtils.ts\nimport type { FamilyFormData, FamilyFormErrors } from './db'; // Import types\n\ninterface FormParseResult {\n\tvalues: FamilyFormData;\n\terrors: FamilyFormErrors | null;\n}\n\n/**\n Extracts and sanitizes family data from FormData for Create/Update.\n Handles type conversions and null values. Returns typed values and errors.\n @param formData The FormData object from the request.\n @returns Object containing sanitized values and potential validation errors.\n */\nexport function getFamilyDataFromForm(formData: FormData): FormParseResult {\n\tconst values: Partial = {}; // Use Partial initially\n\tconst errors: FamilyFormErrors = {};\n\n\t// Helper to safely get string values\n\tconst getString = (key: keyof FamilyFormData): string | undefined =>\n\t\tformData.get(key as string)?.toString();\n\n\tvalues.name = getString('name') ?? '';\n\tvalues.description = getString('description') ?? '';\n\tvalues.visual_studio_version = getString('visual_studio_version') ?? '';\n\tvalues.download_page_section_heading = getString('download_page_section_heading') ?? '';\n\n\tconst availability_id_str = getString('availability_id');\n\tconst group_id_str = getString('group_id');\n\tconst sort_order_str = getString('sort_order');\n\tconst download_page_order_str = getString('download_page_order');\n\tvalues.release_date = getString('release_date') || null; // Handle empty string as null\n\n\t// --- Basic Validation ---\n\tif (!values.name) errors.name = 'Name is required';\n\tif (!values.description) errors.description = 'Description is required';\n\tif (!values.visual_studio_version)\n\t\terrors.visual_studio_version = 'Visual Studio Version is required';\n\tif (!values.download_page_section_heading)\n\t\terrors.download_page_section_heading = 'Download Page Section Heading is required';\n\n\t// --- Type Conversion & Validation ---\n\n\t// Optional BigInt FKs (keep as string)\n\tvalues.availability_id =\n\t\tavailability_id_str && availability_id_str !== '' ? availability_id_str : null;\n\tvalues.group_id = group_id_str && group_id_str !== '' ? group_id_str : null;\n\n\t// Sort Order (required number, default 6000)\n\tif (sort_order_str === undefined || sort_order_str === '') {\n\t\terrors.sort_order = 'Sort order is required';\n\t\tvalues.sort_order = 6000; // Use default even on error for form refill\n\t} else {\n\t\tconst parsed = parseInt(sort_order_str, 10);\n\t\tif (isNaN(parsed)) {\n\t\t\terrors.sort_order = 'Sort order must be a valid number';\n\t\t\tvalues.sort_order = 6000; // Default\n\t\t} else {\n\t\t\tvalues.sort_order = parsed;\n\t\t}\n\t}\n\n\t// Download Page Order (required number, default 3200)\n\tif (download_page_order_str === undefined || download_page_order_str === '') {\n\t\terrors.download_page_order = 'Download page order is required';\n\t\tvalues.download_page_order = 3200; // Default\n\t} else {\n\t\tconst parsed = parseInt(download_page_order_str, 10);\n\t\tif (isNaN(parsed)) {\n\t\t\terrors.download_page_order = 'Download page order must be a valid number';\n\t\t\tvalues.download_page_order = 3200; // Default\n\t\t} else {\n\t\t\tvalues.download_page_order = parsed;\n\t\t}\n\t}\n\n\t// Clean up release date (allow empty string -> null)\n\tif (values.release_date && values.release_date.trim() === '') {\n\t\tvalues.release_date = null;\n\t}\n\n\t// --- Final Check ---\n\tconst hasErrors = Object.keys(errors).length > 0;\n\n\t// Cast to final type, ensuring all required fields are present (even if derived from defaults)\n\t// This relies on defaults being set correctly above.\n\tconst finalValues = values as FamilyFormData;\n\n\treturn {\n\t\tvalues: finalValues,\n\t\terrors: hasErrors ? errors : null\n\t};\n}",
        "locale": "en",
        "slug": "technical-posts/svelte-forms-technical-notes",
        "folder": ""
    },
    {
        "title": "A CRUD repository for MSSQL for JavaScript/TypeScript",
        "description": "A CRUD repository for MSSQL for JavaScript/TypeScript",
        "date_created": "2025-05-31T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sql-server",
            "repository",
            "mssql"
        ],
        "content": "Types used\n\nimport { z } from \"zod\";\n\nexport const categoryRulesSchema = z.object({\n    // public int id { get; set; }\n    // C# 'int' is a non-nullable integer.\n    id: z.number().int().nullable(),\n\n    // public string? description { get; set; }\n    // C# 'string?' means the string can be null.\n    // In Zod, .nullable() means the value can be the type or null.\n    // If the key itself might be missing (undefined), you'd use .optional() or .nullish().\n    // Assuming if the property exists, it's either a string or null:\n    description: z.string().nullable(),\n\n    // public string? rule { get; set; }\n    rule: z.string().nullable(),\n\n    // public string? value { get; set; }\n    value: z.string().nullable(),\n\n    // public string? category { get; set; }\n    category: z.string().nullable(),\n});\n\n// If you want to infer a TypeScript type from this schema:\nexport type CategoryRules = z.infer;\n\nUsing the upsert operation\n\nasync function upsertCategoryRule(model: CategoryRules) {\n    const request = ms.pool.request();\n    request.input(\"id\", ms.sql.Int, model.id);\n    request.input(\"description\", ms.sql.NVarChar, model.description);\n    request.input(\"rule\", ms.sql.NVarChar, model.rule);\n    request.input(\"value\", ms.sql.NVarChar, model.value);\n    request.input(\"category\", ms.sql.NVarChar, model.category);\n\n    const result = await request.execute(\"dbo.rp_categoryrulesUpsert\");\n\n    console.log(result);\n\n    return result.rowsAffected0];\n}\n\nlet categoryRulesMode: CategoryRules = {\n    id: 37,\n    description: \"Ruby's Taps and Tails\",\n    rule: \"includes\",\n    value: \"Taps and Tails\",\n    category: \"fast food\",\n};\n\nconst count = await upsertCategoryRule(model).catch((err) => {\n    console.error(\"Error executing select query:\", err);\n    return 0;\n});\n\n\n\n\n\n\n\nThe result from calling an upsert procedure with mssql when a record is updated is: \n\n{\n  recordsets: [\n    [\n      [Object ...], columns: [Object ...], toTable: [Function: value]\n    ]\n  ],\n  recordset: [\n    {\n      id: 37,\n      description: \"Ruby's Taps and Tails\",\n      rule: \"includes\",\n      value: \"xxxxxxxx\",\n      category: \"fast food\",\n      ActionStatus: \"UPDATED\",\n    }, columns: {\n      id: [Object ...],\n      description: [Object ...],\n      rule: [Object ...],\n      value: [Object ...],\n      category: [Object ...],\n      ActionStatus: [Object ...],\n    }, toTable: [Function: value]\n  ],\n  output: {},\n  rowsAffected: [],\n  returnValue: 0,\n}\n\nThe result from calling an upsert procedure with mssql when a record is updated is: \n\n{\n  recordsets: [\n    [\n      [Object ...], columns: [Object ...], toTable: [Function: value]\n    ]\n  ],\n  recordset: [\n    {\n      id: 38,\n      description: \"Millers Brewz\",\n      rule: \"includes\",\n      value: \"xxxxxxxx\",\n      category: \"fast food\",\n      ActionStatus: \"INSERTED\",\n    }, columns: {\n      id: [Object ...],\n      description: [Object ...],\n      rule: [Object ...],\n      value: [Object ...],\n      category: [Object ...],\n      ActionStatus: [Object ...],\n    }, toTable: [Function: value]\n  ],\n  output: {},\n  rowsAffected: [],\n  returnValue: 0,\n}\n\n\nGemini cleaned up my Upsert stored proc and added returning whether a row was added or updated and used a transaction. \n\nOkay, you can modify your stored procedure to return this information by adding an ActionStatus column to your result set and ensuring you correctly capture and return the ID.\n\nHere's the modified stored procedure:\n\nCREATE OR ALTER PROC [dbo].[rp_categoryrulesUpsert]\n    @id INT = NULL,\n    @description VARCHAR(150),\n    @rule VARCHAR(50),\n    @value VARCHAR(500),\n    @category VARCHAR(150)\nAS\nBEGIN\n    SET NOCOUNT ON;\n    SET XACT_ABORT ON;\n\n    DECLARE @ActionStatus VARCHAR(10);\n    DECLARE @OutputId INT;\n\n    BEGIN TRY\n        BEGIN TRAN;\n\n        IF @id IS NOT NULL AND @id > 0 -- Check for a valid, positive ID indicating an update attempt\n        BEGIN\n            -- Attempt to update\n            UPDATE [dbo].[categoryrules]\n            SET    [description] = @description,\n                   [rule] = @rule,\n                   [value] = @value,\n                   [category] = @category\n            WHERE  [id] = @id;\n\n            IF @@ROWCOUNT > 0 -- Check if any row was actually updated\n            BEGIN\n                SET @ActionStatus = 'UPDATED';\n                SET @OutputId = @id;\n\n                SELECT\n                    cr.[id],\n                    cr.[description],\n                    cr.[rule],\n                    cr.[value],\n                    cr.[category],\n                    @ActionStatus AS ActionStatus\n                FROM\n                    [dbo].[categoryrules] cr\n                WHERE\n                    cr.[id] = @OutputId;\n            END\n            ELSE\n            BEGIN\n                -- No row found with the given @id to update.\n                SET @ActionStatus = 'NOT_FOUND';\n                SET @OutputId = @id; -- The ID that was attempted\n\n                SELECT\n                    @OutputId AS id,\n                    CAST(NULL AS VARCHAR(150)) AS description,\n                    CAST(NULL AS VARCHAR(50)) AS [rule], -- Changed 'AS rule' to 'AS [rule]'\n                    CAST(NULL AS VARCHAR(500)) AS value,\n                    CAST(NULL AS VARCHAR(150)) AS category,\n                    @ActionStatus AS ActionStatus;\n            END\n        END\n        ELSE\n        BEGIN\n            -- Insert new row\n            INSERT INTO [dbo].[categoryrules, rule], [value], [category])\n            VALUES (@description, @rule, @value, @category);\n\n            SET @OutputId = SCOPE_IDENTITY(); -- Get the newly inserted ID\n            SET @ActionStatus = 'INSERTED';\n\n            SELECT\n                cr.[id],\n                cr.[description],\n                cr.[rule],\n                cr.[value],\n                cr.[category],\n                @ActionStatus AS ActionStatus\n            FROM\n                [dbo].[categoryrules] cr\n            WHERE\n                cr.[id] = @OutputId;\n        END\n\n        COMMIT TRAN;\n    END TRY\n    BEGIN CATCH\n        IF @@TRANCOUNT > 0\n            ROLLBACK TRAN;\n\n        THROW;\n    END CATCH;\nEND\nGO\n\n\n\n\n-- Example Usage:\n\n-- 1. Insert a new rule (assuming ID 1, 2, 3 don't exist or you want a new one)\nEXEC [dbo].[rp_categoryrulesUpsert]\n    @id = NULL, -- Or 0, or -1, or simply omit if default NULL is desired for insert\n    @description = 'New Rule Description',\n    @rule = 'NEW_RULE',\n    @value = 'Value for new rule',\n    @category = 'General';\nGO\n\n-- Get the ID from the previous insert (e.g., assume it was 100)\nDECLARE @InsertedId INT = 100; -- Replace with actual ID if known from app logic\n\n-- 2. Update the previously inserted rule\nEXEC [dbo].[rp_categoryrulesUpsert]\n    @id = @InsertedId, -- Provide the ID of the row to update\n    @description = 'Updated Rule Description',\n    @rule = 'UPDATED_RULE_CODE',\n    @value = 'New value after update',\n    @category = 'Specifics';\nGO\n\n-- 3. Attempt to update a non-existent rule\nEXEC [dbo].[rp_categoryrulesUpsert]\n    @id = 9999, -- Assuming this ID does not exist\n    @description = 'Non-existent Rule Update',\n    @rule = 'NON_EXIST',\n    @value = 'WontBeSaved',\n    @category = 'Test';\nGO\n\nKey Changes and Explanations:\n\n@ActionStatus VARCHAR(10): A variable to store whether an 'INSERTED', 'UPDATED', or 'NOT_FOUND' action occurred.\n@OutputId INT: A variable to store the ID that was either inserted or updated.\nIF @id IS NOT NULL AND @id > 0:\n    This condition is used to determine if the operation is an update. If @id is provided and is a positive integer, it's treated as an update attempt.\n    Your original IF @Id > 0 would implicitly treat NULL as not greater than 0, leading to the ELSE (insert) block. Making it IS NOT NULL AND @id > 0 is more explicit.\nUpdate Path (IF @id IS NOT NULL AND @id > 0):\n    The UPDATE statement is executed.\n    IF @@ROWCOUNT > 0: This is crucial. @@ROWCOUNT returns the number of rows affected by the last statement. If it's greater than 0, the update was successful.\n        @ActionStatus is set to 'UPDATED'.\n        @OutputId is set to the input @id.\n        The SELECT statement then retrieves the updated row and includes the ActionStatus.\n    ELSE (for @@ROWCOUNT = 0): If @@ROWCOUNT is 0 after an update attempt with a specific @id, it means no row with that id was found.\n        @ActionStatus is set to 'NOT_FOUND'.\n        @OutputId is set to the @id that was attempted.\n        A specific SELECT statement is used to return a single row with the id and ActionStatus of 'NOT_FOUND', and NULLs for other data columns to maintain a consistent result set structure.\nInsert Path (ELSE):\n    The INSERT statement is executed.\n    SET @OutputId = SCOPE_IDENTITY();: SCOPE_IDENTITY() reliably gets the ID of the row inserted within the current scope and batch.\n    @ActionStatus is set to 'INSERTED'.\n    The SELECT statement retrieves the newly inserted row along with the ActionStatus.\nTRY...CATCH Block:\n    Added for robust error handling. If any error occurs within the TRY block, execution jumps to the CATCH block.\n    IF @@TRANCOUNT > 0 ROLLBACK TRAN;: If a transaction is active and an error occurs, it's rolled back to maintain data integrity.\n    THROW;: Re-throws the original error so the calling application can be aware of it.\nConsistent SELECT Statement:\n    Each logical path (UPDATED, INSERTED, NOT_FOUND) now returns a result set with the same columns: id, description, rule, value, category, and the new ActionStatus. This makes it easier for the client application to process the result.\n\nNow, when you execute this stored procedure, the result set will always include the ActionStatus column, telling you exactly what happened, and the id column will contain the relevant ID.\n\nThis is a template \n\nUSE [YourDatabaseName]\nGO\n\n/* Object:  StoredProcedure [dbo].[TableName_Upsert] */\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE OR ALTER PROC [dbo].[TableName_Upsert]\n    @id INT = NULL,\n    -- Add your table-specific parameters here\n    @column1 VARCHAR(150),\n    @column2 VARCHAR(50),\n    @column3 VARCHAR(500),\n    @column4 VARCHAR(150)\n    -- Add more parameters as needed\nAS\nBEGIN\n    SET NOCOUNT ON;\n    SET XACT_ABORT ON;\n\n    DECLARE @ActionStatus VARCHAR(10);\n    DECLARE @OutputId INT;\n\n    BEGIN TRY\n        BEGIN TRAN;\n\n        IF @id IS NOT NULL AND @id > 0 -- Check for a valid, positive ID indicating an update attempt\n        BEGIN\n            -- Attempt to update\n            UPDATE [dbo].[YourTableName]\n            SET    [column1] = @column1,\n                   [column2] = @column2,\n                   [column3] = @column3,\n                   [column4] = @column4\n                   -- Add more columns as needed\n            WHERE  [id] = @id;\n\n            IF @@ROWCOUNT > 0 -- Check if any row was actually updated\n            BEGIN\n                SET @ActionStatus = 'UPDATED';\n                SET @OutputId = @id;\n\n                SELECT\n                    t.[id],\n                    t.[column1],\n                    t.[column2],\n                    t.[column3],\n                    t.[column4],\n                    -- Add more columns as needed\n                    @ActionStatus AS ActionStatus\n                FROM\n                    [dbo].[YourTableName] t\n                WHERE\n                    t.[id] = @OutputId;\n            END\n            ELSE\n            BEGIN\n                -- No row found with the given @id to update.\n                SET @ActionStatus = 'NOT_FOUND';\n                SET @OutputId = @id; -- The ID that was attempted\n\n                SELECT\n                    @OutputId AS id,\n                    CAST(NULL AS VARCHAR(150)) AS column1,\n                    CAST(NULL AS VARCHAR(50)) AS column2,\n                    CAST(NULL AS VARCHAR(500)) AS column3,\n                    CAST(NULL AS VARCHAR(150)) AS column4,\n                    -- Add more columns as needed with appropriate CAST types\n                    @ActionStatus AS ActionStatus;\n            END\n        END\n        ELSE\n        BEGIN\n            -- Insert new row\n            INSERT INTO [dbo].[YourTableName, [column2], [column3], [column4])\n            VALUES (@column1, @column2, @column3, @column4);\n\n            SET @OutputId = SCOPE_IDENTITY(); -- Get the newly inserted ID\n            SET @ActionStatus = 'INSERTED';\n\n            SELECT\n                t.[id],\n                t.[column1],\n                t.[column2],\n                t.[column3],\n                t.[column4],\n                -- Add more columns as needed\n                @ActionStatus AS ActionStatus\n            FROM\n                [dbo].[YourTableName] t\n            WHERE\n                t.[id] = @OutputId;\n        END\n\n        COMMIT TRAN;\n    END TRY\n    BEGIN CATCH\n        IF @@TRANCOUNT > 0\n            ROLLBACK TRAN;\n\n        THROW;\n    END CATCH;\nEND\nGO",
        "locale": "en",
        "slug": "technical-posts/mssql-repository-upsert",
        "folder": ""
    },
    {
        "title": "what is a docker file",
        "description": "what is a docker file",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "docker",
            "containers"
        ],
        "content": "Okay, let's break down what a Dockerfile is.\n\nIn simple terms:\n\nA Dockerfile is a text file that contains a set of instructions on how to build a Docker image. Think of it like a recipe or a blueprint for creating your containerized application environment.\nIn essence, the Dockerfile is the source code for your Docker image, making environment creation automated, repeatable, and shareable.\n\nHere's a more detailed explanation:\n\nBlueprint for Images: Docker images are the static, packaged snapshots of your application, its dependencies, libraries, configuration files, and everything else needed to run it. The Dockerfile defines exactly what goes into that image and how it should be configured.\n\nText-Based Instructions: It's just a plain text file (usually named Dockerfile with no extension) where each line represents an instruction Docker follows during the image build process.\n\nAutomation: Instead of manually setting up an environment, installing software, copying files, and configuring settings every time, you write these steps once in the Dockerfile. Docker then automates the entire process of creating the image based on these instructions.\n\nReproducibility & Consistency: Because the Dockerfile precisely defines the environment, anyone with that Dockerfile can build the exact same image. This ensures consistency across different development machines, testing environments, and production servers.\n\nLayered Structure: Each instruction in a Dockerfile typically creates a new \"layer\" in the Docker image. Docker caches these layers. If you change an instruction later in the file, Docker can reuse the cached layers from the earlier, unchanged instructions, making subsequent builds much faster.\n\nCommon Instructions in a Dockerfile:\n\nFROM: Specifies the base image to start from (e.g., FROM ubuntu:latest, FROM python:3.9-slim). This is usually the first instruction.\nWORKDIR: Sets the working directory for subsequent instructions (RUN, CMD, ENTRYPOINT, COPY, ADD).\nCOPY: Copies files or directories from your local machine (the build context) into the image's filesystem.\nADD: Similar to COPY, but with extra features like unpacking compressed files and fetching remote URLs (though COPY is often preferred for simplicity).\nRUN: Executes commands during the image build process (e.g., RUN apt-get update && apt-get install -y some-package, RUN pip install -r requirements.txt). Each RUN creates a new layer.\nEXPOSE: Informs Docker that the container will listen on the specified network ports at runtime (it doesn't actually publish the port).\nCMD: Specifies the default command to run when a container is started from the image. There can only be one CMD, and it can be overridden when running the container.\nENTRYPOINT: Also specifies a command to run when the container starts, but it's harder to override than CMD. Often used to make the container act like an executable.\n\nSimple Example (Python App):\n\nFROM python:3.9-slim\n\nSet the working directory in the container\nWORKDIR /app\n\nCopy the current directory contents into the container at /app\nCOPY . /app\n\nInstall any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\nMake port 80 available to the world outside this container\nEXPOSE 80\n\nDefine environment variable\nENV NAME World\n\nRun app.py when the container launches\nCMD [\"python\", \"app.py\"]\n\nHow it's Used:\n\nYou create a file named Dockerfile in your project directory.\nYou run the command docker build -t your-image-name . in that directory.\nDocker reads the Dockerfile, executes the instructions step-by-step, creating layers.\nThe final output is a Docker image named your-image-name that you can then use to run containers (docker run your-image-name).",
        "locale": "en",
        "slug": "technical-posts/what-is-a-docker-file",
        "folder": ""
    },
    {
        "title": "types versus interfaces",
        "description": "types versus interfaces",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "That's a great question, and it gets to the heart of when and why to use TypeScript's strong typing. Here's a breakdown of the pros and cons of typing your JSON like that, to help you decide if it's a good idea for your specific scenario:\n\nPros of Typing Your JSON (Strongly Recommended in Most Cases):\n\nEarly Error Detection:\n\n    TypeScript will immediately flag errors at compile time (or while you're coding) if your JSON data doesn't conform to the expected type. This is much better than runtime errors that might only surface in production.\n    You'll catch typos, missing properties, and incorrect data types before your code ever runs.\n\nImproved Code Maintainability:\n\n    Well-typed code is easier to understand and reason about. Other developers (and your future self) can see exactly what data structure to expect.\n    When you need to refactor, the compiler will help ensure you haven't broken any dependencies.\n\nEnhanced Developer Experience:\n\n    Code editors with TypeScript support provide autocompletion, inline documentation, and type-checking on the fly.\n    You get code hints and refactoring assistance that can drastically speed up your development.\n\nClear Communication of Data Structure:\n\n    Types act as a clear and concise form of documentation for your data. They help describe the \"shape\" of your data, which can be more effective than writing long comments.\n\nMore Robust Applications:\n\n    Type checking can prevent unexpected behaviors and bugs related to incorrect assumptions about the shape or type of incoming data, making your applications more stable and predictable.\n\nCons of Typing Your JSON (Considerations):\n\nInitial Setup Cost:\n\n    Typing JSON initially requires some effort. You need to define the correct interfaces/types, which takes a little more time up front.\n    For very simple or rapidly changing JSON structures, the initial typing effort might feel like overkill.\n\nType Maintenance:\n\n    If your JSON structure changes frequently, you must update your TypeScript types accordingly.\n    This can be tedious if your data changes very often, or you're not sure what to expect. However, you may need to do this with javascript as well (i.e. if your JSON has new keys)\n\nIncreased Code Size (Marginal):\n\n    TypeScript types can add slightly to your file sizes, but this difference is typically negligible, particularly after minification.\n    For larger projects, the benefits far outweigh this small cost.\n\nLearning Curve:\n\n    If you're completely new to TypeScript, there will be a learning curve to understand how to define the types. But once you are up to speed, this cost will be minimal.\n\nRigidity:\n\nIf you have a JSON source that may have optional properties it may be more difficult to type without having a more complex type that includes | undefined. However, this can also be useful because it forces you to define how you would handle missing data.\n\nOver Typing:\n\nIf you find yourself typing every single key and type and have deeply nested objects, that can be too much work. If that is the case, then you should avoid typing nested objects, unless you are actually using all of the keys. This is also a benefit to your code because you won't need to worry about making a type change to a key if you are not using that key anyway.\n\nWhen is Typing JSON a \"Good Idea\" vs. \"More Trouble Than It's Worth?\"\n\nGood Idea (Strongly Recommended):\n\n    Medium to Large Projects: If your project is large, complex, or going to be maintained for a long time, typing your JSON will almost certainly pay off.\n    Data from External Sources: When your JSON comes from an API or external source, typing becomes crucial to protect your application from unexpected data formats.\n    Shared Data Structures: When multiple parts of your application use the same JSON structure, types help ensure consistency across your project.\n    Collaborative Projects: If multiple developers are working on the same project, explicit typing prevents errors and makes the code easier to work with as a team.\n    If you want to avoid runtime errors: If you want to avoid crashes on the front end due to incorrect json data, then you should type your JSON.\n\nPotentially More Trouble Than It's Worth (Use with Caution):\n\n    Very Small, Throwaway Projects: In very small, one-off scripts, the initial type setup may not be worth it, but even then, you may find yourself using those scripts later.\n    Rapidly Changing/Unpredictable JSON Structures: If your JSON structure changes extremely quickly and you can't accurately pin down the types, you might consider skipping types. However, even in this case, it may be useful to have the major keys typed, so you don't get undefined errors.\n    Experimental or Research Code: If you're experimenting or in the research phase, and the data is not important, you can use less strict typing (e.g., use any to move very quickly) then type later when the structures are clearer.\n\nMy Recommendation:\n\nIn most cases, especially when working on a project that's more than a few lines of code, typing your JSON with TypeScript is absolutely a good idea. The benefits in terms of code quality, maintainability, and fewer bugs far outweigh the initial setup cost. You may also be surprised that, over time, the initial cost of types will be small, and the payoff is big, since you do not need to worry about what the json types are.\n\nThat being said, you don't need to type everything to the most extreme degree. If you find yourself spending too much time on typing, you may be over typing your data. For example, you may not need to type keys in an object that are not being used.\n\nBy weighing the pros and cons, you'll have a clearer understanding of when and why to type your JSON data in TypeScript.",
        "locale": "en",
        "slug": "technical-posts/types-versus-interfaces",
        "folder": ""
    },
    {
        "title": "svelte-component-input",
        "description": "svelte-component-input",
        "date_created": "2025-05-22T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte-component"
        ],
        "content": "Properties:\ndomName: string;\nlabelText: string;\nvalue?: string | number;\nrequired?: boolean;\nerrorMessage?: string;\nplaceholder?: string;\nclassName?: string;\nsurroundWithFormFieldDiv?: boolean;\n\nComponent source code:\n\n\n\timport { core } from 'zod/v4';\n\n\tinterface ComponentProps {\n\t\tdomName: string;\n\t\tlabelText: string;\n\t\tvalue?: string | number;\n\t\trequired?: boolean;\n\t\terrorMessage?: string;\n\t\tplaceholder?: string;\n\t\tclassName?: string;\n\t\tsurroundWithFormFieldDiv?: boolean;\n\t}\n\n\tconst {\n\t\tdomName,\n\t\tlabelText,\n\t\tvalue,\n\t\trequired = true,\n\t\terrorMessage,\n\t\tplaceholder,\n\t\tclassName,\n\t\tsurroundWithFormFieldDiv = true\n\t}: ComponentProps = $props();\n\n\tlet fieldValue = $state(value ?? '');\n\n\n{#snippet inputHTML()}\n\t{labelText}\n\t\n\t{#if errorMessage}\n\t\t{errorMessage}\n\t{/if}\n{/snippet}\n\n{#if surroundWithFormFieldDiv}\n\t\n\t\t{@render inputHTML()}\n\t\n{:else}\n\t{@render inputHTML()}\n{/if}",
        "locale": "en",
        "slug": "technical-posts/svelte-component-input-text",
        "folder": ""
    },
    {
        "title": "svelte-reactivity-avoids-need-for-data-sveltekit-reload",
        "description": "svelte-reactivity-avoids-need-for-data-sveltekit-reload",
        "date_created": "2025-10-26T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "A group of pages comprise 'success stories.' Each success story is rendered in \n\nsrc/routes/en/success-stories/[slug]+page.svelte\n\nThe right side of this page lists the currently available success stories. The list of available success stories is provided in this array\" \n\nexport const successStories = [\n\t\t{ slug: 'aerospace-defense', title: 'Aerospace/Defense' },\n\t\t{ slug: 'healthcare-management', title: 'Health Care Management Services' },\n\t\t{ slug: 'health-medical', title: 'Health/Medical Care' },\n\t\t{ slug: 'financial-services', title: 'International Financial Services' },\n\t\t{ slug: 'marine-harbor-management', title: 'Marine Harbor Management' },\n\t\t{ slug: 'component-manufacturer', title: 'Micro Component Manufacturer' },\n\t\t{ slug: 'state-infrastructure', title: 'State Infrastructure Management' }\n\t];\n\nThis navigation lists is rendered as a unordered list. I wanted the currently-displayed success story's link to be marked as such in the list, like this: \n\n![[image-54.png|733x478]]\n\nThis code gets the current page's slug: \n\nimport { page } from '$app/state';\n\n...\n\nlet currentSlug = page.params.slug;\n\nThis loop renders the list of links, assigning the active class to the current link and conditionally adds the (this article) text to the link.\n\n\n\t{#each successStories as story}\n\t\t\n\t\t\t\n\t\t\t\t{story.title}\n\t\t\t\t{#if currentSlug === story.slug}\n\t\t\t\t\t(this article)\n\t\t\t\t{/if}\n\t\t\t\n\t\t\n\t{/each}\n\n\nOn the first display of the page, this works like a champ. However, when \n\n\nGreat! Let me explain why this works in detail:\n\nHow Svelte 5 Reactivity Works with $derived\n\nThe Problem Before:\nWhen you were using:\nlet currentSlug = page.params.slug;\n\nThis created a one-time assignment. The variable currentSlug was set when the component first loaded, but it didn't track changes to page.params.slug. When you clicked a link to navigate to a different success story, SvelteKit's client-side navigation would update the page object, but currentSlug remained stuck with its original value.\n\nThe Solution with $derived:\nlet currentSlug = $derived(page.params.slug);\n\nThe $derived rune creates a reactive derived state. Here's what happens:\n\nDependency Tracking: When you use $derived, Svelte 5 automatically tracks that currentSlug depends on page.params.slug.\n\nAutomatic Re-evaluation: Whenever page.params.slug changes (when you navigate to a different success story), Svelte automatically re-runs the $derived expression and updates currentSlug.\n\nDOM Updates: Because currentSlug is used in your template to determine the active class:\n   \n   When currentSlug updates, Svelte re-evaluates this expression for each list item and updates the classes in the DOM.\n\nWhy $app/state Matters:\n\nIn Svelte 5, page from $app/state is a reactive state object. This means:\nIt's not just a regular JavaScript object\nIt's designed to work with Svelte 5's reactivity system\nWhen properties change (like page.params.slug), any $derived values that depend on it automatically update\n\nThe Flow:\nUser clicks link  /en/success-stories/aerospace-defense\nSvelteKit updates page.params.slug to 'aerospace-defense'\n$derived detects the change and updates currentSlug\nTemplate expressions using currentSlug re-evaluate\nThe new active list item gets the 'active' class, the old one loses it\nCSS styling updates to show the new active state\n\nWhy data-sveltekit-reload Wasn't Needed:\n\nWith data-sveltekit-reload, you were forcing a full page reload, which:\nRe-ran all JavaScript\nRe-created the component from scratch\nLost the benefits of SvelteKit's client-side navigation\nWas slower and less efficient\n\nWith $derived, the reactivity system handles updates efficiently without needing to reload anything!",
        "locale": "en",
        "slug": "technical-posts/svelte-reactivity-avoids-need-for-data-sveltekit-reload",
        "folder": ""
    },
    {
        "title": "Update Family table",
        "description": "Update Family table",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sql",
            "downloads"
        ],
        "content": "This SQL is saved as update-last-release-date.sql. It should be run after posting a new release set. This SQL updates the release_date in the Family table. This column drives the ability of the\n\nWITH\nadded_row_number AS (\n\tSELECT *, ROW_NUMBER() OVER\n\t(\n\t\tPARTITION BY family_name\n\t\tORDER BY family_name ASC, release_date DESC\n\t)\n\tAS row_number\n\tFROM ReleaseSet\n),\nlast_release_date AS (\n\tSELECT release_date, family_name\n\tFROM added_row_number\n\tWHERE row_number = 1\n)\n\nUPDATE Family SET family.release_date =\n(\n    SELECT release_date\n\tFROM last_release_date\n\tWHERE Family.name = last_release_date.family_name\n)",
        "locale": "en",
        "slug": "technical-posts/updating-each-family-last-release-date",
        "folder": ""
    },
    {
        "title": "Sveltekit browser object",
        "description": "Sveltekit browser object",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "This imports a Boolean value that indicates if the page is being seen with a browser or on the server side.\n\nimport { browser } from '$app/environment';\n\nThere arebalso these objects in the environment store.\n_ dev\n_ building \\* version",
        "locale": "en",
        "slug": "technical-posts/sveltekit-browser-object",
        "folder": ""
    },
    {
        "title": "Webcast scheduling workflow",
        "description": "Webcast scheduling workflow",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "lakeb2b",
            "marketing"
        ],
        "content": "1. Schedule a[Webcast with GotoWebinar\nId: 6452680152106201181\nDate: Tuesday, November 25th, 1 to 2pm central\nUrl: https://www.asna.com/en/webcasts/rpg-future-2025-11-25;\n2. Create email\nCopy and save email for step 6.\n3. Create Landing page\n4. Check GoTo API key\n\n5. Get LakeB2B list from SQL Server \n\n6. Schedule Webcast with Constant Contact",
        "locale": "en",
        "slug": "technical-posts/schedule-landing-page-webcast",
        "folder": ""
    },
    {
        "title": "What does & mean in Rust",
        "description": "What does & mean in Rust",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "The ampersand (&) in file_path: &str signifies that the function is expecting a reference to a str (string slice) rather than owning the string data itself.\n\nHeres whats happening:\n\n&str (String slice): This is a reference to a string slice, which is a view into a part of a string. It points to some sequence of characters stored elsewhere, typically in a string (String) or in the programs binary (for string literals like \"hello\").\n\n    &: Denotes a reference. This allows the function to borrow the data instead of taking ownership of it. This is important for memory safety and efficient data handling in Rust. By borrowing, the function doesn't need to take ownership of the string, and the original owner (caller of the function) remains responsible for managing the string's memory.\n\nWhy use &str?\n\nEfficiency: Passing a reference (&str) is more efficient than copying the whole string, especially for large strings.\nOwnership and Borrowing: Rust's ownership system ensures only one owner of the data at a time unless its borrowed. By borrowing (&), the function doesn't take ownership of the string, allowing the caller to continue using it afterward.\nString literals: &str is commonly used with string literals (like \"hello\") because they are immutable and stored in the program's binary, meaning they are naturally borrowed.\n\nIn your example, file_path: &str means that the read_lines_from_file function borrows a string slice, making it efficient and preventing the function from taking ownership of the file_path. The caller remains responsible for the string's memory, and the function can safely use it without needing to copy it or manage its memory.\n\nIs the & like pointer in C\n\nYes, in a sense, the & in Rust is similar to a pointer in C, but with some important differences tied to Rust's focus on safety and ownership.\n\nHow Rust references (&T) are like C pointers:\n\nAddress of data: Both Rust references (&T) and C pointers store the address of a value in memory. So, when you use &str in Rust, youre passing the address of the string slice, just like passing a pointer to data in C.\nEfficiency: Just like pointers in C, passing a reference in Rust doesn't involve copying the actual data, only the address, making it more efficient for large data structures.\n\nKey differences between Rust references and C pointers:\n\nOwnership and Borrowing:\n    In C, pointers don't enforce ownership rules, so you can have multiple pointers to the same data without restrictions, which can lead to bugs (e.g., dangling pointers, double frees).\n    In Rust, references are tied to Rust's ownership and borrowing system, which enforces strict rules at compile time. For example:\n        You can have multiple immutable references (&T), or a single mutable reference (&mut T), but not both at the same time. This prevents data races and ensures memory safety.\nSafety:\n    C pointers can be null, leading to potential runtime crashes if dereferenced.\n    Rust references (&T) are guaranteed by the compiler to always be valid and non-null. Rust also ensures that references are only valid as long as the data they point to exists (lifetime management), preventing issues like dangling pointers.\nDereferencing:\n\n    In C, you need to explicitly dereference a pointer using the * operator to access the value.\n    In Rust, dereferencing is often implicit with references. For example, when you access a field or method of a reference, Rust automatically dereferences for you in most cases (though you can use * explicitly for dereferencing when necessary).\n\nNullability:\n    C pointers can be null, but Rust references are always valid. If you need something like a nullable pointer in Rust, you would use Option, which is an explicit, safe way to represent something that might be null.\n\nSummary:\n\nRust references (&) are conceptually similar to C pointers in that they both refer to the memory location of data. However, Rust adds compile-time checks to ensure safety, prevent common errors like null or dangling pointers, and manage memory lifetimes in a way that avoids many bugs you might encounter with raw pointers in C.",
        "locale": "en",
        "slug": "technical-posts/what-does-ampersand-mean-in-rust",
        "folder": ""
    },
    {
        "title": "Sveltekit ActionData for forms processing",
        "description": "Sveltekit ActionData for forms processing",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "forms"
        ],
        "content": "In SvelteKit, form is a special prop that contains form submission data and is automatically populated when using SvelteKit's form actions. Here's a breakdown:\n\nType Definition:\n\nexport let form: ActionData;\n\nThis declares form as a prop with type ActionData (imported from ./$types)\nActionData typically contains the return data from your form actions\n\nUsage Context:\n\nWhen you submit a form using SvelteKit's form actions (using ``)\nThe form prop automatically receives the data returned by your action handlers\nIt persists across page loads/navigation\nUseful for showing form submission results, errors, or success messages\n\nHere's a typical example of how it might be used:\n\n\n    import type { ActionData } from './$types';\n    export let form: ActionData;\n\n\n\n    \n    {#if form?.error}\n        {form.error}\n    {/if}\n    Submit\n\n\nThe commented-out getPermissionState function in your code suggests this form might be handling some kind of permission toggle, where the form data would include a permission field.\n\nTo see what data you're actually getting, you can check the console output from your console.log('form data in page', form); statement.",
        "locale": "en",
        "slug": "technical-posts/sveltekit-actiondata-for-forms-processing",
        "folder": ""
    },
    {
        "title": "WezTerm configuration",
        "description": "WezTerm configuration",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "I don't think I'll ever use it, but here is a WezTerm config for Windows:\n\nlocal wezterm = require(\"wezterm\")\nlocal config = wezterm.config_builder()\n\nconfig.font_size = 15\nconfig.window_decorations = \"RESIZE\"\n\nconfig.window_background_opacity = 0.7\n\n\nconfig.default_prog = { 'C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe' }\n\nreturn config",
        "locale": "en",
        "slug": "technical-posts/wezterm-configuration",
        "folder": ""
    },
    {
        "title": "Supabase with C#",
        "description": "Supabase with C#",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "using System;\nusing System.Windows.Forms;\nusing Npgsql;\nusing Dapper;\nusing System.Configuration; // Add reference to System.Configuration.ConfigurationManager\nusing Microsoft.Extensions.Configuration; // For appsettings.json\n\nnamespace SupabaseWinForms\n{\n    public partial class MainForm : Form\n    {\n        private string _connectionString;\n\n        public MainForm()\n        {\n            InitializeComponent();\n\n            // Option 1: Read connection string from app.config (classic .NET Framework way)\n            //_connectionString = ConfigurationManager.ConnectionStrings[\"SupabaseConnectionString\"]?.ConnectionString;\n\n            // Option 2: Read connection string from appsettings.json (preferred .NET 8 way)\n            var config = new ConfigurationBuilder()\n                .SetBasePath(AppDomain.CurrentDomain.BaseDirectory)\n                .AddJsonFile(\"appsettings.json\", optional: true, reloadOnChange: true)\n                .Build();\n\n            _connectionString = config.GetConnectionString(\"SupabaseConnectionString\");\n\n\n            if (string.IsNullOrEmpty(_connectionString))\n            {\n                MessageBox.Show(\"Connection string not found. Please check your configuration.\", \"Error\", MessageBoxButtons.OK, MessageBoxIcon.Error);\n                Application.Exit();\n            }\n        }\n\n        private void btnGetData_Click(object sender, EventArgs e)\n        {\n            try\n            {\n                using (var connection = new NpgsqlConnection(_connectionString))\n                {\n                    connection.Open();\n\n                    // Example query to retrieve data\n                    var results = connection.Query(\"SELECT * FROM my_table;\");  // Replace my_table with your actual table name\n                    // 'MyDataType' is a class/struct defined to match the columns in your table.\n\n                    // Display the data (example: in a TextBox)\n                    txtData.Text = \"\"; //Clear the textbox before adding new data\n                    foreach (var item in results)\n                    {\n                        txtData.AppendText(item.ToString() + Environment.NewLine); // Assuming MyDataType has a meaningful ToString() or override it.\n                    }\n                }\n            }\n            catch (Exception ex)\n            {\n                MessageBox.Show($\"Error: {ex.Message}\", \"Error\", MessageBoxButtons.OK, MessageBoxIcon.Error);\n            }\n        }\n\n        // Example of inserting data (create this event handler for a button, e.g., btnInsertData_Click)\n        private void btnInsertData_Click(object sender, EventArgs e)\n        {\n            try\n            {\n                using (var connection = new NpgsqlConnection(_connectionString))\n                {\n                    connection.Open();\n\n                    // Example: Using parameterized query to insert data\n                    var sql = \"INSERT INTO my_table (column1, column2) VALUES (@Column1Value, @Column2Value);\"; // Replace with your table and column names\n\n                    var parameters = new\n                    {\n                        Column1Value = txtColumn1.Text, // Get value from a TextBox for Column1\n                        Column2Value = txtColumn2.Text  // Get value from a TextBox for Column2\n                    };\n\n                    int rowsAffected = connection.Execute(sql, parameters);\n\n                    MessageBox.Show($\"{rowsAffected} row(s) inserted.\", \"Success\", MessageBoxButtons.OK, MessageBoxIcon.Information);\n                }\n            }\n            catch (Exception ex)\n            {\n                MessageBox.Show($\"Error: {ex.Message}\", \"Error\", MessageBoxButtons.OK, MessageBoxIcon.Error);\n            }\n        }\n\n        // Example of a data type to map results to (create a class/struct like this)\n        public class MyDataType\n        {\n            public int Id { get; set; }  // Must match column name and type in your database table!\n            public string? Column1 { get; set; } // Make sure column name and type matches database.\n            public string? Column2 { get; set; }\n\n            public override string ToString()\n            {\n                return $\"Id: {Id}, Column1: {Column1}, Column2: {Column2}\";\n            }\n        }\n    }\n}\n\nKey improvements and explanations:\n\nError Handling: Includes try-catch blocks around database operations. This is essential for a robust application. It catches exceptions and displays informative error messages to the user (or logs them for debugging). Without this, your app will crash unexpectedly if the database is unavailable or the connection string is wrong.\nConnection String Configuration: The code provides two options for managing the connection string:\n    app.config (Classic .NET Framework): This is the older way and is still perfectly valid. If you use this, you must add a reference to System.Configuration.ConfigurationManager in your project. The connection string is then stored in the app.config file, which you'll need to create.\n    appsettings.json (.NET 8 Preferred): This is the recommended approach for modern .NET applications. This is more flexible. You'll need to install the Microsoft.Extensions.Configuration and Microsoft.Extensions.Configuration.Json NuGet packages. This example uses AddJsonFile to add the JSON configuration. Also you need to ensure Copy to Output Directory for the appsettings.json file is set to Copy if newer or Copy always.\nConnection String Validation: Checks if the connection string is null or empty after loading it. This prevents a crash later if the connection string is missing from the configuration file.\nusing Statement: Uses using blocks for the NpgsqlConnection. This ensures that the connection is properly closed and disposed of, even if an exception occurs. This is crucial for resource management and preventing connection leaks.\nParameterized Queries: The insert example uses parameterized queries. This is absolutely critical for security. It prevents SQL injection attacks. Never, ever concatenate user input directly into SQL queries. Dapper handles the parameterization safely.\nClearer Code Structure: Separates the connection string loading, the data retrieval, and the error handling into distinct steps, making the code easier to read and understand.\nData Type Mapping (MyDataType): Includes an example MyDataType class. Crucially, the properties in this class must match the column names and types in your PostgreSQL table. Dapper uses this class to automatically map the database results to C# objects. Add the override ToString() for easy display.\nComplete Example with Insert: Adds a btnInsertData_Click example to demonstrate how to insert data using Dapper and parameterized queries.\nClearer Error Messages: The error messages now include the actual exception message (ex.Message), making it easier to diagnose problems.\nSupabase Note: This code assumes you've configured Supabase with a public schema or that you have the necessary authentication mechanisms (API keys, Row Level Security) set up correctly to allow your C# application to access the database. Supabase's security relies on these configurations, which are beyond the scope of this code example.\nNuGet Packages: Emphasizes that you need to install the Npgsql and Dapper NuGet packages. Also indicates which NuGet Packages are needed for using appsettings.json.\ntxtData.Text = \"\";: Clears the txtData textbox before adding new results, so you don't just keep appending to the existing text.\nAsync/Await: While this example is synchronous for simplicity, in a real-world WinForms application, you'd want to use async and await with Dapper's asynchronous methods (QueryAsync, ExecuteAsync) to avoid blocking the UI thread. That is beyond the scope of this core example, but important for responsiveness.\nConfiguration Builders: Demonstrates using ConfigurationBuilder to read appsettings.json file. It also includes setting the base path and enabling reload on change.\nConnection String retrieval from Configuration: The example reads connection strings defined under the \"ConnectionStrings\" section in the appsettings.json. Also it includes checking for null/empty connections string for better error handling.\n\nHow to Use:\n\nCreate a New WinForms Project: In Visual Studio, create a new C# Windows Forms App project (.NET 8).\nInstall NuGet Packages:\n    Right-click on your project in Solution Explorer, and choose \"Manage NuGet Packages...\".\n    Search for and install:\n        Npgsql\n        Dapper\n        Microsoft.Extensions.Configuration (If using appsettings.json)\n        Microsoft.Extensions.Configuration.Json (If using appsettings.json)\nDesign Your Form:\n    Open Form1.cs (or whatever your main form is named) in the designer.\n    Add a Button (named btnGetData), a TextBox (named txtData, set Multiline to true), and optionally, TextBoxes (txtColumn1, txtColumn2) and another Button (btnInsertData) for the insert example.\nAdd the Code:\n    Copy the C# code above into your Form1.cs file, replacing the default code.\nConfigure Your Connection String:\n    Option 1 (app.config): Create an app.config file in your project (if it doesn't exist already). Add the following configuration section:\n        \n        \n            \n                \n            \n            \n                \n            \n        \n        Option 2 (appsettings.json): Create a file called appsettings.json in your project. Make sure the Copy to Output Directory property for this file is set to Copy if newer or Copy always. Add the following JSON:\n        {\n            \"ConnectionStrings\": {\n                \"SupabaseConnectionString\": \"Host=your_supabase_host;Database=your_supabase_database;Username=your_supabase_username;Password=your_supabase_password;SSL Mode=Require;Trust Server Certificate=true\"\n            }\n        }\n    Important: Replace your_supabase_host, your_supabase_database, your_supabase_username, and your_supabase_password with your actual Supabase PostgreSQL credentials.\n    SSL Mode: The SSL Mode=Require;Trust Server Certificate=true part is essential for connecting securely to Supabase. Supabase enforces SSL.\nModify MyDataType: Adjust the MyDataType class to match the columns in your my_table table (or whatever table you're querying).\nBuild and Run: Build your project and run it. Click the \"Get Data\" button to retrieve data from your Supabase PostgreSQL database and display it in the txtData textbox. Test the \"Insert Data\" button as well, filling in the appropriate text boxes.\n\nTroubleshooting:\n\nConnection Errors: Double-check your connection string. Make sure the host, database, username, and password are correct. Ensure SSL Mode is set correctly. Verify that your network allows connections to your Supabase database.\nMissing NuGet Packages: Make sure you've installed the Npgsql and Dapper NuGet packages.\nColumn Mapping Errors: The properties in your MyDataType class must exactly match the column names (case-insensitively) and types in your database table.\nSecurity: Never hardcode your database credentials directly into your code. Use environment variables or a secure configuration management system for production applications.\nAsynchronous Operations: For larger datasets or potentially long-running queries, use asynchronous operations (async and await with Dapper's asynchronous methods) to prevent blocking the UI thread.\nSupabase Security: Ensure you've configured Supabase's Row Level Security (RLS) policies correctly to control access to your data.\n\nThis complete example should provide you with a solid foundation for building a C# .NET 8 WinForms application that connects to Supabase's PostgreSQL database using Dapper. Remember to adapt the code to your specific table structure and application requirements. Always prioritize security by using parameterized queries and protecting your database credentials.\n\n\nYou are absolutely right to point that out! My apologies. I've updated the answer to focus on .NET 8 (Core) and remove .NET Framework references. The previous answer mixed .NET Framework and .NET Core practices.\n\nusing System;\nusing System.Windows.Forms;\nusing Npgsql;\nusing Dapper;\nusing Microsoft.Extensions.Configuration; // For appsettings.json\n\nnamespace SupabaseWinForms\n{\n    public partial class MainForm : Form\n    {\n        private string _connectionString;\n\n        public MainForm()\n        {\n            InitializeComponent();\n\n            // Read connection string from appsettings.json (preferred .NET 8 way)\n            var config = new ConfigurationBuilder()\n                .SetBasePath(AppDomain.CurrentDomain.BaseDirectory)\n                .AddJsonFile(\"appsettings.json\", optional: true, reloadOnChange: true)\n                .Build();\n\n            _connectionString = config.GetConnectionString(\"SupabaseConnectionString\");\n\n\n            if (string.IsNullOrEmpty(_connectionString))\n            {\n                MessageBox.Show(\"Connection string not found. Please check your configuration.\", \"Error\", MessageBoxButtons.OK, MessageBoxIcon.Error);\n                Application.Exit();\n            }\n        }\n\n        private void btnGetData_Click(object sender, EventArgs e)\n        {\n            try\n            {\n                using (var connection = new NpgsqlConnection(_connectionString))\n                {\n                    connection.Open();\n\n                    // Example query to retrieve data\n                    var results = connection.Query(\"SELECT * FROM my_table;\");  // Replace my_table with your actual table name\n                    // 'MyDataType' is a class/struct defined to match the columns in your table.\n\n                    // Display the data (example: in a TextBox)\n                    txtData.Text = \"\"; //Clear the textbox before adding new data\n                    foreach (var item in results)\n                    {\n                        txtData.AppendText(item.ToString() + Environment.NewLine); // Assuming MyDataType has a meaningful ToString() or override it.\n                    }\n                }\n            }\n            catch (Exception ex)\n            {\n                MessageBox.Show($\"Error: {ex.Message}\", \"Error\", MessageBoxButtons.OK, MessageBoxIcon.Error);\n            }\n        }\n\n        // Example of inserting data (create this event handler for a button, e.g., btnInsertData_Click)\n        private void btnInsertData_Click(object sender, EventArgs e)\n        {\n            try\n            {\n                using (var connection = new NpgsqlConnection(_connectionString))\n                {\n                    connection.Open();\n\n                    // Example: Using parameterized query to insert data\n                    var sql = \"INSERT INTO my_table (column1, column2) VALUES (@Column1Value, @Column2Value);\"; // Replace with your table and column names\n\n                    var parameters = new\n                    {\n                        Column1Value = txtColumn1.Text, // Get value from a TextBox for Column1\n                        Column2Value = txtColumn2.Text  // Get value from a TextBox for Column2\n                    };\n\n                    int rowsAffected = connection.Execute(sql, parameters);\n\n                    MessageBox.Show($\"{rowsAffected} row(s) inserted.\", \"Success\", MessageBoxButtons.OK, MessageBoxIcon.Information);\n                }\n            }\n            catch (Exception ex)\n            {\n                MessageBox.Show($\"Error: {ex.Message}\", \"Error\", MessageBoxButtons.OK, MessageBoxIcon.Error);\n            }\n        }\n\n        // Example of a data type to map results to (create a class/struct like this)\n        public class MyDataType\n        {\n            public int Id { get; set; }  // Must match column name and type in your database table!\n            public string? Column1 { get; set; } // Make sure column name and type matches database.\n            public string? Column2 { get; set; }\n\n            public override string ToString()\n            {\n                return $\"Id: {Id}, Column1: {Column1}, Column2: {Column2}\";\n            }\n        }\n    }\n}\n\nKey changes and why they are important for .NET 8:\n\nRemoved System.Configuration: The System.Configuration namespace and ConfigurationManager are part of the .NET Framework and not the preferred way to handle configuration in .NET 8 (or any .NET Core version). It's been completely removed from the code.\nMicrosoft.Extensions.Configuration: We now use the Microsoft.Extensions.Configuration and Microsoft.Extensions.Configuration.Json packages (install via NuGet) for reading the configuration from appsettings.json. This is the standard way of managing configuration in .NET 8. It is much more flexible and extensible.\nappsettings.json: The code reads the connection string from appsettings.json. This file should be placed in your project's root directory, and its \"Copy to Output Directory\" property should be set to \"Copy if newer\" or \"Copy always\" so it's available when the application runs.\nConfigurationBuilder: Uses the ConfigurationBuilder class to build a configuration object from the appsettings.json file. The SetBasePath and AddJsonFile methods are essential for locating and loading the configuration file.\nDependency Injection (Potential): While this simple example doesn't use full-blown dependency injection, using Microsoft.Extensions.Configuration makes it much easier to integrate with dependency injection in a more complex .NET 8 application. You could inject an IConfiguration instance into your form or other classes.\n\nTo use this updated code:\n\nCreate a .NET 8 Windows Forms App Project: In Visual Studio, create a new \"Windows Forms App\" project and ensure you select \".NET 8\" as the target framework.\nInstall NuGet Packages: Install the following NuGet packages:\n    Npgsql\n    Dapper\n    Microsoft.Extensions.Configuration\n    Microsoft.Extensions.Configuration.Json\nDesign Your Form: As before, add a button, textbox, etc. to your form.\nCopy the C# Code: Copy the C# code above into your form's code-behind file.\nCreate and Configure appsettings.json: Create a file named appsettings.json in the root of your project. Add the connection string as shown in the previous response. Crucially: Set the Copy to Output Directory property of appsettings.json to \"Copy if newer\" or \"Copy always\".\nUpdate Connection String: Modify the connection string in appsettings.json with your Supabase credentials.\nAdjust MyDataType: Update the MyDataType class to match your table structure.\nBuild and Run: Build and run your application.\n\nThis version is now fully aligned with .NET 8 best practices and will avoid the issues associated with using .NET Framework components in a .NET Core application. The reliance on appsettings.json is the standard approach for configuration in modern .NET. Remember to thoroughly test and adapt the code to your specific Supabase setup and data structure.",
        "locale": "en",
        "slug": "technical-posts/supabase-with-csharp",
        "folder": ""
    },
    {
        "title": "Stop HTML background activity with CSS inert attribute",
        "description": "Stop HTML background activity with CSS inert attribute",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "You can do this by toggling the inert attribute, but that requires JavaScript. This little chunk of JavaScript is perfect for the job.\n\nbody:has(dialog[open]) {\n    overflow: hidden;\n}",
        "locale": "en",
        "slug": "technical-posts/stop-background-scroll-with-html-dialog-element",
        "folder": ""
    },
    {
        "title": "technical debt article ideas",
        "description": "technical debt article ideas",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "technical-debt"
        ],
        "content": "Okay, let's brainstorm some blog ideas about the perils of technical debt for enterprise applications. We'll aim for a mix of angles, targeting different audiences and levels of technical expertise.\n\nBroad & Engaging Titles (for a general audience):\n\nTechnical Debt: The Silent Killer of Enterprise Innovation: Focuses on the impact on speed of delivery and innovation.\nIs Your Enterprise Application Cracking Under the Weight of Technical Debt?: A dramatic, attention-grabbing title that uses a metaphor.\nThe Hidden Costs of Neglecting Technical Debt: A Guide for Enterprise Leaders: Appeals to decision-makers and focuses on the bottom line.\nTechnical Debt in Enterprise: Why \"Pay Later\" Can Bankrupt Your Digital Transformation: Links technical debt directly to strategic business goals.\nBeyond the Code: Understanding the Business Impact of Technical Debt in Enterprise Applications: Emphasizes the business repercussions, not just technical ones.\n\nMore Specific & Technical Titles (for a technically savvy audience):\n\nThe Technical Debt Spiral: How Poor Decisions Compound in Enterprise Systems: Explores the cascading effects of bad code.\nUntangling the Web: Strategies for Managing Technical Debt in Large-Scale Enterprise Applications: Focuses on practical solutions and tactics.\nWhen Legacy Bites Back: Identifying and Prioritizing Technical Debt in Mature Enterprise Systems: Targets companies with older, established software.\nRefactoring for the Future: Addressing Technical Debt as a Continuous Process in Enterprise Development: Promotes a proactive, ongoing approach.\nMicroservices vs. Monoliths: How Architectural Decisions Can Mitigate or Amplify Technical Debt in Enterprise: Explores the link between architecture and debt.\n\nContent Focus & Blog Angle Ideas:\n\nThe \"Why\" of Technical Debt:\n\n    The Accumulation of Debt: Explain how technical debt arises (e.g., rushed deadlines, lack of planning, skills gaps).\n    The Analogy of Financial Debt: Compare it to financial debt to make it easier to understand.\n    Different Types of Technical Debt: Differentiate between deliberate and accidental debt.\n\nThe Consequences of Ignoring Technical Debt:\n\n    Slowed Development Cycles: Explain how it makes future development more complex and time-consuming.\n    Increased Bugs and Downtime: Discuss the impact on application stability and user experience.\n    Higher Maintenance Costs: Show how fixing debt later is more expensive than addressing it early.\n    Stifled Innovation: Illustrate how it hinders the ability to adapt to changing business needs.\n    Security Risks: Explain how poorly maintained code can create vulnerabilities.\n    Talent Retention: Talk about how frustrating it can be for developers to work with technical debt.\n\nStrategies for Managing & Reducing Technical Debt:\n\n    Identifying and Measuring Technical Debt: Discuss tools and techniques for assessing the problem.\n    Prioritizing Debt Reduction: Explain how to decide which areas to focus on first.\n    Refactoring and Code Optimization: Explore technical solutions for cleaning up the code.\n    Automation and Testing: Highlight the importance of automated processes to prevent future issues.\n    Cultivating a Culture of Quality: Emphasize the need for a proactive, company-wide approach.\n\nSpecific Examples & Case Studies:\n    Real-World Examples: Provide concrete scenarios of how technical debt has hurt real companies.\n    Success Stories: Share examples of organizations that have successfully reduced their technical debt.\n    \"Before and After\" Comparisons: Illustrate the impact of addressing technical debt with measurable results.\n\nBlog Post Formats & Styles:\n\nListicles: \"5 Signs Your Enterprise Application is Drowning in Technical Debt\"\nHow-To Guides: \"How to Create a Technical Debt Reduction Plan\"\nCase Studies: \"How Company X Overcame its Technical Debt Woes\"\nInterviews with Experts: \"Insights on Technical Debt from a Seasoned Enterprise Architect\"\nOpinion Pieces: \"The Urgent Need to Prioritize Technical Debt Reduction in 2024\"\n\nKey Takeaways to Emphasize Throughout:\n\nTechnical Debt is Inevitable, but Manageable: Don't scare people, just educate them.\nProactive Management is Key: Don't wait until it's a crisis.\nIt's a Business Problem, Not Just a Technical One: Appeal to different audiences.\nAddressing Technical Debt Pays Dividends: Emphasize the long-term benefits.\n\nBy using a combination of these ideas, you can create a compelling series of blog posts that will help educate and inform your audience about the perils of technical debt in enterprise applications. Remember to tailor your content to the specific needs and interests of your target audience. Good luck!\nN",
        "locale": "en",
        "slug": "technical-posts/technical-debt-article-ideas",
        "folder": ""
    },
    {
        "title": "ways to create dynamic queries for mssql - SQL Server with TypeScript.",
        "description": "ways to create dynamic queries for mssql - SQL Server with TypeScript.",
        "date_created": "2025-05-30T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript",
            "sql-server",
            "mssql"
        ],
        "content": "You're right, while parameterized queries are essential for security (preventing SQL injection), simply replacing values with placeholders might not feel like a huge leap in \"flexibility\" if you're still manually crafting the entire SQL string.\n\nThe key to flexibility with parameterized queries is to dynamically build the SQL string itself based on conditions, and then use parameters for the *values* within that dynamically built string.\n\nLet's break down how you can make your select function more flexible:\n\n1. Parameterize the Current Query (Security First!)\n\nEven before adding flexibility, let's parameterize what you have. This is non-negotiable for security.\n\nimport * as mssql from 'mssql';\n\n// Assume 'ms' is your initialized mssql object with a pool\n// For example:\n// const ms = {\n//     pool: await new mssql.ConnectionPool(config).connect()\n// };\n\nasync function selectWithParams(year: number, month: number, days: number[]) {\n    if (!ms.pool.connected) {\n        console.error(\"Database not connected\");\n        return;\n    }\n    try {\n        const request = ms.pool.request();\n\n        // For the IN clause, we need to generate placeholders\n        const dayPlaceholders = days.map((_, index) => @day${index}).join(','); // @day0,@day1\n\n        const query = `\n            SELECT Classification, PostDate, Debit\n            FROM rpbudget\n            WHERE Year(PostDate) = @year\n              AND Month(PostDate) = @month\n              AND Day(PostDate) IN (${dayPlaceholders})\n        `;\n\n        request.input('year', mssql.Int, year);\n        request.input('month', mssql.Int, month);\n        days.forEach((day, index) => {\n            request.input(day${index}, mssql.Int, day);\n        });\n\n        console.log(\"Executing Query:\", query); // For debugging\n        console.log(\"Parameters:\", { year, month, days }); // For debugging\n\n        const result = await request.query(query);\n\n        if (result.recordset.length > 0) {\n            console.log(\"Results:\");\n            for (const row of result.recordset) {\n                console.log(\n                    Classification: ${row.Classification}, PostDate: ${new Date(row.PostDate).toLocaleDateString()}, Debit: ${row.Debit}\n                );\n            }\n        } else {\n            console.log(\"No records found for the given criteria.\");\n        }\n        return result.recordset;\n    } catch (err) {\n        console.error(\"SQL error\", err);\n        throw err; // Re-throw or handle appropriately\n    }\n}\n\n// Example Usage:\nasync function main() {\n    // Mock mssql pool for standalone example\n    global.ms = {\n        pool: {\n            connected: true,\n            request: () => {\n                const inputs: Record = {};\n                return {\n                    input: (name: string, type: any, value: any) => { inputs[name] = value; return this; },\n                    query: async (sql: string) => {\n                        console.log(\"Mock Query:\", sql);\n                        console.log(\"Mock Inputs:\", inputs);\n                        // Simulate finding some data if inputs match typical values\n                        if (inputs['year'] === 2025 && inputs['month'] === 5 && inputs['day0'] === 14) {\n                            return {\n                                recordset: [\n                                    { Classification: 'Revenue', PostDate: new Date(2025, 4, 14), Debit: 0, Credit: 100 },\n                                    { Classification: 'Expense', PostDate: new Date(2025, 4, 15), Debit: 50, Credit: 0 }\n                                ],\n                                rowsAffected: [2]\n                            };\n                        }\n                        return { recordset: [], rowsAffected: [0] };\n                    }\n                };\n            }\n        } as any // Type assertion for mock\n    };\n\n    await selectWithParams(2025, 5, [14, 15]);\n    await selectWithParams(2024, 1, [1, 2, 3]);\n}\n\nmain().catch(console.error);\n\n2. Building SQL Dynamically (More Flexibility)\n\nNow, let's make the query construction more dynamic. You might want to:\nSelect different columns.\nQuery different tables (be careful with this for security if table names come from user input).\nHave optional filter conditions.\nChange sorting.\n\nimport * as mssql from 'mssql';\n\n// Assume 'ms' is your initialized mssql object with a pool\n\ninterface QueryOptions {\n    tableName: string;\n    columns?: string[]; // e.g., ['Classification', 'PostDate', 'Debit']\n    filters?: {\n        year?: number;\n        month?: number;\n        days?: number[];\n        classification?: string;\n        // Add more potential filter fields\n    };\n    orderBy?: { column: string; direction: 'ASC' | 'DESC' };\n    limit?: number;\n}\n\nasync function flexibleSelect(options: QueryOptions) {\n    if (!ms.pool.connected) {\n        console.error(\"Database not connected\");\n        return;\n    }\n    try {\n        const request = ms.pool.request();\n        let query = SELECT ;\n\n        // Columns\n        if (options.columns && options.columns.length > 0) {\n            // IMPORTANT: Sanitize column names if they ever come from user input.\n            // For now, we assume they are developer-defined and safe.\n            query += options.columns.join(', ');\n        } else {\n            query += '*'; // Default to all columns\n        }\n\n        // Table\n        // IMPORTANT: Table names from user input are a HUGE security risk if not whitelisted.\n        // For this example, assume it's controlled by the developer.\n        query +=  FROM ${options.tableName};\n\n        // Filters (WHERE clause)\n        const conditions: string[] = [];\n        if (options.filters) {\n            if (options.filters.year !== undefined) {\n                conditions.push(Year(PostDate) = @year);\n                request.input('year', mssql.Int, options.filters.year);\n            }\n            if (options.filters.month !== undefined) {\n                conditions.push(Month(PostDate) = @month);\n                request.input('month', mssql.Int, options.filters.month);\n            }\n            if (options.filters.days && options.filters.days.length > 0) {\n                const dayPlaceholders = options.filters.days.map((_, index) => @day${index}).join(',');\n                conditions.push(Day(PostDate) IN (${dayPlaceholders}));\n                options.filters.days.forEach((day, index) => {\n                    request.input(day${index}, mssql.Int, day);\n                });\n            }\n            if (options.filters.classification) {\n                conditions.push(Classification = @classification);\n                request.input('classification', mssql.NVarChar, options.filters.classification);\n            }\n            // Add more filter conditions here\n        }\n\n        if (conditions.length > 0) {\n            query +=  WHERE ${conditions.join(' AND ')};\n        }\n\n        // Order By\n        if (options.orderBy) {\n            // Sanitize column name for ORDER BY if it can come from user input\n            query +=  ORDER BY ${options.orderBy.column} ${options.orderBy.direction};\n        }\n\n        // Limit (TOP clause in SQL Server)\n        if (options.limit !== undefined) {\n            // Note: SQL Server uses TOP, not LIMIT.\n            // We need to inject it after SELECT\n            if (query.toLowerCase().startsWith(\"select *\")) {\n                query = SELECT TOP (@limit) * + query.substring(\"select *\".length);\n            } else if (query.toLowerCase().startsWith(\"select \")) {\n                 query = SELECT TOP (@limit)  + query.substring(\"select \".length);\n            }\n            request.input('limit', mssql.Int, options.limit);\n        }\n\n\n        console.log(\"Executing Query:\", query);\n        const paramsForLog: any = {};\n        (request as any).parameters.forEach((v: any, k: string) => paramsForLog[k] = v.value); // Accessing internal parameters for logging\n        console.log(\"Parameters:\", paramsForLog);\n\n\n        const result = await request.query(query);\n\n        if (result.recordset.length > 0) {\n            console.log(\"Results:\");\n            result.recordset.forEach(row => console.log(row));\n        } else {\n            console.log(\"No records found for the given criteria.\");\n        }\n        return result.recordset;\n\n    } catch (err) {\n        console.error(\"SQL error\", err);\n        throw err;\n    }\n}\n\n// Example Usage for flexibleSelect:\nasync function mainFlexible() {\n    // Mock mssql pool for standalone example (same as above)\n    global.ms = {\n        pool: {\n            connected: true,\n            request: () => {\n                const inputs: Record = {};\n                let currentSql = \"\";\n                return {\n                    input: (name: string, type: any, value: any) => { inputs[name] = value; return this; },\n                    query: async (sql: string) => {\n                        currentSql = sql;\n                        console.log(\"Mock Query:\", currentSql);\n                        console.log(\"Mock Inputs:\", inputs);\n                        // Simulate some data based on query\n                        if (currentSql.includes(\"rpbudget\") && inputs['year'] === 2025) {\n                             return {\n                                recordset: [\n                                    { Classification: 'Revenue', PostDate: new Date(2025, 4, 14), Debit: 0, Credit: 100 },\n                                    { Classification: 'Expense', PostDate: new Date(2025, 4, 15), Debit: 50, Credit: 0 }\n                                ],\n                                rowsAffected: [2]\n                            };\n                        }\n                        if (currentSql.includes(\"Users\") && inputs['classification'] === 'Admin') {\n                            return {\n                                recordset: [ { UserID: 1, UserName: 'Alice', Classification: 'Admin' } ],\n                                rowsAffected: [1]\n                            }\n                        }\n                        return { recordset: [], rowsAffected: [0] };\n                    },\n                    get parameters() { // Mock for logging parameters\n                        const paramMap = new Map();\n                        Object.entries(inputs).forEach(([key, value]) => paramMap.set(key, { value }));\n                        return paramMap;\n                    }\n                };\n            }\n        } as any // Type assertion for mock\n    };\n\n    console.log(\"\\n--- Flexible Select Example 1 ---\");\n    await flexibleSelect({\n        tableName: 'rpbudget',\n        columns: ['Classification', 'PostDate', 'Debit'],\n        filters: {\n            year: 2025,\n            month: 5,\n            days: [14, 15]\n        },\n        orderBy: { column: 'PostDate', direction: 'DESC' },\n        limit: 10\n    });\n\n    console.log(\"\\n--- Flexible Select Example 2 (different table/filters) ---\");\n    await flexibleSelect({\n        tableName: 'Users', // Fictional table\n        columns: ['UserID', 'UserName', 'Classification'],\n        filters: {\n            classification: 'Admin'\n        }\n    });\n\n    console.log(\"\\n--- Flexible Select Example 3 (no filters, all columns) ---\");\n    await flexibleSelect({\n        tableName: 'rpbudget',\n        limit: 5\n    });\n}\n\nmainFlexible().catch(console.error);\n\nKey aspects of the \"flexibleSelect\" function:\n\nQueryOptions Interface: Defines the structure for passing various query modifications.\nDynamic Column Selection: Allows specifying which columns to retrieve.\nDynamic Table Selection: Allows specifying the table. CRITICAL SECURITY NOTE: If tableName can ever be influenced by external user input, you must implement a whitelist of allowed table names to prevent SQL injection. Never directly concatenate user input into table names without strict validation.\nConditional WHERE Clauses: Filters are only added if provided in the options.\nParameterization is Still Used: All values in the WHERE clause are parameterized using request.input().\nDynamic ORDER BY and TOP (LIMIT): Added based on options. Again, be cautious with orderBy.column if it comes from user input (whitelist valid column names).\nTOP Clause for SQL Server: SQL Server uses TOP N instead of LIMIT N. The logic for inserting TOP is a bit naive here and might need refinement for more complex SELECT statements, but it demonstrates the idea.\n\n3. Using a Query Builder (e.g., Knex.js)\n\nFor even more complex scenarios or if you want to abstract away SQL syntax further, a query builder like Knex.js is an excellent choice. Knex can generate SQL for various databases (including SQL Server) and handles parameterization automatically.\n\nimport knex from 'knex'; // npm install knex mssql\n\n// Initialize Knex (typically done once)\n// const db = knex({\n//   client: 'mssql',\n//   connection: {\n//     server: 'your_server.database.windows.net',\n//     user: 'your_username',\n//     password: 'your_password',\n//     database: 'your_database',\n//     options: {\n//       encrypt: true // For Azure SQL or if your server requires it\n//     }\n//   }\n// });\n\n// Mock Knex for example\nconst db = knex({ client: 'mssql', connection: {} }); // Won't actually connect\nconst originalQueryBuilderToSQL = db.queryBuilder().toSQL;\ndb.queryBuilder().toSQL = function() { // Intercept toSQL for logging\n    const result = originalQueryBuilderToSQL.call(this);\n    console.log(\"Knex SQL:\", result.sql);\n    console.log(\"Knex Bindings:\", result.bindings);\n    // Simulate a result for demonstration\n    if (result.sql.includes(\"rpbudget\") && result.bindings.includes(2025)) {\n        return {\n            sql: result.sql,\n            bindings: result.bindings,\n            // Simulate that Knex would execute this and return data\n            mockExecute: async () => [\n                { Classification: 'Revenue', PostDate: new Date(2025, 4, 14), Debit: 0, Credit: 100 },\n                { Classification: 'Expense', PostDate: new Date(2025, 4, 15), Debit: 50, Credit: 0 }\n            ]\n        };\n    }\n     if (result.sql.includes(\"Users\") && result.bindings.includes('Admin')) {\n        return {\n            sql: result.sql,\n            bindings: result.bindings,\n            mockExecute: async () => [ { UserID: 1, UserName: 'Alice', Classification: 'Admin' } ]\n        };\n    }\n    return { ...result, mockExecute: async () => [] };\n};\n\n\ninterface KnexQueryOptions {\n    tableName: string;\n    columns?: string[];\n    filters?: {\n        year?: number;\n        month?: number;\n        days?: number[];\n        classification?: string;\n    };\n    orderBy?: { column: string; direction: 'asc' | 'desc' }; // Knex uses lowercase\n    limit?: number;\n}\n\nasync function selectWithKnex(options: KnexQueryOptions) {\n    let query = db(options.tableName); // Start with the table\n\n    if (options.columns && options.columns.length > 0) {\n        query = query.select(options.columns);\n    } else {\n        query = query.select('*');\n    }\n\n    if (options.filters) {\n        if (options.filters.year !== undefined) {\n            query = query.whereRaw('Year(PostDate) = ?', [options.filters.year]);\n        }\n        if (options.filters.month !== undefined) {\n            query = query.whereRaw('Month(PostDate) = ?', [options.filters.month]);\n        }\n        if (options.filters.days && options.filters.days.length > 0) {\n            query = query.whereIn(db.raw('Day(PostDate)'), options.filters.days);\n        }\n        if (options.filters.classification) {\n            query = query.where('Classification', options.filters.classification);\n        }\n    }\n\n    if (options.orderBy) {\n        query = query.orderBy(options.orderBy.column, options.orderBy.direction);\n    }\n\n    if (options.limit !== undefined) {\n        query = query.limit(options.limit);\n    }\n\n    // To see the generated SQL and bindings:\n    const sqlQuery = query.toSQL().toNative ? query.toSQL().toNative() : query.toSQL();\n    // console.log(\"Knex SQL:\", sqlQuery.sql);\n    // console.log(\"Knex Bindings:\", sqlQuery.bindings);\n\n    try {\n        // const results = await query; // This executes the query\n        const simulatedQueryResult = (query as any).toSQL(); // Using our mock\n        const results = await simulatedQueryResult.mockExecute();\n\n        if (results.length > 0) {\n            console.log(\"Knex Results:\");\n            results.forEach(row => console.log(row));\n        } else {\n            console.log(\"No records found via Knex for the given criteria.\");\n        }\n        return results;\n    } catch (err) {\n        console.error(\"Knex SQL error\", err);\n        throw err;\n    }\n}\n\n// Example Usage for Knex:\nasync function mainKnex() {\n    console.log(\"\\n--- Knex Select Example 1 ---\");\n    await selectWithKnex({\n        tableName: 'rpbudget',\n        columns: ['Classification', 'PostDate', 'Debit'],\n        filters: {\n            year: 2025,\n            month: 5,\n            days: [14, 15]\n        },\n        orderBy: { column: 'PostDate', direction: 'desc' },\n        limit: 10\n    });\n\n    console.log(\"\\n--- Knex Select Example 2 (different table/filters) ---\");\n    await selectWithKnex({\n        tableName: 'Users', // Fictional table\n        columns: ['UserID', 'UserName', 'Classification'],\n        filters: {\n            classification: 'Admin'\n        }\n    });\n}\n\nmainKnex().catch(console.error);\n\nChoosing the Right Approach:\n\nParameterized Queries (like selectWithParams): Absolutely essential for security. Use this as your baseline.\nDynamic SQL Building (like flexibleSelect): Good for when you need to construct different queries based on application logic, but want to keep dependencies minimal and have fine-grained control over the SQL. Parameterize all user-provided values. Be extremely careful with dynamic table/column names.\nQuery Builders (like Knex.js): Excellent for complex queries, improving code readability, reducing raw SQL, and easier database migrations if you ever switch DBs. They handle parameterization under the hood. This is often the preferred method for larger applications.\n\nRemember, always prioritize security. Parameterize inputs to prevent SQL injection, and if you're dynamically building parts of the query structure (like table or column names) from potentially unsafe sources, use strict whitelisting.",
        "locale": "en",
        "slug": "technical-posts/mssql-dynamic-queries",
        "folder": ""
    },
    {
        "title": "sveltekit-async-form-handling",
        "description": "sveltekit-async-form-handling",
        "date_created": "2025-05-25T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "forms"
        ],
        "content": "Beyond making a traditional round-trip form more snappy, use:enhance can be exploited to do interesting Ajax-like work. This document explains how the use:enhance callback function works. \n\n\n  import { enhance, applyAction } from '$app/forms';\n\n\tfunction doWork() {\n\t\t...\n\t}\n\n\n...\n\n {\n\t\t// This function is explained in detail below. \n\t\treturn async ({ result, formData }) => {\n\t\t\tif (result.type === 'success' && result.data?.updatedItem) {\n\t\t\t\tdoWork(result.data.updatedItem);\n\t\t\t}\n\t\t\tawait applyAction(result); // SvelteKit's default behavior\n\t\t};\n\t}}\n\t\n\tUpdate\n\n\n// This is the callback function passed to use:enhance\n// It's executed when the form is about to be submitted.\n// It can return an \"update\" function that SvelteKit will call\n// AFTER the form submission (fetch request to the server action) is complete.\n() => {\n    // This \"update\" function is returned.\n    // It will be called by SvelteKit after the server action responds.\n    return async ({ result, formData }) => {\n        // 'result': This object contains information about the outcome of the server action.\n        //   - result.type: 'success', 'failure', 'redirect', 'error'\n        //   - result.status: The HTTP status code from the server action.\n        //   - result.data: Any data returned by the server action (e.g., the updated item, an error message).\n        //                  This is what you return from your action in +page.server.js.\n        //                  For example: return { success: true, updatedItem: { ... } }\n\n        // 'formData': The FormData object that was submitted with the form.\n        //             Useful if you need to access the submitted values on the client-side\n        //             for the optimistic update, though often the result.data is preferred.\n\n        // 1. Optimistic Update Logic:\n        if (result.type === 'success' && result.data?.updatedItem) {\n            // Check if the server action reported success.\n            // AND check if the 'data' object returned by the action contains\n            // a property named 'updatedItem'. The ?. is optional chaining.\n\n            // If both conditions are true, it means the server successfully updated the item\n            // and sent back the updated version of the item.\n\n            optimisticUpdate(result.data.updatedItem);\n            // This calls a local Svelte component function named optimisticUpdate.\n            // You would define this function elsewhere in your  tag.\n            // Its job is to update your local Svelte state (the items array in the example)\n            // immediately with the updatedItem data from the server.\n            // This makes the UI change before SvelteKit might re-run a load function\n            // to fetch all items again. It makes the app feel faster.\n        }\n\n        // 2. Apply SvelteKit's Default Behavior for the Action Result:\n        await applyAction(result);\n        // 'applyAction' is a function provided by SvelteKit (specifically by enhance).\n        // This is crucial! It tells SvelteKit to:\n        //    - Update $page.form with result.data if the action was a 'success' or 'failure'.\n        //      This is how form-specific error messages or success messages are typically displayed.\n        //    - Update $page.status with result.status.\n        //    - Handle redirects if result.type is 'redirect'.\n        //    - Invalidate data from load functions if the action indicates it should\n        //      (or by default, actions invalidate all data unless invalidateAll: false is specified).\n        //      This would cause relevant load functions to re-run and fetch fresh data.\n\n        // If you OMIT await applyAction(result), then:\n        //    - $page.form won't be updated with the action's result.\n        //    - load functions might not re-run even if data changed on the server.\n        //    - Redirects from the action won't happen.\n    };\n}\n\nIn simpler terms:\n\nBefore Submission (Outer Function):\n    When the user clicks \"Update\" on a row, use:enhance intercepts the form submission.\n    The outer () => { ... } part of the code runs. Its only job here is to return the inner async ({ result, formData }) => { ... } function.\n\nAfter Server Responds (Inner async Function):\n    SvelteKit sends the form data to your ?/updateItem server action using fetch.\n    Your server action processes the data, updates the database (or in-memory store), and returns a response (e.g., { success: true, updatedItem: ... }).\n    Once SvelteKit receives this response, it calls the inner async function you provided, passing in the result (containing the server's response) and the original formData.\n\nInside the Inner async Function:\n    Optimistic Part:\n        It checks if the server said \"Success!\" and gave back the updatedItem.\n        If yes, it immediately calls your optimisticUpdate function. This function (which you write) directly modifies your Svelte component's local items array. So, the user sees the row update on the screen instantly.\n    Standard SvelteKit Part:\n        await applyAction(result): This then tells SvelteKit to do its normal post-action work: update special stores like $page.form, handle potential redirects, and trigger data re-fetching if necessary. Even though you did an optimistic update, applyAction ensures everything is consistent with the server's final state and handles things like server-side validation errors being displayed.\n\nWhy is this optimisticUpdate beneficial?\n\nPerceived Speed: The UI updates immediately, making the application feel very responsive, even if the server takes a moment to process the request.\nBetter User Experience: Users get instant feedback that their action has likely succeeded.\n\nThe optimisticUpdate function itself (defined in your Svelte component's `` tag) would look something like this:\n\n\n    // ... other imports and reactive variables like 'items' ...\n    export let data;\n    let items = data.items; // Assuming items is passed from load\n\n    function optimisticUpdate(updatedItemFromServer) {\n        items = items.map(item =>\n            item.id === updatedItemFromServer.id ? { ...item, ...updatedItemFromServer } : item\n        );\n    }\n\n    // ... rest of your component logic ...\n\n\nThis function finds the item in the local items array by its ID and replaces it with (or merges it with) the updatedItemFromServer data.",
        "locale": "en",
        "slug": "technical-posts/sveltekit-async-form-handling",
        "folder": ""
    },
    {
        "title": "setting up a new PC",
        "description": "setting up a new PC",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "pc"
        ],
        "content": "Chrome\nCyberDuck\nEverything\nFileSeek?\nGit\nNode\nMountainDuck\nPython\nS3 Browser\nSQL Server Dev Edition\nVisual Studio Code\nVisual Studio\nVPN\n\n![[pc-installation-20240822102031924.webp|500]]\n\nS3 Browser config for our Digital Ocean Spaces account for download readmes.\n\n![[envoy-setup-20240822121412639.webp|500]]\n\naccess key\n    DO00P28T8473PADXAZRW\nsecret key\n    Q1uKVZom3H3qQ7Q1UZbMEXkUZLDSBxibOHG8tYdOOKo",
        "locale": "en",
        "slug": "technical-posts/new-pc-setup",
        "folder": ""
    },
    {
        "title": "svelte-sveltekit things to know",
        "description": "svelte-sveltekit things to know",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "State\n    locals, stores, context, url, local storage, cookies\nCSS\n    PostCSS\nComponents\n    Partition complexity\n    Reuse\n    Extend markdown\nPrerender\n    Find all routes\nHooks.ts\nTypeScript\n    Bun\n    value of typing",
        "locale": "en",
        "slug": "technical-posts/svelte-sveltekit-things-to-know",
        "folder": ""
    },
    {
        "title": "Understanding Rust's unwrap function",
        "description": "Understanding Rust's unwrap function",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "Question\n\nWhat is unwrap doing in this line of Rust:\n\nlet re = Regex::new(r\"^\\s*#include\\s+\").unwrap();\n\nAnswer\n\nIn Rust, the unwrap() method is used to extract the value from a Result or Option type, assuming that it contains a value. In your example:\n\nlet re = Regex::new(r\"^\\s*#include\\s+\").unwrap();\n\nHere's what's happening:\n\nRegex::new(r\"^\\s*#include\\s+\") attempts to create a new regex pattern. This function returns a Result, which means it could either succeed (returning a Regex) or fail (returning an error).\n\nBy calling unwrap(), you're saying, \"I expect this operation to succeed, and if it doesn't, panic (crash) the program and print the error message.\"\n\nUsing unwrap() is convenient for quick prototyping or in situations where you're sure the operation will succeed, but in production code, it's generally better to handle errors more gracefully, such as using match or if let to manage potential failures.",
        "locale": "en",
        "slug": "technical-posts/rust-unwrap-method",
        "folder": ""
    },
    {
        "title": "Type predicate function",
        "description": "Type predicate function",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "typescript"
        ],
        "content": "type NonNegativeNumber = number;\n\nfunction isValidNonNegativeNumber(num: number): num is NonNegativeNumber {\n    return typeof num === \"number\" && num >= 0;\n}\n\n// Example usage:\n\nlet age: NonNegativeNumber = 25; // Valid\nlet count: NonNegativeNumber = 0; // Valid\n// let height: NonNegativeNumber = -5; // Invalid, TypeScript won't catch this directly\n\nfunction processAge(age: number): void {\n    // or NonNegativeNumber to restrict input\n    if (isValidNonNegativeNumber(age)) {\n        console.log(Age is: ${age});\n    } else {\n        console.log(\"Invalid age.  Age must be a non-negative number.\");\n    }\n}\n\nprocessAge(30); // Output: Age is: 30\nprocessAge(0); // Output: Age is: 0\nprocessAge(-5); // Output: Invalid age.  Age must be a non-negative number.\n\nExplanation:\n\ntype NonNegativeNumber = number;: This creates a type alias. It doesn't create a completely new type, but it gives the existing number type a more descriptive name ( NonNegativeNumber). This improves readability. However, TypeScript's type system at runtime is based on the underlying types of the variables. At runtime, there are only numbers in this case.\n\nisValidNonNegativeNumber(num: number): num is NonNegativeNumber: This is a type predicate function. This is the crucial part. It allows us to perform runtime validation and inform the TypeScript compiler that a particular variable is actually of the NonNegativeNumber type after the validation succeeds.\n\n    num: number: The function accepts any number as input.\n    num is NonNegativeNumber: This is the type predicate syntax. It tells TypeScript that if the function returns true, then the TypeScript compiler should treat the input num as being of type NonNegativeNumber within the scope where the function returned true.\n    Inside the function, typeof num === 'number' && num >= 0 performs the runtime validation.\n\nImportant Considerations:\n\nRuntime vs. Compile Time: TypeScript's type system is primarily for compile-time checking. At runtime, TypeScript code is usually converted to JavaScript, and JavaScript doesn't have static types in the same way. Therefore, using only type NonNegativeNumber = number; will not prevent you from assigning negative numbers at runtime. The type predicate ensures this.\n\nRuntime Validation is Essential: You must perform runtime validation (like the isValidNonNegativeNumber function) if you need to enforce the \"non-negative\" constraint at runtime. The type system helps catch errors early in development, but it can't guarantee that a variable will always be non-negative at runtime (especially if data comes from external sources, user input, or other untyped code).\n\nAlternative (Limited) Approaches: You could consider using libraries like io-ts or zod for more sophisticated runtime type checking and validation. These libraries provide ways to define types with validation rules that are enforced at runtime. However, they usually involve more overhead than a simple type predicate function.",
        "locale": "en",
        "slug": "technical-posts/type-predicate-function",
        "folder": ""
    },
    {
        "title": "Svelte forms with remote functions are awesome!",
        "description": "This article explains how to use Svelte 5 forms with remote functions.",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-30T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte",
            "forms"
        ],
        "content": "\nCreating a form with Svelte 5 for use with a remote function\n\nCreate a form schema  \n\nYou have an interesting option we'll discuss later if you locate the form schema in a separate module (as opposed to defining it in the form's remote function). I put mine under the src/lib/types folder but you can put your schemas anywhere. \n\nsrc/lib/types/form1.schema.ts\n\nForm schema are defined with any standard schema compliant validation library (ie, Zod, Valibot, or Arktype). I'm using Zod in this example. Zod has a rich set of schema definition options. \n\nimport { z } from 'zod';\n\nexport const form1Schema = z.object({\n\ttitle: z.string().nonempty({ message: \"Hey, foo! Can't be empty\" }),\n\tcontent: z.string().nonempty()\n});\n\nUsing a schema is part of the magic of Svelte 5 forms. It provides:\nStrongly-typed, easy access to form fields\nThe ability to populate a form with the correct tag and attributes for every field.\nVery effective form validation for the server- and client-side\nFreedom from third-party forms libraries like SuperForms. \nCreate a remote function\n\nTo reduce learning friction, this example Svelte 5 form remote function doesn't do very much, but it does enough to understand how things work. There aren't any rules as to where a  remote function must reside, but it anywhere you want. While it may be prudent to co-locate with them their corresponding +page.svelte, I like the idea of knowing where all my remote functions are located and put them in src\\lib\\remote-funcs.  \n\nsrc\\lib\\remote-funcs\\form.remote.ts\n\nimport { z } from 'zod';\nimport { form } from '$app/server';\nimport { error, redirect, invalid } from '@sveltejs/kit';\nimport { form1Schema } from '$lib/types/form1schema';\n\n// You cannot export a schema from a remote function. Put them in a shared module.\n\nexport const createPost = form(form1Schema, async (data, issue) => {\n\t// Descructure the form fields from the data object.\n\tconst { title, content } = data;\n\n\t// See a field value in the console.\n\tconsole.log(title);\n\n\tif (title == 'ROGER') {\n\t\t// use the issue object to throw custom error messages.\n\t\tinvalid(issue.title('Title cannot be ROGER'), issue.content('Title cannot be PENCE'));\n\t}\n\n\tredirect(303, /results);\n});\n\nCreate a form \n\nInitially, the only unusual thing with the form declaration is its ` tag. The {...createPost} populates the type and action` attributes for you. In this case, the form fields were created manually. We'll see in a moment how the form schema helps with this.\n\n\n\timport { createPost } from '$lib/remote-funcs/form.remote';\n\n\n\n\t\n\t\tTitle\n\t\t\t\n\t\n\n\t\n\t\tWrite your post\n\t\t\t\t\n\t\n\n\tSubmit\n\n\nFor this example, Svelte5 rendered this  `` tag\n\n\n\nas \n\n\n\nIf you dig into the code Svelte 5 generates, you'll find that a remote function gets an alias assigned to it. For example, the remote function file provided above is assigned the lig978e alias. You won't ever know, or care, what this alias value is. \n\ninit_remote_functions(m, \"src/lib/remote-funcs/form.remote.ts\", \"1ig978e\");\n\nWith this alias, and the function name (createPost, in this case) your form has the info it needs to issue the correct HTTP POST. \n\nAn alternative manually creating the form fields yourself, is to use the schema and the spread operator to do the work for you:\n\n\n\timport { createPost } from '$lib/remote-funcs/form.remote';\n\n\n\n\t\n\t\tTitle\n\t\t\n\t\n\n\t\n\t\tWrite your post\n\t\t\n\t\n\n\tSubmit\n\n\nUsing the schema fields like this is mostly an easy way to populate the name attribute. This code:\n\n\n\nRenders as:\n\n\n\nI think the point of using the more verbose spread is to ensure the form field names correspond to the schema field names. If you use the schema spread, you can also add other attributes to the tag (eg, id or class). \n\npreflight\n\nerror messages",
        "locale": "en",
        "slug": "technical-posts/svelte-forms-with-remote-functions",
        "folder": ""
    },
    {
        "title": "Svelte binding with the getter/setter pattern",
        "description": "Svelte binding with the getter/setter pattern",
        "date_created": "2025-07-16T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "svelte"
        ],
        "content": "rogerpence: what is this line doing in this svelte 5 exmaple:\n\nbind:value={() => left, updateLeft}\n\n\nGitHub Copilot: This line is using Svelte 5's derived binding syntax to create a two-way binding with a derived value.\n\nHere's what bind:value={() => left, updateLeft} does:\n\n() => left - This is a getter function that returns the current value of the left derived state\nupdateLeft - This is a setter function that gets called when the input value changes\n\nSince left is a $derived value (calculated as total - spent), it's read-only by default. You can't directly bind to it with a simple bind:value={left} because derived values are computed from other state.\n\nThe derived binding syntax allows you to:\nRead the derived value through the getter function\nWrite to it indirectly through the setter function, which updates the underlying state (spent) that the derived value depends on\n\nWhen the user moves the second range slider:\nThe input calls updateLeft(newValue)\nupdateLeft calculates and updates spent = total - left\nThis causes left to be recalculated automatically\nBoth sliders stay in sync\n\nThis pattern is useful when you want to provide multiple ways to interact with related data (in this case, adjusting either \"spent\" or \"left\" amounts).\n\nrogerpence: how does 'left' and 'updateLeft' interact on that line. Is that two separate function calls\n\nGitHub Copilot: No, it's not two separate function calls. The syntax () => left, updateLeft is a single expression using the comma operator.\n\nHere's how it works:\n\n() => left, updateLeft evaluates to just updateLeft (the comma operator returns the last value)\nBut Svelte's compiler recognizes this specific pattern and treats it as a getter/setter pair:\n   Getter: () => left - function that returns the current value\n   Setter: updateLeft - function that handles updates\n\nSo the bind directive receives:\nA getter function to read the current value of left\nA setter function to update the underlying state when the input changes\n\nWhen the range slider moves:\nSvelte calls updateLeft(newValue) with the new slider value\nupdateLeft sets spent = total - left \nThis causes left to be recalculated (since it's $derived(total - spent))\nSvelte calls the getter () => left to get the updated value for display\n\nThis pattern allows you to bind to derived/computed values that can't be directly mutated, by providing both a way to read the current value and a way to update the underlying state that the derived value depends on.\n\nrogerpence: does that then pass the current value of 'left' to the 'updateLeft' function\n\nGitHub Copilot: No, the getter function () => left doesn't pass the current value to updateLeft.\n\nHere's what actually happens:\n\nThe getter () => left is used by Svelte to read the current value for display in the input\nThe setter updateLeft receives the new value from the input when the user moves the slider\n\nSo the flow is:\nUser moves the slider to a new position (e.g., 30)\nSvelte calls updateLeft(30) with the new slider value\nupdateLeft calculates spent = total - left  spent = 100 - 30  spent = 70\nThe left derived value recalculates: left = total - spent  left = 100 - 70  left = 30\nSvelte calls the getter () => left to update the input's displayed value\n\nThe current value of left is not passed to updateLeft. Instead, updateLeft receives whatever new value the user set on the range slider, then updates the underlying spent state accordingly.\n\nThis creates a bidirectional relationship where both sliders stay in sync - moving either the \"spent\" slider or the \"left\" slider updates the other automatically.\n\nrogerpence: I don't understand how 'updateLeft' is called with the correct argument\n\nGitHub Copilot: Great question! The key is understanding how Svelte's binding system works.\n\nWhen you write bind:value={() => left, updateLeft}, Svelte's compiler recognizes this as a getter/setter pair and automatically handles calling updateLeft with the correct argument.\n\nHere's what Svelte does behind the scenes:\n\nWhen the input value changes (user moves the slider), Svelte automatically calls:\n   updateLeft(newInputValue)\n\nThe newInputValue comes directly from the range input's current position - Svelte passes whatever value the user set on the slider\n\nSo if the user drags the \"left\" slider to position 30:\nThe range input's value becomes 30\nSvelte automatically calls updateLeft(30)\nYour function executes: spent = total - left  spent = 100 - 30  spent = 70\nThe derived left recalculates and stays at 30\n\nYou don't need to manually pass arguments - Svelte's binding system handles this automatically. The setter function (updateLeft) will always receive the new value from the input element as its first parameter.\n\nThis is part of Svelte's \"magic\" - it automatically wires up the data flow between the DOM input and your state management functions.\n\nrogerpence: so it's the bind: value that gets the currently value to the 'updateLeft' function\n\nGitHub Copilot: Yes, exactly! \n\nThe bind:value directive is what handles getting the current input value to the updateLeft function.\n\nWhen you write bind:value={() => left, updateLeft}:\n\nbind:value creates a two-way binding with the input element\nWhen the user moves the slider, bind:value automatically captures the new input value\nbind:value then calls updateLeft(newValue) with that captured value\n\nSo the flow is:\nUser drags slider to position 30\nbind:value detects the input change and gets the value 30\nbind:value calls updateLeft(30)\nYour function executes: spent = total - 30 = 70\n\nThe bind:value directive is doing all the heavy lifting - it's monitoring the input for changes and automatically calling your setter function with the new value. You don't have to manually wire up event listeners or extract values from DOM events.\n\nThis is the core of Svelte's reactivity system - the binding directives handle the connection between DOM elements and your JavaScript functions/state.\n\nrogerpence: I know I seem a little slow... but after that call to 'updateLeft' the new derived value of 'left' gets assigned to the bind:value, right?\n\nGitHub Copilot: You're not slow at all - this is a subtle but important distinction!\n\nAfter updateLeft is called, the new derived value of left doesn't get \"assigned\" to bind:value. Instead:\n\nUser moves slider  bind:value calls updateLeft(newValue)\nupdateLeft updates spent = total - newValue\nleft automatically recalculates (because it's $derived(total - spent))\nbind:value calls the getter function () => left to read the updated value\nThe input's display value updates to match the new left value\n\nSo it's more accurate to say that bind:value reads the new derived value using the getter function, rather than having it \"assigned\" to it.\n\nThe flow is:\nWrite: bind:value  updateLeft()  updates spent  left recalculates\nRead: bind:value  (() => left)()  gets current left value  updates input display\n\nThe bind:value is constantly synchronizing between the input and your state using both the getter (to read) and setter (to write), rather than having values assigned to it.\n\nThis two-way synchronization is what keeps both sliders perfectly in sync!\n\nrogerpence: I said it this way: \" new derived value of 'left' gets assigned to the bind:value,\"\nisn't that what steps 4 and 5 are doing\n\nGitHub Copilot: You're absolutely right! I was being overly pedantic about the word \"assigned.\"\n\nYes, steps 4 and 5 are essentially doing what you described - the new derived value of left gets assigned to the bind:value.\n\nWhen I said:\nbind:value calls the getter function () => left to read the updated value\nThe input's display value updates to match the new left value\n\nThat is the new derived value of left getting assigned to the bind:value! \n\nThe bind:value directive takes that returned value from the getter and assigns it to the input element's value property, which updates what the user sees on the slider.\n\nSo your understanding is correct - after updateLeft runs and left recalculates, that new derived value does get assigned back to the bind:value to keep the input synchronized with the state.\n\nThanks for pushing back on my explanation - you were right to call that out!\n\nrogerpence: witht his comma-separated syntax, is the getter always on the left and setter on the right?\n\nGitHub Copilot: Yes, with this comma-separated syntax in Svelte 5, the getter is always on the left and the setter is always on the right.\n\nThe pattern is:\nbind:value={getter, setter}\n\nSo in your example:\n() => left (left side) = getter function\nupdateLeft (right side) = setter function\n\nThis is a consistent convention in Svelte 5's derived binding syntax. The order matters because Svelte's compiler expects:\nFirst expression = how to read the current value\nSecond expression = how to handle updates\n\nIf you swapped them around like bind:value={updateLeft, () => left}, it wouldn't work correctly because Svelte would try to use updateLeft as the getter and () => left as the setter.\n\nSo yes, it's always getter, setter - left to right.\n\nrogerpence: very interesting. Svelte 5 has a lot more subtle power than Svelte 4\n\nGitHub Copilot: Absolutely! Svelte 5 is a major leap forward in terms of both power and elegance.\n\nThis getter/setter binding syntax is a perfect example - it solves a real problem that was clunky in Svelte 4. Before, if you wanted to bind to a computed/derived value, you'd have to:\n\nCreate a separate writable store\nUse reactive statements with $: \nManually sync between multiple pieces of state\nOften end up with complex, hard-to-follow reactive chains\n\nNow with Svelte 5's runes and this binding syntax, you can:\nUse $derived for clean, declarative computed state\nBind directly to derived values with simple getter/setter pairs\nKeep all the logic in one place and easy to reason about\n\nThe runes system ($state, $derived, $effect) really unlocks a lot of subtle but powerful patterns like this. It feels much more like modern reactive programming while still maintaining Svelte's simplicity.\n\nPlus the fact that this binding pattern automatically handles the bidirectional sync between both sliders with just a few lines of code - that's the kind of \"it just works\" magic that makes Svelte special, but now with even more control and clarity about what's happening under the hood.",
        "locale": "en",
        "slug": "technical-posts/svelte-binding-with-getter-and-setter",
        "folder": ""
    },
    {
        "title": "Weirdo routing error",
        "description": "Weirdo routing error",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "rogerpence: this sveltekit app has some prerendered routes. Pages from a prerendered route load fine the first time, but when refreshed they throw a 404. This app works fine in dev mode. I think this has something to do with the refresh failing because the .HTML extension is missing\n\nGitHub Copilot: Let's analyze and fix the prerendering 404 issue in SvelteKit:\n\nPlan:\n\nConfigure Vercel to handle HTML extensions properly\nEnsure proper route fallbacks\nAdd correct prerendering config\n\n// svelte.config.js\nconst config = {\n    kit: {\n        adapter: adapter({\n            // Force Vercel to handle all routes, with or without .html\n            edge: false,\n            split: false,\n        }),\n        prerender: {\n            handleMissingId: \"ignore\", // Handle missing prerender targets gracefully\n            entries: [\"*\"], // Prerender all routes\n            crawl: true, // Automatically detect pages to prerender\n        },\n    },\n};\n\n// vercel.json\n{\n    \"rewrites\": [\n        { \"source\": \"/(.*)\", \"destination\": \"/\" },\n        { \"source\": \"/:path\", \"destination\": \"/:path.html\" }\n    ]\n}\n\n// +layout.js\nexport const prerender = true;\nexport const csr = true;\nexport const trailingSlash = \"always\"; // Forces consistent URL format\n\nThis configuration:\n\nHandles both /path and /path.html URLs\nEnsures proper fallback for client-side routing\nMaintains consistent URL format\nPreserves prerendering while fixing refresh 404s",
        "locale": "en",
        "slug": "technical-posts/weirdo-routing-error",
        "folder": ""
    },
    {
        "title": "VS Code Prettier/ESLint configuration",
        "description": "VS Code Prettier/ESLint configuration",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "vs-code"
        ],
        "content": "To configure Prettier to use 4 spaces instead of tabs, update your .prettierrc file:\n\n.prettierrc\n\n{\n    \"useTabs\": false,\n    \"tabWidth\": 4,\n    \"singleQuote\": true,\n    \"trailingComma\": \"none\",\n    \"printWidth\": 120,\n    \"plugins\": [\"prettier-plugin-svelte\"],\n    \"overrides\": [\n        {\n            \"files\": \"*.svelte\",\n            \"options\": {\n                \"parser\": \"svelte\"\n            }\n        }\n    ]\n}\n\neslint.config.js\n\nimport prettier from \"eslint-config-prettier\";\nimport js from \"@eslint/js\";\nimport { includeIgnoreFile } from \"@eslint/compat\";\nimport svelte from \"eslint-plugin-svelte\";\nimport globals from \"globals\";\nimport { fileURLToPath } from \"node:url\";\nimport ts from \"typescript-eslint\";\nimport svelteConfig from \"./svelte.config.js\";\n\nconst gitignorePath = fileURLToPath(new URL(\"./.gitignore\", import.meta.url));\n\nexport default ts.config(\n    includeIgnoreFile(gitignorePath),\n    js.configs.recommended,\n    ...ts.configs.recommended,\n    ...svelte.configs.recommended,\n    prettier,\n    ...svelte.configs.prettier,\n    {\n        languageOptions: {\n            globals: { ...globals.browser, ...globals.node },\n        },\n        rules: {\n            \"no-undef\": \"off\",\n            indent: [\"error\", 4], // Add this line to enforce 4 spaces\n        },\n    },\n    {\n        files: [\n            \"*/.svelte\",\n            \"*/.svelte.ts\",\n            \"*/.svelte.js\",\n            \"*/.js\",\n            \"*/.ts\",\n        ],\n        languageOptions: {\n            parserOptions: {\n                projectService: true,\n                extraFileExtensions: [\".svelte\"],\n                parser: ts.parser,\n                svelteConfig,\n            },\n        },\n    }\n);\n\nConfigure VS Code to automatically run Prettier on files when they're saved. Here's how to set it up:\n\n1. Install the Prettier Extension\n\nIf you haven't already, install the official Prettier extension:\n\nOpen Extensions view (Ctrl+Shift+X)\nSearch for \"Prettier - Code formatter\"\nClick Install\n\n2. Configure VS Code Settings\n\nAdd these settings to your VS Code settings.json file:\n\n// For User Settings: File > Preferences > Settings > (click icon to edit in JSON)\n{\n    \"editor.formatOnSave\": true,\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n    \"[typescript]\": {\n        \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n    },\n    \"[javascript]\": {\n        \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n    },\n    \"[svelte]\": {\n        \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n    }\n}\n\n3. Create a Workspace Settings File (Optional)\n\nFor project-specific settings, create or edit .vscode/settings.json:\n\n{\n    \"editor.formatOnSave\": true,\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n    \"prettier.requireConfig\": true\n}\n\nThis way, Prettier will automatically format your code according to your .prettierrc settings every time you save a file.",
        "locale": "en",
        "slug": "technical-posts/vs-code-prettier-eslint-configuration",
        "folder": ""
    },
    {
        "title": "mysql-nuggets",
        "description": "mysql-nuggets",
        "date_created": "2025-07-07T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "mysql"
        ],
        "content": "select email_address, product, DATE_FORMAT(date_requested, '%Y-%m-%d') AS iso_date from downloads\nwhere email_address like '%blattenb%'\norder by iso_date, product",
        "locale": "en",
        "slug": "technical-posts/mysql-nuggets",
        "folder": ""
    },
    {
        "title": "Supabase credentials",
        "description": "Supabase credentials",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-29T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "postgres"
        ],
        "content": "\nSupabase credentials\n\n\nDATABASE_URL=postgresql://postgres.nrtgiufboxlhbspclizl:RJpmDsNKteZ7xSyn@aws-0-us-west-1.pooler.supabase.com:6543/postgres\n\nPG_HOST=aws-0-us-west-1.pooler.supabase.com\nPG_PORT=6543\nPG_USER=postgres.nrtgiufboxlhbspclizl\nPG_PASSWORD=RJpmDsNKteZ7xSyn\nPG_DATABASE=postgres\n\nenv Files\nSUPABASE_HOST: aws-0-us-west-1.pooler.supabase.com",
        "locale": "en",
        "slug": "technical-posts/supabase-credentials",
        "folder": ""
    },
    {
        "title": "MSO is sneaky",
        "description": "MSO is sneaky",
        "date_created": "2025-07-22T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "mso"
        ],
        "content": "MSO is very sneaky. It hijacks your HTML and does lots of nefarious and mysterious things, including:\nCSS gets \"helpfully\" rewritten\nFont sizes have mysterious minimums  \nTable layouts work differently than literally everywhere else\nMSO conditionals have magical powers\n\n\n\n\n\n\nrogerpence: The RowSpacer component renders a row in an HTML for emails intended for Windows Outlook.\n\nI am calling it like this:\n\n\nto get a 14\" high spacer in the email. \n\nThe code present works just fine, emit one table for !mso and one for mso content.  I render the two TDs with this code:\n\n\n\t{@html td}\n\t{@html msoTd}\n\n\ni noticed that in both cases the height, heightMso, padding, and paddingMso are exactly the same (14 and \"0 0 14px 0\", respectively)\n\nGiven that the markup is the same, I thougth I could remove this line\n{@html msoTd}            \n\nbut if do, Outlook does respect the spacing specified. If I include that line, it does. How can that be! The markup is the same in both cases?\n\nGitHub Copilot: This is a classic Outlook rendering quirk! Even though the markup appears identical, there are subtle differences in how Outlook processes MSO conditional comments versus regular HTML. Here's what's happening:\n\nWhy both TDs are needed despite identical markup:\n\n1. MSO Conditional Comment Processing\nOutlook's rendering engine processes MSO conditionals (``) at a different stage than regular HTML parsing. When Outlook sees:\n\n\ncontent\n\n\ncontent\n\n\nIt actually:\nIgnores the [if !mso] block completely (doesn't even parse it)\nProcesses the [if mso] block as native HTML\n\n2. CSS Property Inheritance Differences\nEven with identical styles, Outlook treats MSO-conditional content differently:\n\nNon-MSO TD: Outlook tries to \"fix\" the CSS and may ignore certain properties\nMSO TD: Outlook trusts the CSS more because it's in an MSO block\n\n3. Font Size Minimum Enforcement\nYour font-size: 1px is being handled differently:\nNon-MSO: Outlook might enforce minimum font size (8-12px)\nMSO: Outlook respects the 1px because it's in an MSO conditional\n\nHere's what's really different in processing:\n\nconst td = `\n    \n    &#8203;\n    `;\n    //  Outlook ignores this completely\n\nconst msoTd = `       \n    \n    &#8203;\n    `;\n    //  Outlook processes this as trusted MSO content\n\nSolutions:\n\nOption 1: Simplify with proper MSO handling\n\n    \n    &nbsp;\n    \n    \n    \n    &nbsp;\nOption 2: Use different approaches per client\n\n    \n    &nbsp;\n    \n    \n    \n    &#8203;\nWhy your current approach works:\n\nYour dual-TD approach works because:\nNon-Outlook clients see only the first TD (due to MSO conditionals)\nOutlook sees both TDs but the MSO-conditional one \"wins\" in the rendering priority\nThe MSO conditional signals to Outlook: \"trust these styles completely\"\n\nBottom line: Keep both TDs. The MSO conditional isn't just about different markupit's about telling Outlook to process the styles with higher fidelity. Even identical markup behaves differently when wrapped in MSO conditionals versus not.\n\nEmail development, especially for Outlook, is absolutely wild. It's one of those areas where logic goes out the window and you end up with these bizarre workarounds that make no sense until you understand Outlook's quirky rendering engine.\n\nThe fact that identical markup behaves differently based on MSO conditional wrapping is just... chef's kiss \n\nYou'll probably encounter more of these \"wait, that makes no sense but it works\" moments as you build out your email components. Outlook has so many rendering peculiarities:\n\nCSS gets \"helpfully\" rewritten\nFont sizes have mysterious minimums  \nTable layouts work differently than literally everywhere else\nMSO conditionals have magical powers\n\nKeep that dual-TD pattern - it's going to save you a lot of headaches down the road!",
        "locale": "en",
        "slug": "technical-posts/mso-is-sneaky",
        "folder": ""
    },
    {
        "title": "Style a button like a link",
        "description": "Style a button like a link",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css"
        ],
        "content": "This needs a little tweaking\n\n.button-as-link {\n\tbackground-color: inherit;\n\ttext-decoration: underline;\n\toutline: none;\n\tborder: none;\n\tcolor: gray;\n\tpadding: 5px 10px;\n\ttext-align: center;\n\tdisplay: inline-block;\n\tfont-size: 16px;\n\tcursor: pointer;\n}",
        "locale": "en",
        "slug": "technical-posts/style-a-button-like-a-link",
        "folder": ""
    },
    {
        "title": "Backup SQL Server with PowerShell",
        "description": "Backup SQL Server with PowerShell",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities",
            "powershell"
        ],
        "content": "See also: [[PowerShell tips and techniques]]\n\nget-sqldatabase -serverinstance thumb-vm\n  | where { $_.Name -ne 'tempdb' }\n  | backup-sqldatabase\n\ncopy-item \"C:\\Program Files\\Microsoft SQL Server\\MSSQL16.MSSQLSERVER\\MSSQL\\Backup\\*.bak\" -destination \"c:\\users\\thumb\\documents\\sql-server-backup\"",
        "locale": "en",
        "slug": "technical-posts/sql-server-backup-with-powershell",
        "folder": ""
    },
    {
        "title": "My Starship configuration file",
        "description": "My Starship configuration file",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "utilities"
        ],
        "content": "Starship command line utility\n\nStarship ANSI colors\n\n\nThe starship.toml file location:\n\nc:\\users\\thumb\\documents\\.config\n\nMy starship.toml config file:\n\n#git_branch]\n#symbol = ' '\n#truncation_length = 4\n#truncation_symbol = ''\n##ignore_branches = ['master', 'main']\n\n\ncommand_timeout = 500\nformat = \"$time$directory$git_branch$cmd_duration$character\"\nformat = \"$time$directory$git_branch$git_status$python$character\"\n[line_break]\ndisabled = true\n\n[character]\nsuccess_symbol is the right pointy shape.\nI don't where I got the right-pointing arrow symbol!\nsuccess_symbol= \"[\"\n#success_symbol = \" \"\nerror_symbol = \" \"\n\ncmd_duration]\nmin_time = 500\nformat = \"took [$duration \"\n\ndirectory]\nread_only = \" \"\ntruncation_length = 3\ntruncation_symbol = \"~/\"\nstyle = \"fg:yellow bg:blue\"\nformat=\"[ $path \"\n\n\ngit_branch]\n#symbol = \"  \"\nsymbol = ''\nstyle = \"fg:black bg:36\"\nformat = \"[$symbol$branch\"\n\ntime]\ndisabled = false\n#format = '[  $time  '\nformat = ' $time '\ntime_format = \"%I:%M\"\nutc_time_offset = \"+7\"\nstyle = \"fg:black bg:69\"\n\ngit_commit]\ndisabled = true\n\n[git_state]\ndisabled = true\n\n[git_status]\ndisabled = false\nmodified = '!'\nstyle=\"fg:black bg:green\"\nformat='[$modified'\n\npackage]\ndisabled = true\n\n[python]\ndisabled=false\nsymbol = ' '\nsymbol = ' env '\nstyle=\"fg:black bg:#F5B041\"\nformat='[$virtualenv'",
        "locale": "en",
        "slug": "technical-posts/starship-command-line-configuration-file",
        "folder": ""
    },
    {
        "title": "What is a crate in Rust",
        "description": "What is a crate in Rust",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "In Rust, a crate is a package of Rust code. It can be a binary crate, which is an executable program, or a library crate, which provides functionality that can be used by other crates. Heres a more detailed breakdown:\n\nTypes of Crates\n\nBinary Crate:\n\n    Contains a main function and can be compiled into an executable.\n    You run it with cargo run.\n    Examples: command-line tools, applications.\n\nLibrary Crate:\n    Does not contain a main function.\n    Provides functionality through its public API for use in other crates.\n    You include it in your project using Cargo.toml.\n    Examples: utility libraries, frameworks.\n\nCrate Structure\n\nA crate has the following structure:\n\nCargo.toml: The configuration file where you define dependencies, metadata, and other settings for your crate.\nsrc/lib.rs: For library crates, this is where you define the crates public API.\nsrc/main.rs: For binary crates, this is where the main function resides.\n\nCreating and Using Crates\n\nCreating a Crate:\n    Use cargo new crate_name to create a new crate. By default, it creates a binary crate with a main function.\nUsing External Crates:\n    Add dependencies to your Cargo.toml to use external crates. For example:\n\n    [dependencies]\n    regex = \"1\"\n\nUsing Crates in Code:\n    Import and use functions or types from external crates with use statements. For example:\n\n    use regex::Regex;\n\n    fn main() {\n        let re = Regex::new(r\"^\\d+$\").unwrap();\n        println!(\"{}\", re.is_match(\"123\")); // true\n    }\n\nExample\n\nHeres a minimal example of a library crate:\n\nCargo.toml:\n\n    [package]\n    name = \"my_library\"\n    version = \"0.1.0\"\n    edition = \"2021\"\n\n    [dependencies]\n\nsrc/lib.rs:\n    pub fn greet(name: &str) -> String {\n        format!(\"Hello, {}!\", name)\n    }\n\nTo use this library crate in another project:\n\nAdd it to Cargo.toml as a dependency:\n\n    [dependencies]\n    my_library = { path = \"../my_library\" }\n\nUse it in your code:\n\n    use my_library::greet;\n\n    fn main() {\n        println!(\"{}\", greet(\"World\"));\n    }\n\nUnderstanding crates is fundamental to Rust development, as they help organize code into manageable pieces and facilitate code reuse.",
        "locale": "en",
        "slug": "technical-posts/what-is-a-crate-in-rust",
        "folder": ""
    },
    {
        "title": "mothra-notes",
        "description": "mothra-notes",
        "date_created": "2025-09-08T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "mothra",
            "office-setup"
        ],
        "content": "Mothra's back panel\n\n![[mothra-back-panel.jpg]]",
        "locale": "en",
        "slug": "technical-posts/mothra-notes",
        "folder": ""
    },
    {
        "title": "Target mso and not mso targets",
        "description": "Target mso and not mso targets",
        "date_created": "2025-07-23T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "outlook",
            "mso"
        ],
        "content": "[!info]\nSee the RowSpacer component for an example of this syntax in action.\n\nTargeting MSO Environments\n\nTo include content specifically for Microsoft Outlook, the following syntax is used:\n\n\n  ... content for MSO clients ...\n\n\nThis code leverages conditional comments, a feature originally introduced by Microsoft for Internet Explorer and also supported by the Word-based rendering engine used in many versions of Outlook. Other email clients will interpret this block as a standard HTML comment and ignore the content within.\n\nTargeting Non-MSO Environments\n\nTo include content for email clients other than Microsoft Outlook, the syntax is slightly different:\n\n\n  ... content for non-MSO clients ...\n\n\nHere's a breakdown of why this specific syntax is necessary:\n\n``: This is a conditional comment that targets environments that are not Microsoft Outlook.\n`: This is a crucial part of the syntax. For non-MSO clients, this closes the initial comment, allowing them to render the content that follows. Outlook, however, will see the [if !mso] condition as false and will ignore everything until it finds the `.\n``: This closes the conditional for Outlook and the new comment for other clients.\n\nThis clever use of comments ensures that only non-MSO clients will render the enclosed HTML.\n\nIn summary, the syntax you've inquired about is the standard and correct approach for handling the rendering differences in Outlook and other email clients, a common challenge in HTML email development.",
        "locale": "en",
        "slug": "technical-posts/target-mso-and-not-mso-targets",
        "folder": ""
    },
    {
        "title": "Svelte 5 upgrade issues --after getting the app compiled enough to run",
        "description": "Svelte 5 upgrade issues --after getting the app compiled enough to run",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Algolia\n\nAlgolia upgrade breaks search. Sent an email to support. See [[Algolia v5 upgrade]]\n\nMark up changes\n\nen/products/wings page failed because the content.md file had a caption element within a figure element for this image: \\* https://nyc3.digitaloceanspaces.com/asna-assets/images/asna-com/wings-before-after.webp\n\n[!danger]\nWatch for mark up errors to cause bigger problems than you would expect. HTML don't care, but Svelte does!\n\nsvelte:component deprecated\n\nThe en/products/wings page reads a content.md file for most of its content. With Svelte 4, the markdown was rendered with the svelte:component like this:\n\nexport let data;\n\n\n\n\n\t\n\n\nWith Svelte 5, that changes to:\n\nconst { data } = $props();\nconst Markdown = data.content;\n\n\n\n\n\t\n\n\nThe markdown is effectively treated like an on-the-fly component.\n\nsvelte.config.js change\n\nThe orginal svelte.config.js file had the order the two preprocessors reversed (with viteProcess() coming first). Although the app worked that way, the Svelte 5 upgrade said mdsvex should come before viteProcess(), so I did change the order just for the superstitious value.\n\nconst config = {\n\t// Consult https://kit.svelte.dev/docs/integrations#preprocessors\n\t// for more information about preprocessors\n\textensions: ['.svelte', '.md'],\n\tpreprocess: [mdsvex(mdsvexOptions), vitePreprocess()],\n\nZod forms validation\n\nZod's 2.x superValidate now needs a second argument that wraps the form schema in the zod function (which must be imported as show)\n\nimport { zod } from 'sveltekit-superforms/adapters';\n\nThe first arg to superValidate needs to be form data and the second needs to be the results of the zod call.\n\nconst formData = await event.request.formData();\nconst form = await superValidate(formData, zod(contactFormSchema));\n\nThis isn't right! That's only for form actions, not load functions. Watch for problems with load functions.",
        "locale": "en",
        "slug": "technical-posts/svelte-5-upgrade-issues--after-getting-the-app-compiled-enough-to-run",
        "folder": ""
    },
    {
        "title": "Nesting WebForms projects as virtual directories",
        "description": "Nesting WebForms projects as virtual directories",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "web-dev"
        ],
        "content": "Yes, it is absolutely doable to add other WebForms projects as virtual directory children in your existing C# WebForms website. Here's a breakdown of how to do it, along with considerations and best practices:\n\nUnderstanding the Setup\n\nParent Website: Your existing main WebForms website. This will be hosted under a particular URL (e.g., www.example.com).\nChild WebForms Projects (Virtual Directories): Each of these will be a separate WebForms project, living as a \"subdirectory\" of your parent website. Their URLs will be like www.example.com/ChildApp1, www.example.com/ChildApp2, etc.\nSeparate Applications: Each child project acts as a separate ASP.NET application within the parent site's IIS application pool. This means they have their own web.config files, session state (if not configured to share), and their own set of application events (e.g., Application_Start).\n\nSteps to Configure (IIS - Internet Information Services)\n\nBuild Your Projects: Make sure all your WebForms projects (the parent and all the children) are built successfully.\n\nDeploy the Parent Website: Deploy the compiled parent website to your IIS server. This usually involves copying the contents of the bin folder and the other necessary files (ASPX pages, CSS, images, etc.) to the directory specified for the website in IIS.\n\nCreate Virtual Directories in IIS: This is the key step.\n\n    Open IIS Manager: Type inetmgr in the Windows Run dialog (Win + R) and press Enter.\n    Locate Your Parent Website: In the Connections pane on the left, expand your server name, then \"Sites,\" and then find your parent website.\n    Right-Click on the Website: Right-click on your parent website and select \"Add Virtual Directory...\"\n    Configure the Virtual Directory:\n        Alias: This is the name of the \"subdirectory\" that will be used in the URL. For example, if you want your child application to be accessible at www.example.com/ChildApp1, then set the alias to ChildApp1.\n        Physical Path: This is the path to the root directory of your compiled child WebForms project (e.g., C:\\inetpub\\wwwroot\\ParentSite\\ChildApp1Project). IMPORTANT: This should point to the folder containing the web.config file of the child project. This is critical for IIS to recognize it as a separate ASP.NET application.\n    Repeat for Each Child Project: Repeat steps 3-5 for each of the WebForms projects you want to add as virtual directories.\n\nConvert Virtual Directory to Application: This is often crucial for proper isolation and configuration.\n\n    In IIS Manager, select the Virtual Directory you created.\n    Right-click on it and choose \"Convert to Application...\"\n    Accept the default settings (Application Pool) unless you have specific pool requirements. This step ensures that each child virtual directory runs in its own application within IIS. Without this, the child directory might just be treated as static content, and your ASP.NET code won't execute.\n\nImportant Considerations and Best Practices\n\nApplication Pools: Consider creating separate application pools for each of your child WebForms projects. This provides better isolation. If one child application crashes or has a memory leak, it's less likely to affect the other applications or the parent website. To do this, during the \"Convert to Application...\" step, specify a different Application Pool for each child.\nweb.config Files:\n\n    Each child project must have its own web.config file in its root directory. This is how IIS knows it's a separate ASP.NET application.\n    web.config Inheritance: By default, child web.config files inherit settings from the parent web.config. This can be useful (e.g., for sharing connection strings or authentication settings), but it can also cause conflicts. Use the ` element in the parent web.config` to control which settings are inherited by the child applications. Example to prevent inheritance:\n\n    \n       \n    \n\n    Unique appSettings and connectionStrings: Ensure that the appSettings and connectionStrings in each web.config are tailored to that specific application and don't conflict.\n\nSession State: By default, each application will have its own session state. If you want to share session state between the parent and child applications (or between the children), you'll need to configure session state appropriately (e.g., using a State Server or SQL Server mode). Be aware of the implications of shared session state (e.g., potential for collisions if session keys are not unique).\nAuthentication and Authorization: Carefully plan your authentication and authorization strategy. Each child application can have its own authentication scheme. If you want a single sign-on experience across all applications, you'll need to implement a shared authentication mechanism (e.g., using ASP.NET Identity, a custom authentication provider, or an external identity provider like Azure AD).\nVirtual Pathing: Within your code, when you generate URLs, make sure to account for the virtual directory structure. Use Page.ResolveUrl(\"/SomePage.aspx\") or VirtualPathUtility.ToAbsolute(\"/SomePage.aspx\", Request.ApplicationPath) to correctly resolve paths relative to the application root. The Request.ApplicationPath property is crucial for getting the virtual directory path. Example:\n\n    string myUrl = VirtualPathUtility.ToAbsolute(\"~/ChildApp1/MyPage.aspx\", Request.ApplicationPath);\n\nRelative Paths: Be careful with relative paths in CSS, JavaScript, and images. They will be relative to the location of the page that references them. Use the ~ operator to specify paths relative to the application root.\n\nVisual Studio Development: When developing, you can attach the Visual Studio debugger to the w3wp.exe process for each application pool that your websites are running in. This allows you to debug each application independently. You might need to run Visual Studio as administrator.\n\nDeployment Strategy: Consider using a deployment tool or process (e.g., MSDeploy, Azure DevOps pipelines) to automate the deployment of your WebForms projects to IIS. This makes deployments more consistent and reduces the risk of errors.\n\nShared Code/Libraries: If you have code that needs to be shared between the parent and child applications, put it in a separate class library project. Add a reference to that class library in each of the WebForms projects. Deploy the DLL of the class library to the bin folder of each WebForms project.\n\nExample Scenario\n\nLet's say you have a parent website called MyMainWebsite at www.example.com. You want to add two child applications:\n\nChildApp1: A blog at www.example.com/Blog\nChildApp2: An e-commerce store at www.example.com/Store\n\nYou'd create three separate WebForms projects in Visual Studio: MyMainWebsite, ChildApp1, and ChildApp2.\nAfter building, you'd deploy MyMainWebsite to a directory like C:\\inetpub\\wwwroot\\MyMainWebsite.\nIn IIS Manager, you'd create two virtual directories:\n    Alias: Blog, Physical path: C:\\inetpub\\wwwroot\\MyMainWebsite\\ChildApp1\n    Alias: Store, Physical path: C:\\inetpub\\wwwroot\\MyMainWebsite\\ChildApp2\nYou'd \"Convert to Application...\" both the Blog and Store virtual directories. Consider using separate Application Pools for each.\nYou'd carefully configure the web.config files in each project, paying attention to inheritance, authentication, session state, and connection strings.\n\nBy following these steps, you can successfully create a WebForms website with child WebForms projects hosted as virtual directories, allowing you to build modular and manageable applications. Remember to test thoroughly to ensure that all aspects of your application are working correctly.",
        "locale": "en",
        "slug": "technical-posts/nesting-webforms-projects-as-virtual-directories",
        "folder": ""
    },
    {
        "title": "SQL Server full text search",
        "description": "SQL Server full text search",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sql-server",
            "db"
        ],
        "content": "see if full text search is installed\n\nSELECT SERVERPROPERTY('IsFullTextInstalled') AS [Full Text Search Installed];\n\nSQL to see which databases have full text search enabled.\n\nSELECT name as [DBName], is_fulltext_enabled FROM sys.databases\n\nEnable full text search\n\nEXEC sp_fulltext_database 'enable'\n\nSELECT SERVERPROPERTY('IsFullTextInstalled') AS [Full Text Search Installed];\n\nThe database needs a full-text catalog.\n\nThis catalog is in the database's Storage/Full Text Catalogs node.\n\nTo create one, right click on Storage/Full Text and select \"New Full-Text Catalog.\" You can name it anything you like; I use \"full-text-search-catalog.\"\n\nAny table you want to full-text search needs a full-text index",
        "locale": "en",
        "slug": "technical-posts/sql-server-full-text-search",
        "folder": ""
    },
    {
        "title": "Using TsDoc",
        "description": "Using TSDoc",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "tsdoc",
            "typescript"
        ],
        "content": "| TSDoc Tag         | Description                                                                                          |\n| :---------------- | :--------------------------------------------------------------------------------------------------- |\n| @alpha          | Indicates that an API item is still in an early, unstable stage and may change frequently.           |\n| @beta           | Signifies that an API item is stable enough for broader testing but may still have breaking changes. |\n| @internal       | Marks an API item as intended for internal use only within its package or project.                   |\n| @public         | Explicitly declares an API item as part of the public API, meant for external consumers.             |\n| @deprecated     | Indicates that an API item is no longer recommended for use and may be removed in a future version.  |\n| @example        | Provides a code example demonstrating how to use the API item.                                       |\n| @param          | Describes a parameter of a function or method.                                                       |\n| @returns        | Describes the return value of a function or method.                                                  |\n| @remarks        | Provides additional, more detailed information about the API item.                                   |\n| @see            | Links to related documentation or other API items.                                                   |\n| @throws         | Describes an error that the API item might throw.                                                    |\n| @privateRemarks | Provides internal remarks that are not published in the public documentation.                        |\n| @virtual        | Indicates that a member is expected to be overridden by subclasses.                                  |\n| @override       | Indicates that a member overrides a member from a base class.                                        |\n| @sealed         | Indicates that a class or member cannot be inherited from or overridden.                             |\n| @readonly       | Indicates that a property is read-only.                                                              |\n| @eventProperty  | Used to document properties that represent events.                                                   |\n| @enum           | Used to document enumeration members.                                                                |\n\nExamples\n\n/**\n Represents a basic point in 2D space.\n @alpha This API is new and may change frequently.\n @beta This class is stable enough for broader testing.\n @public\n @sealed\n @exampletypescript\n const p = new Point(10, 20);\n console.log(p.toString()); // Output: Point(10, 20)\n */\nexport class Point {\n  /**\n   The x-coordinate of the point.\n   @readonly\n   */\n  public readonly x: number;\n\n  /**\n   The y-coordinate of the point.\n   @internal For internal use within the Point class.\n   @privateRemarks We considered making this a getter, but decided against it for performance.\n   */\n  private _y: number;\n\n  /**\n   Creates a new Point instance.\n   @param x - The initial x-coordinate.\n   @param y - The initial y-coordinate.\n   @throws {Error} If x or y are not finite numbers.\n   @deprecated Use Point.fromCoordinates instead for better readability.\n   */\n  constructor(x: number, y: number) {\n    if (!Number.isFinite(x) || !Number.isFinite(y)) {\n      throw new Error(\"Coordinates must be finite numbers.\");\n    }\n    this.x = x;\n    this._y = y;\n    this._initializeInternalState();\n  }\n\n  /**\n   Initializes some internal state for the point.\n   @internal\n   */\n  private _initializeInternalState(): void {\n    // ...\n  }\n\n  /**\n   Gets the y-coordinate of the point.\n   @returns The y-coordinate.\n   */\n  public getY(): number {\n    return this._y;\n  }\n\n  /**\n   Returns a string representation of the point.\n   @remarks This method is useful for debugging and logging.\n   @returns A string in the format \"Point(x, y)\".\n   @see {@link https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/toString | Object.toString()}\n   */\n  public toString(): string {\n    return Point(${this.x}, ${this._y});\n  }\n\n  /**\n   Creates a new Point instance from given coordinates.\n   @param x The x-coordinate.\n   @param y The y-coordinate.\n   @returns A new Point object.\n   */\n  public static fromCoordinates(x: number, y: number): Point {\n    return new Point(x, y);\n  }\n}\n\n/**\n Represents a colored point, extending the base Point class.\n @beta\n @virtual\n */\nexport class ColoredPoint extends Point {\n  /**\n   The color of the point.\n   */\n  public color: string;\n\n  /**\n   Creates a new ColoredPoint instance.\n   @param x - The x-coordinate.\n   @param y - The y-coordinate.\n   @param color - The color of the point.\n   */\n  constructor(x: number, y: number, color: string) {\n    super(x, y);\n    this.color = color;\n  }\n\n  /**\n   Overrides the toString method to include color information.\n   @override\n   @returns A string in the format \"ColoredPoint(x, y, color)\".\n   */\n  public toString(): string {\n    return ColoredPoint(${this.x}, ${this._y}, ${this.color});\n  }\n}\n\n/**\n Represents a click event.\n @eventProperty\n */\nexport interface ClickEvent {\n  /**\n   The x-coordinate of the click.\n   */\n  x: number;\n  /**\n   The y-coordinate of the click.\n   */\n  y: number;\n}\n\n/**\n Represents the status of an operation.\n @enum\n */\nexport enum OperationStatus {\n  /**\n   The operation is pending.\n   */\n  Pending = \"PENDING\",\n  /**\n   The operation was successful.\n   */\n  Success = \"SUCCESS\",\n  /**\n   The operation failed.\n   */\n  Failure = \"FAILURE\",\n}",
        "locale": "en",
        "slug": "technical-posts/tsdoc-tags",
        "folder": ""
    },
    {
        "title": "New ASNA home page research",
        "description": "New ASNA home page research",
        "date_created": "2025-10-13T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-com"
        ],
        "content": "\nGood mega menu example\n\nCurious, take the quiz? \nClear, fixed path experience\nPaint a picture of what life could be like with your products",
        "locale": "en",
        "slug": "technical-posts/new-asna-homepage",
        "folder": ""
    },
    {
        "title": "mso-email-two-column-component",
        "description": "mso-email-two-column-component",
        "date_created": "2025-07-27T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "mso"
        ],
        "content": "Raw HTML from Gemini that renders two columns with text in the right and an image in the left. \n\n\n        \n            \n                \n                    \n                        \n                            \n                                \n                                    \n                                        Your RPG applications face an uncertain future\n                                    \n                                    \n                                        Your business depends on your core RPG applications to keep your business\n                                        running. Can you keep them running when your RPG programming team retires?\n                                    \n                                    \n                                        This is a paragraph of text that will appear in the left column of the email. It\n                                        is now vertically centered to align with the image on the right.\n                                     -->\n                                \n                            \n                        \n                    \n                \n                \n                    \n                        \n                            \n                                \n                                    \n                                \n                            \n                        \n                    \n                \n            \n        \n    \n\nGemini produced this table to show a centered image\n\n\n        \n            \n                \n                    \n        \n        \n        \n        \n                    \n                        \n                            \n                                \n                                    \n                                        \n                                    \n                                \n                            \n                        \n                    \n                    \n        \n        \n        \n        \n                \n            \n        \n    ",
        "locale": "en",
        "slug": "technical-posts/mso-email-two-column-component",
        "folder": ""
    },
    {
        "title": "An intro to SQL PARTITION-BY",
        "description": "An intro to SQL PARTITION-BY (this specific SQL works on SQL Server but may work on other platforms)",
        "date_created": "2025-02-02T00:00:00.000Z",
        "date_updated": "2025-12-04T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sql"
        ],
        "content": "Here is a query using a Common Table Expression (CTE) and a Window Function. This is widely considered the most efficient and readable way to solve this problem in SQL Server.\n\nIt calculates the count of each email address \"on the fly\" and attaches that count to every row. You can then simply filter for rows where that count is greater than 1.\n\n[!info]\nNote the use of a Common Table Expression) below\n\nWITH EmailCounts AS (\n    SELECT \n        *,\n        COUNT(*) OVER (PARTITION BY email) as EmailCount\n    FROM list_history\n)\nSELECT *\nFROM EmailCounts\nWHERE EmailCount > 1\nORDER BY email;\n\nThe query above finds and count alls email addresses in list_history for each email address that occurs more than once.\n\nHow it works:\nCOUNT(*) OVER (PARTITION BY email): This creates a new temporary column (EmailCount). It looks at the email column and counts how many times that specific email appears in the entire table, then assigns that number to the current row.\nWITH EmailCounts AS (...): This wraps the logic in a temporary result set (the CTE) so we can filter by the new column we just created.\nWHERE EmailCount > 1: This filters the final result to show only the rows where the email appears more than once.\n\nAlternative Method (Using JOIN)\nIf you prefer standard grouping logic, you can use a CTE to find the duplicates first and then join back to the main table:\n\nWITH Duplicates AS (\n    SELECT email\n    FROM list_history\n    GROUP BY email\n    HAVING COUNT(*) > 1\n)\nSELECT lh.*\nFROM list_history lh\nINNER JOIN Duplicates d ON lh.email = d.email\nORDER BY lh.email;",
        "locale": "en",
        "slug": "technical-posts/sql-partition-by",
        "folder": ""
    },
    {
        "title": "What does Rust's 'mut' keyword do",
        "description": "What does Rust's 'mut' keyword do",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "rust"
        ],
        "content": "In Rust, mut is a keyword used to declare that a variable is mutable, meaning that its value can be changed after it is initially set.\n\nDetailed Explanation:\n\nlet: This keyword is used to declare a variable in Rust.\nmut: This keyword is used to indicate that the variable's value can be modified. Without mut, the variable is immutable, meaning its value cannot be changed once it's assigned.\n\nIn Your Example:\n\nlet mut lines = Vec::new();\n\nHeres what each part means:\n\nlet: Declares a new variable named lines.\nmut: Indicates that the variable lines is mutable, so you can modify its contents or reassign it.\nlines: The name of the variable.\nVec::new(): Creates a new, empty vector of the default type Vec. The specific type of T would be inferred from the context or explicitly defined elsewhere in the code.\n\nWhy Use mut?\n\nIn Rust, variables are immutable by default. This immutability helps to prevent unintended side effects and makes code more predictable and safer. However, if you need to change the value of a variable, you must explicitly mark it as mutable using mut.\n\nFor example, if you have a mutable vector:\n\nlet mut lines = Vec::new(); // Create a mutable vector\nlines.push(\"First line\".to_string()); // Modify the vector by adding an element\nlines.push(\"Second line\".to_string()); // Modify the vector by adding another element\n\nIn this case:\n\nlines.push(...) is allowed because lines is mutable. If you had declared lines without mut, trying to call push would result in a compile-time error.\n\nImmutable Variable Example:\n\nIf you declared lines without mut:\n\nlet lines = Vec::new(); // Create an immutable vector\n// lines.push(\"Some line\".to_string()); // This would cause a compile-time error\n\nHere, lines cannot be modified or reassigned. Any attempt to call methods that mutate the vector, like push, would result in an error.\n\nSummary:\n\nmut allows a variable to be modified after it is initialized.\nIn your example, let mut lines = Vec::new(); creates a mutable vector that can be modified by adding elements or performing other mutations.\nWithout mut, the variable would be immutable, and any attempts to change its value would result in a compile-time error.",
        "locale": "en",
        "slug": "technical-posts/what-does-rusts-mut-keyword-do",
        "folder": ""
    },
    {
        "title": "What does the svelte-kit json object do",
        "description": "What does the svelte-kit json object do",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-12-18T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "json",
            "svelte"
        ],
        "content": "The json function from @sveltejs/kit is a convenience helper function used on the server-side (in +server.js files, or server load functions and actions in +page.server.js or +layout.server.js) to create and return a standard Web Response object with a JSON payload and the correct Content-Type header.\n\nHere's a breakdown of what it does and why it's useful:\n\nSerializes Data to JSON:\n\n    It takes a JavaScript object (or any value that can be JSON-serialized) as its first argument.\n    It internally uses JSON.stringify() to convert this data into a JSON string.\n\nCreates a Response Object:\n\n    SvelteKit endpoints and server functions are expected to return a standard Response object (part of the Web Fetch API). The json function handles the creation of this Response object for you.\n\nSets the Content-Type Header:\n\n    It automatically sets the Content-Type header of the Response to application/json. This is crucial for clients (like browsers or other services) to understand that the body of the response is JSON data.\n\nAllows Setting Status Code and Custom Headers:\n    You can optionally provide a second argument, an options object, to customize the response:\n        status: To set the HTTP status code (e.g., 200 for OK, 201 for Created, 404 for Not Found, 500 for Internal Server Error). Defaults to 200 if not specified.\n        headers: An object to set additional custom HTTP headers.\n\nWhy is it used?\n\nWithout the json helper, you would have to manually construct the Response object like this:\n\n// src/routes/api/data/+server.js\n\nexport function GET() {\n    const myData = { message: \"Hello, world!\" };\n    return new Response(JSON.stringify(myData), {\n        status: 200,\n        headers: {\n            \"Content-Type\": \"application/json\",\n        },\n    });\n}\n\nWith the json helper, it becomes much more concise and less error-prone:\n\n// src/routes/api/data/+server.js\nimport { json } from \"@sveltejs/kit\";\n\nexport function GET() {\n    const myData = { message: \"Hello, world!\" };\n    return json(myData); // Defaults to status 200\n}\n\nExamples:\n\nBasic API Endpoint:\n\n    // src/routes/api/items/+server.js\n    import { json } from \"@sveltejs/kit\";\n\n    export async function GET() {\n        const items = [\n            { id: 1, name: \"Item 1\" },\n            { id: 2, name: \"Item 2\" },\n        ];\n        return json(items); // Responds with status 200 and Content-Type: application/json\n    }\n\nResponding with a different status code (e.g., after creating a resource):\n\n    // src/routes/api/items/+server.js\n    import { json } from \"@sveltejs/kit\";\n\n    export async function POST({ request }) {\n        const newItemData = await request.json();\n        // ... logic to save newItemData to a database ...\n        const createdItem = { id: 3, ...newItemData };\n        return json(createdItem, { status: 201 }); // 201 Created\n    }\n\nResponding with an error:\n\n    // src/routes/api/items/[id]/+server.js\n    import { json } from \"@sveltejs/kit\";\n\n    export async function GET({ params }) {\n        const item = findItemById(params.id); // Assume this function exists\n        if (!item) {\n            return json(\n                { message: Item with ID ${params.id} not found },\n                { status: 404 }\n            );\n        }\n        return json(item);\n    }\n\nIn a server load function:\n\n    // src/routes/products/+page.server.js\n    import { json } from \"@sveltejs/kit\";\n\n    export async function load() {\n        // In a real app, you'd fetch this from a DB or API\n        const products = [\n            { id: \"p1\", name: \"Super Widget\" },\n            { id: \"p2\", name: \"Mega Gadget\" },\n        ];\n        // Note: In load functions, you usually just return the object directly.\n        // SvelteKit handles serializing it for the client.\n        // json() is more common for standalone API endpoints or when you need\n        // to specifically control the Response object being formed by the load function,\n        // for instance, if the load function itself is acting like an API for a form action.\n        // However, if you were to directly call this endpoint via fetch, returning json() would be appropriate.\n        return { products }; // SvelteKit handles this correctly for page data\n\n        // If this load function was being used by a form action that expects a JSON response:\n        // return json({ products }); // This would also work but is often not needed here\n    }\n\n    Correction for load function usage: While you can use json() in load, it's more common to just return the plain JavaScript object. SvelteKit's load function mechanism will automatically serialize this data for the page. The json() utility is most vital for dedicated API endpoints (+server.js) or when an action needs to return a JSON response (e.g., to be consumed by client-side JavaScript using enhance with fetch).\n\nIn summary, @sveltejs/kit's json function is a server-side utility that simplifies the process of returning JSON data from your SvelteKit application, ensuring the correct Response object is constructed with the appropriate headers and status code.",
        "locale": "en",
        "slug": "technical-posts/what-does-the-svelte-kit-json-function-do",
        "folder": ""
    },
    {
        "title": "Mongo notes from early stages of ASNA.com",
        "description": "Mongo notes from early stages of ASNA.com",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "asna-svelte",
            "sveltekit"
        ],
        "content": "!info]\nThis is from very early in Project Ocho's life--which was originally called Vulture (because it was gonna eat the dead remains of the WordPress site). I gave up on using Mongo a data store very shortly after the site was launched. I'm leaving this notes here if I ever have the bad fortune to want to try to use Mongo again.\n\nVulture's database is a Mongo Atlas DB. This is a NoSQL DB.\nThe Mongo Atlas database instance credentials are:\n\nrogerpence\n7EvxzPxIxwT5M35J\n\ndatabase: vulture\ncollection: downloads\nconnection string: mongodb+srv://rogerpence:7EvxzPxIxwT5M35J@cluster0.rqhvs2d.mongodb.net/?retryWrites=true&w=majority\n\nProject Vulture - ASNA.com V\n\n[Project overview From Project Overview browse collections\nDatabases Click \"Connect\" button next to Cluster0\" then click \"Drivers\" to see connection string details. The account name and credentials are above.\nWork with database users\nUse Mongo Compass to view and work with databases and collections\n\nSerializing a Mongo collection\n\nThe Mongo _id property is a nested object in the collection and can cause problems serialization for Svelte.\n\nThe canonical solution to this problem is to iterate over the collection to substitute the ToString() value of _id in the result:\n\nconst urlList = await collection.find().toArray();\n\nconst serializedUrls = urlList.map((item) =>\n\tJSON.parse(\n\t\tJSON.stringify(item, (key, value) => (key === '_id' ? value.toString(value) : value))\n\t)\n);\n\nreturn serializedUrls\n\nThis article says to use serializedUrls which is a much cleaner to fix the issue.\n\nIt first offers this function:\n\nconst serializeNonPOJOs = (value) => {\n\treturn JSON.parse(JSON.stringify(value));\n};\n\nI'm not sure if the solution above screws up data types in the Json doc, as it unconditionally stringifies every property (the map solution farther above does not do).\n\nbut then goes on to explain that this is less verbose:\n\nserializedUrls = structuredClone(urlList)\n\nbut using structuredClone returns an empty _id property.\n\nExample\n\nGiven this two object Mongo collection:\n\n![[Pasted image 20231015120801.png]]\n\nand using this query to fetch it:\n\nconst data = await testCollection.find().toArray();\n\nThis is the raw data returned. Note the _id property is an object.\n\n[\n  {\n    _id: new ObjectId(\"652c19bac046c5d0785e73e8\"),\n    name: 'Spaceman',\n    age: 70\n  },\n  {\n    _id: new ObjectId(\"652c1b6dc046c5d0785e73ec\"),\n    name: 'Niko',\n    age: 65\n  }\n]\n\nThe embedded _id object property causes this error which, when you look closely, references the _id property:\n\nError: Data returned from load while rendering / is not serializable: Cannot stringify arbitrary non-POJOs (data.downloads[0]._id)\n\nModifying the collection with structuredClone\n\nconst fixedData = structuredClone(data)\n\nreturns:\n\n[\n  { _id: {}, name: 'Spaceman', age: 70 },\n  { _id: {}, name: 'Niko', age: 65 }\n]\n\nnote that the _id property is an empty object, This data returns correctly but fetching the _id property returns [object Object]\n\nModifying the collection with serializeNonPOJOs(data)\n\nconst fixedData = serializeNonPOJOs(data)\n\nreturns:\n\n[\n  { _id: '652c19bac046c5d0785e73e8', name: 'Spaceman', age: 70 },\n  { _id: '652c1b6dc046c5d0785e73ec', name: 'Niko', age: 65 }\n]\n\nwhich includes the correct string value for the _id property.\n\nFor most use cases, use the serializeNonPOJOs function. However, if you do need to directly manipulate a Mongo collection value the serialization method using the Map function is good to know about.",
        "locale": "en",
        "slug": "technical-posts/mongodb-notes",
        "folder": ""
    },
    {
        "title": "SvelteKit context and locals server-side objects",
        "description": "SvelteKit context and locals server-side objects",
        "date_created": "2025-06-07T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "Server-Side Context Objects in SvelteKit\n\nYes, you're correct. There are two main types of context objects in SvelteKit, and both are server-side only:\n\n1. locals Object\nServer-side only: Available only in server-side code\nPer-request: Each request gets its own locals object\nSet in hooks: Populated via the handle hook in hooks.server.ts\nAccessible in: \n  Server route handlers (+page.server.ts, +layout.server.ts)\n  API endpoints (+server.ts)\n  Server hooks\nNot accessible in: Client-side components or code\n\n2. Svelte Context API\nThe standard Svelte context API (getContext/setContext)\nWhen used in SvelteKit server components, it only works on the server\nNot shared between client and server\n\nYour pino-logger.ts implementation correctly handles this by requiring the pool to be passed explicitly, rather than trying to access it from a context that might not be available:\n\ninfo: (message: string, data?: any, pool?: Pool) => {\n    pinoLogger.info(data || {}, message);\n    if (!dev && pool)\n        logToDB(pool, 'info', message, data).catch((e) =>\n            pinoLogger.error('Failed to write to DB: ' + e.message)\n        );\n}\n\nThis is a good design because:\nIt makes dependencies explicit\nIt works in both server and client contexts\nIt's more testable and maintainable\n\nIf you wanted to reduce the need to pass the pool explicitly, you could enhance your logger in hooks.server.ts to create a request-specific logger with the pool already bound:\n\nexport const handle: Handle = async ({ event, resolve }) => {\n    event.locals.pool = pool;\n    event.locals.logger = {\n        ...logger,\n        // Pre-bind the pool to the logger methods\n        reqInfo: (message: string, data?: any) => logger.info(message, data, pool),\n        reqError: (message: string, data?: any) => logger.error(message, data, pool),\n        reqWarn: (message: string, data?: any) => logger.warn(message, data, pool)\n    };\n    \n    const response = await resolve(event);\n    return response;\n};",
        "locale": "en",
        "slug": "technical-posts/sveltekit-server-side-context-objects",
        "folder": ""
    },
    {
        "title": "SvelteKit study guide",
        "description": "SvelteKit study guide",
        "date_created": "2025-08-14T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "{$key} https://youtu.be/Xbdiwq_88iE?t=211\nSvelteKit boundaries - https://youtu.be/Xbdiwq_88iE?t=363\nTyler Codes - https://www.youtube.com/@TylerCodes  - Postgres and Drizzle\nSleep function - https://youtu.be/Xbdiwq_88iE?t=565\n\tawait new Promise((resolve, _) => setTimeout(resolve, 1000))\nI stopped at: https://youtu.be/Xbdiwq_88iE?t=615",
        "locale": "en",
        "slug": "technical-posts/sveltekit-study-guide",
        "folder": ""
    },
    {
        "title": "Create symlinks with PowerShell",
        "description": "How to create symlinks (SymbolicLink) with PowerShell. This is what enables the Brainiac Svelte project to share, in real time, markdown posts from Obsidian. PowerShell's SymbolicLink is like DOS's MKLINK command. This article also includes an explanation of PowerShell's symbolic links.",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-05T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "windows",
            "powershell"
        ],
        "content": "A junction point (also called a soft link or symbolic directory link) in Windows is a type of symbolic link that refers to another directory on the same volume. It's essentially a pointer from one directory to another. Think of it as a shortcut, but a much more integrated one.\n\nHere's a breakdown of what it is and how it differs from other linking methods:\n\nKey Characteristics of Junctions:\n\nDirectory-Only: Junctions can only point to directories, not individual files. This is a key distinction from symbolic links, which can link to both files and directories.\n\nLocal Volume Requirement: The target directory must be on the same volume (same hard drive or partition) as the junction point itself. You can't create a junction that points to a directory on a different drive letter.\n\nTransparent Access: When you access the junction point, Windows treats it as if you're directly accessing the contents of the target directory. Applications don't know they're being redirected.\n\nPersistency: Junctions are persistent. They remain intact across reboots.\n\nPhysical File Path Changes: Junctions rely on the physical file path of the target directory. If the target directory is moved or renamed, the junction will break, and attempting to access it will result in an error.\n\nHow Symlinks Work:\n\nA symlink echoes a folder to another virtual folder. Any changes to the files in either folder are synchronized very quickly. \n\nFor example:\n\nI want to echo the contents of my Obsidian Brainiac vault to another (virtual) folder. This lets me create a SvelteKit app that works with the virtual folder. \n\nNew-Item -ItemType SymbolicLink -Path  -Target \n\nWhere: \n \n``: Specifies the full path and name of the symbolic link you want to create.This is the \"shortcut\" or \"alias.\"\n``: Specifies the full path to the original file or directory that the symbolic link will point to.This is the \"source\" or \"destination.\"\n\nExample:\n\nThe link path I want to create is:  C:\\Users\\thumb\\Documents\\projects\\svelte\\brainiac\\src\\markdown \nThe source path is  C:\\Users\\thumb\\Documents\\resilio-envoy\\Obsidian\\brainiac (which is the Obsidian vault)\n\nNew-Item -ItemType SymbolicLink \n         -path C:\\Users\\thumb\\Documents\\projects\\svelte\\brainiac\\src\\markdown\n         -target C:\\Users\\thumb\\Documents\\resilio-envoy\\Obsidian\\brainiac\n\nIn this case, the symlink folder makes it to look the SvelteKit project that the Obsidian vault markdown lives in the project's src\\markdown folder. Any changes made to the contents in the markdown folder are immediately also applied to the original Obsidian vault folder (its unlikely there would be changes, but there could be a typo fixed or something like that). \n\n[!info]\nThe PowerShell command must be run as admin.\n\n\n\n\nThis provides:\n\nRedirection: When an application tries to access LinkDirectory, the NTFS file system recognizes it as a junction point. Instead of directly accessing the contents of LinkDirectory (which doesn't actually contain any files), the file system transparently redirects the application to the contents of TargetDirectory.\n\nSeamless Operation: The application continues to function as if it were directly accessing the files and subdirectories within TargetDirectory. It's unaware of the redirection.\n\nDifferences from Other Types of Links:\n\nHard Links: Hard links can only point to files, not directories, and like junctions, must be on the same volume. Multiple hard links point to the same underlying file data. If you delete one hard link, the file data remains as long as at least one hard link exists.\n\nSymbolic Links (Symlinks): Symlinks are more flexible. They can point to files or directories and can point to locations on different volumes or even network shares (though network symlinks can have security implications). They are the most versatile type of link, but may require administrator privileges to create. However, some older applications may not handle symlinks correctly. Symlinks are created with mklink /D for directory symlinks and mklink (without any options) for file symlinks.\n\nShortcuts (.lnk files): Shortcuts are not part of the file system. They are simply files that contain information about the target file or directory. They don't provide transparent access like junctions and symlinks. When you click on a shortcut, it opens the target in its associated application. Shortcuts can point to locations on different volumes or network shares.\n\nPersistence\n\njunctions do persist across reboots in Windows. Once created, a junction point will remain intact and functional even after the system is restarted. The operating system stores the information about the junction point in the NTFS file system, so it's automatically recreated each time the computer starts.\n\nUse Cases for Junctions:\n\nMoving System Folders (Carefully): Sometimes used (though less common now due to symlinks) to relocate system folders (like Program Files, Users) to a different partition without breaking software. This should be done with extreme caution.\n\nOrganizing Large Projects: Creating junction points can help organize complex projects by allowing you to access files from multiple locations as if they were in a single directory.\n\nBackwards Compatibility: Sometimes used to maintain backwards compatibility with older applications that expect files to be in a specific location, even if those files have been moved.\n\nExample:\n\nLet's say you have a directory called C:\\MyProject\\SourceFiles and you want to access those files from a new directory called C:\\WorkArea. You can create a junction point:\n\nmklink /J \"C:\\WorkArea\" \"C:\\MyProject\\SourceFiles\"\n\nNow, if you navigate to C:\\WorkArea, you'll see the same files and subdirectories that are in C:\\MyProject\\SourceFiles. Any changes you make in C:\\WorkArea will be reflected in C:\\MyProject\\SourceFiles, and vice versa.\n\nRemove a junction\n\nYou remove a junction point in Windows just like you would remove a regular directory: by using the rmdir command (remove directory) or by deleting it through File Explorer.\n\n1. Using the Command Prompt (rmdir)\n\nOpen the Command Prompt as an administrator (right-click on the Start button and select \"Command Prompt (Admin)\" or \"Windows Terminal (Admin)\").\nUse the rmdir command (or the rd command, which is an alias for rmdir) to remove the junction point:\n\n    rmdir \"JunctionPointPath\"\n\n    Replace \"JunctionPointPath\" with the full path to the junction point you want to remove. For example:\n\n    rmdir \"C:\\WorkArea\"\n\n    If the path contains spaces, be sure to enclose it in quotation marks.\n\nImportant: This command only removes the junction point itself (the directory that acts as the link). It does not delete the contents of the target directory that the junction points to. Those files and directories remain untouched.\n\n2. Using File Explorer\n\nOpen File Explorer.\nNavigate to the junction point you want to remove.\nRight-click on the junction point directory.\nSelect \"Delete\".\nConfirm the deletion if prompted.\n\nImportant: Just like with the rmdir command, deleting the junction point through File Explorer only removes the junction itself, not the contents of the directory it points to.\n\nExample:\n\nLet's say you have a junction point at C:\\WorkArea that points to C:\\MyProject\\SourceFiles.\n\nUsing Command Prompt:\n\n    rmdir \"C:\\WorkArea\"\n\nUsing File Explorer:\n\n    Open File Explorer.\n    Navigate to C:\\WorkArea.\n    Right-click on the WorkArea folder.\n    Select \"Delete\".\n\nAfter performing either of these steps, the C:\\WorkArea directory (the junction point) will be removed. The files and subdirectories in C:\\MyProject\\SourceFiles will remain intact.\n\nError Handling:\n\nIf you get an \"Access is denied\" error, ensure that you are running the Command Prompt as an administrator. You might also need to take ownership of the junction point directory if your user account doesn't have sufficient permissions.\n\nIf you try to delete the target directory that the junction points to, you'll be deleting the actual files and directories. Be very careful not to accidentally delete the wrong directory! Double-check the path before deleting anything.\n\nIn Summary:\n\nJunctions are a powerful tool in Windows for creating symbolic directory links within the same volume, providing transparent access to the target directory. They offer a more integrated solution than shortcuts, but are less flexible than symbolic links. Understanding the differences between these link types is crucial for choosing the right one for your needs.",
        "locale": "en",
        "slug": "technical-posts/symboliclinks-powershell",
        "folder": ""
    },
    {
        "title": "sveltekit-template-page-server-ts",
        "description": "sveltekit-template-page-server-ts",
        "date_created": "2025-05-26T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "import type { PageServerLoad, Actions } from './$types';\nimport { fail } from '@sveltejs/kit';\n\nimport { gotoRegistrantFormSchema as formSchema } from '$lib/form-schema/gotoRegistrantFormSchema';\nimport type { WebinarRegistrationFormData, GotoWebinarResponse } from '$lib/types/goto-webinar';\n\nexport const load: PageServerLoad = async ({ cookies }) => {\n    let formData = {\n        company: 'ASNA',\n        first_name: 'Roger',\n        last_name: 'Pence',\n        email: 'rp@asna.com'\n    };\n\n    return { formData };\n};\n\nexport const actions: Actions = {\n    create: async (event) => {\n        const rawFormData = Object.fromEntries(await event.request.formData()) as WebinarRegistrationFormData;\n\n        try {\n            // Have Zod validate the form with its schema.\n            const formData = formSchema.parse(rawFormData) as WebinarRegistrationFormData;\n\n            if (registrationInfo.success) {\n                throw redirect(303, '/success');\n            }\n\n            return {\n                registrationInfo\n            };\n        } catch (err: any) {\n            // Fetch the Zod errors:\n            const { fieldErrors: errors } = err.flatten();\n\n            return fail(400, {\n                data: rawFormData,\n                registrationInfo,\n                errors\n            });\n        }\n    }\n};",
        "locale": "en",
        "slug": "technical-posts/sveltekit-template-page-server-ts",
        "folder": ""
    },
    {
        "title": "Using stored procedures with mssql and Node",
        "description": "Using stored procedures with mssql and Node",
        "date_created": "2025-07-05T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "mssql"
        ],
        "content": "This code is here: \n\nC:\\Users\\thumb\\Documents\\projects\\svelte\\sql-server\n\nA fully-typed example\n\npnpm add -D @types/mssql\n\nTyped example code: \n\nimport { MSSqlConnect } from \"./MSSqlConnect\";\nimport { sqlConfig } from \"./mssql-config\"; // Assuming you have a config file for SQL Server configuration\nimport { IResult, RequestError } from \"mssql\";\n\nconst ms: MSSqlConnect = new MSSqlConnect(sqlConfig);\nconsole.log(\"Starting connection...\");\n\nawait ms.connectPool();\nconst request = ms.pool.request();\n\nconst id: number = 0;\nconst description: string = \"ColleenD\";\nconst rule: string = \"equals\";\nconst value: string = \"Roger\";\nconst category: string = \"test\";\n\n// Add input parameter for the year\nrequest.input(\"id\", ms.sql.Int, id);\nrequest.input(\"description\", ms.sql.NVarChar, description);\nrequest.input(\"rule\", ms.sql.NVarChar, rule);\nrequest.input(\"value\", ms.sql.NVarChar, value);\nrequest.input(\"category\", ms.sql.NVarChar, category);\n\ntry {\n    // Execute the stored procedure instead of a raw query\n    const result: IResult = await request.execute(\"dbo.source_upsert\");\n    console.log(JSON.stringify(result, null, 4));\n} catch (error: unknown) {\n    const sqlError = error as RequestError;\n    if (\n        sqlError.originalError &&\n        \"info\" in sqlError.originalError &&\n        (sqlError.originalError as any).info?.message\n            ?.toLowerCase()\n            .includes(\"unique key constraint\")\n    ) {\n        console.error(Row with Description = ${description} already exists);\n    } else {\n        console.error(sqlError.message || \"Unknown error occurred\");\n    }\n}\n\nawait ms.closePool().catch((err) => {\n    console.error(\"Error closing MSSqlConnect pool:\", err);\n});\n\nIn this case, full object returned from a successful request is:\n\nOn update: \n\n{\n    \"recordsets\": [\n        [\n            {\n                \"id\": 685,\n                \"description\": \"ColleenD\",\n                \"rule\": \"equals\",\n                \"value\": \"Roger\",\n                \"category\": \"test\",\n                \"ActionStatus\": \"INSERTED\"\n            }\n        ]\n    ],\n    \"recordset\": [\n        {\n            \"id\": 685,\n            \"description\": \"ColleenD\",\n            \"rule\": \"equals\",\n            \"value\": \"Roger\",\n            \"category\": \"test\",\n            \"ActionStatus\": \"INSERTED\"\n        }\n    ],\n    \"output\": {},\n    \"rowsAffected\": [],\n    \"returnValue\": 0\n}\n\nOn insert:\n\n{\n    \"recordsets\": [\n        [\n            {\n                \"id\": 686,\n                \"description\": \"ColleenD\",\n                \"rule\": \"equals\",\n                \"value\": \"Roger\",\n                \"category\": \"test\",\n                \"ActionStatus\": \"INSERTED\"\n            }\n        ]\n    ],\n    \"recordset\": [\n        {\n            \"id\": 686,\n            \"description\": \"ColleenD\",\n            \"rule\": \"equals\",\n            \"value\": \"Roger\",\n            \"category\": \"test\",\n            \"ActionStatus\": \"INSERTED\"\n        }\n    ],\n    \"output\": {},\n    \"rowsAffected\": [],\n    \"returnValue\": 0\n}\n\nOn error: \n\n{\n    \"code\": \"EREQUEST\",\n    \"originalError\": {\n        \"info\": {\n            \"name\": \"ERROR\",\n            \"handlerName\": \"onErrorMessage\",\n            \"number\": 2627,\n            \"state\": 1,\n            \"class\": 14,\n            \"message\": \"Violation of UNIQUE KEY constraint 'UniqueDescription'. Cannot insert duplicate key in object 'dbo.source'. The duplicate key value is (ColleenD).\",\n            \"serverName\": \"DESKTOP-FT1088C\",\n            \"procName\": \"dbo.source_upsert\",\n            \"lineNumber\": 64\n        }\n    },\n    \"name\": \"RequestError\",\n    \"number\": 2627,\n    \"lineNumber\": 64,\n    \"state\": 1,\n    \"class\": 14,\n    \"serverName\": \"DESKTOP-FT1088C\",\n    \"procName\": \"dbo.source_upsert\",\n    \"precedingErrors\": []\n}\n\nThe code as a function for the back end:\n\nimport { MSSqlConnect } from \"./MSSqlConnect\";\nimport { sqlConfig } from \"./mssql-config\"; // Assuming you have a config file for SQL Server configuration\nimport { IResult, RequestError } from \"mssql\";\n\ninterface SourceUpsertParams {\n    id: number;\n    description: string;\n    rule: string;\n    value: string;\n    category: string;\n}\n\ninterface SourceUpsertResult {\n    success: boolean;\n    message: string;\n    data?: any;\n    error?: string;\n}\n\nexport async function upsertSource(\n    params: SourceUpsertParams\n): Promise {\n    const ms: MSSqlConnect = new MSSqlConnect(sqlConfig);\n\n    try {\n        await ms.connectPool();\n        const request = ms.pool.request();\n\n        // Add input parameters\n        request.input(\"id\", ms.sql.Int, params.id);\n        request.input(\"description\", ms.sql.NVarChar, params.description);\n        request.input(\"rule\", ms.sql.NVarChar, params.rule);\n        request.input(\"value\", ms.sql.NVarChar, params.value);\n        request.input(\"category\", ms.sql.NVarChar, params.category);\n\n        // Execute the stored procedure\n        const result: IResult = await request.execute(\"dbo.source_upsert\");\n\n        return {\n            success: true,\n            message: \"Source record upserted successfully\",\n            data: result.recordset,\n        };\n    } catch (error: unknown) {\n        const sqlError = error as RequestError;\n\n        if (\n            sqlError.originalError &&\n            \"info\" in sqlError.originalError &&\n            (sqlError.originalError as any).info?.message\n                ?.toLowerCase()\n                .includes(\"unique key constraint\")\n        ) {\n            return {\n                success: false,\n                message: Row with Description = ${params.description} already exists,\n                error: \"DUPLICATE_KEY\",\n            };\n        } else {\n            return {\n                success: false,\n                message: \"Failed to upsert source record\",\n                error: sqlError.message || \"Unknown error occurred\",\n            };\n        }\n    } finally {\n        await ms.closePool().catch((err) => {\n            console.error(\"Error closing MSSqlConnect pool:\", err);\n        });\n    }\n}\n\n// Example usage (can be removed in production)\nasync function testUpsertSource() {\n    const params: SourceUpsertParams = {\n        id: 0,\n        description: \"ColleenD\",\n        rule: \"equals\",\n        value: \"Roger\",\n        category: \"test\",\n    };\n\n    const result = await upsertSource(params);\n\n    if (result.success) {\n        console.log(\"Success:\", result.message);\n        console.log(\"Data:\", result.data);\n    } else {\n        console.error(\"Error:\", result.message);\n        console.error(\"Details:\", result.error);\n    }\n}\n\n// Uncomment to test the function\nawait testUpsertSource();",
        "locale": "en",
        "slug": "technical-posts/stored-p;rocedures-with-mssql",
        "folder": ""
    },
    {
        "title": "Sveltekit dynamic routing and fetching the url.",
        "description": "Sveltekit dynamic routing and fetching the url.",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "sveltekit"
        ],
        "content": "Consider a dynamic route like this:\n\n![[image.png]]\n\nWithout constraints, [name] can be any value. See [[Using Sveltekit ParamMatcher to constraint dynamic routes]] for more info on constraining dynamic routes.\n\nIn a +page.server.ts or +server.ts file, fetch the url like this:\n\nexport async function load({ url }): Promise {\n    console.log('url', url);\n\nThe following properties comprise the url value:\n\nURL {\n  href: 'http://localhost:5173/test-dyno/bob?state=ca',\n  origin: 'http://localhost:5173',\n  protocol: 'http:',\n  username: '',\n  password: '',\n  host: 'localhost:5173',\n  hostname: 'localhost',\n  port: '5173',\n  pathname: '/test-dyno/bob',\n  search: '?state=ca',\n  searchParams: URLSearchParams { 'state' => 'ca' },\n  hash: ''\n}\n\nFor example, fetch the pathname with:\n\nconst path = url.pathname;",
        "locale": "en",
        "slug": "technical-posts/sveltekit-routing",
        "folder": ""
    },
    {
        "title": "Tailwind tips",
        "description": "Tailwind tips",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "css",
            "tailwind"
        ],
        "content": "developedbyed Tailwind tips\n\nUse CSS delegation (parent styles ripple down to children)\nAnimate a button\nResponsive design -- shows a simple nav bar\nTailwind is mobile first -- work upwards with sizes and other changes\n-",
        "locale": "en",
        "slug": "technical-posts/tailwind-tips",
        "folder": ""
    },
    {
        "title": "sveltekit-posting-data-to-an-endpoint",
        "description": "sveltekit-posting-data-to-an-endpoint",
        "date_created": "2025-06-20T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "At line 69 I have some stubbed in to call the assignCategoryRule action in +page.server.ts but I don't think that form-based code is correct. I want to pass the categoryData object to the form actcion and then have the assignCategoryRule form action call the post API at /api/update-category\n\n\n  You're correct: the current form setup is using SvelteKit's form actions, which only send form data (not your categoryData object) to the assignCategoryRule action. If you want to send the categoryData object to your /api/update-category endpoint via the form action, you need to:\n\nSend the data from the client to the form action (as JSON or as form fields).\nHave the form action call the API using fetch on the server side.\n\nHere's how you can do it:\n\n1. Pass categoryData to the form\n\nSince HTML forms only send string fields, you can serialize your categoryData as JSON in a hidden input:\n\n\n {\n    return async ({ result, formData }) => {\n      if (result.type === 'success') {\n        console.log(result);\n        console.log(formData);\n      }\n      await applyAction(result);\n    };\n  }}\n  \n  \n  Update\n\n`\n\n2. Parse and forward the data in the form action\n\nUpdate your assignCategoryRule action to parse the JSON and call the API:\n\n// ...existing code...\nassignCategoryRule: async ({ request, fetch }) => {\n  const rawFormData = Object.fromEntries(await request.formData());\n  let categoryData;\n\n  try {\n    categoryData = JSON.parse(rawFormData.categoryData as string);\n  } catch (err) {\n    return fail(400, { message: 'Invalid category data' });\n  }\n\n  try {\n    // Call the API endpoint on the server\n    const apiResponse = await fetch('/api/update-category', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify(categoryData)\n    });\n\n    const apiResult = await apiResponse.json();\n\n    if (!apiResult.success) {\n      return fail(500, { message: apiResult.message });\n    }\n\n    return { success: true, apiResult };\n  } catch (err: any) {\n    return fail(500, { message: err.message || 'API call failed' });\n  }\n}\n// ...existing code...`\n\n\nSummary:\nSerialize your object as a hidden input in the form.\nParse it in the form action and forward it to your API.\n\nLet me know if you want a more dynamic example for each row, or if you want to use a pure fetch-based approach instead of SvelteKit form actions!",
        "locale": "en",
        "slug": "technical-posts/sveltekit-posting-data-to-an-endpoint",
        "folder": ""
    },
    {
        "title": "Sveltekit links",
        "description": "Sveltekit links",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "YouTube channels\n\nCodevolution\nConsulting Ninja\nCooper Codes\nGuiBibeau\n\\*Huntabyte\nJames B Quick\nJeffrey Codes JavaScript\n\\*Joy of Code\nNoah Glazer\nSmolJames\n    Sveltekit blog with Markdown\n    Sveltekit CRUD app with Firebase Authentication\n    Sveltekit CRUD app with Firebase\nSvelteRust\nSvelte Society\nWeb Dev Cody\n\\*WebJeda\nYour Code Lab\n\nOther free videos\n\nLevelUp Tutorials\n\nArticles\n\nCool Svelte stuff\n\n\\*Svelte French Toast\n\nOther cool stuff\n\nCSS Open Props\nPostCSS\nPostCSS Just In Time props\n-",
        "locale": "en",
        "slug": "technical-posts/sveltekit-resources",
        "folder": ""
    },
    {
        "title": "Submit extra data with Superform",
        "description": "Submit extra data with Superform",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit"
        ],
        "content": "In the ContactForm.svelte component I tried to submit extra data to the server with this SuperForms FAQ but it didn't work.\n\nexport let questions =[]\n\n...\n\nonMount(() => {\n\tallQuestions = questions.join(';');\n});\n\n...\n\nconst { form, errors } = superForm(data.form, {\n\tonSubmit({ formData }) {\n\t\tconsole.log(questions);\n\t\tformData.set('questions', allQuestions);\n\t}\n});\n\nexport const actions = {\n\tcreate: async (event) => {\n\t\tconst formData = await event.request.formData();\n\t\tconst form = await superValidate(event, contactFormSchema);\n\t\tconsole.log(form.data);\n\n\t\tif (!form.valid) {\n\t\t\treturn fail(400, {\n\t\t\t\tform\n\t\t\t});\n\t\t}\n\n\t\tif (formData.has('questions')) {\n\t\t\tconsole.log(formData.questions);\n\t\t}\n\t\t...\n\nThe questions data (and the questions key in formData) didn't make it to the server.\n\nI resolved the issue for now by adding a hidden field to the form.",
        "locale": "en",
        "slug": "technical-posts/submit-extra-data-with-svelte-superform",
        "folder": ""
    },
    {
        "title": "tailwind-example-menu-with-dropdowns",
        "description": "tailwind-example-menu-with-dropdowns",
        "date_created": "2025-09-29T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "tailwind"
        ],
        "content": "This documentation explains what each unique Tailwind CSS class block is doing in your navigation menu.\n\n\nSee route menu-tw-41 in the Svelte tailwind-4-2 project.\n\nMain Navigation Container\nclass=\"flex list-none m-0 p-0 bg-slate-700 h-15 shadow-md relative z-[1000]\"\nflex - Sets display to flexbox for horizontal layout\nlist-none - Removes default list bullets/numbering\nm-0 - Sets margin to 0 on all sides\np-0 - Sets padding to 0 on all sides\nbg-slate-700 - Sets background color to dark gray-blue\nh-15 - Sets height to 3.75rem (60px)\nshadow-md - Adds medium drop shadow\nrelative - Sets position relative for z-index context\nz-[1000] - Sets z-index to 1000 (high stacking order)\n\nTop-Level Menu Items (Dropdown Parents)\nclass=\"relative group\"\nrelative - Creates positioning context for absolutely positioned dropdowns\ngroup - Establishes hover group for child elements to respond to parent hover\n\nMain Navigation Links\nclass=\"flex items-center h-full px-6 text-white no-underline font-medium cursor-pointer transition-colors duration-200 hover:bg-slate-600\"\nflex - Sets display to flexbox\nitems-center - Vertically centers flex items\nh-full - Sets height to 100% of parent\npx-6 - Sets horizontal padding to 1.5rem\ntext-white - Sets text color to white\nno-underline - Removes text decoration\nfont-medium - Sets font weight to 500 (medium)\ncursor-pointer - Changes cursor to pointer on hover\ntransition-colors - Animates color changes\nduration-200 - Sets transition duration to 200ms\nhover:bg-slate-600 - Changes background to lighter slate on hover\n\nDropdown Arrow Icons\nclass=\"h-4 ml-2 fill-white transition-transform duration-200 group-hover:rotate-180\"\nh-4 - Sets height to 1rem\nml-2 - Sets left margin to 0.5rem\nfill-white - Sets SVG fill color to white\ntransition-transform - Animates transform changes\nduration-200 - Sets transition duration to 200ms\ngroup-hover:rotate-180 - Rotates 180 when parent group is hovered\n\nDropdown Menu Container\nclass=\"absolute top-full left-0 min-w-[280px] list-none p-0 m-0 bg-white rounded-b-lg shadow-[0_8px_24px_rgba(0,0,0,0.15)] border border-gray-200 border-t-0 opacity-0 invisible translate-y-2.5 transition-all duration-250 group-hover:opacity-100 group-hover:visible group-hover:translate-y-0\"\nabsolute - Positions absolutely relative to parent\ntop-full - Positions at 100% from top (below parent)\nleft-0 - Aligns to left edge of parent\nmin-w-[280px] - Sets minimum width to 280px\nlist-none - Removes list styling\np-0 m-0 - Removes padding and margin\nbg-white - Sets white background\nrounded-b-lg - Rounds bottom corners (large radius)\nshadow-[0_8px_24px_rgba(0,0,0,0.15)] - Custom shadow (x=0, y=8px, blur=24px, opacity=0.15)\nborder border-gray-200 - Adds light gray border\nborder-t-0 - Removes top border\nopacity-0 - Initially transparent\ninvisible - Initially hidden from screen readers\ntranslate-y-2.5 - Initially moved down 10px\ntransition-all duration-250 - Animates all properties over 250ms\ngroup-hover:opacity-100 - Becomes opaque on parent hover\ngroup-hover:visible - Becomes visible on parent hover\ngroup-hover:translate-y-0 - Returns to normal position on parent hover\n\nDropdown Spacer\nclass=\"h-2\"\nh-2 - Sets height to 0.5rem (8px spacing gap)\n\nDropdown Content Wrapper\nclass=\"py-2\"\npy-2 - Sets vertical padding to 0.5rem (top and bottom)\n\nDropdown Links (Standard)\nclass=\"block px-6 py-3 text-gray-800 no-underline whitespace-nowrap transition-colors duration-200 hover:bg-red-500 hover:text-black\"\nblock - Sets display to block (full width)\npx-6 - Sets horizontal padding to 1.5rem\npy-3 - Sets vertical padding to 0.75rem\ntext-gray-800 - Sets dark gray text color\nno-underline - Removes link underlines\nwhitespace-nowrap - Prevents text wrapping\ntransition-colors duration-200 - Animates color changes over 200ms\nhover:bg-red-500 - Red background on hover\nhover:text-black - Black text on hover\n\nDropdown Links (Indented)\nclass=\"block px-6 py-3 pl-10 text-gray-800 no-underline whitespace-nowrap transition-colors duration-200 hover:bg-red-500 hover:text-black\"\nSame as standard dropdown links, plus:\npl-10 - Sets left padding to 2.5rem (creates indentation)\n\nDropdown Subtext\nclass=\"text-sm text-gray-600 mt-1 font-normal\"\ntext-sm - Sets font size to 0.875rem (14px)\ntext-gray-600 - Sets medium gray text color\nmt-1 - Sets top margin to 0.25rem\nfont-normal - Sets font weight to 400 (normal)\n\nButton Element (Contact Tech Support)\nclass=\"block px-6 py-3 text-gray-800 no-underline whitespace-nowrap transition-colors duration-200 hover:bg-red-500 hover:text-black bg-transparent border-none w-full text-left cursor-pointer text-base font-inherit\"\nSame as dropdown links, plus:\nbg-transparent - Transparent background\nborder-none - Removes button borders\nw-full - Sets width to 100%\ntext-left - Left-aligns text\ncursor-pointer - Pointer cursor\ntext-base - Base font size (1rem)\nfont-inherit - Inherits font family from parent\n\nSeparator List Items\nclass=\"m-0 p-0\"\nm-0 - Removes all margins\np-0 - Removes all padding\n\nHorizontal Rule Separators\nclass=\"border-0 h-px bg-gray-200 my-2\"\nborder-0 - Removes default border\nh-px - Sets height to 1px\nbg-gray-200 - Light gray background color\nmy-2 - Sets vertical margin to 0.5rem (top and bottom)\n\nExternal Link Icons\nclass=\"fa-regular fa-arrow-up-right-from-square ml-2 text-sm opacity-70\"\nfa-regular fa-arrow-up-right-from-square - Font Awesome icon classes\nml-2 - Sets left margin to 0.5rem\ntext-sm - Small text size\nopacity-70 - 70% opacity (semi-transparent)",
        "locale": "en",
        "slug": "technical-posts/tailwind-example-menu-with-dropdowns",
        "folder": ""
    },
    {
        "title": "SQL Server login issue",
        "description": "SQL Server login issue",
        "date_created": "2025-06-20T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sqlserver"
        ],
        "content": "My local SQL Server instance has an issue.\n\n\nLet's perform the definitive test to isolate whether the problem is with the SSMS application itself or something deeper in your machine's SQL client configuration.\n\nCritical Test: Connect with sqlcmd\n\nThis test bypasses the entire SSMS user interface and its saved settings. It will tell us what the fundamental SQL Native Client on your machine does when asked to connect.\n\nOpen a standard Windows Command Prompt (not PowerShell, just to keep it simple). You can do this by pressing Win+R, typing cmd, and pressing Enter.\n\nRun the following command. The -E flag specifically requests a trusted connection (Windows Authentication).\n    If you are connecting to the default instance on your local machine, use this:\n        sqlcmd -S . -E\n    If you are connecting to a named instance (e.g., SQLEXPRESS), use this, replacing SQLEXPRESS with your instance name:\n        sqlcmd -S .\\SQLEXPRESS -E\n\nIf the connection is successful, you will see a 1%3E prompt.\n\nAt the prompt, type this command and press Enter:\n    SELECT SUSER_NAME();\n\nThen type GO and press Enter.\n\n    1> SELECT SUSER_NAME();\n    2> GO\n\nObserve the output. This is the crucial result.\n\nScenario A: sqlcmd shows your Windows login (YOUR_DOMAIN\\thumb)\n\nThis is the most likely outcome.\n\nConclusion: The problem is 100% isolated to your SQL Server Management Studio (SSMS) application. Your underlying client drivers are working correctly. A hidden or corrupt setting within SSMS is forcing the node_user login.\n\nSolution: Reset the SSMS User Profile (The \"Scorched Earth\" Fix)\n    Since you've checked all the UI-accessible settings, the only remaining option is to reset your SSMS profile, which will clear any corrupt state.\n\n    Close all instances of SSMS.\n    Open File Explorer. In the address bar, type %APPDATA%\\Microsoft\\ and press Enter.\n    Find the folder named SQL Server Management Studio. Open it.\n    You will see a folder named after your SSMS version (e.g., 18.0, 19.0).\n    Rename this folder to something else, like 19.0_old.\n    Restart SSMS. It will take a moment to start as it rebuilds a fresh user profile. All your recent server history will be gone, but this will almost certainly fix the problem.\n\nScenario B: sqlcmd ALSO shows node_user\n\nThis would be extremely unusual, but if it happens:\n\nConclusion: The problem is not with SSMS but is at a deeper level, affecting all SQL client tools on your computer.\n\nSolution: Check for a SQL Server Client Alias\n    This becomes the number one suspect. An alias can intercept a server name and redirect the connection.\n\n    Press the Windows key, type cliconfg, and run the application that appears.\n    Go to the \"Alias\" tab.\n    Look for an alias that matches the server name you are trying to connect to (e.g., . or localhost or YOURMACHINE\\SQLEXPRESS).\n    If you find one, select it and click Remove.\n    Click OK and try connecting again, first with sqlcmd and then with SSMS.\n\nPlease perform the sqlcmd test and let me know what it returns. The result will give us the final answer.>)\n\nGiving node_user god powers\n\nI need to reinstall SQL Server to fix the weirdo issue above. But until I do...\n\nI tried to do this in SQL Server:\n\nALTER SERVER ROLE sysadmin ADD MEMBER [node_user];\n\nbut got the error:\n\nCannot alter the server role 'sysadmin', because it does not exist or you do not have permission.\n\nI used the instructions below to give node_user god powers.\n\nGemini said:\nThis error message is the final, definitive proof we needed.\n\nThis is the classic \"chicken-and-egg\" problem, and it confirms our theory 100%.\n\nHere is what's happening:\n\nYou are launching SSMS and selecting \"Windows Authentication\".\nSome hidden client-side configuration is forcing your connection to use the node_user login instead.\nYou then run the command: ALTER SERVER ROLE sysadmin ADD MEMBER [node_user];\nSQL Server receives this command from the user who is currently logged in, which is node_user.\nSQL Server correctly says, \"The login node_user does not have permission to modify the sysadmin role.\"\nThe result is the error you see: ...or you do not have permission.\n\nYou are stuck in a loop: to fix the problem, you need to log in as a real administrator, but the problem itself is preventing you from logging in as a real administrator.\n\nThe Solution: Bypass the Faulty SSMS Configuration\n\nWe need to establish a connection to SQL Server that is a guaranteed, true administrator connection, bypassing whatever is broken in your SSMS client. Here are three ways to do that, from most recommended to least.\n\nMethod 1: Use sqlcmd (The Most Reliable Way)\n\nThis command-line tool bypasses all the SSMS UI settings and will give you a clean connection.\n\nOpen a Windows Command Prompt (cmd.exe).\nConnect to your instance using the -E (trusted/Windows) flag.\n    For a default instance: sqlcmd -S . -E\n    For a named instance (like SQLEXPRESS): sqlcmd -S .\\SQLEXPRESS -E\nYou will get a 1> prompt. Type the command to grant the permission:\n    ALTER SERVER ROLE sysadmin ADD MEMBER [node_user];\nPress Enter. You will get a 2> prompt. Type GO and press Enter.\n\n    C:\\> sqlcmd -S . -E\n    1> ALTER SERVER ROLE sysadmin ADD MEMBER [node_user];\n    2> GO\n    1>\n    If you see no error message and it returns to the 1> prompt, it has worked. node_user is now a sysadmin. You can type exit to leave sqlcmd.\n\nMethod 2: Use the sa Account\n\nIf you set a password for the sa (System Administrator) account during the SQL Server installation, you can use that. This login is always a sysadmin.\n\nIn the SSMS \"Connect to Server\" dialog, change Authentication to \"SQL Server Authentication\".\nEnter sa for the Login.\nEnter the sa Password you set during installation.\nClick Connect.\nNow that you are connected as sa, you have full permissions. Open a new query window and successfully run:\n    ALTER SERVER ROLE sysadmin ADD MEMBER [node_user];\n\nMethod 3: Run SSMS As a Different User\n\nThis is a clever Windows trick that forces SSMS to start with a completely fresh user profile, bypassing your corrupted one.\n\nFind the Ssms.exe application. It's usually in a location like:\n    C:\\Program Files (x86)\\Microsoft SQL Server Management Studio 19\\Common7\\IDE\\\nHold down the Shift key, then right-click on Ssms.exe.\nSelect \"Run as different user\".\nYou will be prompted for credentials. Enter the username and password for another account on your computer that has administrator rights. (e.g., the built-in Administrator account if it's enabled, or another local admin account you created).\nThis will launch a new instance of SSMS under a clean profile. Connect using Windows Authentication.\nIt will now connect correctly as that user. Run the ALTER command.\n\nChoose whichever method is easiest for you. sqlcmd is the most direct fix for this specific situation.",
        "locale": "en",
        "slug": "technical-posts/sql-server-login-issue",
        "folder": ""
    },
    {
        "title": "Using @custom-media for ASNA.com responsiveness",
        "description": "Using @custom-media for ASNA.com responsiveness",
        "date_created": "2025-05-18T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "asna-com"
        ],
        "content": "See also:\n\n\\*[Add PostCSS to Sveltekit project]]\n\nThe following viewport breakpoints are defined with CSS custom properties in _resets.css. These custom properties define the breakpoints used at asna.com.\n\n@custom-media --media-small       (width   768px) and (width  1024px) and (width  1920px);\n\n@custom-media --lt-large       (width   768px) and (width  1024px) and (width  1920px);\n\nThe CSS breakpoints for asna.com is defined \"typical desktop\" first with exceptions added as necessary. It's best to to define exceptions CSS's nesting feature. For example, the CSS below defines the typical desktop rules for div.header-info and uses CSS nesting to add an exception for the --media-small viewport.\n\ndiv.header-info {\n    display: flex;\n    justify-content: flex-end;\n    align-items: center;\n    padding-right: 2rem;;\n\n    @media (--media-small) {\n        flex-direction: column-reverse;\n        justify-content: flex-start;\n        align-items: flex-start;\n    }\n}\n\nUsing CSS nesting works well with the componentized nature of asna.com's CSS.\n\nCSS nesting\n\nCSS nesting became widely available about halfway through asna.com development. Initially, to resolve nesting's limited browser availability, asna.com used the PostCSS [postcss-nesting plugin (not to be confused with the postcss-nested plugin).\n\nThe initial CSS nesting spec allowed CSS like this:\n\ndiv.header-info {\n    display: flex;\n    justify-content: flex-end;\n    align-items: center;\n    padding-right: 2rem;;\n\n    @media (--media-small) {\n        flex-direction: column-reverse;\n        justify-content: flex-start;\n        align-items: flex-start;\n    }\n\n    & option {\n        padding: .4rem;\n        cursor: inherit;\n    }\n    ...\n\nChrome was the first to offer full nesting support. The original spec required that a nested rule start with an & if the rule didn't start with a symbol (ie, @ or .). However, by the time FireFox aquired full nesting support, the & requirement had been removed from the spec.\n\nAs of early October, Chromium-based browsers still need the &, so the asna.com CSS includes it. It's presence is ignored by FireFox (per the new spec).\n\nThis change made for interesting Can I use reporting on the nesting feature. Earlier this year, Chrome and Safari was green and Firefox was red. When FireFox went green (by implementing the new spec), Chrome and Safari revered back to partially-supported status. When Chrome and Safari, with a little testing, during CSS refactoring the & could be removed, but it's not going to hurt anything to leave it in place.",
        "locale": "en",
        "slug": "technical-posts/sveltekit-responsive-strategy",
        "folder": ""
    },
    {
        "title": "Sveltekit nuggets",
        "description": "Sveltekit nuggets",
        "date_created": "2025-02-01T00:00:00.000Z",
        "date_updated": "2025-11-03T00:00:00.000Z",
        "date_published": null,
        "pinned": false,
        "tags": [
            "sveltekit",
            "nuggets"
        ],
        "content": "Set a page title (or do other things in the head tag)\n\n\n\tServer Data Example\n\nThe docs say, \"In server-side rendering (SSR) mode, contents of ` are returned separately from the rest of your HTML.\" I'm not sure of the impact of this, but I think it means use this sparingly. Anything you know needs to be the head should be added to the app.html` file.\n\nChange the URL at runtime\n\nconst newUrl = $page.url.origin + $page.url.pathname + '/?family=' + family;\nwindow.history.pushState({}, document.title, newUrl);\n\nDetermine dev or production \n\nimport { dev } from '$app/environment'; // Import SvelteKit's environment\n\n...\nif (dev) {\n    // Running dev mode.\n}\nelse {\n\t// Running in production mode.\n}\n\nBind an input tag to a checkbox\n\n\n\tlet show = true\n\nInstall Svelte command line \n\nEven if you intend to use the Sveltekit CLI with PNPM, install sv with npm.\nnpm install -g sv\n\nWith sv installed, create a new Sveltekit project. This creates a folder named \\. Change to it after the project is created. \n\nnpx sv create \n\nGet dev/prod domains at runtime\n\n.env file \n\nDOMAIN_DEVELOPMENT=\"http://localhost:5173\"\nDOMAIN_PRODUCTION=\"https://asna.com\"\n\nExample use\n\nimport { env } from '$env/dynamic/private'; // For accessing environment variables securely\nimport { dev } from '$app/environment'; // Import SvelteKit's environment\n\nfunction getDomain() {\n    if (dev) {\n        return env.DOMAIN_DEVELOPMENT;\n    } else {\n        return env.DOMAIN_PRODUCTION;\n    }\n}\n\nCopy data to the clipboard\n\n\nimport { browser } from '$app/environment';\n\nfunction updateClipboard(newClip: string) {\n\tnavigator.clipboard.writeText(newClip).then(\n\t\t() => {\n\t\t\tconsole.log('clipboard successfully set');\n\t\t},\n\t\t() => {\n\t\t\tconsole.log('clipboard successfully set');\n\t\t}\n\t);\n}\n\nfunction copyTokenToClipboard(token: string) {\n\tupdateClipboard(token);\n}\n\n\n\n\n copyTokenToClipboard(data.LOGMEIN_ORGANIZER_ID)}>Copy\n\n\n\n\t// This CSS is probably a little overboard to style the button.\n\t\n    .button-as-link {\n        display: inline-block; /* Make it behave like a link */\n        /* 1. Reset Button Defaults */\n        background: none; /* Remove default background */\n        border: none; /* Remove default border */\n        padding: 0; /* Remove default padding */\n        font: inherit; /* Inherit font from parent */\n        color: #007bff; /* Standard link blue color (adjust as needed) */\n        text-decoration: underline; /* Add underline, like a link */\n        cursor: pointer; /* Show pointer cursor, like a link */\n        text-align: left; /* Align text to the left if button was centering it */\n\n        /* Optional: Remove browser-specific button styling (especially for Safari/iOS) */\n        -webkit-appearance: none;\n        -moz-appearance: none;\n        appearance: none;\n    }\n\n    /* 2. Style Hover & Focus States (like a link) */\n    .button-as-link:hover,\n    .button-as-link:focus {\n        color: #0056b3; /* Darker blue on hover/focus */\n        text-decoration: none; /* Often links lose underline on hover */\n        /* Or, to keep underline and just change color:\n     text-decoration: underline;\n  */\n    }\n\n    /* Optional: Style Active State (when clicked) */\n    .button-as-link:active {\n        color: #004085; /* Even darker blue or different color for active state */\n    }\n\n    /* 3. Style Disabled State (if you use it) */\n    .button-as-link:disabled {\n        color: #6c757d; /* Muted color for disabled state */\n        text-decoration: none; /* No underline for disabled */\n        cursor: not-allowed; /* Not-allowed cursor for disabled */\n    }\n\n    /* Optional: Ensure focus outline is visible for accessibility */\n    .button-as-link:focus-visible {\n        /* Modern browsers */\n        outline: 2px solid #007bff;\n        outline-offset: 2px;\n    }\n    /* Fallback for older browsers (might show on click too) */\n    .button-as-link:focus {\n        outline: 2px solid #007bff;\n        outline-offset: 2px;\n    }\n    /* If you want to remove the default button focus ring before adding your own */\n    .button-as-link:focus {\n        outline: none; /* First remove default, then add custom below */\n    }\n    .button-as-link:focus-visible {\n        outline: 2px solid #007bff;\n        outline-offset: 2px;\n    }\n\nAdd an internal CSS file to Svelte\n\nin top-level +layout.svelte\n\nwhere src\\main.css\n\n\n    let { children } = $props();    \n\n    import '../main.css';\n\n\n\n  {@render children()}\n",
        "locale": "en",
        "slug": "technical-posts/sveltekit-nuggets",
        "folder": ""
    }
]